[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v1",
                "updated": "2025-03-12T17:43:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09218v1",
                "updated": "2025-03-12T10:05:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:05:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N2C2: Nearest Neighbor Enhanced Confidence Calibration for Cross-Lingual\n  In-Context Learning"
                },
                "summary": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements of in-context learning (ICL) show language models can\nsignificantly improve their performance when demonstrations are provided.\nHowever, little attention has been paid to model calibration and prediction\nconfidence of ICL in cross-lingual scenarios. To bridge this gap, we conduct a\nthorough analysis of ICL for cross-lingual sentiment classification. Our\nfindings suggest that ICL performs poorly in cross-lingual scenarios,\nexhibiting low accuracy and presenting high calibration errors. In response, we\npropose a novel approach, N2C2, which employs a -nearest neighbors augmented\nclassifier for prediction confidence calibration. N2C2 narrows the prediction\ngap by leveraging a datastore of cached few-shot instances. Specifically, N2C2\nintegrates the predictions from the datastore and incorporates confidence-aware\ndistribution, semantically consistent retrieval representation, and adaptive\nneighbor combination modules to effectively utilize the limited number of\nsupporting instances. Evaluation on two multilingual sentiment classification\ndatasets demonstrates that N2C2 outperforms traditional ICL. It surpasses fine\ntuning, prompt tuning and recent state-of-the-art methods in terms of accuracy\nand calibration errors."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Víctor Gutiérrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17363v3",
                "updated": "2025-03-12T07:23:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    23,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T17:40:09Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    17,
                    40,
                    9,
                    0,
                    55,
                    0
                ],
                "title": "KV-Edit: Training-Free Image Editing for Precise Background Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Edit: Training-Free Image Editing for Precise Background Preservation"
                },
                "summary": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background consistency remains a significant challenge in image editing\ntasks. Despite extensive developments, existing works still face a trade-off\nbetween maintaining similarity to the original image and generating content\nthat aligns with the target. Here, we propose KV-Edit, a training-free approach\nthat uses KV cache in DiTs to maintain background consistency, where background\ntokens are preserved rather than regenerated, eliminating the need for complex\nmechanisms or expensive training, ultimately generating new content that\nseamlessly integrates with the background within user-provided regions. We\nfurther explore the memory consumption of the KV cache during editing and\noptimize the space complexity to $O(1)$ using an inversion-free method. Our\napproach is compatible with any DiT-based generative model without additional\ntraining. Experiments demonstrate that KV-Edit significantly outperforms\nexisting approaches in terms of both background and image quality, even\nsurpassing training-based methods. Project webpage is available at\nhttps://xilluill.github.io/projectpages/KV-Edit"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang",
                "arxiv_comment": "Project webpage is available at\n  https://xilluill.github.io/projectpages/KV-Edit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v2",
                "updated": "2025-03-12T03:40:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    40,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08966v1",
                "updated": "2025-03-12T00:12:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T00:12:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    0,
                    12,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Performance Models for a Two-tiered Storage System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Models for a Two-tiered Storage System"
                },
                "summary": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work describes the design, implementation and performance analysis of a\ndistributed two-tiered storage software. The first tier functions as a\ndistributed software cache implemented using solid-state devices~(NVMes) and\nthe second tier consists of multiple hard disks~(HDDs). We describe an online\nlearning algorithm that manages data movement between the tiers. The software\nis hybrid, i.e. both distributed and multi-threaded. The end-to-end performance\nmodel of the two-tier system was developed using queuing networks and\nbehavioral models of storage devices. We identified significant parameters that\naffect the performance of storage devices and created behavioral models for\neach device. The performance of the software was evaluated on a many-core\ncluster using non-trivial read/write workloads. The paper provides examples to\nillustrate the use of these models."
                },
                "authors": [
                    {
                        "name": "Aparna Sasidharan"
                    },
                    {
                        "name": "Xian-He"
                    },
                    {
                        "name": "Jay Lofstead"
                    },
                    {
                        "name": "Scott Klasky"
                    }
                ],
                "author_detail": {
                    "name": "Scott Klasky"
                },
                "author": "Scott Klasky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08941v1",
                "updated": "2025-03-11T22:44:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T22:44:38Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    22,
                    44,
                    38,
                    1,
                    70,
                    0
                ],
                "title": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BCZT/LSMO/BCZT multilayer films for high temperature energy storage\n  capacitors"
                },
                "summary": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ba0.85Ca0.15Zr0.1Ti0.9O3/La0.8Sr0.2MnO3/Ba0.85Ca0.15Zr0.1Ti0.9O3\n(BCZT/LSMO/BCZT) sandwich films were elaborated using the sol-gel spin coating\nprocess. The dielectric properties displayed excellent thermal stability with\nthe temperature coefficient of capacitance, TCC, remaining within 10% between\n-50 C and 300 C. The high energy storage density, Wrec, of 11.8 J/cm3 observed\nin this sandwich films, is nearly twice as high as that of the BCZT films, with\nan efficiency, n, of 77% under a weak electric field of 800 kV/cm. Furthermore,\nthe stability of Wrec and n was observed along the studied temperature interval\nmaking them promising candidates for high-temperature energy storage\ncapacitors."
                },
                "authors": [
                    {
                        "name": "Afaak Lakouader"
                    },
                    {
                        "name": "Abdelilah Lahmar"
                    },
                    {
                        "name": "Spela Kunej"
                    },
                    {
                        "name": "Daoud Mezzane"
                    },
                    {
                        "name": "Jamal Belhadi"
                    },
                    {
                        "name": "El Hassan Choukri"
                    },
                    {
                        "name": "Lahoucine Hajji"
                    },
                    {
                        "name": "Mbarek Amjoud"
                    },
                    {
                        "name": "Zdravko Kutnjak"
                    },
                    {
                        "name": "Igor A. Lukyanchuk"
                    },
                    {
                        "name": "Mimoun El Marssi"
                    }
                ],
                "author_detail": {
                    "name": "Mimoun El Marssi"
                },
                "author": "Mimoun El Marssi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08879v1",
                "updated": "2025-03-11T20:45:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T20:45:02Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    20,
                    45,
                    2,
                    1,
                    70,
                    0
                ],
                "title": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know What to Drop: Self-Attention Guided KV Cache Eviction for\n  Efficient Long-Context Inference"
                },
                "summary": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-context inference is critical as large language models (LLMs)\nadopt context windows of ranging from 128K to 1M tokens. However, the growing\nkey-value (KV) cache and the high computational complexity of attention create\nsignificant bottlenecks in memory usage and latency. In this paper, we find\nthat attention in diverse long-context tasks exhibits sparsity, and LLMs\nimplicitly \"know\" which tokens can be dropped or evicted at the head level\nafter the pre-filling stage. Based on this insight, we propose Self-Attention\nGuided Eviction~(SAGE-KV), a simple and effective KV eviction cache method for\nlong-context inference. After prefilling, our method performs a one-time top-k\nselection at both the token and head levels to compress the KV cache, enabling\nefficient inference with the reduced cache. Evaluations on LongBench and three\nlong-context LLMs (Llama3.1-8B-Instruct-128k, Llama3-8B-Prolong-512k-Instruct,\nand Qwen2.5-7B-Instruct-128k) show that SAGE-KV maintains accuracy comparable\nto full attention while significantly improving efficiency. Specifically,\nSAGE-KV achieves 4x higher memory efficiency with improved accuracy over the\nstatic KV cache selection method StreamLLM, and 2x higher memory efficiency\nwith better accuracy than the dynamic KV cache selection method Quest."
                },
                "authors": [
                    {
                        "name": "Guangtao Wang"
                    },
                    {
                        "name": "Shubhangi Upasani"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Darshan Gandhi"
                    },
                    {
                        "name": "Jonathan Li"
                    },
                    {
                        "name": "Changran Hu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Urmish Thakker"
                    }
                ],
                "author_detail": {
                    "name": "Urmish Thakker"
                },
                "author": "Urmish Thakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08640v1",
                "updated": "2025-03-11T17:30:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T17:30:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    30,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Many-Shot In-Context Learning with Dynamic Block-Sparse\n  Attention"
                },
                "summary": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-shot in-context learning has recently shown promise as an alternative to\nfinetuning, with the major advantage that the same model can be served for\nmultiple tasks. However, this shifts the computational burden from\ntraining-time to inference-time, making deployment of many-shot ICL challenging\nto justify in-practice. This cost is further increased if a custom\ndemonstration set is retrieved for each inference example. We present Dynamic\nBlock-Sparse Attention, a training-free framework for retrieval-based many-shot\nin-context learning. By combining carefully designed block-sparse attention and\nretrieval of cached groups of demonstrations, we achieve comparable per-example\nlatency to finetuning while maintaining on average >95% of the best method's\naccuracy across strong ICL and finetuning baselines. We hope that this will\nfurther enable the deployment of many-shot ICL at scale."
                },
                "authors": [
                    {
                        "name": "Emily Xiao"
                    },
                    {
                        "name": "Chin-Jou Li"
                    },
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Amanda Bertsch"
                    }
                ],
                "author_detail": {
                    "name": "Amanda Bertsch"
                },
                "author": "Amanda Bertsch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v3",
                "updated": "2025-03-11T16:35:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    16,
                    35,
                    59,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Rewrite the methods section. Add more ablation studies and results in\n  LongVideoBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08461v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08461v1",
                "updated": "2025-03-11T14:10:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T14:10:58Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    10,
                    58,
                    1,
                    70,
                    0
                ],
                "title": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Optimizing Multimodal LLM Serving through Lightweight\n  KV-Cache Compression Framework"
                },
                "summary": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Large Language Models (MLLMs) serving systems commonly employ\nKV-cache compression to reduce memory footprint. However, existing compression\nmethods introduce significant processing overhead and queuing delays,\nparticularly in concurrent serving scenarios. We present \\texttt{FastCache}, a\nnovel serving framework that effectively addresses these challenges through two\nkey innovations: (1) a dynamic batching strategy that optimizes request\nscheduling across prefill, compression, and decode stages, and (2) an efficient\nKV-cache memory pool mechanism that eliminates memory fragmentation while\nmaintaining high GPU utilization. Our comprehensive experiments on the GQA and\nMileBench datasets demonstrate that \\texttt{FastCache} achieves up to\n19.3$\\times$ reduction in Time-To-First-Token (TTFT) and 12.1$\\times$\nimprovement in throughput compared to state-of-the-art baselines. The system\nmaintains stable performance under high-concurrency scenarios (up to 40 req/s)\nwhile reducing average memory consumption by 20\\%. These results establish\n\\texttt{FastCache} as an efficient solution for real-world LLM serving systems\nwith KV-cache compression."
                },
                "authors": [
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Ruixuan Li"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "14 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08461v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08461v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v2",
                "updated": "2025-03-11T14:02:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    14,
                    2,
                    4,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v3",
                "updated": "2025-03-11T13:13:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The text-guided video inpainting technique has significantly improved the\nperformance of content generation applications. A recent family for these\nimprovements uses diffusion models, which have become essential for achieving\nhigh-quality video inpainting results, yet they still face performance\nbottlenecks in temporal consistency and computational efficiency. This\nmotivates us to propose a new video inpainting framework using optical\nFlow-guided Efficient Diffusion (FloED) for higher video coherence.\nSpecifically, FloED employs a dual-branch architecture, where the time-agnostic\nflow branch restores corrupted flow first, and the multi-scale flow adapters\nprovide motion guidance to the main inpainting branch. Besides, a training-free\nlatent interpolation method is proposed to accelerate the multi-step denoising\nprocess using flow warping. With the flow attention cache mechanism, FLoED\nefficiently reduces the computational cost of incorporating optical flow.\nExtensive experiments on background restoration and object removal tasks show\nthat FloED outperforms state-of-the-art diffusion-based methods in both quality\nand efficiency. Our codes and models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    },
                    {
                        "name": "Qihua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Qihua Zhou"
                },
                "author": "Qihua Zhou",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08407v1",
                "updated": "2025-03-11T13:10:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-11T13:10:41Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    13,
                    10,
                    41,
                    1,
                    70,
                    0
                ],
                "title": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildSeg3D: Segment Any 3D Objects in the Wild from 2D Images"
                },
                "summary": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in interactive 3D segmentation from 2D images have\ndemonstrated impressive performance. However, current models typically require\nextensive scene-specific training to accurately reconstruct and segment\nobjects, which limits their applicability in real-time scenarios. In this\npaper, we introduce WildSeg3D, an efficient approach that enables the\nsegmentation of arbitrary 3D objects across diverse environments using a\nfeed-forward mechanism. A key challenge of this feed-forward approach lies in\nthe accumulation of 3D alignment errors across multiple 2D views, which can\nlead to inaccurate 3D segmentation results. To address this issue, we propose\nDynamic Global Aligning (DGA), a technique that improves the accuracy of global\nmulti-view alignment by focusing on difficult-to-match 3D points across images,\nusing a dynamic adjustment function. Additionally, for real-time interactive\nsegmentation, we introduce Multi-view Group Mapping (MGM), a method that\nutilizes an object mask cache to integrate multi-view segmentations and respond\nrapidly to user prompts. WildSeg3D demonstrates robust generalization across\narbitrary scenes, thereby eliminating the need for scene-specific training.\nSpecifically, WildSeg3D not only attains the accuracy of state-of-the-art\n(SOTA) methods but also achieves a $40\\times$ speedup compared to existing SOTA\nmodels. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Yansong Guo"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v5",
                "updated": "2025-03-11T09:17:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    9,
                    17,
                    2,
                    1,
                    70,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "The paper is accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06304v2",
                "updated": "2025-03-11T03:26:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    11,
                    3,
                    26,
                    20,
                    1,
                    70,
                    0
                ],
                "published": "2025-03-08T18:42:34Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    42,
                    34,
                    5,
                    67,
                    0
                ],
                "title": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization and Benchmarking of Monolithically Stackable Gain Cell\n  Memory for Last-Level Cache"
                },
                "summary": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Last Level Cache (LLC) is the processor's critical bridge between on-chip\nand off-chip memory levels - optimized for high density, high bandwidth, and\nlow operation energy. To date, high-density (HD) SRAM has been the conventional\ndevice of choice; however, with the slowing of transistor scaling, as reflected\nin the industry's almost identical HD SRAM cell size from 5 nm to 3 nm,\nalternative solutions such as 3D stacking with advanced packaging like hybrid\nbonding are pursued (as demonstrated in AMD's V-cache). Escalating data demands\nnecessitate ultra-large on-chip caches to decrease costly off-chip memory\nmovement, pushing the exploration of device technology toward monolithic 3D\n(M3D) integration where transistors can be stacked in the back-end-of-line\n(BEOL) at the interconnect level. M3D integration requires fabrication\ntechniques compatible with a low thermal budget (<400 degC). Among promising\nBEOL device candidates are amorphous oxide semiconductor (AOS) transistors,\nparticularly desirable for their ultra-low leakage (<fA/um), enabling\npersistent data retention (>seconds) when used in a gain-cell configuration.\nThis paper examines device, circuit, and system-level tradeoffs when optimizing\nBEOL-compatible AOS-based 2-transistor gain cell (2T-GC) for LLC. A cache\nearly-exploration tool, NS-Cache, is developed to model caches in advanced 7\nand 3 nm nodes and is integrated with the Gem5 simulator to systematically\nbenchmark the impact of the newfound density/performance when compared to\nHD-SRAM, MRAM, and 1T1C eDRAM alternatives for LLC."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jungyoun Kwak"
                    },
                    {
                        "name": "Junmo Lee"
                    },
                    {
                        "name": "Minji Shon"
                    },
                    {
                        "name": "Mohammadhosein Gholamrezaei"
                    },
                    {
                        "name": "Kevin Skadron"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 15 Figures, 6 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07545v1",
                "updated": "2025-03-10T17:12:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T17:12:47Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    17,
                    12,
                    47,
                    0,
                    69,
                    0
                ],
                "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing, Predictions, and LLMs: Challenges and Open Problems"
                },
                "summary": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Queueing systems present many opportunities for applying machine-learning\npredictions, such as estimated service times, to improve system performance.\nThis integration raises numerous open questions about how predictions can be\neffectively leveraged to improve scheduling decisions. Recent studies explore\nqueues with predicted service times, typically aiming to minimize job time in\nthe system. We review these works, highlight the effectiveness of predictions,\nand present open questions on queue performance. We then move to consider an\nimportant practical example of using predictions in scheduling, namely Large\nLanguage Model (LLM) systems, which presents novel scheduling challenges and\nhighlights the potential for predictions to improve performance. In particular,\nwe consider LLMs performing inference. Inference requests (jobs) in LLM systems\nare inherently complex; they have variable inference times, dynamic memory\nfootprints that are constrained by key-value (KV) store memory limitations, and\nmultiple possible preemption approaches that affect performance differently. We\nprovide background on the important aspects of scheduling in LLM systems, and\nintroduce new models and open problems that arise from them. We argue that\nthere are significant opportunities for applying insights and analysis from\nqueueing theory to scheduling in LLM systems."
                },
                "authors": [
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Rana Shahout"
                    }
                ],
                "author_detail": {
                    "name": "Rana Shahout"
                },
                "author": "Rana Shahout",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07518v1",
                "updated": "2025-03-10T16:41:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T16:41:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    41,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "TokenButler: Token Importance is Predictable",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenButler: Token Importance is Predictable"
                },
                "summary": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rely on the Key-Value (KV) Cache to store token\nhistory, enabling efficient decoding of tokens. As the KV-Cache grows, it\nbecomes a major memory and computation bottleneck, however, there is an\nopportunity to alleviate this bottleneck, especially because prior research has\nshown that only a small subset of tokens contribute meaningfully to each\ndecoding step. A key challenge in finding these critical tokens is that they\nare dynamic, and heavily input query-dependent. Existing methods either risk\nquality by evicting tokens permanently, or retain the full KV-Cache but rely on\nretrieving chunks (pages) of tokens at generation, failing at dense,\ncontext-rich tasks. Additionally, many existing KV-Cache sparsity methods rely\non inaccurate proxies for token importance. To address these limitations, we\nintroduce TokenButler, a high-granularity, query-aware predictor that learns to\nidentify these critical tokens. By training a light-weight predictor with less\nthan 1.2% parameter overhead, TokenButler prioritizes tokens based on their\ncontextual, predicted importance. This improves perplexity & downstream\naccuracy by over 8% relative to SoTA methods for estimating token importance.\nWe evaluate TokenButler on a novel synthetic small-context co-referential\nretrieval task, demonstrating near-oracle accuracy. Code, models and\nbenchmarks: https://github.com/abdelfattah-lab/TokenButler"
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Ahmed F AbouElhamayed"
                    },
                    {
                        "name": "Yifei Gao"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07474v1",
                "updated": "2025-03-10T15:49:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T15:49:20Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    49,
                    20,
                    0,
                    69,
                    0
                ],
                "title": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Rotational Symmetry Breaking Charge-density Wave Order in\n  Kagome Superconductor (Rb, K)V$_3$Sb$_5$ by Ultrafast Pump-probe Experiments"
                },
                "summary": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently discovered Kagome superconductor AV$_3$Sb$_5$ (where A refers to\nK, Rb, Cs) has stimulated widespread research interest due to its interplay of\nnon-trivial topology and unconventional correlated physics including\ncharge-density waves (CDW) and superconductivity. The essential prerequisite to\nunderstanding the microscopic mechanisms of this complex electronic landscape\nis to unveil the configuration and symmetry of the charge-density wave order.\nAs to now, little consensus has been made on what symmetry is broken. Herein,\nwe clarify the microscopic structure and symmetry breaking of the CDW phase in\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$ by ultrafast time-resolved reflectivity. Our\napproach is based on extracting coherent phonon spectra induced by\nthree-dimensional CDW and comparing them to calculated phonon frequencies via\ndensity-functional theory. The combination of these experimental results and\ncalculations provides compelling evidence that the CDW structure of both\ncompounds prevailing up to T$_{\\text{CDW}}$ is the 2 $\\times$ 2 $\\times$ 2\nstaggered inverse Star-of-David pattern with interlayer $\\pi$ phase shift, in\nwhich the six-fold rotational symmetry is broken. These observations thus\ncorroborate six-fold rotational symmetry breaking throughout the CDW phase of\nRbV$_3$Sb$_5$ and KV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Qinwen Deng"
                    },
                    {
                        "name": "Hengxin Tan"
                    },
                    {
                        "name": "Brenden R. Ortiz"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Binghai Yan"
                    },
                    {
                        "name": "Liang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wu"
                },
                "author": "Liang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v2",
                "updated": "2025-03-10T12:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    12,
                    10,
                    30,
                    0,
                    69,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v1",
                "updated": "2025-03-10T09:49:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposure Bias Reduction for Enhancing Diffusion Transformer Feature\n  Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis problem, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing the impact of caching on the generation of intermediate processes. So\nthe lack of exploration provides us with room for analysis and improvement. In\nthis paper, we analyze the impact of caching on the SNR of the diffusion\nprocess and discern that feature caching intensifies the denoising procedure,\nand we further identify this as a more severe exposure bias issue. Drawing on\nthis insight, we introduce EB-Cache, a joint cache strategy that aligns the\nNon-exposure bias (which gives us a higher performance ceiling) diffusion\nprocess. Our approach incorporates a comprehensive understanding of caching\nmechanisms and offers a novel perspective on leveraging caches to expedite\ndiffusion processes. Empirical results indicate that EB-Cache optimizes model\nperformance while concurrently facilitating acceleration. Specifically, in the\n50-step generation process, EB-Cache achieves 1.49$\\times$ acceleration with\n0.63 FID reduction from 3.69, surpassing prior acceleration methods. Code will\nbe available at\n\\href{https://github.com/aSleepyTree/EB-Cache}{https://github.com/aSleepyTree/EB-Cache}."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07027v1",
                "updated": "2025-03-10T08:07:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T08:07:17Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    8,
                    7,
                    17,
                    0,
                    69,
                    0
                ],
                "title": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyControl: Adding Efficient and Flexible Control for Diffusion\n  Transformer"
                },
                "summary": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Unet-based diffusion models, such as ControlNet and\nIP-Adapter, have introduced effective spatial and subject control mechanisms.\nHowever, the DiT (Diffusion Transformer) architecture still struggles with\nefficient and flexible control. To tackle this issue, we propose EasyControl, a\nnovel framework designed to unify condition-guided diffusion transformers with\nhigh efficiency and flexibility. Our framework is built on three key\ninnovations. First, we introduce a lightweight Condition Injection LoRA Module.\nThis module processes conditional signals in isolation, acting as a\nplug-and-play solution. It avoids modifying the base model weights, ensuring\ncompatibility with customized models and enabling the flexible injection of\ndiverse conditions. Notably, this module also supports harmonious and robust\nzero-shot multi-condition generalization, even when trained only on\nsingle-condition data. Second, we propose a Position-Aware Training Paradigm.\nThis approach standardizes input conditions to fixed resolutions, allowing the\ngeneration of images with arbitrary aspect ratios and flexible resolutions. At\nthe same time, it optimizes computational efficiency, making the framework more\npractical for real-world applications. Third, we develop a Causal Attention\nMechanism combined with the KV Cache technique, adapted for conditional\ngeneration tasks. This innovation significantly reduces the latency of image\nsynthesis, improving the overall efficiency of the framework. Through extensive\nexperiments, we demonstrate that EasyControl achieves exceptional performance\nacross various application scenarios. These innovations collectively make our\nframework highly efficient, flexible, and suitable for a wide range of tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Yirui Yuan"
                    },
                    {
                        "name": "Yiren Song"
                    },
                    {
                        "name": "Haofan Wang"
                    },
                    {
                        "name": "Jiaming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiaming Liu"
                },
                "author": "Jiaming Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06923v1",
                "updated": "2025-03-10T05:09:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-10T05:09:42Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    5,
                    9,
                    42,
                    0,
                    69,
                    0
                ],
                "title": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Reusing to Forecasting: Accelerating Diffusion Models with\n  TaylorSeers"
                },
                "summary": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have revolutionized high-fidelity image and\nvideo synthesis, yet their computational demands remain prohibitive for\nreal-time applications. To solve this problem, feature caching has been\nproposed to accelerate diffusion models by caching the features in the previous\ntimesteps and then reusing them in the following timesteps. However, at\ntimesteps with significant intervals, the feature similarity in diffusion\nmodels decreases substantially, leading to a pronounced increase in errors\nintroduced by feature caching, significantly harming the generation quality. To\nsolve this problem, we propose TaylorSeer, which firstly shows that features of\ndiffusion models at future timesteps can be predicted based on their values at\nprevious timesteps. Based on the fact that features change slowly and\ncontinuously across timesteps, TaylorSeer employs a differential method to\napproximate the higher-order derivatives of features and predict features in\nfuture timesteps with Taylor series expansion. Extensive experiments\ndemonstrate its significant effectiveness in both image and video synthesis,\nespecially in high acceleration ratios. For instance, it achieves an almost\nlossless acceleration of 4.99$\\times$ on FLUX and 5.00$\\times$ on HunyuanVideo\nwithout additional training. On DiT, it achieves $3.41$ lower FID compared with\nprevious SOTA at $4.53$$\\times$ acceleration. %Our code is provided in the\nsupplementary materials and will be made publicly available on GitHub. Our\ncodes have been released in Github:https://github.com/Shenyi-Z/TaylorSeer"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05116v2",
                "updated": "2025-03-10T02:41:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    10,
                    2,
                    41,
                    21,
                    0,
                    69,
                    0
                ],
                "published": "2025-03-07T03:27:33Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    3,
                    27,
                    33,
                    4,
                    66,
                    0
                ],
                "title": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Piccolo: Large-Scale Graph Processing with Fine-Grained In-Memory\n  Scatter-Gather"
                },
                "summary": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph processing requires irregular, fine-grained random access patterns\nincompatible with contemporary off-chip memory architecture, leading to\ninefficient data access. This inefficiency makes graph processing an extremely\nmemory-bound application. Because of this, existing graph processing\naccelerators typically employ a graph tiling-based or processing-in-memory\n(PIM) approach to relieve the memory bottleneck. In the tiling-based approach,\na graph is split into chunks that fit within the on-chip cache to maximize data\nreuse. In the PIM approach, arithmetic units are placed within memory to\nperform operations such as reduction or atomic addition. However, both\napproaches have several limitations, especially when implemented on current\nmemory standards (i.e., DDR). Because the access granularity provided by DDR is\nmuch larger than that of the graph vertex property data, much of the bandwidth\nand cache capacity are wasted. PIM is meant to alleviate such issues, but it is\ndifficult to use in conjunction with the tiling-based approach, resulting in a\nsignificant disadvantage. Furthermore, placing arithmetic units inside a memory\nchip is expensive, thereby supporting multiple types of operation is thought to\nbe impractical. To address the above limitations, we present Piccolo, an\nend-to-end efficient graph processing accelerator with fine-grained in-memory\nrandom scatter-gather. Instead of placing expensive arithmetic units in\noff-chip memory, Piccolo focuses on reducing the off-chip traffic with\nnon-arithmetic function-in-memory of random scatter-gather. To fully benefit\nfrom in-memory scatter-gather, Piccolo redesigns the cache and MHA of the\naccelerator such that it can enjoy both the advantage of tiling and in-memory\noperations. Piccolo achieves a maximum speedup of 3.28$\\times$ and a geometric\nmean speedup of 1.62$\\times$ across various and extensive benchmarks."
                },
                "authors": [
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Dogeun Kim"
                    },
                    {
                        "name": "Jun Sung"
                    },
                    {
                        "name": "Taehee Kwon"
                    },
                    {
                        "name": "Jae Hyung Ju"
                    },
                    {
                        "name": "Frank Liu"
                    },
                    {
                        "name": "Yeonkyu Choi"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "arxiv_comment": "HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v3",
                "updated": "2025-03-09T17:43:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    17,
                    43,
                    28,
                    6,
                    68,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v2",
                "updated": "2025-03-09T16:14:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    16,
                    14,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have proven effective in generating\nhigh-quality videos but are hindered by high computational costs. Existing\nvideo DiT sampling acceleration methods often rely on costly fine-tuning or\nexhibit limited generalization capabilities. We propose Asymmetric Reduction\nand Restoration (AsymRnR), a training-free and model-agnostic method to\naccelerate video DiTs. It builds on the observation that redundancies of\nfeature tokens in DiTs vary significantly across different model blocks,\ndenoising steps, and feature types. Our AsymRnR asymmetrically reduces\nredundant tokens in the attention operation, achieving acceleration with\nnegligible degradation in output quality and, in some cases, even improving it.\nWe also tailored a reduction schedule to distribute the reduction across\ncomponents adaptively. To further accelerate this process, we introduce a\nmatching cache for more efficient reduction. Backed by theoretical foundations\nand extensive experimental validation, AsymRnR integrates into state-of-the-art\nvideo DiTs and offers substantial speedup."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06594v1",
                "updated": "2025-03-09T12:54:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T12:54:05Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    12,
                    54,
                    5,
                    6,
                    68,
                    0
                ],
                "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Decoder-only: Large Language Models Can be Good Encoders for\n  Machine Translation"
                },
                "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks."
                },
                "authors": [
                    {
                        "name": "Yingfeng Luo"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Bei Li"
                    },
                    {
                        "name": "Qinghong Zhang"
                    },
                    {
                        "name": "Yongqi Gao"
                    },
                    {
                        "name": "Ziqiang Xu"
                    },
                    {
                        "name": "Peinan Feng"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06545v1",
                "updated": "2025-03-09T10:31:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T10:31:51Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    31,
                    51,
                    6,
                    68,
                    0
                ],
                "title": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantCache: Adaptive Importance-Guided Quantization with Hierarchical\n  Latent and Layer Caching for Video Generation"
                },
                "summary": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Diffusion Transformers (DiTs) have emerged as a dominant\narchitecture in video generation, surpassing U-Net-based models in terms of\nperformance. However, the enhanced capabilities of DiTs come with significant\ndrawbacks, including increased computational and memory costs, which hinder\ntheir deployment on resource-constrained devices. Current acceleration\ntechniques, such as quantization and cache mechanism, offer limited speedup and\nare often applied in isolation, failing to fully address the complexities of\nDiT architectures. In this paper, we propose QuantCache, a novel training-free\ninference acceleration framework that jointly optimizes hierarchical latent\ncaching, adaptive importance-guided quantization, and structural\nredundancy-aware pruning. QuantCache achieves an end-to-end latency speedup of\n6.72$\\times$ on Open-Sora with minimal loss in generation quality. Extensive\nexperiments across multiple video generation benchmarks demonstrate the\neffectiveness of our method, setting a new standard for efficient DiT\ninference. The code and models will be available at\nhttps://github.com/JunyiWuCode/QuantCache."
                },
                "authors": [
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/JunyiWuCode/QuantCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06433v1",
                "updated": "2025-03-09T04:14:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "published": "2025-03-09T04:14:06Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    4,
                    14,
                    6,
                    6,
                    68,
                    0
                ],
                "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seesaw: High-throughput LLM Inference via Model Re-sharding"
                },
                "summary": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To improve the efficiency of distributed large language model (LLM)\ninference, various parallelization strategies, such as tensor and pipeline\nparallelism, have been proposed. However, the distinct computational\ncharacteristics inherent in the two stages of LLM inference-prefilling and\ndecoding-render a single static parallelization strategy insufficient for the\neffective optimization of both stages. In this work, we present Seesaw, an LLM\ninference engine optimized for throughput-oriented tasks. The key idea behind\nSeesaw is dynamic model re-sharding, a technique that facilitates the dynamic\nreconfiguration of parallelization strategies across stages, thereby maximizing\nthroughput at both phases. To mitigate re-sharding overhead and optimize\ncomputational efficiency, we employ tiered KV cache buffering and\ntransition-minimizing scheduling. These approaches work synergistically to\nreduce the overhead caused by frequent stage transitions while ensuring maximum\nbatching efficiency. Our evaluation demonstrates that Seesaw achieves a\nthroughput increase of up to 1.78x (1.36x on average) compared to vLLM, the\nmost widely used state-of-the-art LLM inference engine."
                },
                "authors": [
                    {
                        "name": "Qidong Su"
                    },
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Muralidhar Andoorveedu"
                    },
                    {
                        "name": "Chenhao Jiang"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00776v3",
                "updated": "2025-03-09T02:19:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    9,
                    2,
                    19,
                    22,
                    6,
                    68,
                    0
                ],
                "published": "2024-12-01T11:43:46Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    11,
                    43,
                    46,
                    6,
                    336,
                    0
                ],
                "title": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Mamba as a Continual Learner: Meta-learning Selective State\n  Space Models for Efficient Continual Learning"
                },
                "summary": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning (CL) aims to efficiently learn from a non-stationary data\nstream, without storing or recomputing all seen samples. CL enables prediction\non new tasks by incorporating sequential training samples. Building on this\nconnection between CL and sequential modeling, meta-continual learning (MCL)\naims to meta-learn an efficient continual learner as a sequence prediction\nmodel, with advanced sequence models like Transformers being natural choices.\nHowever, despite decent performance, Transformers rely on a linearly growing\ncache to store all past representations, conflicting with CL's objective of not\nstoring all seen samples and limiting efficiency. In this paper, we focus on\nmeta-learning sequence-prediction-based continual learners without retaining\nall past representations. While attention-free models with fixed-size hidden\nstates (e.g., Linear Transformers) align with CL's essential goal and\nefficiency needs, they have shown limited effectiveness in MCL in previous\nliterature. Given Mamba's strong sequence modeling performance and\nattention-free nature, we explore a key question: Can attention-free models\nlike Mamba perform well on MCL? By formulating Mamba and the SSM for MCL tasks,\nwe propose MambaCL, a meta-learned continual learner. To enhance MambaCL's\ntraining, we introduce selectivity regularization, leveraging the connection\nbetween Mamba and Transformers to guide its behavior over sequences.\nFurthermore, we study how Mamba and other models perform across various MCL\nscenarios through extensive and well-designed experiments. Our results\nhighlight the promising performance and strong generalization of Mamba and\nattention-free models in MCL, demonstrating its potential for efficient\ncontinual learning and adaptation."
                },
                "authors": [
                    {
                        "name": "Chongyang Zhao"
                    },
                    {
                        "name": "Dong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Gong"
                },
                "author": "Dong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v2",
                "updated": "2025-03-08T21:55:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    21,
                    55,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06302v1",
                "updated": "2025-03-08T18:30:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T18:30:54Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    18,
                    30,
                    54,
                    5,
                    67,
                    0
                ],
                "title": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing AI and Digital Twins for Next-Generation Network\n  Optimization, Forecasting, and Security"
                },
                "summary": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs) are virtual representations of physical\nnetworks, designed to enable real-time monitoring, simulation, and optimization\nof network performance. When integrated with machine learning (ML) techniques,\nparticularly federated learning (FL) and reinforcement learning (RL), DNTs\nemerge as powerful solutions for managing the complexities of network\noperations. This article presents a comprehensive analysis of the synergy of\nDNTs, FL, and RL techniques, showcasing their collective potential to address\ncritical challenges in 6G networks. We highlight key technical challenges that\nneed to be addressed, such as ensuring network reliability, achieving joint\ndata-scenario forecasting, and maintaining security in high-risk environments.\nAdditionally, we propose several pipelines that integrate DNT and ML within\ncoherent frameworks to enhance network optimization and security. Case studies\ndemonstrate the practical applications of our proposed pipelines in edge\ncaching and vehicular networks. In edge caching, the pipeline achieves over 80%\ncache hit rates while balancing base station loads. In autonomous vehicular\nsystem, it ensure a 100% no-collision rate, showcasing its reliability in\nsafety-critical scenarios. By exploring these synergies, we offer insights into\nthe future of intelligent and adaptive network systems that automate\ndecision-making and problem-solving."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Minghong Fang"
                    },
                    {
                        "name": "Dianwei Chen"
                    },
                    {
                        "name": "Xianfeng Yang"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Accepted by IEEE Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03708v2",
                "updated": "2025-03-08T14:48:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    14,
                    48,
                    15,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-05T17:59:19Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    17,
                    59,
                    19,
                    2,
                    64,
                    0
                ],
                "title": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Video Tokenization: A Conditioned Diffusion-based Approach"
                },
                "summary": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video tokenizers typically use the traditional Variational\nAutoencoder (VAE) architecture for video compression and reconstruction.\nHowever, to achieve good performance, its training process often relies on\ncomplex multi-stage training tricks that go beyond basic reconstruction loss\nand KL regularization. Among these tricks, the most challenging is the precise\ntuning of adversarial training with additional Generative Adversarial Networks\n(GANs) in the final stage, which can hinder stable convergence. In contrast to\nGANs, diffusion models offer more stable training processes and can generate\nhigher-quality results. Inspired by these advantages, we propose CDT, a novel\nConditioned Diffusion-based video Tokenizer, that replaces the GAN-based\ndecoder with a conditional causal diffusion model. The encoder compresses\nspatio-temporal information into compact latents, while the decoder\nreconstructs videos through a reverse diffusion process conditioned on these\nlatents. During inference, we incorporate a feature cache mechanism to generate\nvideos of arbitrary length while maintaining temporal continuity and adopt\nsampling acceleration technique to enhance efficiency. Trained using only a\nbasic MSE diffusion loss for reconstruction, along with KL term and LPIPS\nperceptual loss from scratch, extensive experiments demonstrate that CDT\nachieves state-of-the-art performance in video reconstruction tasks with just a\nsingle-step sampling. Even a scaled-down version of CDT (3$\\times$ inference\nspeedup) still performs comparably with top baselines. Moreover, the latent\nvideo generation model trained with CDT also exhibits superior performance. The\nsource code and pretrained weights will be released shortly, so please stay\ntuned for updates!"
                },
                "authors": [
                    {
                        "name": "Nianzu Yang"
                    },
                    {
                        "name": "Pandeng Li"
                    },
                    {
                        "name": "Liming Zhao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Chen-Wei Xie"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Yun Zheng"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06015v1",
                "updated": "2025-03-08T02:35:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "published": "2025-03-08T02:35:16Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    2,
                    35,
                    16,
                    5,
                    67,
                    0
                ],
                "title": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML-based Adaptive Prefetching and Data Placement for US HEP Systems"
                },
                "summary": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although benefits from caching in US HEP are well-known, current caching\nstrategies are not adaptive i.e. they do not adapt to changing cache access\npatterns. Newer developments such as High Luminosity - Large Hadron Collider\n(HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward\nstreaming readout based Data Acquisition systems (DAQs) will increase the data\nproduction exponentially and hence burden the storage, compute \\& network\ninfrastructures. Moreover, existing caching frameworks are optimized to reduce\nlatency, but not optimized for storage. This in combination with limited cache\ncapacities relative to total data makes it difficult to achieve data locality.\n  In this work, we present Machine Learning-aided (ML) caching strategies.\nSpecifically, first we present a Long Short-Term Memory-based (LSTM) hourly\ncache usage prediction. Second, we present an hourly file-level access\nprediction model based on CatboostRegressor. To date, most ML-based cache\nprediction strategies in HEP have focused on daily cache usage and limited\nworks tackled hourly cache usage and even less strategies addressed hourly\nfile-level access prediction. File-level access prediction allows for the\ndesign of intelligent prefetching and data placement strategies with\nfine-grained control. We validated our cache prediction strategies using data\ncollected from SoCal MINI caches in August 2024. We are currently extending\nWRENCH simulator to reflect the US HEP ecosystem at the storage, network and\ncompute levels. We plan to deploy our cache prediction strategies into WRENCH\nand later perform extensive analysis with complex data access patterns and\ncandidate infrastructure configurations."
                },
                "authors": [
                    {
                        "name": "Venkat Sai Suman Lamba Karanam"
                    },
                    {
                        "name": "Sarat Sasank Barla"
                    },
                    {
                        "name": "Byrav Ramamurthy"
                    },
                    {
                        "name": "Derek Weitzel"
                    }
                ],
                "author_detail": {
                    "name": "Derek Weitzel"
                },
                "author": "Derek Weitzel",
                "arxiv_comment": "Submitted as a contribution to the CHEP 2024 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05941v1",
                "updated": "2025-03-07T21:16:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T21:16:41Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    21,
                    16,
                    41,
                    4,
                    66,
                    0
                ],
                "title": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choosing Augmentation Parameters in OSQP- A New Approach based on\n  Conjugate Directions"
                },
                "summary": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a new method to select the augmentation parameters in the\noperator splitting quadratic program (OSQP) algorithm so as to reduce the\ncomputation time of overall algorithm. The selection is based upon the\ninformation of conjugate directions of the coefficient matrix of a linear\nsystem of equations present in the algorithm. This selection makes it possible\nto cache these conjugate directions, instead of computing them at each\niteration, resulting in faster computation of the solution of the linear system\nthus reducing the overall computation time. This reduction is demonstrated by a\nnumerical example."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Avinash Kumar"
                },
                "author": "Avinash Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18668v2",
                "updated": "2025-03-07T18:57:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    57,
                    52,
                    4,
                    66,
                    0
                ],
                "published": "2024-02-28T19:28:27Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    19,
                    28,
                    27,
                    2,
                    59,
                    0
                ],
                "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple linear attention language models balance the recall-throughput\n  tradeoff"
                },
                "summary": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based."
                },
                "authors": [
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Michael Zhang"
                    },
                    {
                        "name": "Aman Timalsina"
                    },
                    {
                        "name": "Silas Alberti"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "Christopher Ré"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Ré"
                },
                "author": "Christopher Ré",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v4",
                "updated": "2025-03-07T17:47:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    17,
                    47,
                    42,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99% KV cache IO and\nnearly 100% IO for partial results during attention calculation, DeFT achieves\nup to 2.23/3.59x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms. Our code is available at https://github.com/LINs-lab/DeFT."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v4, accepted by ICLR'25\n  (https://openreview.net/forum?id=2c7pfOqu9k). Our code is available at\n  https://github.com/LINs-lab/DeFT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v1",
                "updated": "2025-03-07T15:54:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, reducing reliance on expensive vector database lookups.\nWe evaluate Proximity on the MMLU and MedRAG benchmarks, demonstrating that it\nsignificantly improves retrieval efficiency while maintaining response\naccuracy. Proximity reduces retrieval latency by up to 59% while maintaining\naccuracy and lowers the computational burden on the vector database. We also\nexperiment with different similarity thresholds and quantify the trade-off\nbetween speed and recall. Our work shows that approximate caching is a viable\nand effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v4",
                "updated": "2025-03-07T14:49:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    49,
                    7,
                    4,
                    66,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Caching for LLM Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Caching for LLM Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "Accepted at 2025 IEEE 39th International Parallel and Distributed\n  Processing Symposium (IPDPS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v1",
                "updated": "2025-03-07T05:31:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04982v1",
                "updated": "2025-03-06T21:21:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:21:18Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    21,
                    18,
                    3,
                    65,
                    0
                ],
                "title": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LVLM-Compress-Bench: Benchmarking the Broader Impact of Large\n  Vision-Language Model Compression"
                },
                "summary": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent efforts in understanding the compression impact on large\nlanguage models (LLMs) in terms of their downstream task performance and\ntrustworthiness on relatively simpler uni-modal benchmarks (for example,\nquestion answering, common sense reasoning), their detailed study on\nmulti-modal Large Vision-Language Models (LVLMs) is yet to be unveiled. Towards\nmitigating this gap, we present LVLM-Compress-Bench, a framework to first\nthoroughly study the broad impact of compression on the generative performance\nof LVLMs with multi-modal input driven tasks. In specific, we consider two\nmajor classes of compression for autoregressive models, namely KV cache and\nweight compression, for the dynamically growing intermediate cache and static\nweights, respectively.\n  We use four LVLM variants of the popular LLaVA framework to present our\nanalysis via integrating various state-of-the-art KV and weight compression\nmethods including uniform, outlier-reduced, and group quantization for the KV\ncache and weights. With this framework we demonstrate on ten different\nmulti-modal datasets with different capabilities including recognition,\nknowledge, language generation, spatial awareness, visual reasoning,\nhallucination and visual illusion identification, toxicity, stereotypes and\nbias. In specific, our framework demonstrates the compression impact on both\ngeneral and ethically critical metrics leveraging a combination of real world\nand synthetic datasets to encompass diverse societal intersectional attributes.\nExtensive experimental evaluations yield diverse and intriguing observations on\nthe behavior of LVLMs at different quantization budget of KV and weights, in\nboth maintaining and losing performance as compared to the baseline model with\nFP16 data format.\n  Code will be open-sourced at\nhttps://github.com/opengear-project/LVLM-compress-bench."
                },
                "authors": [
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Sungduk Yu"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Tiep Le"
                    },
                    {
                        "name": "Sharath Nittur Sridhar"
                    },
                    {
                        "name": "David Cobbley"
                    },
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Vasudev Lal"
                    }
                ],
                "author_detail": {
                    "name": "Vasudev Lal"
                },
                "author": "Vasudev Lal",
                "arxiv_comment": "This work has been accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04973v1",
                "updated": "2025-03-06T21:07:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "published": "2025-03-06T21:07:41Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    21,
                    7,
                    41,
                    3,
                    65,
                    0
                ],
                "title": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond RAG: Task-Aware KV Cache Compression for Comprehensive Knowledge\n  Reasoning"
                },
                "summary": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incorporating external knowledge in large language models (LLMs) enhances\ntheir utility across diverse applications, but existing methods have\ntrade-offs. Retrieval-Augmented Generation (RAG) fetches evidence via\nsimilarity search, but key information may fall outside top ranked results.\nLong-context models can process multiple documents but are computationally\nexpensive and limited by context window size. Inspired by students condensing\nstudy material for open-book exams, we propose task-aware key-value (KV) cache\ncompression, which compresses external knowledge in a zero- or few-shot setup.\nThis enables LLMs to reason efficiently over a compacted representation of all\nrelevant information. Experiments show our approach outperforms both RAG and\ntask-agnostic compression methods. On LongBench v2, it improves accuracy by up\nto 7 absolute points over RAG with a 30x compression rate, while reducing\ninference latency from 0.43s to 0.16s. A synthetic dataset highlights that RAG\nperforms well when sparse evidence suffices, whereas task-aware compression is\nsuperior for broad knowledge tasks."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Fabio Petroni"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v2",
                "updated": "2025-03-06T06:39:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    6,
                    6,
                    39,
                    56,
                    3,
                    65,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought"
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Kai Fan"
                    }
                ],
                "author_detail": {
                    "name": "Kai Fan"
                },
                "author": "Kai Fan",
                "arxiv_comment": "Camera ready version for NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01801v2",
                "updated": "2025-03-05T20:36:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    20,
                    36,
                    51,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-03T18:32:31Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    32,
                    31,
                    0,
                    62,
                    0
                ],
                "title": "TUNA: Tuning Unstable and Noisy Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TUNA: Tuning Unstable and Noisy Cloud Applications"
                },
                "summary": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autotuning plays a pivotal role in optimizing the performance of systems,\nparticularly in large-scale cloud deployments. One of the main challenges in\nperforming autotuning in the cloud arises from performance variability. We\nfirst investigate the extent to which noise slows autotuning and find that as\nlittle as $5\\%$ noise can lead to a $2.5$x slowdown in converging to the\nbest-performing configuration. We measure the magnitude of noise in cloud\ncomputing settings and find that while some components (CPU, disk) have almost\nno performance variability, there are still sources of significant variability\n(caches, memory). Furthermore, variability leads to autotuning finding unstable\nconfigurations. As many as $63.3\\%$ of the configurations selected as \"best\"\nduring tuning can have their performance degrade by $30\\%$ or more when\ndeployed. Using this as motivation, we propose a novel approach to improve the\nefficiency of autotuning systems by (a) detecting and removing outlier\nconfigurations and (b) using ML-based approaches to provide a more stable true\nsignal of de-noised experiment results to the optimizer. The resulting system,\nTUNA (Tuning Unstable and Noisy Cloud Applications) enables faster convergence\nand robust configurations. Tuning postgres running mssales, an enterprise\nproduction workload, we find that TUNA can lead to $1.88$x lower running time\non average with $2.58x$ lower standard deviation compared to traditional\nsampling methodologies."
                },
                "authors": [
                    {
                        "name": "Johannes Freischuetz"
                    },
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Brian Kroth"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_doi": "10.1145/3689031.3717480",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3717480",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 20 figures, EuroSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03751v1",
                "updated": "2025-03-05T18:59:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T18:59:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    18,
                    59,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera\n  Control"
                },
                "summary": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GEN3C, a generative video model with precise Camera Control and\ntemporal 3D Consistency. Prior video models already generate realistic videos,\nbut they tend to leverage little 3D information, leading to inconsistencies,\nsuch as objects popping in and out of existence. Camera control, if implemented\nat all, is imprecise, because camera parameters are mere inputs to the neural\nnetwork which must then infer how the video depends on the camera. In contrast,\nGEN3C is guided by a 3D cache: point clouds obtained by predicting the\npixel-wise depth of seed images or previously generated frames. When generating\nthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache with\nthe new camera trajectory provided by the user. Crucially, this means that\nGEN3C neither has to remember what it previously generated nor does it have to\ninfer the image structure from the camera pose. The model, instead, can focus\nall its generative power on previously unobserved regions, as well as advancing\nthe scene state to the next frame. Our results demonstrate more precise camera\ncontrol than prior work, as well as state-of-the-art results in sparse-view\nnovel view synthesis, even in challenging settings such as driving scenes and\nmonocular dynamic video. Results are best viewed in videos. Check out our\nwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/"
                },
                "authors": [
                    {
                        "name": "Xuanchi Ren"
                    },
                    {
                        "name": "Tianchang Shen"
                    },
                    {
                        "name": "Jiahui Huang"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Merlin Nimier-David"
                    },
                    {
                        "name": "Thomas Müller"
                    },
                    {
                        "name": "Alexander Keller"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Jun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Gao"
                },
                "author": "Jun Gao",
                "arxiv_comment": "To appear in CVPR 2025. Website:\n  https://research.nvidia.com/labs/toronto-ai/GEN3C/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v3",
                "updated": "2025-03-05T14:43:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    43,
                    1,
                    2,
                    64,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "arxiv_comment": "Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07714v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07714v5",
                "updated": "2025-03-05T07:39:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    7,
                    39,
                    3,
                    2,
                    64,
                    0
                ],
                "published": "2024-03-12T14:57:40Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    14,
                    57,
                    40,
                    1,
                    72,
                    0
                ],
                "title": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StableToolBench: Towards Stable Large-Scale Benchmarking on Tool\n  Learning of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have witnessed remarkable advancements in recent\nyears, prompting the exploration of tool learning, which integrates LLMs with\nexternal tools to address diverse real-world challenges. Assessing the\ncapability of LLMs to utilise tools necessitates large-scale and stable\nbenchmarks. However, previous works relied on either hand-crafted online tools\nwith limited scale, or large-scale real online APIs suffering from instability\nof API status. To address this problem, we introduce StableToolBench, a\nbenchmark evolving from ToolBench, proposing a virtual API server and stable\nevaluation system. The virtual API server contains a caching system and API\nsimulators which are complementary to alleviate the change in API status.\nMeanwhile, the stable evaluation system designs solvable pass and win rates\nusing GPT-4 as the automatic evaluator to eliminate the randomness during\nevaluation. Experimental results demonstrate the stability of StableToolBench,\nand further discuss the effectiveness of API simulators, the caching system,\nand the evaluator system."
                },
                "authors": [
                    {
                        "name": "Zhicheng Guo"
                    },
                    {
                        "name": "Sijie Cheng"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Shihao Liang"
                    },
                    {
                        "name": "Yujia Qin"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07714v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07714v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v1",
                "updated": "2025-03-05T04:54:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v1",
                "updated": "2025-03-04T19:51:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02812v1",
                "updated": "2025-03-04T17:37:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T17:37:49Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    17,
                    37,
                    49,
                    1,
                    63,
                    0
                ],
                "title": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression"
                },
                "summary": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM."
                },
                "authors": [
                    {
                        "name": "Nathan Godey"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Éric de la Clergerie"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02758v1",
                "updated": "2025-03-04T16:21:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T16:21:33Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    16,
                    21,
                    33,
                    1,
                    63,
                    0
                ],
                "title": "Efficient and Optimal No-Regret Caching under Partial Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Optimal No-Regret Caching under Partial Observation"
                },
                "summary": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online learning algorithms have been successfully used to design caching\npolicies with sublinear regret in the total number of requests, with no\nstatistical assumption about the request sequence. Most existing algorithms\ninvolve computationally expensive operations and require knowledge of all past\nrequests. However, this may not be feasible in practical scenarios like caching\nat a cellular base station. Therefore, we study the caching problem in a more\nrestrictive setting where only a fraction of past requests are observed, and we\npropose a randomized caching policy with sublinear regret based on the classic\nonline learning algorithm Follow-the-Perturbed-Leader (FPL). Our caching policy\nis the first to attain the asymptotically optimal regret bound while ensuring\nasymptotically constant amortized time complexity in the partial observability\nsetting of requests. The experimental evaluation compares the proposed solution\nagainst classic caching policies and validates the proposed approach under\nsynthetic and real-world request traces."
                },
                "authors": [
                    {
                        "name": "Younes Ben Mazziane"
                    },
                    {
                        "name": "Francescomaria Faticanti"
                    },
                    {
                        "name": "Sara Alouf"
                    },
                    {
                        "name": "Giovanni Neglia"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Neglia"
                },
                "author": "Giovanni Neglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03157v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03157v2",
                "updated": "2025-03-04T13:01:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    13,
                    1,
                    7,
                    1,
                    63,
                    0
                ],
                "published": "2024-07-03T14:34:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    14,
                    34,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "Let the Code LLM Edit Itself When You Edit the Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let the Code LLM Edit Itself When You Edit the Code"
                },
                "summary": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhenyu He"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shengjie Luo"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Di He"
                    }
                ],
                "author_detail": {
                    "name": "Di He"
                },
                "author": "Di He",
                "arxiv_comment": "ICLR 2025 Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03157v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03157v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02508v1",
                "updated": "2025-03-04T11:19:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:19:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    19,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Q&C: When Quantization Meets Cache in Efficient Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Q&C: When Quantization Meets Cache in Efficient Image Generation"
                },
                "summary": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and cache mechanisms are typically applied individually for\nefficient Diffusion Transformers (DiTs), each demonstrating notable potential\nfor acceleration. However, the promoting effect of combining the two mechanisms\non efficient generation remains under-explored. Through empirical\ninvestigation, we find that the combination of quantization and cache\nmechanisms for DiT is not straightforward, and two key challenges lead to\nsevere catastrophic performance degradation: (i) the sample efficacy of\ncalibration datasets in post-training quantization (PTQ) is significantly\neliminated by cache operation; (ii) the combination of the above mechanisms\nintroduces more severe exposure bias within sampling distribution, resulting in\namplified error accumulation in the image generation process. In this work, we\ntake advantage of these two acceleration mechanisms and propose a hybrid\nacceleration method by tackling the above challenges, aiming to further improve\nthe efficiency of DiTs while maintaining excellent generation capability.\nConcretely, a temporal-aware parallel clustering (TAP) is designed to\ndynamically improve the sample selection efficacy for the calibration within\nPTQ for different diffusion steps. A variance compensation (VC) strategy is\nderived to correct the sampling distribution. It mitigates exposure bias\nthrough an adaptive correction factor generation. Extensive experiments have\nshown that our method has accelerated DiTs by 12.7x while preserving\ncompetitive generation capability. The code will be available at\nhttps://github.com/xinding-sys/Quant-Cache."
                },
                "authors": [
                    {
                        "name": "Xin Ding"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02504v1",
                "updated": "2025-03-04T11:15:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T11:15:47Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    15,
                    47,
                    1,
                    63,
                    0
                ],
                "title": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy efficiency of cache eviction algorithms for Zipf distributed\n  objects"
                },
                "summary": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a summary analysis of the Least Frequently Used (LFU) and\nPerfect Least Frequently Used (PLFU) cache eviction algorithms on real data,\ntransferred on Content Delivery Nettworks (CDNs), as well as on Zipf\ndistributed samples. In light of the growing emphasis on energy efficiency in\nCDNs in recent years due to rising energy costs, this paper considers and\ndiscusses the total CPU time required to run a cache algorithm. The total CPU\ntime represents a novel metric for evaluating cache performance, and it is\ncontrasted with the conventional Cache Hit Ratio (CHR) metric. Furthermore, a\nnew algorithm with an admission policy and the eviction strategy that of PLFU\nis presented. The results demonstrate that it is a simple and straightforward\nalgorithm to implement and offers high CHR and low CPU time."
                },
                "authors": [
                    {
                        "name": "Emese Sziklay"
                    },
                    {
                        "name": "Tamás Jursonovics"
                    }
                ],
                "author_detail": {
                    "name": "Tamás Jursonovics"
                },
                "author": "Tamás Jursonovics",
                "arxiv_comment": "13 pages, 7 figures, ICRIC 2023, Volume 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02398v1",
                "updated": "2025-03-04T08:41:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T08:41:40Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    8,
                    41,
                    40,
                    1,
                    63,
                    0
                ],
                "title": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence"
                },
                "summary": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents."
                },
                "authors": [
                    {
                        "name": "Yunxiao Shi"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Zeqi Zhang"
                    },
                    {
                        "name": "Xing Zi"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Min Xu"
                    }
                ],
                "author_detail": {
                    "name": "Min Xu"
                },
                "author": "Min Xu",
                "arxiv_comment": "draft paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v1",
                "updated": "2025-03-04T03:18:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05787v2",
                "updated": "2025-03-03T18:23:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    18,
                    23,
                    47,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-08T18:57:07Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    18,
                    57,
                    7,
                    4,
                    313,
                    0
                ],
                "title": "RefreshKV: Updating Small KV Cache During Long-form Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RefreshKV: Updating Small KV Cache During Long-form Generation"
                },
                "summary": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long sequences of tokens given a long-context input is a very\ncompute-intensive inference scenario for large language models (LLMs). One\nprominent inference speed-up approach is to construct a smaller key-value (KV)\ncache, relieving LLMs from computing attention over a long sequence of tokens.\nWhile such methods work well to generate short sequences, their performance\ndegrades rapidly for long-form generation. Most KV compression happens once,\nprematurely removing tokens that can be useful later in the generation. We\npropose a new inference method, RefreshKV, that flexibly alternates between\nfull context attention and attention over a subset of input tokens during\ngeneration. After each full attention step, we update the smaller KV cache\nbased on the attention pattern over the entire input. Applying our method to\noff-the-shelf LLMs achieves comparable speedup to eviction-based methods while\nimproving performance for various long-form generation tasks. Lastly, we show\nthat continued pretraining with our inference setting brings further gains in\nperformance."
                },
                "authors": [
                    {
                        "name": "Fangyuan Xu"
                    },
                    {
                        "name": "Tanya Goyal"
                    },
                    {
                        "name": "Eunsol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Eunsol Choi"
                },
                "author": "Eunsol Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01586v1",
                "updated": "2025-03-03T14:26:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T14:26:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    14,
                    26,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and\n  Joint Low-Rank Projection"
                },
                "summary": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) enables each attention head to capture\nmulti-frequency information along the sequence dimension and is widely applied\nin foundation models. However, the nonlinearity introduced by RoPE complicates\noptimization of the key state in the Key-Value (KV) cache for RoPE-based\nattention. Existing KV cache compression methods typically store key state\nbefore rotation and apply the transformation during decoding, introducing\nadditional computational overhead. This paper introduces EliteKV, a flexible\nmodification framework for RoPE-based models supporting variable KV cache\ncompression ratios. EliteKV first identifies the intrinsic frequency preference\nof each head using RoPElite, selectively restoring linearity to certain\ndimensions of key within attention computation. Building on this, joint\nlow-rank compression of key and value enables partial cache sharing.\nExperimental results show that with minimal uptraining on only $0.6\\%$ of the\noriginal training data, RoPE-based models achieve a $75\\%$ reduction in KV\ncache size while preserving performance within a negligible margin.\nFurthermore, EliteKV consistently performs well across models of different\nscales within the same family."
                },
                "authors": [
                    {
                        "name": "Yuhao Zhou"
                    },
                    {
                        "name": "Sirui Song"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Senjie Jin"
                    },
                    {
                        "name": "Xiaoran Fan"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01483v1",
                "updated": "2025-03-03T12:43:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T12:43:06Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    43,
                    6,
                    0,
                    62,
                    0
                ],
                "title": "KurTail : Kurtosis-based LLM Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KurTail : Kurtosis-based LLM Quantization"
                },
                "summary": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the challenges of quantizing a large language model (LLM) is the\npresence of outliers. Outliers often make uniform quantization schemes less\neffective, particularly in extreme cases such as 4-bit quantization. We\nintroduce KurTail, a new post-training quantization (PTQ) scheme that leverages\nKurtosis-based rotation to mitigate outliers in the activations of LLMs. Our\nmethod optimizes Kurtosis as a measure of tailedness. This approach enables the\nquantization of weights, activations, and the KV cache in 4 bits. We utilize\nlayer-wise optimization, ensuring memory efficiency. KurTail outperforms\nexisting quantization methods, offering a 13.3\\% boost in MMLU accuracy and a\n15.5\\% drop in Wiki perplexity compared to QuaRot. It also outperforms\nSpinQuant with a 2.6\\% MMLU gain and reduces perplexity by 2.9\\%, all while\nreducing the training cost. For comparison, learning the rotation using\nSpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas\nour method requires only a single GPU, making it a more accessible solution for\nconsumer GPU."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Akhondzadeh"
                    },
                    {
                        "name": "Aleksandar Bojchevski"
                    },
                    {
                        "name": "Evangelos Eleftheriou"
                    },
                    {
                        "name": "Martino Dazzi"
                    }
                ],
                "author_detail": {
                    "name": "Martino Dazzi"
                },
                "author": "Martino Dazzi",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01348v1",
                "updated": "2025-03-03T09:38:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:38:20Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    38,
                    20,
                    0,
                    62,
                    0
                ],
                "title": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Optimization of 3D Stencil Computation on ARM Scalable\n  Vector Extension"
                },
                "summary": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stencil computation is essential in high-performance computing, especially\nfor large-scale tasks like liquid simulation and weather forecasting.\nOptimizing its performance can reduce both energy consumption and computation\ntime, which is critical in disaster prediction. This paper explores\noptimization techniques for 7-point 3D stencil computation on ARM's Scalable\nVector Extension (SVE), using the Roofline model and tools like Gem5 and cacti.\nWe evaluate software optimizations such as vectorization and tiling, as well as\nhardware adjustments in ARM SVE vector lengths and cache configurations. The\nstudy also examines performance, power consumption, and chip area trade-offs to\nidentify optimal configurations for ARM-based systems."
                },
                "authors": [
                    {
                        "name": "Hongguang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongguang Chen"
                },
                "author": "Hongguang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01330v1",
                "updated": "2025-03-03T09:12:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:12:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    12,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeightedKV: Attention Scores Weighted Key-Value Cache Merging for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) use key-value (KV) cache to reduce redundant\ncomputation in autoregressive generation. However, the KV cache size increases\nlinearly during generation, leading to excessive memory usage, especially for\nlong texts. Most KV cache compression methods evict the unimportant KV pairs to\nmaintain a fixed cache size, which leads to the permanent loss of tokens during\ngeneration. However, singular value decomposition shows that \\textit{values} do\nnot exhibit a strong low-rank property as \\textit{keys} do, suggesting that\ninformation is distributed more evenly across \\textit{values}, in contrast to\nits more redundant distribution within \\textit{keys}. Therefore, methods that\nevict both \\textit{keys} and \\textit{values} risk losing crucial information\nand compromise context integrity, ultimately degrading the output quality. To\naddress this problem, we propose WeightedKV, a novel, training-free approach\nthat discards the \\textit{keys} of less important tokens, while merging their\n\\textit{values} into neighboring tokens via a convex combination weighted by\ntheir average attention scores. In this way, the retained \\textit{keys} serve\nas anchors that guide the generation process, while the merged \\textit{values}\nprovide a rich contextual backdrop. We assess our method on four widely used\nlanguage modeling datasets, demonstrating superior performance compared to all\nbaseline methods, particularly with a lower budget ratio."
                },
                "authors": [
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01323v1",
                "updated": "2025-03-03T09:04:51Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T09:04:51Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    9,
                    4,
                    51,
                    0,
                    62,
                    0
                ],
                "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheQuant: Comprehensively Accelerated Diffusion Models"
                },
                "summary": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gradually gained prominence in the field of image\nsynthesis, showcasing remarkable generative capabilities. Nevertheless, the\nslow inference and complex networks, resulting from redundancy at both temporal\nand structural levels, hinder their low-latency applications in real-world\nscenarios. Current acceleration methods for diffusion models focus separately\non temporal and structural levels. However, independent optimization at each\nlevel to further push the acceleration limits results in significant\nperformance degradation. On the other hand, integrating optimizations at both\nlevels can compound the acceleration effects. Unfortunately, we find that the\noptimizations at these two levels are not entirely orthogonal. Performing\nseparate optimizations and then simply integrating them results in\nunsatisfactory performance. To tackle this issue, we propose CacheQuant, a\nnovel training-free paradigm that comprehensively accelerates diffusion models\nby jointly optimizing model caching and quantization techniques. Specifically,\nwe employ a dynamic programming approach to determine the optimal cache\nschedule, in which the properties of caching and quantization are carefully\nconsidered to minimize errors. Additionally, we propose decoupled error\ncorrection to further mitigate the coupled and accumulated errors step by step.\nExperimental results show that CacheQuant achieves a 5.18 speedup and 4\ncompression for Stable Diffusion on MS-COCO, with only a 0.02 loss in CLIP\nscore. Our code are open-sourced: https://github.com/BienLuky/CacheQuant ."
                },
                "authors": [
                    {
                        "name": "Xuewen Liu"
                    },
                    {
                        "name": "Zhikai Li"
                    },
                    {
                        "name": "Qingyi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Qingyi Gu"
                },
                "author": "Qingyi Gu",
                "arxiv_comment": "CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01281v1",
                "updated": "2025-03-03T08:06:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "published": "2025-03-03T08:06:55Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    8,
                    6,
                    55,
                    0,
                    62,
                    0
                ],
                "title": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DCI: A Coordinated Allocation and Filling Workload-Aware Dual-Cache\n  Allocation GNN Inference Acceleration System"
                },
                "summary": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are powerful tools for processing\ngraph-structured data, increasingly used for large-scale real-world graphs via\nsampling-based inference methods. However, inherent characteristics of neighbor\nsampling lead to redundant data loading during GNN inference, compounded by\ninefficient data transfers between host and GPU memory, resulting in slow\ninference and low resource utilization. Existing methods to accelerate GNN\ninference face several challenges: (1) low practical GPU memory utilization,\n(2) overlooking adjacency matrix locality, and (3) long preprocessing time. To\naddress these challenges, we introduce DCI, an efficient workload-aware\ndual-cache allocation system for GNN inference acceleration. DCI allocates\ncache capacities for both node features and adjacency matrices based on\nworkload patterns during the pre-sampling phase, leveraging a lightweight\ncache-filling algorithm to optimize data loading efficiency. Experimental\nresults demonstrate that DCI accelerates sampling and node feature loading,\nachieving end-to-end inference speedups of 1.18$\\times$ to 11.26$\\times$\ncompared to DGL, and 1.14$\\times$ to 13.68$\\times$ over RAIN, while reducing\npreprocessing time by 52.8\\% to 98.7\\%. Additionally, DCI outperforms\nstate-of-the-art single-cache inference systems by achieving speedup of\n1.08$\\times$ to 1.32$\\times$. We also compared DCI with DUCATI's dual-cache\npopulation strategy. Our lightweight population algorithm allows DCI to achieve\nnearly the same inference speed while keeping preprocessing time to less than\n20\\% of that required by DUCATI."
                },
                "authors": [
                    {
                        "name": "Yi Luo"
                    },
                    {
                        "name": "Yaobin Wang"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Yingchen Song"
                    },
                    {
                        "name": "Huan Wu"
                    },
                    {
                        "name": "Qingfeng Wang"
                    },
                    {
                        "name": "Jun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Huang"
                },
                "author": "Jun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v2",
                "updated": "2025-03-03T05:49:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    49,
                    41,
                    0,
                    62,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v1",
                "updated": "2025-03-02T18:12:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v2",
                "updated": "2025-03-02T14:37:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    14,
                    37,
                    53,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00695v1",
                "updated": "2025-03-02T02:26:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "published": "2025-03-02T02:26:21Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    2,
                    26,
                    21,
                    6,
                    61,
                    0
                ],
                "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition from video enables various downstream\napplications. Transformer-based sliding window approaches have set the\nstate-of-the-art by capturing rich spatial-temporal features. However, while\ntransformers can theoretically handle arbitrary-length sequences, in practice\nthey are limited by memory and compute constraints, resulting in fixed context\nwindows that struggle with maintaining temporal consistency across lengthy\nsurgical procedures. This often leads to fragmented predictions and limited\nprocedure-level understanding. To address these challenges, we propose Memory\nof Surgery (MoS), a framework that enriches temporal modeling by incorporating\nboth semantic interpretable long-term surgical history and short-term\nimpressions. MoSFormer, our enhanced transformer architecture, integrates MoS\nusing a carefully designed encoding and fusion mechanism. We further introduce\nstep filtering to refine history representation and develop a memory caching\npipeline to improve training and inference stability, mitigating shortcut\nlearning and overfitting. MoSFormer demonstrates state-of-the-art performance\non multiple benchmarks. On the Challenging BernBypass70 benchmark, it attains\n88.0 video-level accuracy and phase-level metrics of 70.7 precision, 68.7\nrecall, and 66.3 F1 score, outperforming its baseline with 2.1 video-level\naccuracy and phase-level metrics of 4.6 precision, 3.6 recall, and 3.8 F1\nscore. Further studies confirms the individual and combined benefits of\nlong-term and short-term memory components through ablation and counterfactual\ninference. Qualitative results shows improved temporal consistency. The\naugmented temporal context enables procedure-level understanding, paving the\nway for more comprehensive surgical video analysis."
                },
                "authors": [
                    {
                        "name": "Hao Ding"
                    },
                    {
                        "name": "Xu Lian"
                    },
                    {
                        "name": "Mathias Unberath"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Unberath"
                },
                "author": "Mathias Unberath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07295v2",
                "updated": "2025-03-02T01:39:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    2,
                    1,
                    39,
                    57,
                    6,
                    61,
                    0
                ],
                "published": "2024-10-09T16:21:38Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    16,
                    21,
                    38,
                    2,
                    283,
                    0
                ],
                "title": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IterGen: Iterative Semantic-aware Structured LLM Generation with\n  Backtracking"
                },
                "summary": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used for tasks such as natural\nlanguage and code generation, but their outputs often suffer from issues like\nhallucination, toxicity, and incorrect results. Current libraries for\nstructured LLM generation rely on left-to-right decoding without support for\nbacktracking, limiting the ability to correct or refine outputs mid-generation.\n  To address this, we introduce IterGen, a user-friendly library for iterative,\ngrammar-guided LLM generation that enables users to move both forward and\nbackward within the generated output based on grammar symbols. By leveraging a\nsymbol-to-position mapping and maintaining the key-value (KV) cache state,\nIterGen ensures efficient and structured generation while allowing for\ncorrections during the process. We demonstrate IterGen's effectiveness in two\nimportant applications: reducing privacy leakage in LLM outputs and improving\nthe accuracy of LLM-generated SQL and Vega-Lite queries.\n  Our code and additional resources are available at https://structuredllm.com."
                },
                "authors": [
                    {
                        "name": "Shubham Ugare"
                    },
                    {
                        "name": "Rohan Gumaste"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Gagandeep Singh"
                    },
                    {
                        "name": "Sasa Misailovic"
                    }
                ],
                "author_detail": {
                    "name": "Sasa Misailovic"
                },
                "author": "Sasa Misailovic",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00540v1",
                "updated": "2025-03-01T15:53:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T15:53:33Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    15,
                    53,
                    33,
                    5,
                    60,
                    0
                ],
                "title": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Video Question-Answering with In-context Video KV-Cache\n  Retrieval"
                },
                "summary": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose ReKV, a novel training-free approach that enables efficient\nstreaming video question-answering (StreamingVQA), by seamlessly integrating\nwith existing Video Large Language Models (Video-LLMs). Traditional VideoQA\nsystems struggle with long videos, as they must process entire videos before\nresponding to queries, and repeat this process for each new question. In\ncontrast, our approach analyzes long videos in a streaming manner, allowing for\nprompt responses as soon as user queries are received. Building on a common\nVideo-LLM, we first incorporate a sliding-window attention mechanism, ensuring\nthat input frames attend to a limited number of preceding frames, thereby\nreducing computational overhead. To prevent information loss, we store\nprocessed video key-value caches (KV-Caches) in RAM and disk, reloading them\ninto GPU memory as needed. Additionally, we introduce a retrieval method that\nleverages an external retriever or the parameters within Video-LLMs to retrieve\nonly query-relevant KV-Caches, ensuring both efficiency and accuracy in\nquestion answering. ReKV enables the separation of video encoding and\nquestion-answering across different processes and GPUs, significantly enhancing\nthe efficiency of StreamingVQA. Through comprehensive experimentation, we\nvalidate the efficacy and practicality of our approach, which significantly\nboosts efficiency and enhances applicability over existing VideoQA models."
                },
                "authors": [
                    {
                        "name": "Shangzhe Di"
                    },
                    {
                        "name": "Zhelun Yu"
                    },
                    {
                        "name": "Guanghao Zhang"
                    },
                    {
                        "name": "Haoyuan Li"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Bolin Li"
                    },
                    {
                        "name": "Wanggui He"
                    },
                    {
                        "name": "Fangxun Shu"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "arxiv_comment": "Accepted to ICLR 2025. Code: https://github.com/Becomebright/ReKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00392v1",
                "updated": "2025-03-01T07:56:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T07:56:42Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    7,
                    56,
                    42,
                    5,
                    60,
                    0
                ],
                "title": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Sparse Attention: Algorithm and System Co-design for\n  Efficient Attention in LLM Serving"
                },
                "summary": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts has become a critical capability for modern large\nlanguage models (LLMs). However, serving long-context LLMs comes with\nsignificant inference costs due to the high memory overhead of the key-value\n(KV) cache. Existing work leverages dynamic sparse attention algorithms (DSAes)\nto mitigate the KV cache overhead, but these algorithms rely on top-$k$ KV\ncache selection, which results in a trade-off between accuracy and efficiency.\nA larger $k$ improves accuracy but decreases efficiency, while a smaller $k$\nboosts efficiency but compromises accuracy. To overcome this trade-off, this\npaper presents PSA, a $\\underline{P}$rogressive $\\underline{S}$parse\n$\\underline{A}$ttention mechanism that integrates algorithmic innovations with\nsystem co-design to achieve both high inference accuracy and improved\nefficiency in LLM serving. The PSA algorithm adaptively adjusts the KV cache\nbudget of different tokens and layers according to their real attention weight\ndistributions, rather than relying on a fixed budget $k$. This enables high\naccuracy while minimizing KV cache usage. To further enhance execution\nefficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU\ninterleaving and synchronization overhead during PSA computation. Additionally,\nwe implement unified GPU memory management that optimizes PSA's memory\nutilization by accounting for uneven memory requirements across different model\nlayers. Extensive experimental results demonstrate that PSA reduces KV cache\nusage for attention computation by up to 2.4$\\times$ and 8.8$\\times$, and\nincreases end-to-end serving throughput by up to 1.4$\\times$ and 2.0$\\times$,\ncompared to state-of-the-art DSAes and systems without sparse attention,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v6",
                "updated": "2025-03-01T05:43:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    5,
                    43,
                    19,
                    5,
                    60,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3706628.3708873",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708873",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.03058v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00323v1",
                "updated": "2025-03-01T03:20:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "published": "2025-03-01T03:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    1,
                    3,
                    20,
                    30,
                    5,
                    60,
                    0
                ],
                "title": "FLStore: Efficient Federated Learning Storage for non-training workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLStore: Efficient Federated Learning Storage for non-training workloads"
                },
                "summary": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an approach for privacy-preserving Machine\nLearning (ML), enabling model training across multiple clients without\ncentralized data collection. With an aggregator server coordinating training,\naggregating model updates, and storing metadata across rounds. In addition to\ntraining, a substantial part of FL systems are the non-training workloads such\nas scheduling, personalization, clustering, debugging, and incentivization.\nMost existing systems rely on the aggregator to handle non-training workloads\nand use cloud services for data storage. This results in high latency and\nincreased costs as non-training workloads rely on large volumes of metadata,\nincluding weight parameters from client updates, hyperparameters, and\naggregated updates across rounds, making the situation even worse. We propose\nFLStore, a serverless framework for efficient FL non-training workloads and\nstorage. FLStore unifies the data and compute planes on a serverless cache,\nenabling locality-aware execution via tailored caching policies to reduce\nlatency and costs. Per our evaluations, compared to cloud object store based\naggregator server FLStore reduces per request average latency by 71% and costs\nby 92.45%, with peak improvements of 99.7% and 98.8%, respectively. Compared to\nan in-memory cloud cache based aggregator server, FLStore reduces average\nlatency by 64.6% and costs by 98.83%, with peak improvements of 98.8% and\n99.6%, respectively. FLStore integrates seamlessly with existing FL frameworks\nwith minimal modifications, while also being fault-tolerant and highly\nscalable."
                },
                "authors": [
                    {
                        "name": "Ahmad Faraz Khan"
                    },
                    {
                        "name": "Samuel Fountain"
                    },
                    {
                        "name": "Ahmed M. Abdelmoniem"
                    },
                    {
                        "name": "Ali R. Butt"
                    },
                    {
                        "name": "Ali Anwar"
                    }
                ],
                "author_detail": {
                    "name": "Ali Anwar"
                },
                "author": "Ali Anwar",
                "arxiv_comment": "11 pages, 19 figures, 2 tables This paper has been accepted at the\n  The Eighth Annual Conference on Machine Learning and Systems (MLSys 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v4",
                "updated": "2025-02-28T18:04:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    18,
                    4,
                    52,
                    4,
                    59,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21117v1",
                "updated": "2025-02-28T14:54:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:54:35Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    54,
                    35,
                    4,
                    59,
                    0
                ],
                "title": "Distributed Data Access in Industrial Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Data Access in Industrial Edge Networks"
                },
                "summary": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless edge networks in smart industrial environments increasingly operate\nusing advanced sensors and autonomous machines interacting with each other and\ngenerating huge amounts of data. Those huge amounts of data are bound to make\ndata management (e.g., for processing, storing, computing) a big challenge.\nCurrent data management approaches, relying primarily on centralized data\nstorage, might not be able to cope with the scalability and real time\nrequirements of Industry 4.0 environments, while distributed solutions are\nincreasingly being explored. In this paper, we introduce the problem of\ndistributed data access in multi-hop wireless industrial edge deployments,\nwhereby a set of consumer nodes needs to access data stored in a set of data\ncache nodes, satisfying the industrial data access delay requirements and at\nthe same time maximizing the network lifetime. We prove that the introduced\nproblem is computationally intractable and, after formulating the objective\nfunction, we design a two-step algorithm in order to address it. We use an open\ntestbed with real devices for conducting an experimental investigation on the\nperformance of the algorithm. Then, we provide two online improvements, so that\nthe data distribution can dynamically change before the first node in the\nnetwork runs out of energy. We compare the performance of the methods via\nsimulations for different numbers of network nodes and data consumers, and we\nshow significant lifetime prolongation and increased energy efficiency when\nemploying the method which is using only decentralized low-power wireless\ncommunication instead of the method which is using also centralized local area\nwireless communication."
                },
                "authors": [
                    {
                        "name": "Theofanis P. Raptis"
                    },
                    {
                        "name": "Andrea Passarella"
                    },
                    {
                        "name": "Marco Conti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Conti"
                },
                "author": "Marco Conti",
                "arxiv_doi": "10.1109/JSAC.2020.2980917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSAC.2020.2980917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.21117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work was funded by the EC through the FoF-RIA Project AUTOWARE\n  (No. 723909)",
                "arxiv_journal_ref": "IEEE Journal on Selected Areas in Communications, vol. 38, no. 5,\n  pp. 915-927, May 2020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.21079v1",
                "updated": "2025-02-28T14:11:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T14:11:20Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    14,
                    11,
                    20,
                    4,
                    59,
                    0
                ],
                "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free and Adaptive Sparse Attention for Efficient Long Video\n  Generation"
                },
                "summary": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating high-fidelity long videos with Diffusion Transformers (DiTs) is\noften hindered by significant latency, primarily due to the computational\ndemands of attention mechanisms. For instance, generating an 8-second 720p\nvideo (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500\nPFLOPs consumed by attention computations. To address this issue, we propose\nAdaSpa, the first Dynamic Pattern and Online Precise Search sparse attention\nmethod. Firstly, to realize the Dynamic Pattern, we introduce a blockified\npattern to efficiently capture the hierarchical sparsity inherent in DiTs. This\nis based on our observation that sparse characteristics of DiTs exhibit\nhierarchical and blockified structures between and within different modalities.\nThis blockified approach significantly reduces the complexity of attention\ncomputation while maintaining high fidelity in the generated videos. Secondly,\nto enable Online Precise Search, we propose the Fused LSE-Cached Search with\nHead-adaptive Hierarchical Block Sparse Attention. This method is motivated by\nour finding that DiTs' sparse pattern and LSE vary w.r.t. inputs, layers, and\nheads, but remain invariant across denoising steps. By leveraging this\ninvariance across denoising steps, it adapts to the dynamic nature of DiTs and\nallows for precise, real-time identification of sparse indices with minimal\noverhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can\nbe integrated seamlessly with existing DiTs, requiring neither additional\nfine-tuning nor a dataset-dependent profiling. Extensive experiments validate\nthat AdaSpa delivers substantial acceleration across various models while\npreserving video quality, establishing itself as a robust and scalable approach\nto efficient video generation."
                },
                "authors": [
                    {
                        "name": "Yifei Xia"
                    },
                    {
                        "name": "Suhan Ling"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v3",
                "updated": "2025-02-28T13:23:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    23,
                    56,
                    4,
                    59,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baseline. Also, we establish a theoretical upper\nbound by an Oracle with LLMs and perform an in-depth linguistic analysis to\nunderstand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v3",
                "updated": "2025-02-28T13:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    13,
                    8,
                    44,
                    4,
                    59,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20812v1",
                "updated": "2025-02-28T07:56:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "published": "2025-02-28T07:56:37Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    7,
                    56,
                    37,
                    4,
                    59,
                    0
                ],
                "title": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Vector Database Management Systems: A Software Testing\n  Roadmap for 2030"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) and AI-driven applications\nhas propelled Vector Database Management Systems (VDBMSs) into the spotlight as\na critical infrastructure component. VDBMS specializes in storing, indexing,\nand querying dense vector embeddings, enabling advanced LLM capabilities such\nas retrieval-augmented generation, long-term memory, and caching mechanisms.\nHowever, the explosive adoption of VDBMS has outpaced the development of\nrigorous software testing methodologies tailored for these emerging systems.\nUnlike traditional databases optimized for structured data, VDBMS face unique\ntesting challenges stemming from the high-dimensional nature of vector data,\nthe fuzzy semantics in vector search, and the need to support dynamic data\nscaling and hybrid query processing. In this paper, we begin by conducting an\nempirical study of VDBMS defects and identify key challenges in test input\ngeneration, oracle definition, and test evaluation. Drawing from these\ninsights, we propose the first comprehensive research roadmap for developing\neffective testing methodologies tailored to VDBMS. By addressing these\nchallenges, the software testing community can contribute to the development of\nmore reliable and trustworthy VDBMS, enabling the full potential of LLMs and\ndata-intensive AI applications."
                },
                "authors": [
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Yinglin Xie"
                    },
                    {
                        "name": "Zhao Liu"
                    },
                    {
                        "name": "Xinyi Hou"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v1",
                "updated": "2025-02-27T23:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Inference"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general VQA benchmarks, and show that CoT\nincreases overall VQA performance by up to 7.7% under the same budget, and\nspecifically boosts the performance of apprentice VLMs by up to 36.6%."
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "Mingyuan, Jize, and Haozhen contributed equally, while Minjia,\n  Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15896v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15896v3",
                "updated": "2025-02-27T21:50:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    50,
                    48,
                    3,
                    58,
                    0
                ],
                "published": "2023-12-26T06:16:12Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    6,
                    16,
                    12,
                    1,
                    360,
                    0
                ],
                "title": "WWW: What, When, Where to Compute-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WWW: What, When, Where to Compute-in-Memory"
                },
                "summary": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrix multiplication is the dominant computation during Machine Learning\n(ML) inference. To efficiently perform such multiplication operations,\nCompute-in-memory (CiM) paradigms have emerged as a highly energy efficient\nsolution. However, integrating compute in memory poses key questions, such as\n1) What type of CiM to use: Given a multitude of CiM design characteristics,\ndetermining their suitability from architecture perspective is needed. 2) When\nto use CiM: ML inference includes workloads with a variety of memory and\ncompute requirements, making it difficult to identify when CiM is more\nbeneficial than standard processing cores. 3) Where to integrate CiM: Each\nmemory level has different bandwidth and capacity, creating different data\nreuse opportunities for CiM integration.\n  To answer such questions regarding on-chip CiM integration for accelerating\nML workloads, we use an analytical architecture-evaluation methodology with\ntailored mapping algorithm. The mapping algorithm aims to achieve highest\nweight reuse and reduced data movements for a given CiM prototype and workload.\nOur analysis considers the integration of CiM prototypes into the cache levels\nof a tensor-core-like architecture, and shows that CiM integrated memory\nimproves energy efficiency by up to 3.4x and throughput by up to 15.6x compared\nto established baseline with INT-8 precision. We believe the proposed work\nprovides insights into what type of CiM to use, and when and where to optimally\nintegrate it in the cache hierarchy for efficient matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Tanvi Sharma"
                    },
                    {
                        "name": "Mustafa Ali"
                    },
                    {
                        "name": "Indranil Chakraborty"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "added supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15896v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15896v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20547v1",
                "updated": "2025-02-27T21:42:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T21:42:49Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    21,
                    42,
                    49,
                    3,
                    58,
                    0
                ],
                "title": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Attempt to Catch Up with JIT Compilers: The False Lead of Optimizing\n  Inline Caches"
                },
                "summary": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Just-in-Time (JIT) compilers are able to specialize the code they\ngenerate according to a continuous profiling of the running programs. This\ngives them an advantage when compared to Ahead-of-Time (AoT) compilers that\nmust choose the code to generate once for all.\n  Inquiry: Is it possible to improve the performance of AoT compilers by adding\nDynamic Binary Modification (DBM) to the executions?\n  Approach: We added to the Hopc AoT JavaScript compiler a new optimization\nbased on DBM to the inline cache (IC), a classical optimization dynamic\nlanguages use to implement object property accesses efficiently.\n  Knowledge: Reducing the number of memory accesses as the new optimization\ndoes, does not shorten execution times on contemporary architectures.\n  Grounding: The DBM optimization we have implemented is fully operational on\nx86_64 architectures. We have conducted several experiments to evaluate its\nimpact on performance and to study the reasons of the lack of acceleration.\n  Importance: The (negative) result we present in this paper sheds new light on\nthe best strategy to be used to implement dynamic languages. It tells that the\nold days were removing instructions or removing memory reads always yielded to\nspeed up is over. Nowadays, implementing sophisticated compiler optimizations\nis only worth the effort if the processor is not able by itself to accelerate\nthe code. This result applies to AoT compilers as well as JIT compilers."
                },
                "authors": [
                    {
                        "name": "Aurore Poirier"
                    },
                    {
                        "name": "Erven Rohou"
                    },
                    {
                        "name": "Manuel Serrano"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Serrano"
                },
                "arxiv_affiliation": "Inria - University of Côte d'Azur, France",
                "author": "Manuel Serrano",
                "arxiv_doi": "10.22152/programming-journal.org/2026/10/6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.22152/programming-journal.org/2026/10/6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.20547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Art, Science, and Engineering of Programming, 2025, Vol. 10,\n  Issue 1, Article 6",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v1",
                "updated": "2025-02-27T17:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "Long-Context Inference with Retrieval-Augmented Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Context Inference with Retrieval-Augmented Speculative Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference, particularly in managing key-value (KV) caches, presents significant\nefficiency challenges. While Speculative Decoding (SD) traditionally\naccelerates inference using smaller draft models, its effectiveness diminishes\nsubstantially in long-context scenarios due to memory-bound KV cache\noperations. We present Retrieval-Augmented Speculative Decoding (RAPID), which\nleverages RAG for both accelerating and enhancing generation quality in\nlong-context inference. RAPID introduces the RAG drafter-a draft LLM operating\non shortened retrieval contexts-to speculate on the generation of long-context\ntarget LLMs. Our approach enables a new paradigm where same-scale or even\nlarger LLMs can serve as RAG drafters while maintaining computational\nefficiency. To fully leverage the potentially superior capabilities from\nstronger RAG drafters, we develop an inference-time knowledge transfer dynamic\nthat enriches the target distribution by RAG. Extensive experiments on the\nLLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates\nthe strengths of both approaches, achieving significant performance\nimprovements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with\nmore than 2x speedups. Our analyses reveal that RAPID achieves robust\nacceleration beyond 32K context length and demonstrates superior generation\nquality in real-world applications."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v2",
                "updated": "2025-02-27T15:29:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    15,
                    29,
                    3,
                    3,
                    58,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v3",
                "updated": "2025-02-27T12:30:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    30,
                    43,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance. Our code has been made available at\nhttps://github.com/SalesforceAIResearch/ThinK."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "ICLR 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v2",
                "updated": "2025-02-27T12:15:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    12,
                    15,
                    38,
                    3,
                    58,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uroš Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uroš Seljak"
                },
                "author": "Uroš Seljak",
                "arxiv_comment": "36 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16235v2",
                "updated": "2025-02-27T06:39:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    27,
                    6,
                    39,
                    6,
                    3,
                    58,
                    0
                ],
                "published": "2025-02-22T14:13:37Z",
                "published_parsed": [
                    2025,
                    2,
                    22,
                    14,
                    13,
                    37,
                    5,
                    53,
                    0
                ],
                "title": "Dynamic Parallel Tree Search for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Parallel Tree Search for Efficient LLM Reasoning"
                },
                "summary": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by\nstructuring problem-solving as a spanning tree. However, recent methods focus\non search accuracy while overlooking computational efficiency. The challenges\nof accelerating the ToT lie in the frequent switching of reasoning focus, and\nthe redundant exploration of suboptimal solutions. To alleviate this dilemma,\nwe propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework\nthat aims to dynamically optimize the reasoning path in inference. It includes\nthe Parallelism Streamline in the generation phase to build up a flexible and\nadaptive parallelism with arbitrary paths by fine-grained cache management and\nalignment. Meanwhile, the Search and Transition Mechanism filters potential\ncandidates to dynamically maintain the reasoning focus on more possible\nsolutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with\nMath500 and GSM8K datasets show that DPTS significantly improves efficiency by\n2-4x on average while maintaining or even surpassing existing reasoning\nalgorithms in accuracy, making ToT-based reasoning more scalable and\ncomputationally efficient."
                },
                "authors": [
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Wentao Jiang"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Yongcheng Jing"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Yingjie Wang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05783v1",
                "updated": "2025-02-26T21:03:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T21:03:02Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    21,
                    3,
                    2,
                    2,
                    57,
                    0
                ],
                "title": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge representation and scalable abstract reasoning for simulated\n  democracy in Unity"
                },
                "summary": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel form of scalable knowledge representation about agents in\na simulated democracy, e-polis, where real users respond to social challenges\nassociated with democratic institutions, structured as Smart Spatial Types, a\nnew type of Smart Building that changes architectural form according to the\nphilosophical doctrine of a visitor. At the end of the game players vote on the\nSmart City that results from their collective choices. Our approach uses\ndeductive systems in an unusual way: by integrating a model of democracy with a\nmodel of a Smart City we are able to prove quality aspects of the simulated\ndemocracy in different urban and social settings, while adding ease and\nflexibility to the development. Second, we can infer and reason with abstract\nknowledge, which is a limitation of the Unity platform; third, our system\nenables real-time decision-making and adaptation of the game flow based on the\nplayer's abstract state, paving the road to explainability. Scalability is\nachieved by maintaining a dual-layer knowledge representation mechanism for\nreasoning about the simulated democracy that functions in a similar way to a\ntwo-level cache. The lower layer knows about the current state of the game by\ncontinually processing a high rate of events produced by the in-built physics\nengine of the Unity platform, e.g., it knows of the position of a player in\nspace, in terms of his coordinates x,y,z as well as their choices for each\nchallenge. The higher layer knows of easily-retrievable, user-defined abstract\nknowledge about current and historical states, e.g., it knows of the political\ndoctrine of a Smart Spatial Type, a player's philosophical doctrine, and the\ncollective philosophical doctrine of a community players with respect to\ncurrent social issues."
                },
                "authors": [
                    {
                        "name": "Eleftheria Katsiri"
                    },
                    {
                        "name": "Alexandros Gazis"
                    },
                    {
                        "name": "Angelos Protopapas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Protopapas"
                },
                "author": "Angelos Protopapas",
                "arxiv_comment": "23 pages, 11 figures, 76 references. This article is under review at\n  WSEAS Transactions on Information Science and Applications from 02.2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3; C.5.2; C.5.3; C.5.5; C.5.m; C.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v3",
                "updated": "2025-02-26T11:47:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    11,
                    47,
                    58,
                    2,
                    57,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%. The code is available at\nhttps://github.com/HArmonizedSS/HASS."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "arxiv_comment": "Published as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v3",
                "updated": "2025-02-26T10:49:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    10,
                    49,
                    33,
                    2,
                    57,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "Jürgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Schmidhuber"
                },
                "author": "Jürgen Schmidhuber",
                "arxiv_comment": "Accepted by TMLR: https://openreview.net/forum?id=xXs2GKXPnH",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v1",
                "updated": "2025-02-26T07:10:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Hours to Minutes: Lossless Acceleration of Ultra Long Sequence\n  Generation up to 100K Tokens"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v2",
                "updated": "2025-02-26T02:48:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    48,
                    22,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18755v1",
                "updated": "2025-02-26T02:16:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "published": "2025-02-26T02:16:46Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    2,
                    16,
                    46,
                    2,
                    57,
                    0
                ],
                "title": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically\n  Adaptive Numerical Type"
                },
                "summary": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are one of the most important killer computer\napplications. The recent algorithmic advancement proposes a fine-grained\ngroup-wise quantization for LLMs, which treats a small set (e.g., 64) of values\nin a tensor as a compression unit. It effectively preserves the model accuracy\nwithout retraining, and has become the standard approach to efficiently deploy\nLLMs. On the other hand, there are works that propose various adaptive data\ntypes to better adapt to different distributions and further reduce the\nrequired bit length for LLMs. In this work, our detailed analysis unveils a key\nfinding that while different tensors exhibit similar distributions, small\ngroups can have markedly different distributions. As such, the group-level\ndiversity requires a new level of adaptivity for which existing adaptive data\ntypes fail to provide.\n  In this paper, we propose MANT, a mathematically adaptive numeric type,\nfeaturing a more flexible encoding paradigm with a wider range of data\ndistribution and more efficient decodingcomputation fusion mechanism to address\nthese challenges. Based on MANT, we develop a supporting framework to assign\nthe appropriate data type for each group adaptively. Meanwhile, the dynamically\ngenerated Key-Value (KV) caches in LLMs introduce further complexity for\nreal-time quantization. To tackle this, we propose an efficient real-time\nquantization mechanism. Besides, we implement a specific processing element\n(PE) to efficiently support MANT and incorporate a real-time quantization unit.\nBy integrating these components into a systolic array, MANT unifies the\ngroup-wise weight and KV cache quantization and addresses the associated\nchallenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)\nspeedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM\naccelerator."
                },
                "authors": [
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Haoyan Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Renyang Guan"
                    },
                    {
                        "name": "Zhendong Hua"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2203.02550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2203.02550v3",
                "updated": "2025-02-25T13:03:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    13,
                    3,
                    44,
                    1,
                    56,
                    0
                ],
                "published": "2022-03-04T19:56:56Z",
                "published_parsed": [
                    2022,
                    3,
                    4,
                    19,
                    56,
                    56,
                    4,
                    63,
                    0
                ],
                "title": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgileWatts: An Energy-Efficient CPU Core Idle-State Architecture for\n  Latency-Sensitive Server Applications"
                },
                "summary": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User-facing applications running in modern datacenters exhibit irregular\nrequest patterns and are implemented using a multitude of services with tight\nlatency requirements. These characteristics render ineffective existing energy\nconserving techniques when processors are idle due to the long transition time\nfrom a deep idle power state (C-state). While prior works propose management\ntechniques to mitigate this inefficiency, we tackle it at its root with\nAgileWatts (AW): a new deep C-state architecture optimized for datacenter\nserver processors targeting latency-sensitive applications. AW is based on\nthree key ideas. First, AW eliminates the latency overhead of saving/restoring\nthe core context (i.e., micro-architectural state) when powering-off/-on the\ncore in a deep idle power state by i) implementing medium-grained power-gates,\ncarefully distributed across the CPU core, and ii) retaining context in the\npower-ungated domain. Second, AW eliminates the flush latency overhead (several\ntens of microseconds) of the L1/L2 caches when entering a deep idle power state\nby keeping L1/L2 cache content power-ungated. A minimal control logic also\nremains power-ungated to serve cache coherence traffic (i.e., snoops)\nseamlessly. AW implements sleep-mode in caches to reduce caches leakage power\nconsumption and lowers a core voltage to the minimum operational voltage level\nto minimize the leakage power of the power-ungated domain. Third, using a\nstate-of-the-art power efficient all-digital phase-locked loop (ADPLL) clock\ngenerator, AW keeps the PLL active and locked during the idle state, further\ncutting precious microseconds of wake-up latency at a negligible power cost.\nOur evaluation with an accurate simulator calibrated against an Intel Skylake\nserver shows that AW reduces the energy consumption of Memcached by up to 71%\n(35% on average) with up to 1% performance degradation."
                },
                "authors": [
                    {
                        "name": "Jawad Haj Yahya"
                    },
                    {
                        "name": "Haris Volos"
                    },
                    {
                        "name": "Davide B. Bartolini"
                    },
                    {
                        "name": "Georgia Antoniou"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Kleovoulos Kalaitzidis"
                    },
                    {
                        "name": "Tom Rollet"
                    },
                    {
                        "name": "Zhirui Chen"
                    },
                    {
                        "name": "Ye Geng"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Yiannakis Sazeides"
                    }
                ],
                "author_detail": {
                    "name": "Yiannakis Sazeides"
                },
                "author": "Yiannakis Sazeides",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2203.02550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2203.02550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18113v1",
                "updated": "2025-02-25T11:36:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-25T11:36:43Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    11,
                    36,
                    43,
                    1,
                    56,
                    0
                ],
                "title": "Accelerating Graph Indexing for ANNS on Modern CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Graph Indexing for ANNS on Modern CPUs"
                },
                "summary": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In high-dimensional vector spaces, Approximate Nearest Neighbor Search (ANNS)\nis a key component in database and artificial intelligence infrastructures.\nGraph-based methods, particularly HNSW, have emerged as leading solutions among\nvarious ANNS approaches, offering an impressive trade-off between search\nefficiency and accuracy. Many modern vector databases utilize graph indexes as\ntheir core algorithms, benefiting from various optimizations to enhance search\nperformance. However, the high indexing time associated with graph algorithms\nposes a significant challenge, especially given the increasing volume of data,\nquery processing complexity, and dynamic index maintenance demand. This has\nrendered indexing time a critical performance metric for users. In this paper,\nwe comprehensively analyze the underlying causes of the low graph indexing\nefficiency on modern CPUs, identifying that distance computation dominates\nindexing time, primarily due to high memory access latency and suboptimal\narithmetic operation efficiency. We demonstrate that distance comparisons\nduring index construction can be effectively performed using compact vector\ncodes at an appropriate compression error. Drawing from insights gained through\nintegrating existing compact coding methods in the graph indexing process, we\npropose a novel compact coding strategy, named Flash, designed explicitly for\ngraph indexing and optimized for modern CPU architectures. By minimizing random\nmemory accesses and maximizing the utilization of SIMD (Single Instruction,\nMultiple Data) instructions, Flash significantly enhances cache hit rates and\narithmetic operations. Extensive experiments conducted on eight real-world\ndatasets, ranging from ten million to one billion vectors, exhibit that Flash\nachieves a speedup of 10.4$\\times$ to 22.9$\\times$ in index construction\nefficiency, while maintaining or improving search performance."
                },
                "authors": [
                    {
                        "name": "Mengzhao Wang"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Xiangyu Ke"
                    },
                    {
                        "name": "Yunjun Gao"
                    },
                    {
                        "name": "Yifan Zhu"
                    },
                    {
                        "name": "Wenchao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenchao Zhou"
                },
                "author": "Wenchao Zhou",
                "arxiv_comment": "SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v3",
                "updated": "2025-02-25T03:42:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    25,
                    3,
                    42,
                    15,
                    1,
                    56,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths. Our code and searched configurations are\navailable at https://github.com/cmd2001/KVTuner."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "arxiv_comment": "36 pages. Code: https://github.com/cmd2001/KVTuner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17606v1",
                "updated": "2025-02-24T19:48:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T19:48:48Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    48,
                    48,
                    0,
                    55,
                    0
                ],
                "title": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELMo-Tune-V2: LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based\n  Key-Value Stores"
                },
                "summary": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log-Structured Merge-tree-based Key-Value Store (LSM-KVS) is a foundational\nstorage engine serving diverse modern workloads, systems, and applications. To\nsuit varying use cases, LSM-KVS allows a vast configuration space that controls\ncore parameters like compaction, flush, and cache sizes, each consuming a\nshared pool of CPU, Memory, and Storage resources. Navigating the LSM-KVS\nconfiguration space necessitates knowledge of the impact of each configuration\non the expected workload and underlying hardware. Beyond expensive and\ntime-intensive human-expert-based tuning, existing LSM-KVS tuning solutions\nfocus on tuning with specific workload expectations while limited to a narrow\nsubset of parameters.\n  This paper introduces ELMo-Tune-V2, a framework that integrates Large\nLanguage Models (LLMs) at its foundation to demonstrate the potential of\napplying modern LLMs in data system optimization problems. ELMo-Tune-V2\nleverages the contextual reasoning, cross-domain, and generative capabilities\nof LLMs to perform 1) self-navigated characterization and modeling of LSM-KVS\nworkloads, 2) automatic tuning across a broad parameter space using\ncross-domain knowledge, and 3) real-time dynamic configuration adjustments for\nLSM-KVS. ELMo-Tune-V2 integrates three innovations: LLM-based workload\nsynthesis for adaptive benchmark generation, feedback-driven iterative\nfine-tuning for configuration refinement, and real-time tuning to handle\nevolving workloads. Through detailed evaluation using RocksDB under several\nreal-world applications across diverse scenarios, ELMo-Tune-V2 achieves\nperformance improvements up to ~14X our YCSB benchmarks compared against\ndefault RocksDB configurations, and our end-to-end tests with upper-level\napplications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and\n26%, respectively."
                },
                "authors": [
                    {
                        "name": "Viraj Thakkar"
                    },
                    {
                        "name": "Qi Lin"
                    },
                    {
                        "name": "Kenanya Keandra Adriel Prasetyo"
                    },
                    {
                        "name": "Raden Haryosatyo Wisjnunandono"
                    },
                    {
                        "name": "Achmad Imam Kistijantoro"
                    },
                    {
                        "name": "Reza Fuad Rachmadi"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17599v2",
                "updated": "2025-03-13T04:04:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    4,
                    4,
                    8,
                    3,
                    72,
                    0
                ],
                "published": "2025-02-24T19:34:52Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    19,
                    34,
                    52,
                    0,
                    55,
                    0
                ],
                "title": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDA: Dynamic KV Cache Allocation for Efficient Multimodal Long-Context\n  Inference"
                },
                "summary": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Multimodal Large Language Models (MLLMs) that incorporate long\ntext-image and text-video modalities, demand substantial resources as their\nmultimodal Key-Value (KV) caches grow with increasing input lengths,\nchallenging inference efficiency. Existing methods for KV cache compression, in\nboth text-only and multimodal LLMs, have neglected attention density variations\nacross layers, thus often adopting uniform or progressive reduction strategies\nfor layer-wise cache allocation. In this work, we propose MEDA, a dynamic\nlayer-wise KV cache allocation method for efficient multimodal long-context\ninference. As its core, MEDA utilizes cross-modal attention entropy to\ndetermine the KV cache size at each MLLMs layer. Given the dynamically\nallocated KV cache size at each layer, MEDA also employs a KV pair selection\nscheme to identify which KV pairs to select and a KV pair merging strategy that\nmerges the selected and non-selected ones to preserve information from the\nentire context. MEDA achieves up to 72% KV cache memory reduction and 2.82\ntimes faster decoding speed, while maintaining or enhancing performance on\nvarious multimodal tasks in long-context settings, including multi-images and\nlong-video scenarios. Our code is released at\nhttps://github.com/AIoT-MLSys-Lab/MEDA."
                },
                "authors": [
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Hui Shen"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Che Liu"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Mi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mi Zhang"
                },
                "author": "Mi Zhang",
                "arxiv_comment": "NAACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v1",
                "updated": "2025-02-24T18:53:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Speculative Decoding with Efficient Drafting and\n  Verification"
                },
                "summary": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding has become a promising technique to mitigate the high\ninference latency of autoregressive decoding in Large Language Models (LLMs).\nDespite its promise, the effective application of speculative decoding in LLMs\nstill confronts three key challenges: the increasing memory demands of the\ndraft model, the distribution shift between the short-training corpora and\nlong-context inference, and inefficiencies in attention implementation. In this\nwork, we enhance the performance of speculative decoding in long-context\nsettings by addressing these challenges. First, we propose a memory-efficient\ndraft model with a constant-sized Key-Value (KV) cache. Second, we introduce\nnovel position indices for short-training data, enabling seamless adaptation\nfrom short-context training to long-context inference. Finally, we present an\ninnovative attention aggregation method that combines fast implementations for\nprefix computation with standard attention for tree mask handling, effectively\nresolving the latency and memory inefficiencies of tree decoding. Our approach\nachieves strong results on various long-context tasks, including\nrepository-level code completion, long-context summarization, and o1-like long\nreasoning tasks, demonstrating significant improvements in latency reduction.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01418v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01418v2",
                "updated": "2025-02-24T18:51:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    51,
                    48,
                    0,
                    55,
                    0
                ],
                "published": "2024-05-02T16:08:03Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    16,
                    8,
                    3,
                    3,
                    123,
                    0
                ],
                "title": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTX: A Write-Optimized Latch-free Graph Data System with Transactional\n  Support -- Extended Version"
                },
                "summary": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces GTX, a standalone main-memory write-optimized graph\ndata system that specializes in structural and graph property updates while\nenabling concurrent reads and graph analytics through ACID transactions. Recent\ngraph systems target concurrent read and write support while guaranteeing\ntransaction semantics. However, their performance suffers from updates with\nreal-world temporal locality over the same vertices and edges due to\nvertex-centric lock contentions. GTX has an adaptive delta-chain locking\nprotocol on top of a carefully designed latch-free graph storage. It eliminates\nvertex-level locking contention, and adapts to real-life workloads while\nmaintaining sequential access to the graph's adjacency lists storage. GTX's\ntransactions further support cache-friendly block level concurrency control,\nand cooperative group commit and garbage collection. This combination of\nfeatures ensures high update throughput and provides low-latency graph\nanalytics. Based on experimental evaluation, in addition to not sacrificing the\nperformance of read-heavy analytical workloads, and having competitive\nperformance similar to state-of-the-art systems, GTX has high read-write\ntransaction throughput. For write-heavy transactional workloads, GTX achieves\nup to 11x better transaction throughput than the best-performing\nstate-of-the-art system."
                },
                "authors": [
                    {
                        "name": "Libin Zhou"
                    },
                    {
                        "name": "Lu Xing"
                    },
                    {
                        "name": "Yeasir Rayhan"
                    },
                    {
                        "name": "Walid. G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid. G. Aref"
                },
                "author": "Walid. G. Aref",
                "arxiv_comment": "technical report for our main paper GTX: A Write-Optimized Latch-free\n  Graph Data System with Transactional Support",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01418v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01418v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17398v1",
                "updated": "2025-02-24T18:26:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "published": "2025-02-24T18:26:22Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    26,
                    22,
                    0,
                    55,
                    0
                ],
                "title": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating IOMMU-Based Shared Virtual Addressing for RISC-V Embedded\n  Heterogeneous SoCs"
                },
                "summary": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedded heterogeneous systems-on-chip (SoCs) rely on domain-specific\nhardware accelerators to improve performance and energy efficiency. In\nparticular, programmable multi-core accelerators feature a cluster of\nprocessing elements and tightly coupled scratchpad memories to balance\nperformance, energy efficiency, and flexibility. In embedded systems running a\ngeneral-purpose OS, accelerators access data via dedicated, physically\naddressed memory regions. This negatively impacts memory utilization and\nperformance by requiring a copy from the virtual host address to the physical\naccelerator address space. Input-Output Memory Management Units (IOMMUs)\novercome this limitation by allowing devices and hosts to use a shared virtual\npaged address space. However, resolving IO virtual addresses can be\nparticularly costly on high-latency memory systems as it requires up to three\nsequential memory accesses on IOTLB miss. In this work, we present a\nquantitative evaluation of shared virtual addressing in RISC-V heterogeneous\nembedded systems. We integrate an IOMMU in an open-source heterogeneous RISC-V\nSoC consisting of a 64-bit host with a 32-bit accelerator cluster. We evaluated\nthe system performance by emulating the design on FPGA and implementing compute\nkernels from the RajaPERF benchmark suite using heterogeneous OpenMP\nprogramming. We measure the transfers and computation time on the host and\naccelerators for systems with different DRAM access latencies. We first show\nthat IO virtual address translation can account for 4.2% up to 17.6% of the\naccelerator's runtime for gemm (General Matrix Multiplication) at low and high\nmemory bandwidth. Then, we show that in systems containing a last-level cache,\nthis IO address translation cost falls to 0.4% and 0.7% under the same\nconditions, making shared virtual addressing and zero-copy offloading suitable\nfor such RISC-V heterogeneous SoCs."
                },
                "authors": [
                    {
                        "name": "Cyril Koenig"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.09600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09600v1",
                "updated": "2025-03-12T17:59:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    42,
                    2,
                    71,
                    0
                ],
                "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09598v1",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:59:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation"
                },
                "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research."
                },
                "authors": [
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09594v1",
                "updated": "2025-03-12T17:58:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    58,
                    6,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:58:06Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    58,
                    6,
                    2,
                    71,
                    0
                ],
                "title": "SimLingo: Vision-Only Closed-Loop Autonomous Driving with\n  Language-Action Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLingo: Vision-Only Closed-Loop Autonomous Driving with\n  Language-Action Alignment"
                },
                "summary": "Integrating large language models (LLMs) into autonomous driving has\nattracted significant attention with the hope of improving generalization and\nexplainability. However, existing methods often focus on either driving or\nvision-language understanding but achieving both high driving performance and\nextensive language understanding remains challenging. In addition, the dominant\napproach to tackle vision-language understanding is using visual question\nanswering. However, for autonomous driving, this is only useful if it is\naligned with the action space. Otherwise, the model's answers could be\ninconsistent with its behavior. Therefore, we propose a model that can handle\nthree different tasks: (1) closed-loop driving, (2) vision-language\nunderstanding, and (3) language-action alignment. Our model SimLingo is based\non a vision language model (VLM) and works using only camera, excluding\nexpensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on\nthe widely used CARLA simulator on the Bench2Drive benchmark and is the winning\nentry at the CARLA challenge 2024. Additionally, we achieve strong results in a\nwide variety of language-related tasks while maintaining high driving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) into autonomous driving has\nattracted significant attention with the hope of improving generalization and\nexplainability. However, existing methods often focus on either driving or\nvision-language understanding but achieving both high driving performance and\nextensive language understanding remains challenging. In addition, the dominant\napproach to tackle vision-language understanding is using visual question\nanswering. However, for autonomous driving, this is only useful if it is\naligned with the action space. Otherwise, the model's answers could be\ninconsistent with its behavior. Therefore, we propose a model that can handle\nthree different tasks: (1) closed-loop driving, (2) vision-language\nunderstanding, and (3) language-action alignment. Our model SimLingo is based\non a vision language model (VLM) and works using only camera, excluding\nexpensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on\nthe widely used CARLA simulator on the Bench2Drive benchmark and is the winning\nentry at the CARLA challenge 2024. Additionally, we achieve strong results in a\nwide variety of language-related tasks while maintaining high driving\nperformance."
                },
                "authors": [
                    {
                        "name": "Katrin Renz"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Elahe Arani"
                    },
                    {
                        "name": "Oleg Sinavski"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Sinavski"
                },
                "author": "Oleg Sinavski",
                "arxiv_comment": "CVPR 2025. 1st Place @ CARLA Challenge 2024. Challenge tech report\n  (preliminary version of SimLingo): arXiv:2406.10165",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09590v1",
                "updated": "2025-03-12T17:57:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    32,
                    2,
                    71,
                    0
                ],
                "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering"
                },
                "summary": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Torresani"
                },
                "author": "Lorenzo Torresani",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02634v3",
                "updated": "2025-03-12T17:57:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    16,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-05T19:44:36Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    19,
                    44,
                    36,
                    6,
                    5,
                    0
                ],
                "title": "Optimal Inference of Asynchronous Boolean Network Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Inference of Asynchronous Boolean Network Models"
                },
                "summary": "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. In addition, we present a new method for pseudo-time assignment for\nsingle-cell RNA sequencing data that is derived from the modeling procedure.\nOur approach greatly simplifies the construction of Boolean network models for\ntime-series datasets, where asynchronicity often occurs. We demonstrate our\nmethodology by integrating real data from transcriptomics experiments. These\nresults significantly expand the applicability of the Boolean network model to\nexperimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associations between phenotype and genomic and epigenomic markers are often\nderived by correlation. Systems Biology aims to make more robust connections\nand uncover broader insights by modeling the cellular mechanisms that produce a\nphenotype. The question of choosing the modeling methodology is of central\nimportance. A model that does not capture biological reality closely enough\nwill not explain the system's behavior. At the same time, highly detailed\nmodels suffer from computational limitations and are likely to overfit the\ndata. Boolean networks strike a balance between complexity and descriptiveness\nand thus have received increasing interest. We previously described an\nalgorithm for fitting Boolean networks to high-throughout experimental data\nthat finds the optimal network with respect to the information in a given\ndataset. In this work, we describe a simple extension that enables the modeling\nof asynchronous dynamics, i.e. different reaction times for different network\nnodes. In addition, we present a new method for pseudo-time assignment for\nsingle-cell RNA sequencing data that is derived from the modeling procedure.\nOur approach greatly simplifies the construction of Boolean network models for\ntime-series datasets, where asynchronicity often occurs. We demonstrate our\nmethodology by integrating real data from transcriptomics experiments. These\nresults significantly expand the applicability of the Boolean network model to\nexperimental data."
                },
                "authors": [
                    {
                        "name": "Guy Karlebach"
                    }
                ],
                "author_detail": {
                    "name": "Guy Karlebach"
                },
                "author": "Guy Karlebach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09579v1",
                "updated": "2025-03-12T17:50:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    50,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:50:42Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    50,
                    42,
                    2,
                    71,
                    0
                ],
                "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs"
                },
                "summary": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data."
                },
                "authors": [
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "16 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09573v1",
                "updated": "2025-03-12T17:43:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:43:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    43,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Diffusion: Interpolating Between Autoregressive and Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer unique benefits over autoregressive models\ndue to their potential for parallelized generation and controllability, yet\nthey lag in likelihood modeling and are limited to fixed-length generation. In\nthis work, we introduce a class of block diffusion language models that\ninterpolate between discrete denoising diffusion and autoregressive models.\nBlock diffusion overcomes key limitations of both approaches by supporting\nflexible-length generation and improving inference efficiency with KV caching\nand parallel token sampling. We propose a recipe for building effective block\ndiffusion models that includes an efficient training algorithm, estimators of\ngradient variance, and data-driven noise schedules to minimize the variance.\nBlock diffusion sets a new state-of-the-art performance among diffusion models\non language modeling benchmarks and enables generation of arbitrary-length\nsequences. We provide the code, along with the model weights and blog post on\nthe project page: https://m-arriola.com/bd3lms/"
                },
                "authors": [
                    {
                        "name": "Marianne Arriola"
                    },
                    {
                        "name": "Aaron Gokaslan"
                    },
                    {
                        "name": "Justin T Chiu"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Zhixuan Qi"
                    },
                    {
                        "name": "Jiaqi Han"
                    },
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Volodymyr Kuleshov"
                    }
                ],
                "author_detail": {
                    "name": "Volodymyr Kuleshov"
                },
                "author": "Volodymyr Kuleshov",
                "arxiv_comment": "ICLR 2025 Oral. We provide the code at\n  https://github.com/kuleshov-group/bd3lms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09572v1",
                "updated": "2025-03-12T17:40:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    40,
                    52,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:40:52Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    40,
                    52,
                    2,
                    71,
                    0
                ],
                "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks"
                },
                "summary": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark."
                },
                "authors": [
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09568v1",
                "updated": "2025-03-12T17:35:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    35,
                    26,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:35:26Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    35,
                    26,
                    2,
                    71,
                    0
                ],
                "title": "Biological Multi-Layer and Single Cell Network-Based Multiomics Models -\n  a Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biological Multi-Layer and Single Cell Network-Based Multiomics Models -\n  a Review"
                },
                "summary": "Recent advances in single cell sequencing and multi-omics techniques have\nsignificantly improved our understanding of biological phenomena and our\ncapacity to model them. Despite combined capture of data modalities showing\nsimilar progress, notably single cell transcriptomics and proteomics,\nsimultaneous multi-omics level probing still remains challenging. As an\nalternative to combined capture of biological data, in this review, we explore\ncurrent and upcoming methods for post-hoc network inference and integration\nwith an emphasis on single cell transcriptomics and proteomics. By examining\nvarious approaches, from probabilistic models to graph-based algorithms, we\noutline the challenges and potential strategies for effectively combining\nbiological data types while simultaneously highlighting the importance of model\nvalidation. With this review, we aim to inform readers of the breadth of tools\ncurrently available for the purpose-specific generation of heterogeneous\nmulti-layer networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in single cell sequencing and multi-omics techniques have\nsignificantly improved our understanding of biological phenomena and our\ncapacity to model them. Despite combined capture of data modalities showing\nsimilar progress, notably single cell transcriptomics and proteomics,\nsimultaneous multi-omics level probing still remains challenging. As an\nalternative to combined capture of biological data, in this review, we explore\ncurrent and upcoming methods for post-hoc network inference and integration\nwith an emphasis on single cell transcriptomics and proteomics. By examining\nvarious approaches, from probabilistic models to graph-based algorithms, we\noutline the challenges and potential strategies for effectively combining\nbiological data types while simultaneously highlighting the importance of model\nvalidation. With this review, we aim to inform readers of the breadth of tools\ncurrently available for the purpose-specific generation of heterogeneous\nmulti-layer networks."
                },
                "authors": [
                    {
                        "name": "Marcello Barylli"
                    },
                    {
                        "name": "Joyaditya Saha"
                    },
                    {
                        "name": "Tineke E. Buffart"
                    },
                    {
                        "name": "Jan Koster"
                    },
                    {
                        "name": "Kristiaan J. Lenos"
                    },
                    {
                        "name": "Louis Vermeulen"
                    },
                    {
                        "name": "Vivek M. Sheraton"
                    }
                ],
                "author_detail": {
                    "name": "Vivek M. Sheraton"
                },
                "author": "Vivek M. Sheraton",
                "arxiv_comment": "10 pages, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.MN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09566v1",
                "updated": "2025-03-12T17:33:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    33,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:33:22Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    33,
                    22,
                    2,
                    71,
                    0
                ],
                "title": "TPDiff: Temporal Pyramid Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPDiff: Temporal Pyramid Video Diffusion Model"
                },
                "summary": "The development of video diffusion models unveils a significant challenge:\nthe substantial computational demands. To mitigate this challenge, we note that\nthe reverse process of diffusion exhibits an inherent entropy-reducing nature.\nGiven the inter-frame redundancy in video modality, maintaining full frame\nrates in high-entropy stages is unnecessary. Based on this insight, we propose\nTPDiff, a unified framework to enhance training and inference efficiency. By\ndividing diffusion into several stages, our framework progressively increases\nframe rate along the diffusion process with only the last stage operating on\nfull frame rate, thereby optimizing computational efficiency. To train the\nmulti-stage diffusion model, we introduce a dedicated training framework:\nstage-wise diffusion. By solving the partitioned probability flow ordinary\ndifferential equations (ODE) of diffusion under aligned data and noise, our\ntraining strategy is applicable to various diffusion forms and further enhances\ntraining efficiency. Comprehensive experimental evaluations validate the\ngenerality of our method, demonstrating 50% reduction in training cost and 1.5x\nimprovement in inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of video diffusion models unveils a significant challenge:\nthe substantial computational demands. To mitigate this challenge, we note that\nthe reverse process of diffusion exhibits an inherent entropy-reducing nature.\nGiven the inter-frame redundancy in video modality, maintaining full frame\nrates in high-entropy stages is unnecessary. Based on this insight, we propose\nTPDiff, a unified framework to enhance training and inference efficiency. By\ndividing diffusion into several stages, our framework progressively increases\nframe rate along the diffusion process with only the last stage operating on\nfull frame rate, thereby optimizing computational efficiency. To train the\nmulti-stage diffusion model, we introduce a dedicated training framework:\nstage-wise diffusion. By solving the partitioned probability flow ordinary\ndifferential equations (ODE) of diffusion under aligned data and noise, our\ntraining strategy is applicable to various diffusion forms and further enhances\ntraining efficiency. Comprehensive experimental evaluations validate the\ngenerality of our method, demonstrating 50% reduction in training cost and 1.5x\nimprovement in inference efficiency."
                },
                "authors": [
                    {
                        "name": "Lingmin Ran"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Project page: https://showlab.github.io/TPDiff/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.01241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.01241v2",
                "updated": "2025-03-12T17:21:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    21,
                    48,
                    2,
                    71,
                    0
                ],
                "published": "2023-07-03T17:25:45Z",
                "published_parsed": [
                    2023,
                    7,
                    3,
                    17,
                    25,
                    45,
                    0,
                    184,
                    0
                ],
                "title": "Data-driven decoding of quantum error correcting codes using graph\n  neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven decoding of quantum error correcting codes using graph\n  neural networks"
                },
                "summary": "To leverage the full potential of quantum error-correcting stabilizer codes\nit is crucial to have an efficient and accurate decoder. Accurate, maximum\nlikelihood, decoders are computationally very expensive whereas decoders based\non more efficient algorithms give sub-optimal performance. In addition, the\naccuracy will depend on the quality of models and estimates of error rates for\nidling qubits, gates, measurements, and resets, and will typically assume\nsymmetric error channels. In this work, instead, we explore a model-free,\ndata-driven, approach to decoding, using a graph neural network (GNN). The\ndecoding problem is formulated as a graph classification task in which a set of\nstabilizer measurements is mapped to an annotated detector graph for which the\nneural network predicts the most likely logical error class. We show that the\nGNN-based decoder can outperform a matching decoder for circuit level noise on\nthe surface code given only simulated experimental data, even if the matching\ndecoder is given full information of the underlying error model. Although\ntraining is computationally demanding, inference is fast and scales\napproximately linearly with the space-time volume of the code. We also find\nthat we can use large, but more limited, datasets of real experimental data\n[Google Quantum AI, Nature {\\bf 614}, 676 (2023)] for the repetition code,\ngiving decoding accuracies that are on par with minimum weight perfect\nmatching. The results show that a purely data-driven approach to decoding may\nbe a viable future option for practical quantum error correction, which is\ncompetitive in terms of speed, accuracy, and versatility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To leverage the full potential of quantum error-correcting stabilizer codes\nit is crucial to have an efficient and accurate decoder. Accurate, maximum\nlikelihood, decoders are computationally very expensive whereas decoders based\non more efficient algorithms give sub-optimal performance. In addition, the\naccuracy will depend on the quality of models and estimates of error rates for\nidling qubits, gates, measurements, and resets, and will typically assume\nsymmetric error channels. In this work, instead, we explore a model-free,\ndata-driven, approach to decoding, using a graph neural network (GNN). The\ndecoding problem is formulated as a graph classification task in which a set of\nstabilizer measurements is mapped to an annotated detector graph for which the\nneural network predicts the most likely logical error class. We show that the\nGNN-based decoder can outperform a matching decoder for circuit level noise on\nthe surface code given only simulated experimental data, even if the matching\ndecoder is given full information of the underlying error model. Although\ntraining is computationally demanding, inference is fast and scales\napproximately linearly with the space-time volume of the code. We also find\nthat we can use large, but more limited, datasets of real experimental data\n[Google Quantum AI, Nature {\\bf 614}, 676 (2023)] for the repetition code,\ngiving decoding accuracies that are on par with minimum weight perfect\nmatching. The results show that a purely data-driven approach to decoding may\nbe a viable future option for practical quantum error correction, which is\ncompetitive in terms of speed, accuracy, and versatility."
                },
                "authors": [
                    {
                        "name": "Moritz Lange"
                    },
                    {
                        "name": "Pontus Havström"
                    },
                    {
                        "name": "Basudha Srivastava"
                    },
                    {
                        "name": "Isak Bengtsson"
                    },
                    {
                        "name": "Valdemar Bergentall"
                    },
                    {
                        "name": "Karl Hammar"
                    },
                    {
                        "name": "Olivia Heuts"
                    },
                    {
                        "name": "Evert van Nieuwenburg"
                    },
                    {
                        "name": "Mats Granath"
                    }
                ],
                "author_detail": {
                    "name": "Mats Granath"
                },
                "author": "Mats Granath",
                "arxiv_comment": "15 pages, 12 figures. V2: Improved results for circuit-level noise up\n  to code-distance d=9. Benchmarked against belief-matching",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.01241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.01241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09533v2",
                "updated": "2025-03-13T05:54:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    54,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T16:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    49,
                    56,
                    2,
                    71,
                    0
                ],
                "title": "Large Language Models for Multi-Facility Location Mechanism Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multi-Facility Location Mechanism Design"
                },
                "summary": "Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents."
                },
                "authors": [
                    {
                        "name": "Nguyen Thach"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Houyu Zhou"
                    },
                    {
                        "name": "Hau Chan"
                    }
                ],
                "author_detail": {
                    "name": "Hau Chan"
                },
                "author": "Hau Chan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09527v1",
                "updated": "2025-03-12T16:42:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    42,
                    26,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:42:26Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    42,
                    26,
                    2,
                    71,
                    0
                ],
                "title": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in\n  3D Action Role-Playing Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in\n  3D Action Role-Playing Games"
                },
                "summary": "Recent advances in Vision-Language-Action models (VLAs) have expanded the\ncapabilities of embodied intelligence. However, significant challenges remain\nin real-time decision-making in complex 3D environments, which demand\nsecond-level responses, high-resolution perception, and tactical reasoning\nunder dynamic conditions. To advance the field, we introduce CombatVLA, an\nefficient VLA model optimized for combat tasks in 3D action role-playing\ngames(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action\npairs collected by an action tracker, where the data is formatted as\naction-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates\ninto an action execution framework, allowing efficient inference through our\ntruncated AoT strategy. Experimental results demonstrate that CombatVLA not\nonly outperforms all existing models on the combat understanding benchmark but\nalso achieves a 50-fold acceleration in game combat. Moreover, it has a higher\ntask success rate than human players. We will open-source all resources,\nincluding the action tracker, dataset, benchmark, model weights, training code,\nand the implementation of the framework at https://combatvla.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Vision-Language-Action models (VLAs) have expanded the\ncapabilities of embodied intelligence. However, significant challenges remain\nin real-time decision-making in complex 3D environments, which demand\nsecond-level responses, high-resolution perception, and tactical reasoning\nunder dynamic conditions. To advance the field, we introduce CombatVLA, an\nefficient VLA model optimized for combat tasks in 3D action role-playing\ngames(ARPGs). Specifically, our CombatVLA is a 3B model trained on video-action\npairs collected by an action tracker, where the data is formatted as\naction-of-thought (AoT) sequences. Thereafter, CombatVLA seamlessly integrates\ninto an action execution framework, allowing efficient inference through our\ntruncated AoT strategy. Experimental results demonstrate that CombatVLA not\nonly outperforms all existing models on the combat understanding benchmark but\nalso achieves a 50-fold acceleration in game combat. Moreover, it has a higher\ntask success rate than human players. We will open-source all resources,\nincluding the action tracker, dataset, benchmark, model weights, training code,\nand the implementation of the framework at https://combatvla.github.io/."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Pi Bu"
                    },
                    {
                        "name": "Yingyao Wang"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Jie Guo"
                    },
                    {
                        "name": "Yingxiu Zhao"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jun Song"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01228v2",
                "updated": "2025-03-12T16:29:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    29,
                    28,
                    2,
                    71,
                    0
                ],
                "published": "2024-11-02T12:32:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    12,
                    32,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "The Interaction Layer: An Exploration for Co-Designing User-LLM\n  Interactions in Parental Wellbeing Support Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Interaction Layer: An Exploration for Co-Designing User-LLM\n  Interactions in Parental Wellbeing Support Systems"
                },
                "summary": "Parenting brings emotional and physical challenges, from balancing work,\nchildcare, and finances to coping with exhaustion and limited personal time.\nYet, one in three parents never seek support. AI systems potentially offer\nstigma-free, accessible, and affordable solutions. Yet, user adoption often\nfails due to issues with explainability and reliability. To see if these issues\ncould be solved using a co-design approach, we developed and tested NurtureBot,\na wellbeing support assistant for new parents. 32 parents co-designed the\nsystem through Asynchronous Remote Communities method, identifying the key\nchallenge as achieving a \"successful chat.\" As part of co-design, parents\nrole-played as NurtureBot, rewriting its dialogues to improve user\nunderstanding, control, and outcomes. The refined prototype, featuring an\nInteraction Layer, was evaluated by 32 initial and 46 new parents, showing\nimproved user experience and usability, with final CUQ score of 91.3/100,\ndemonstrating successful interaction patterns. Our process revealed useful\ninteraction design lessons for effective AI parenting support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parenting brings emotional and physical challenges, from balancing work,\nchildcare, and finances to coping with exhaustion and limited personal time.\nYet, one in three parents never seek support. AI systems potentially offer\nstigma-free, accessible, and affordable solutions. Yet, user adoption often\nfails due to issues with explainability and reliability. To see if these issues\ncould be solved using a co-design approach, we developed and tested NurtureBot,\na wellbeing support assistant for new parents. 32 parents co-designed the\nsystem through Asynchronous Remote Communities method, identifying the key\nchallenge as achieving a \"successful chat.\" As part of co-design, parents\nrole-played as NurtureBot, rewriting its dialogues to improve user\nunderstanding, control, and outcomes. The refined prototype, featuring an\nInteraction Layer, was evaluated by 32 initial and 46 new parents, showing\nimproved user experience and usability, with final CUQ score of 91.3/100,\ndemonstrating successful interaction patterns. Our process revealed useful\ninteraction design lessons for effective AI parenting support."
                },
                "authors": [
                    {
                        "name": "Sruthi Viswanathan"
                    },
                    {
                        "name": "Seray Ibrahim"
                    },
                    {
                        "name": "Ravi Shankar"
                    },
                    {
                        "name": "Reuben Binns"
                    },
                    {
                        "name": "Max Van Kleek"
                    },
                    {
                        "name": "Petr Slovak"
                    }
                ],
                "author_detail": {
                    "name": "Petr Slovak"
                },
                "author": "Petr Slovak",
                "arxiv_doi": "10.1145/3706598.3714088",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714088",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18798v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18798v3",
                "updated": "2025-03-12T16:27:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    27,
                    59,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-26T04:10:18Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    4,
                    10,
                    18,
                    2,
                    57,
                    0
                ],
                "title": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions"
                },
                "summary": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt."
                },
                "authors": [
                    {
                        "name": "Gyeongje Cho"
                    },
                    {
                        "name": "Yeonkyoung So"
                    },
                    {
                        "name": "Jaejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaejin Lee"
                },
                "author": "Jaejin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18798v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18798v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09516v1",
                "updated": "2025-03-12T16:26:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    26,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:26:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    26,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning"
                },
                "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Hansi Zeng"
                    },
                    {
                        "name": "Zhenrui Yue"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09514v1",
                "updated": "2025-03-12T16:25:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    25,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:25:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    25,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "CM-Diff: A Single Generative Network for Bidirectional Cross-Modality\n  Translation Diffusion Model Between Infrared and Visible Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CM-Diff: A Single Generative Network for Bidirectional Cross-Modality\n  Translation Diffusion Model Between Infrared and Visible Images"
                },
                "summary": "The image translation method represents a crucial approach for mitigating\ninformation deficiencies in the infrared and visible modalities, while also\nfacilitating the enhancement of modality-specific datasets. However, existing\nmethods for infrared and visible image translation either achieve\nunidirectional modality translation or rely on cycle consistency for\nbidirectional modality translation, which may result in suboptimal performance.\nIn this work, we present the cross-modality translation diffusion model\n(CM-Diff) for simultaneously modeling data distributions in both the infrared\nand visible modalities. We address this challenge by combining translation\ndirection labels for guidance during training with cross-modality feature\ncontrol. Specifically, we view the establishment of the mapping relationship\nbetween the two modalities as the process of learning data distributions and\nunderstanding modality differences, achieved through a novel Bidirectional\nDiffusion Training (BDT) strategy. Additionally, we propose a Statistical\nConstraint Inference (SCI) strategy to ensure the generated image closely\nadheres to the data distribution of the target modality. Experimental results\ndemonstrate the superiority of our CM-Diff over state-of-the-art methods,\nhighlighting its potential for generating dual-modality datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image translation method represents a crucial approach for mitigating\ninformation deficiencies in the infrared and visible modalities, while also\nfacilitating the enhancement of modality-specific datasets. However, existing\nmethods for infrared and visible image translation either achieve\nunidirectional modality translation or rely on cycle consistency for\nbidirectional modality translation, which may result in suboptimal performance.\nIn this work, we present the cross-modality translation diffusion model\n(CM-Diff) for simultaneously modeling data distributions in both the infrared\nand visible modalities. We address this challenge by combining translation\ndirection labels for guidance during training with cross-modality feature\ncontrol. Specifically, we view the establishment of the mapping relationship\nbetween the two modalities as the process of learning data distributions and\nunderstanding modality differences, achieved through a novel Bidirectional\nDiffusion Training (BDT) strategy. Additionally, we propose a Statistical\nConstraint Inference (SCI) strategy to ensure the generated image closely\nadheres to the data distribution of the target modality. Experimental results\ndemonstrate the superiority of our CM-Diff over state-of-the-art methods,\nhighlighting its potential for generating dual-modality datasets."
                },
                "authors": [
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Chenqiang Gao"
                    },
                    {
                        "name": "Shurui Liu"
                    },
                    {
                        "name": "Junjie Guo"
                    },
                    {
                        "name": "Fang Chen"
                    },
                    {
                        "name": "Fangcen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fangcen Liu"
                },
                "author": "Fangcen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09513v1",
                "updated": "2025-03-12T16:23:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    23,
                    14,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:23:14Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    23,
                    14,
                    2,
                    71,
                    0
                ],
                "title": "RESTRAIN: Reinforcement Learning-Based Secure Framework for\n  Trigger-Action IoT Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESTRAIN: Reinforcement Learning-Based Secure Framework for\n  Trigger-Action IoT Environment"
                },
                "summary": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet of Things (IoT) platforms with trigger-action capability allow event\nconditions to trigger actions in IoT devices autonomously by creating a chain\nof interactions. Adversaries exploit this chain of interactions to maliciously\ninject fake event conditions into IoT hubs, triggering unauthorized actions on\ntarget IoT devices to implement remote injection attacks. Existing defense\nmechanisms focus mainly on the verification of event transactions using\nphysical event fingerprints to enforce the security policies to block unsafe\nevent transactions. These approaches are designed to provide offline defense\nagainst injection attacks. The state-of-the-art online defense mechanisms offer\nreal-time defense, but extensive reliability on the inference of attack impacts\non the IoT network limits the generalization capability of these approaches. In\nthis paper, we propose a platform-independent multi-agent online defense\nsystem, namely RESTRAIN, to counter remote injection attacks at runtime.\nRESTRAIN allows the defense agent to profile attack actions at runtime and\nleverages reinforcement learning to optimize a defense policy that complies\nwith the security requirements of the IoT network. The experimental results\nshow that the defense agent effectively takes real-time defense actions against\ncomplex and dynamic remote injection attacks and maximizes the security gain\nwith minimal computational overhead."
                },
                "authors": [
                    {
                        "name": "Md Morshed Alam"
                    },
                    {
                        "name": "Lokesh Chandra Das"
                    },
                    {
                        "name": "Sandip Roy"
                    },
                    {
                        "name": "Sachin Shetty"
                    },
                    {
                        "name": "Weichao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weichao Wang"
                },
                "author": "Weichao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09504v1",
                "updated": "2025-03-12T16:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    13,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:13:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    13,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-Stage Feature-Level Clustering-Based Mixture of Experts Framework"
                },
                "summary": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).\nHowever, its complex architecture and advantages over dense models in image\nclassification remain unclear. In previous studies, MoE performance has often\nbeen affected by noise and outliers in the input space. Some approaches\nincorporate input clustering for training MoE models, but most clustering\nalgorithms lack access to labeled data, limiting their effectiveness. This\npaper introduces the Double-stage Feature-level Clustering and\nPseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists\nof input feature extraction, feature-level clustering, and a computationally\nefficient pseudo-labeling strategy. This approach reduces the impact of noise\nand outliers while leveraging a small subset of labeled data to label a large\nportion of unlabeled inputs. We propose a conditional end-to-end joint training\nmethod that improves expert specialization by training the MoE model on\nwell-labeled, clustered inputs. Unlike traditional MoE and dense models, the\nDFCP-MoE framework effectively captures input space diversity, leading to\ncompetitive inference results. We validate our approach on three benchmark\ndatasets for multi-class classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) model has succeeded in deep learning (DL).\nHowever, its complex architecture and advantages over dense models in image\nclassification remain unclear. In previous studies, MoE performance has often\nbeen affected by noise and outliers in the input space. Some approaches\nincorporate input clustering for training MoE models, but most clustering\nalgorithms lack access to labeled data, limiting their effectiveness. This\npaper introduces the Double-stage Feature-level Clustering and\nPseudo-labeling-based Mixture of Experts (DFCP-MoE) framework, which consists\nof input feature extraction, feature-level clustering, and a computationally\nefficient pseudo-labeling strategy. This approach reduces the impact of noise\nand outliers while leveraging a small subset of labeled data to label a large\nportion of unlabeled inputs. We propose a conditional end-to-end joint training\nmethod that improves expert specialization by training the MoE model on\nwell-labeled, clustered inputs. Unlike traditional MoE and dense models, the\nDFCP-MoE framework effectively captures input space diversity, leading to\ncompetitive inference results. We validate our approach on three benchmark\ndatasets for multi-class classification tasks."
                },
                "authors": [
                    {
                        "name": "Bakary Badjie"
                    },
                    {
                        "name": "José Cecílio"
                    },
                    {
                        "name": "António Casimiro"
                    }
                ],
                "author_detail": {
                    "name": "António Casimiro"
                },
                "author": "António Casimiro",
                "arxiv_comment": "14 Pages, 1 Figure, and 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09501v1",
                "updated": "2025-03-12T16:05:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    5,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:05:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    5,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning"
                },
                "summary": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Hanjing Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Mark Schmidt"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09499v1",
                "updated": "2025-03-12T16:03:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    3,
                    3,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:03:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    3,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindGYM: Enhancing Vision-Language Models via Synthetic Self-Challenging\n  Questions"
                },
                "summary": "Large vision-language models (VLMs) face challenges in achieving robust,\ntransferable reasoning abilities due to reliance on labor-intensive manual\ninstruction datasets or computationally expensive self-supervised methods. To\naddress these issues, we introduce MindGYM, a framework that enhances VLMs\nthrough synthetic self-challenging questions, consisting of three stages: (1)\nSeed Single-Hop Question Synthesis, generating cognitive questions across\ntextual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based\nqueries) spanning eight semantic areas like ethical analysis; (2) Challenging\nMulti-Hop Question Synthesis, combining seed questions via diverse principles\nlike bridging, visual-textual alignment, to create multi-step problems\ndemanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a\nstructured pipeline that progressively trains the model from scaffolded\nreasoning to standalone inference. By leveraging the model's self-synthesis\ncapability, MindGYM achieves high data efficiency (e.g., +16% gains on\nMathVision-Mini with only 400 samples), computational efficiency (reducing both\ntraining and inference costs), and robust generalization across tasks.\nExtensive evaluations on seven benchmarks demonstrate superior performance over\nstrong baselines, with notable improvements (+15.77% win rates) in reasoning\ndepth and breadth validated via GPT-based scoring. MindGYM underscores the\nviability of self-challenging for refining VLM capabilities while minimizing\nhuman intervention and resource demands. Code and data are released to advance\nmultimodal reasoning research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) face challenges in achieving robust,\ntransferable reasoning abilities due to reliance on labor-intensive manual\ninstruction datasets or computationally expensive self-supervised methods. To\naddress these issues, we introduce MindGYM, a framework that enhances VLMs\nthrough synthetic self-challenging questions, consisting of three stages: (1)\nSeed Single-Hop Question Synthesis, generating cognitive questions across\ntextual (e.g., logical deduction) and multimodal contexts (e.g., diagram-based\nqueries) spanning eight semantic areas like ethical analysis; (2) Challenging\nMulti-Hop Question Synthesis, combining seed questions via diverse principles\nlike bridging, visual-textual alignment, to create multi-step problems\ndemanding deeper reasoning; and (3) Thinking-Induced Curriculum Fine-Tuning, a\nstructured pipeline that progressively trains the model from scaffolded\nreasoning to standalone inference. By leveraging the model's self-synthesis\ncapability, MindGYM achieves high data efficiency (e.g., +16% gains on\nMathVision-Mini with only 400 samples), computational efficiency (reducing both\ntraining and inference costs), and robust generalization across tasks.\nExtensive evaluations on seven benchmarks demonstrate superior performance over\nstrong baselines, with notable improvements (+15.77% win rates) in reasoning\ndepth and breadth validated via GPT-based scoring. MindGYM underscores the\nviability of self-challenging for refining VLM capabilities while minimizing\nhuman intervention and resource demands. Code and data are released to advance\nmultimodal reasoning research."
                },
                "authors": [
                    {
                        "name": "Zhe Xu"
                    },
                    {
                        "name": "Daoyuan Chen"
                    },
                    {
                        "name": "Zhenqing Ling"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Ying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shen"
                },
                "author": "Ying Shen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.08971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.08971v2",
                "updated": "2025-03-12T15:51:20Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    51,
                    20,
                    2,
                    71,
                    0
                ],
                "published": "2024-05-14T21:31:11Z",
                "published_parsed": [
                    2024,
                    5,
                    14,
                    21,
                    31,
                    11,
                    1,
                    135,
                    0
                ],
                "title": "Computation-Aware Kalman Filtering and Smoothing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computation-Aware Kalman Filtering and Smoothing"
                },
                "summary": "Kalman filtering and smoothing are the foundational mechanisms for efficient\ninference in Gauss-Markov models. However, their time and memory complexities\nscale prohibitively with the size of the state space. This is particularly\nproblematic in spatiotemporal regression problems, where the state dimension\nscales with the number of spatial observations. Existing approximate frameworks\nleverage low-rank approximations of the covariance matrix. But since they do\nnot model the error introduced by the computational approximation, their\npredictive uncertainty estimates can be overly optimistic. In this work, we\npropose a probabilistic numerical method for inference in high-dimensional\nGauss-Markov models which mitigates these scaling issues. Our matrix-free\niterative algorithm leverages GPU acceleration and crucially enables a tunable\ntrade-off between computational cost and predictive uncertainty. Finally, we\ndemonstrate the scalability of our method on a large-scale climate dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kalman filtering and smoothing are the foundational mechanisms for efficient\ninference in Gauss-Markov models. However, their time and memory complexities\nscale prohibitively with the size of the state space. This is particularly\nproblematic in spatiotemporal regression problems, where the state dimension\nscales with the number of spatial observations. Existing approximate frameworks\nleverage low-rank approximations of the covariance matrix. But since they do\nnot model the error introduced by the computational approximation, their\npredictive uncertainty estimates can be overly optimistic. In this work, we\npropose a probabilistic numerical method for inference in high-dimensional\nGauss-Markov models which mitigates these scaling issues. Our matrix-free\niterative algorithm leverages GPU acceleration and crucially enables a tunable\ntrade-off between computational cost and predictive uncertainty. Finally, we\ndemonstrate the scalability of our method on a large-scale climate dataset."
                },
                "authors": [
                    {
                        "name": "Marvin Pförtner"
                    },
                    {
                        "name": "Jonathan Wenger"
                    },
                    {
                        "name": "Jon Cockayne"
                    },
                    {
                        "name": "Philipp Hennig"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Hennig"
                },
                "author": "Philipp Hennig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.08971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.08971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20129v2",
                "updated": "2025-03-12T15:47:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    47,
                    8,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-27T14:24:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    24,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking"
                },
                "summary": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Dongming Jin"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09487v1",
                "updated": "2025-03-12T15:46:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    46,
                    12,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T15:46:12Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    46,
                    12,
                    2,
                    71,
                    0
                ],
                "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness"
                },
                "summary": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels."
                },
                "authors": [
                    {
                        "name": "Beier Zhu"
                    },
                    {
                        "name": "Jiequan Cui"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09483v1",
                "updated": "2025-03-12T15:38:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    38,
                    11,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T15:38:11Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    38,
                    11,
                    2,
                    71,
                    0
                ],
                "title": "Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spatially Adaptive $\\ell_1$-Norms Weights for Convolutional\n  Synthesis Regularization"
                },
                "summary": "We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an unrolled algorithm approach for learning spatially adaptive\nparameter maps in the framework of convolutional synthesis-based $\\ell_1$\nregularization. More precisely, we consider a family of pre-trained\nconvolutional filters and estimate deeply parametrized spatially varying\nparameters applied to the sparse feature maps by means of unrolling a FISTA\nalgorithm to solve the underlying sparse estimation problem. The proposed\napproach is evaluated for image reconstruction of low-field MRI and compared to\nspatially adaptive and non-adaptive analysis-type procedures relying on Total\nVariation regularization and to a well-established model-based deep learning\napproach. We show that the proposed approach produces visually and\nquantitatively comparable results with the latter approaches and at the same\ntime remains highly interpretable. In particular, the inferred parameter maps\nquantify\n  the local contribution of each filter in the reconstruction, which provides\nvaluable insight into the algorithm mechanism and could potentially be used to\ndiscard unsuited filters."
                },
                "authors": [
                    {
                        "name": "Andreas Kofler"
                    },
                    {
                        "name": "Luca Calatroni"
                    },
                    {
                        "name": "Christoph Kolbitsch"
                    },
                    {
                        "name": "Kostas Papafitsoros"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Papafitsoros"
                },
                "author": "Kostas Papafitsoros",
                "arxiv_comment": "To be submitted to the EUSIPCO 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09481v1",
                "updated": "2025-03-12T15:36:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    36,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T15:36:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    36,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "BAMBI: Developing Baby Language Models for Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAMBI: Developing Baby Language Models for Italian"
                },
                "summary": "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance."
                },
                "authors": [
                    {
                        "name": "Alice Suozzi"
                    },
                    {
                        "name": "Luca Capone"
                    },
                    {
                        "name": "Gianluca E. Lebani"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05673v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05673v3",
                "updated": "2025-03-12T15:27:49Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    27,
                    49,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-07T14:38:56Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    14,
                    38,
                    56,
                    5,
                    342,
                    0
                ],
                "title": "A generalized Bayesian approach for high-dimensional robust regression\n  with serially correlated errors and predictors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized Bayesian approach for high-dimensional robust regression\n  with serially correlated errors and predictors"
                },
                "summary": "This paper introduces a loss-based generalized Bayesian methodology for\nhigh-dimensional robust regression with serially correlated errors and\npredictors. The proposed framework employs a novel scaled pseudo-Huber (SPH)\nloss function, which smooths the well-known Huber loss, effectively balancing\nquadratic ($\\ell_2$) and absolute linear ($\\ell_1$) loss behaviors. This\nflexibility enables the framework to accommodate both thin-tailed and\nheavy-tailed data efficiently. The generalized Bayesian approach constructs a\nworking likelihood based on the SPH loss, facilitating efficient and stable\nestimation while providing rigorous uncertainty quantification for all model\nparameters. Notably, this approach allows formal statistical inference without\nrequiring ad hoc tuning parameter selection while adaptively addressing a wide\nrange of tail behavior in the errors. By specifying appropriate prior\ndistributions for the regression coefficients--such as ridge priors for small\nor moderate-dimensional settings and spike-and-slab priors for high-dimensional\nsettings--the framework ensures principled inference. We establish rigorous\ntheoretical guarantees for accurate parameter estimation and correct predictor\nselection under sparsity assumptions for a wide range of data generating\nsetups. Extensive simulation studies demonstrate the superior performance of\nour approach compared to traditional Bayesian regression methods based on\n$\\ell_2$ and $\\ell_1$-loss functions. The results highlight its flexibility and\nrobustness, particularly in challenging high-dimensional settings characterized\nby data contamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a loss-based generalized Bayesian methodology for\nhigh-dimensional robust regression with serially correlated errors and\npredictors. The proposed framework employs a novel scaled pseudo-Huber (SPH)\nloss function, which smooths the well-known Huber loss, effectively balancing\nquadratic ($\\ell_2$) and absolute linear ($\\ell_1$) loss behaviors. This\nflexibility enables the framework to accommodate both thin-tailed and\nheavy-tailed data efficiently. The generalized Bayesian approach constructs a\nworking likelihood based on the SPH loss, facilitating efficient and stable\nestimation while providing rigorous uncertainty quantification for all model\nparameters. Notably, this approach allows formal statistical inference without\nrequiring ad hoc tuning parameter selection while adaptively addressing a wide\nrange of tail behavior in the errors. By specifying appropriate prior\ndistributions for the regression coefficients--such as ridge priors for small\nor moderate-dimensional settings and spike-and-slab priors for high-dimensional\nsettings--the framework ensures principled inference. We establish rigorous\ntheoretical guarantees for accurate parameter estimation and correct predictor\nselection under sparsity assumptions for a wide range of data generating\nsetups. Extensive simulation studies demonstrate the superior performance of\nour approach compared to traditional Bayesian regression methods based on\n$\\ell_2$ and $\\ell_1$-loss functions. The results highlight its flexibility and\nrobustness, particularly in challenging high-dimensional settings characterized\nby data contamination."
                },
                "authors": [
                    {
                        "name": "Saptarshi Chakraborty"
                    },
                    {
                        "name": "Kshitij Khare"
                    },
                    {
                        "name": "George Michailidis"
                    }
                ],
                "author_detail": {
                    "name": "George Michailidis"
                },
                "author": "George Michailidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05673v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05673v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13268v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13268v2",
                "updated": "2025-03-12T15:25:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    25,
                    11,
                    2,
                    71,
                    0
                ],
                "published": "2024-07-18T08:21:31Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    8,
                    21,
                    31,
                    3,
                    200,
                    0
                ],
                "title": "Mixture of Experts based Multi-task Supervise Learning from Crowds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts based Multi-task Supervise Learning from Crowds"
                },
                "summary": "Existing truth inference methods in crowdsourcing aim to map redundant labels\nand items to the ground truth. They treat the ground truth as hidden variables\nand use statistical or deep learning-based worker behavior models to infer the\nground truth. However, worker behavior models that rely on ground truth hidden\nvariables overlook workers' behavior at the item feature level, leading to\nimprecise characterizations and negatively impacting the quality of truth\ninference. This paper proposes a new paradigm of multi-task supervised learning\nfrom crowds, which eliminates the need for modeling of items's ground truth in\nworker behavior models. Within this paradigm, we propose a worker behavior\nmodel at the item feature level called Mixture of Experts based Multi-task\nSupervised Learning from Crowds (MMLC). Two truth inference strategies are\nproposed within MMLC. The first strategy, named MMLC-owf, utilizes clustering\nmethods in the worker spectral space to identify the projection vector of the\noracle worker. Subsequently, the labels generated based on this vector are\nconsidered as the inferred truth. The second strategy, called MMLC-df, employs\nthe MMLC model to fill the crowdsourced data, which can enhance the\neffectiveness of existing truth inference methods. Experimental results\ndemonstrate that MMLC-owf outperforms state-of-the-art methods and MMLC-df\nenhances the quality of existing truth inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing truth inference methods in crowdsourcing aim to map redundant labels\nand items to the ground truth. They treat the ground truth as hidden variables\nand use statistical or deep learning-based worker behavior models to infer the\nground truth. However, worker behavior models that rely on ground truth hidden\nvariables overlook workers' behavior at the item feature level, leading to\nimprecise characterizations and negatively impacting the quality of truth\ninference. This paper proposes a new paradigm of multi-task supervised learning\nfrom crowds, which eliminates the need for modeling of items's ground truth in\nworker behavior models. Within this paradigm, we propose a worker behavior\nmodel at the item feature level called Mixture of Experts based Multi-task\nSupervised Learning from Crowds (MMLC). Two truth inference strategies are\nproposed within MMLC. The first strategy, named MMLC-owf, utilizes clustering\nmethods in the worker spectral space to identify the projection vector of the\noracle worker. Subsequently, the labels generated based on this vector are\nconsidered as the inferred truth. The second strategy, called MMLC-df, employs\nthe MMLC model to fill the crowdsourced data, which can enhance the\neffectiveness of existing truth inference methods. Experimental results\ndemonstrate that MMLC-owf outperforms state-of-the-art methods and MMLC-df\nenhances the quality of existing truth inference methods."
                },
                "authors": [
                    {
                        "name": "Tao Han"
                    },
                    {
                        "name": "Huaixuan Shi"
                    },
                    {
                        "name": "Xinyi Ding"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Huamao Gu"
                    },
                    {
                        "name": "Yili Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yili Fang"
                },
                "author": "Yili Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13268v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13268v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14938v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14938v3",
                "updated": "2025-03-12T15:22:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    22,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2024-11-22T13:50:02Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    13,
                    50,
                    2,
                    4,
                    327,
                    0
                ],
                "title": "Bayesian inference of strangeon matter using the measurements of PSR\n  J0437-4715 and GW190814",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference of strangeon matter using the measurements of PSR\n  J0437-4715 and GW190814"
                },
                "summary": "The observations of compact star inspirals from LIGO/Virgo combined with mass\nand radius measurements from NICER provide a valuable tool to study the highly\nuncertain equation of state (EOS) of dense matter at the densities\ncharacteristic of compact stars. In this work, we constrain the solid states of\nstrange-cluster matter, called strangeon matter, as the putative basic units of\nthe ground state of bulk strong matter using a Bayesian statistical method,\nincorporating the mass and radius measurements of PSR J0030+0451, PSR\nJ0740+6620, and the recent data for the $1.4\\ M_{\\odot}$ pulsar PSR J0437-4715.\nWe also include constraints from gravitational wave events GW170817 and\nGW190814. Under the prior assumption of a finite number of quarks in a\nstrangeon, $N_{\\rm q}$, our analysis reveals that current mass-radius\nmeasurements favor a larger $N_{\\rm q}$. Specifically, the results support the\nscenario where a strangeon forms a stable bound state with $N_{\\rm q}=18$,\nsymmetric in color, flavor, and spin spaces, compared to the minimum $N_{\\rm\nq}$ prior. The comparative analyses of the posterior EOS parameter spaces\nderived from three-parameter model and two-parameter model demonstrate a\nconsistent prediction under identical observational constraints. In particular,\nour results indicate that the most probable values of the maximum mass are\nfound to be $3.58^{+0.16}_{-0.12}\\ M_{\\odot}$ ($3.65^{+0.18}_{-0.16}\\\nM_{\\odot}$) at $90\\%$ confidence level for three-parameter (two-parameter) EOS\nconsidering the constraints of GW190814. The corresponding radii for $1.4\\\nM_{\\odot}$ and $2.1\\ M_{\\odot}$ stars are $12.04^{+0.27}_{-0.31}~\\rm km$\n($12.16^{+0.26}_{-0.31}~\\rm km$) and $13.43^{+0.31}_{-0.32}~\\rm km$\n($13.60^{+0.29}_{-0.34}~\\rm km$), respectively. This result may impact\ninterestingly on the research of multiquark states, which could improve our\nunderstanding of the nonperturbative strong force.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The observations of compact star inspirals from LIGO/Virgo combined with mass\nand radius measurements from NICER provide a valuable tool to study the highly\nuncertain equation of state (EOS) of dense matter at the densities\ncharacteristic of compact stars. In this work, we constrain the solid states of\nstrange-cluster matter, called strangeon matter, as the putative basic units of\nthe ground state of bulk strong matter using a Bayesian statistical method,\nincorporating the mass and radius measurements of PSR J0030+0451, PSR\nJ0740+6620, and the recent data for the $1.4\\ M_{\\odot}$ pulsar PSR J0437-4715.\nWe also include constraints from gravitational wave events GW170817 and\nGW190814. Under the prior assumption of a finite number of quarks in a\nstrangeon, $N_{\\rm q}$, our analysis reveals that current mass-radius\nmeasurements favor a larger $N_{\\rm q}$. Specifically, the results support the\nscenario where a strangeon forms a stable bound state with $N_{\\rm q}=18$,\nsymmetric in color, flavor, and spin spaces, compared to the minimum $N_{\\rm\nq}$ prior. The comparative analyses of the posterior EOS parameter spaces\nderived from three-parameter model and two-parameter model demonstrate a\nconsistent prediction under identical observational constraints. In particular,\nour results indicate that the most probable values of the maximum mass are\nfound to be $3.58^{+0.16}_{-0.12}\\ M_{\\odot}$ ($3.65^{+0.18}_{-0.16}\\\nM_{\\odot}$) at $90\\%$ confidence level for three-parameter (two-parameter) EOS\nconsidering the constraints of GW190814. The corresponding radii for $1.4\\\nM_{\\odot}$ and $2.1\\ M_{\\odot}$ stars are $12.04^{+0.27}_{-0.31}~\\rm km$\n($12.16^{+0.26}_{-0.31}~\\rm km$) and $13.43^{+0.31}_{-0.32}~\\rm km$\n($13.60^{+0.29}_{-0.34}~\\rm km$), respectively. This result may impact\ninterestingly on the research of multiquark states, which could improve our\nunderstanding of the nonperturbative strong force."
                },
                "authors": [
                    {
                        "name": "Wen-Li Yuan"
                    },
                    {
                        "name": "Chun Huang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Enping Zhou"
                    },
                    {
                        "name": "Renxin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renxin Xu"
                },
                "author": "Renxin Xu",
                "arxiv_doi": "10.1103/PhysRevD.111.063033",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.063033",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.14938v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14938v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 4 figures, Phys. Rev. D (2025) published",
                "arxiv_journal_ref": "Phys. Rev. D 111, 063033 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05891v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05891v3",
                "updated": "2025-03-12T15:02:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    2,
                    43,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-07T19:24:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    19,
                    24,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MastermindEval: A Simple But Scalable Reasoning Benchmark"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing."
                },
                "authors": [
                    {
                        "name": "Jonas Golde"
                    },
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05891v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05891v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v1",
                "updated": "2025-03-12T14:57:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04789v2",
                "updated": "2025-03-12T14:42:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    42,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-28T06:46:53Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    6,
                    46,
                    53,
                    4,
                    59,
                    0
                ],
                "title": "Ext2Gen: Alignment through Unified Extraction and Generation for Robust\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ext2Gen: Alignment through Unified Extraction and Generation for Robust\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances LLMs by integrating external\nknowledge, but generation remains fragile due to the uncertain placement of\nrelevant chunks and retrieval-induced information overload, leading to\nhallucinations. We propose Ext2Gen, a novel extract-then-generate model that\nenhances RAG robustness by first extracting query-relevant sentences before\ngenerating answers. To optimize this model, we employ preference alignment\nthrough pairwise feedback learning, enabling the model to generate robust\nanswers regardless of variations in retrieval results. Extensive experiments\ndemonstrate that Ext2Gen effectively identifies query-relevant sentences with\nhigh precision and recall, leading to highly reliable answers. Furthermore,\ndeploying our model in a RAG environment reveals that it not only boosts the\nperformance of the base LLM but also synergizes with advanced retrieval\nstrategies like query expansion. The model is available at\nhttps://huggingface.co/DISLab/Ext2Gen-8B-R2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances LLMs by integrating external\nknowledge, but generation remains fragile due to the uncertain placement of\nrelevant chunks and retrieval-induced information overload, leading to\nhallucinations. We propose Ext2Gen, a novel extract-then-generate model that\nenhances RAG robustness by first extracting query-relevant sentences before\ngenerating answers. To optimize this model, we employ preference alignment\nthrough pairwise feedback learning, enabling the model to generate robust\nanswers regardless of variations in retrieval results. Extensive experiments\ndemonstrate that Ext2Gen effectively identifies query-relevant sentences with\nhigh precision and recall, leading to highly reliable answers. Furthermore,\ndeploying our model in a RAG environment reveals that it not only boosts the\nperformance of the base LLM but also synergizes with advanced retrieval\nstrategies like query expansion. The model is available at\nhttps://huggingface.co/DISLab/Ext2Gen-8B-R2."
                },
                "authors": [
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Jeonghwan Choi"
                    },
                    {
                        "name": "Minseok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Minseok Kim"
                },
                "author": "Minseok Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04615v2",
                "updated": "2025-03-12T14:40:24Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    40,
                    24,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-08T16:57:18Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    16,
                    57,
                    18,
                    2,
                    8,
                    0
                ],
                "title": "Doubly Robust and Efficient Calibration of Prediction Sets for Censored\n  Time-to-Event Outcomes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Robust and Efficient Calibration of Prediction Sets for Censored\n  Time-to-Event Outcomes"
                },
                "summary": "Our objective is to construct well-calibrated prediction sets for a\ntime-to-event outcome subject to right-censoring with guaranteed coverage. Our\napproach is inspired by modern conformal inference literature in that, unlike\nclassical frameworks, we obviate the need for a well-specified parametric or\nsemiparametric survival model to accomplish our goal. In contrast to existing\nconformal prediction methods for survival data, which restrict censoring to be\nof Type I, whereby potential censoring times are assumed to be fully observed\non all units in both training and validation samples, we consider the more\ncommon right-censoring setting in which either only the censoring time or only\nthe event time of primary interest is directly observed, whichever comes first.\nUnder a standard conditional independence assumption between the potential\nsurvival and censoring times given covariates, we propose and analyze two\nmethods to construct valid and efficient lower predictive bounds for the\nsurvival time of a future observation. The proposed methods build upon modern\nsemiparametric efficiency theory for censored data, in that the first approach\nincorporates inverse-probability-of-censoring weighting to account for\ncensoring, while the second approach is based on augmenting this method with an\nadditional correction term. For both methods, we formally establish asymptotic\ncoverage guarantees and demonstrate, both theoretically and through empirical\nexperiments, that the augmented approach substantially improves efficiency over\nthe inverse-probability-of-censoring weighting method. Specifically, its\ncoverage error bound is of second-order mixed bias type, that is doubly robust,\nand therefore guaranteed to be asymptotically negligible relative to the\ncoverage error of the non-augmented method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our objective is to construct well-calibrated prediction sets for a\ntime-to-event outcome subject to right-censoring with guaranteed coverage. Our\napproach is inspired by modern conformal inference literature in that, unlike\nclassical frameworks, we obviate the need for a well-specified parametric or\nsemiparametric survival model to accomplish our goal. In contrast to existing\nconformal prediction methods for survival data, which restrict censoring to be\nof Type I, whereby potential censoring times are assumed to be fully observed\non all units in both training and validation samples, we consider the more\ncommon right-censoring setting in which either only the censoring time or only\nthe event time of primary interest is directly observed, whichever comes first.\nUnder a standard conditional independence assumption between the potential\nsurvival and censoring times given covariates, we propose and analyze two\nmethods to construct valid and efficient lower predictive bounds for the\nsurvival time of a future observation. The proposed methods build upon modern\nsemiparametric efficiency theory for censored data, in that the first approach\nincorporates inverse-probability-of-censoring weighting to account for\ncensoring, while the second approach is based on augmenting this method with an\nadditional correction term. For both methods, we formally establish asymptotic\ncoverage guarantees and demonstrate, both theoretically and through empirical\nexperiments, that the augmented approach substantially improves efficiency over\nthe inverse-probability-of-censoring weighting method. Specifically, its\ncoverage error bound is of second-order mixed bias type, that is doubly robust,\nand therefore guaranteed to be asymptotically negligible relative to the\ncoverage error of the non-augmented method."
                },
                "authors": [
                    {
                        "name": "Rebecca Farina"
                    },
                    {
                        "name": "Eric J. Tchetgen Tchetgen"
                    },
                    {
                        "name": "Arun Kumar Kuchibhotla"
                    }
                ],
                "author_detail": {
                    "name": "Arun Kumar Kuchibhotla"
                },
                "author": "Arun Kumar Kuchibhotla",
                "arxiv_comment": "39 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.09673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.09673v2",
                "updated": "2025-03-12T14:36:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    36,
                    48,
                    2,
                    71,
                    0
                ],
                "published": "2023-06-16T08:12:11Z",
                "published_parsed": [
                    2023,
                    6,
                    16,
                    8,
                    12,
                    11,
                    4,
                    167,
                    0
                ],
                "title": "Testing Regular Black Holes with X-ray data of GX 339-4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Regular Black Holes with X-ray data of GX 339-4"
                },
                "summary": "Regular black holes are singularity-free black hole spacetimes proposed to\nsolve the problem of the presence of spacetime singularities that plagues the\nblack holes of general relativity and most theories of gravity. In this work,\nwe consider the regular black holes recently proposed by Mazza, Franzin &\nLiberati and we extend previous studies to get a more stringent observational\nconstraint on the regularization parameter $l$. We study simultaneous\nobservations of NuSTAR and Swift of the Galactic black hole in GX 339-4 during\nits outburst in 2015. The quality of the NuSTAR data is exceptionally good and\nthe spectrum of the source presents both a strong thermal component and\nprominent relativistically blurred reflection features. This permits us to\nmeasure the regularization parameter $l$ from the simultaneous analysis of the\nthermal spectrum and the reflection features. From our analysis, we find the\nconstraint $l/M < 0.44$ (90% CL), which is stronger than previous constraints\ninferred with X-ray and gravitational wave data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regular black holes are singularity-free black hole spacetimes proposed to\nsolve the problem of the presence of spacetime singularities that plagues the\nblack holes of general relativity and most theories of gravity. In this work,\nwe consider the regular black holes recently proposed by Mazza, Franzin &\nLiberati and we extend previous studies to get a more stringent observational\nconstraint on the regularization parameter $l$. We study simultaneous\nobservations of NuSTAR and Swift of the Galactic black hole in GX 339-4 during\nits outburst in 2015. The quality of the NuSTAR data is exceptionally good and\nthe spectrum of the source presents both a strong thermal component and\nprominent relativistically blurred reflection features. This permits us to\nmeasure the regularization parameter $l$ from the simultaneous analysis of the\nthermal spectrum and the reflection features. From our analysis, we find the\nconstraint $l/M < 0.44$ (90% CL), which is stronger than previous constraints\ninferred with X-ray and gravitational wave data."
                },
                "authors": [
                    {
                        "name": "Shafqat Riaz"
                    },
                    {
                        "name": "Michail Kyriazis"
                    },
                    {
                        "name": "Askar B. Abdikamalov"
                    },
                    {
                        "name": "Cosimo Bambi"
                    },
                    {
                        "name": "Swarnim Shashank"
                    }
                ],
                "author_detail": {
                    "name": "Swarnim Shashank"
                },
                "author": "Swarnim Shashank",
                "arxiv_doi": "10.1088/1475-7516/2025/03/022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1475-7516/2025/03/022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2306.09673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.09673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "1+20 pages, 4 figures. v2: refereed version",
                "arxiv_journal_ref": "JCAP 2503:022 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09436v1",
                "updated": "2025-03-12T14:31:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    31,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:31:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    31,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "PromptMap: An Alternative Interaction Style for AI-Based Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMap: An Alternative Interaction Style for AI-Based Image\n  Generation"
                },
                "summary": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output."
                },
                "authors": [
                    {
                        "name": "Krzysztof Adamkiewicz"
                    },
                    {
                        "name": "Paweł W. Woźniak"
                    },
                    {
                        "name": "Julia Dominiak"
                    },
                    {
                        "name": "Andrzej Romanowski"
                    },
                    {
                        "name": "Jakob Karolus"
                    },
                    {
                        "name": "Stanislav Frolov"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Frolov"
                },
                "author": "Stanislav Frolov",
                "arxiv_doi": "10.1145/3708359.3712150",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712150",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To be published in the proceedings of 30th International Conference\n  on Intelligent User Interfaces (IUI '25), March 24-27, 2025, Cagliari, Italy",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09433v1",
                "updated": "2025-03-12T14:30:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection"
                },
                "summary": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark."
                },
                "authors": [
                    {
                        "name": "Richard A. Dubniczky"
                    },
                    {
                        "name": "Krisztofer Zoltán Horvát"
                    },
                    {
                        "name": "Tamás Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Tihanyi"
                },
                "author": "Norbert Tihanyi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19482v2",
                "updated": "2025-03-12T14:25:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    25,
                    10,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T11:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Measuring memorization in language models via probabilistic extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring memorization in language models via probabilistic extraction"
                },
                "summary": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "A. Feder Cooper"
                    }
                ],
                "author_detail": {
                    "name": "A. Feder Cooper"
                },
                "author": "A. Feder Cooper",
                "arxiv_comment": "NAACL 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09419v1",
                "updated": "2025-03-12T14:16:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    16,
                    30,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:16:30Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    16,
                    30,
                    2,
                    71,
                    0
                ],
                "title": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alias-Free Latent Diffusion Models:Improving Fractional Shift\n  Equivariance of Diffusion Latent Space"
                },
                "summary": "Latent Diffusion Models (LDMs) are known to have an unstable generation\nprocess, where even small perturbations or shifts in the input noise can lead\nto significantly different outputs. This hinders their applicability in\napplications requiring consistent results. In this work, we redesign LDMs to\nenhance consistency by making them shift-equivariant. While introducing\nanti-aliasing operations can partially improve shift-equivariance, significant\naliasing and inconsistency persist due to the unique challenges in LDMs,\nincluding 1) aliasing amplification during VAE training and multiple U-Net\ninferences, and 2) self-attention modules that inherently lack\nshift-equivariance. To address these issues, we redesign the attention modules\nto be shift-equivariant and propose an equivariance loss that effectively\nsuppresses the frequency bandwidth of the features in the continuous domain.\nThe resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is\nalso robust to irregular warping. Extensive experiments demonstrate that AF-LDM\nproduces significantly more consistent results than vanilla LDM across various\napplications, including video editing and image-to-image translation. Code is\navailable at: https://github.com/SingleZombie/AFLDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Diffusion Models (LDMs) are known to have an unstable generation\nprocess, where even small perturbations or shifts in the input noise can lead\nto significantly different outputs. This hinders their applicability in\napplications requiring consistent results. In this work, we redesign LDMs to\nenhance consistency by making them shift-equivariant. While introducing\nanti-aliasing operations can partially improve shift-equivariance, significant\naliasing and inconsistency persist due to the unique challenges in LDMs,\nincluding 1) aliasing amplification during VAE training and multiple U-Net\ninferences, and 2) self-attention modules that inherently lack\nshift-equivariance. To address these issues, we redesign the attention modules\nto be shift-equivariant and propose an equivariance loss that effectively\nsuppresses the frequency bandwidth of the features in the continuous domain.\nThe resulting alias-free LDM (AF-LDM) achieves strong shift-equivariance and is\nalso robust to irregular warping. Extensive experiments demonstrate that AF-LDM\nproduces significantly more consistent results than vanilla LDM across various\napplications, including video editing and image-to-image translation. Code is\navailable at: https://github.com/SingleZombie/AFLDM"
                },
                "authors": [
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xingang Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Pan"
                },
                "author": "Xingang Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09414v1",
                "updated": "2025-03-12T14:10:35Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    10,
                    35,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:10:35Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    10,
                    35,
                    2,
                    71,
                    0
                ],
                "title": "Mitigating Membership Inference Vulnerability in Personalized Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Membership Inference Vulnerability in Personalized Federated\n  Learning"
                },
                "summary": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training without the need to share clients' personal data, thereby\npreserving privacy. However, the non-IID nature of the clients' data introduces\nmajor challenges for FL, highlighting the importance of personalized federated\nlearning (PFL) methods. In PFL, models are trained to cater to specific feature\ndistributions present in the population data. A notable method for PFL is the\nIterative Federated Clustering Algorithm (IFCA), which mitigates the concerns\nassociated with the non-IID-ness by grouping clients with similar data\ndistributions. While it has been shown that IFCA enhances both accuracy and\nfairness, its strategy of dividing the population into smaller clusters\nincreases vulnerability to Membership Inference Attacks (MIA), particularly\namong minorities with limited training samples. In this paper, we introduce\nIFCA-MIR, an improved version of IFCA that integrates MIA risk assessment into\nthe clustering process. Allowing clients to select clusters based on both model\nperformance and MIA vulnerability, IFCA-MIR achieves an improved performance\nwith respect to accuracy, fairness, and privacy. We demonstrate that IFCA-MIR\nsignificantly reduces MIA risk while maintaining comparable model accuracy and\nfairness as the original IFCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has emerged as a promising paradigm for collaborative\nmodel training without the need to share clients' personal data, thereby\npreserving privacy. However, the non-IID nature of the clients' data introduces\nmajor challenges for FL, highlighting the importance of personalized federated\nlearning (PFL) methods. In PFL, models are trained to cater to specific feature\ndistributions present in the population data. A notable method for PFL is the\nIterative Federated Clustering Algorithm (IFCA), which mitigates the concerns\nassociated with the non-IID-ness by grouping clients with similar data\ndistributions. While it has been shown that IFCA enhances both accuracy and\nfairness, its strategy of dividing the population into smaller clusters\nincreases vulnerability to Membership Inference Attacks (MIA), particularly\namong minorities with limited training samples. In this paper, we introduce\nIFCA-MIR, an improved version of IFCA that integrates MIA risk assessment into\nthe clustering process. Allowing clients to select clusters based on both model\nperformance and MIA vulnerability, IFCA-MIR achieves an improved performance\nwith respect to accuracy, fairness, and privacy. We demonstrate that IFCA-MIR\nsignificantly reduces MIA risk while maintaining comparable model accuracy and\nfairness as the original IFCA."
                },
                "authors": [
                    {
                        "name": "Kangsoo Jung"
                    },
                    {
                        "name": "Sayan Biswas"
                    },
                    {
                        "name": "Catuscia Palamidessi"
                    }
                ],
                "author_detail": {
                    "name": "Catuscia Palamidessi"
                },
                "author": "Catuscia Palamidessi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09410v1",
                "updated": "2025-03-12T14:01:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    1,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:01:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    1,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "Monte Carlo Diffusion for Generalizable Learning-Based RANSAC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Diffusion for Generalizable Learning-Based RANSAC"
                },
                "summary": "Random Sample Consensus (RANSAC) is a fundamental approach for robustly\nestimating parametric models from noisy data. Existing learning-based RANSAC\nmethods utilize deep learning to enhance the robustness of RANSAC against\noutliers. However, these approaches are trained and tested on the data\ngenerated by the same algorithms, leading to limited generalization to\nout-of-distribution data during inference. Therefore, in this paper, we\nintroduce a novel diffusion-based paradigm that progressively injects noise\ninto ground-truth data, simulating the noisy conditions for training\nlearning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo\nsampling into the diffusion paradigm, approximating diverse data distributions\nby introducing different types of randomness at multiple stages. We evaluate\nour approach in the context of feature matching through comprehensive\nexperiments on the ScanNet and MegaDepth datasets. The experimental results\ndemonstrate that our Monte Carlo diffusion mechanism significantly improves the\ngeneralization ability of learning-based RANSAC. We also develop extensive\nablation studies that highlight the effectiveness of key components in our\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Sample Consensus (RANSAC) is a fundamental approach for robustly\nestimating parametric models from noisy data. Existing learning-based RANSAC\nmethods utilize deep learning to enhance the robustness of RANSAC against\noutliers. However, these approaches are trained and tested on the data\ngenerated by the same algorithms, leading to limited generalization to\nout-of-distribution data during inference. Therefore, in this paper, we\nintroduce a novel diffusion-based paradigm that progressively injects noise\ninto ground-truth data, simulating the noisy conditions for training\nlearning-based RANSAC. To enhance data diversity, we incorporate Monte Carlo\nsampling into the diffusion paradigm, approximating diverse data distributions\nby introducing different types of randomness at multiple stages. We evaluate\nour approach in the context of feature matching through comprehensive\nexperiments on the ScanNet and MegaDepth datasets. The experimental results\ndemonstrate that our Monte Carlo diffusion mechanism significantly improves the\ngeneralization ability of learning-based RANSAC. We also develop extensive\nablation studies that highlight the effectiveness of key components in our\nframework."
                },
                "authors": [
                    {
                        "name": "Jiale Wang"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Wei Ke"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09407v1",
                "updated": "2025-03-12T13:58:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    58,
                    43,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:58:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    58,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Got Compute, but No Data: Lessons From Post-training a Finnish LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Got Compute, but No Data: Lessons From Post-training a Finnish LLM"
                },
                "summary": "As LLMs gain more popularity as chatbots and general assistants, methods have\nbeen developed to enable LLMs to follow instructions and align with human\npreferences. These methods have found success in the field, but their\neffectiveness has not been demonstrated outside of high-resource languages. In\nthis work, we discuss our experiences in post-training an LLM for\ninstruction-following for English and Finnish. We use a multilingual LLM to\ntranslate instruction and preference datasets from English to Finnish. We\nperform instruction tuning and preference optimization in English and Finnish\nand evaluate the instruction-following capabilities of the model in both\nlanguages. Our results show that with a few hundred Finnish instruction samples\nwe can obtain competitive performance in Finnish instruction-following. We also\nfound that although preference optimization in English offers some\ncross-lingual benefits, we obtain our best results by using preference data\nfrom both languages. We release our model, datasets, and recipes under open\nlicenses at https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs gain more popularity as chatbots and general assistants, methods have\nbeen developed to enable LLMs to follow instructions and align with human\npreferences. These methods have found success in the field, but their\neffectiveness has not been demonstrated outside of high-resource languages. In\nthis work, we discuss our experiences in post-training an LLM for\ninstruction-following for English and Finnish. We use a multilingual LLM to\ntranslate instruction and preference datasets from English to Finnish. We\nperform instruction tuning and preference optimization in English and Finnish\nand evaluate the instruction-following capabilities of the model in both\nlanguages. Our results show that with a few hundred Finnish instruction samples\nwe can obtain competitive performance in Finnish instruction-following. We also\nfound that although preference optimization in English offers some\ncross-lingual benefits, we obtain our best results by using preference data\nfrom both languages. We release our model, datasets, and recipes under open\nlicenses at https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant"
                },
                "authors": [
                    {
                        "name": "Elaine Zosa"
                    },
                    {
                        "name": "Ville Komulainen"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "arxiv_comment": "7 pages",
                "arxiv_journal_ref": "Proceedings of the Joint 25th Nordic Conference on Computational\n  Linguistics and 11th Baltic Conference on Human Language Technologies\n  (NoDaLiDa/Baltic-HLT 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09403v1",
                "updated": "2025-03-12T13:53:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    53,
                    57,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:53:57Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    53,
                    57,
                    2,
                    71,
                    0
                ],
                "title": "Multi-Agent Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Image Restoration"
                },
                "summary": "Image restoration (IR) is challenging due to the complexity of real-world\ndegradations. While many specialized and all-in-one IR models have been\ndeveloped, they fail to effectively handle complex, mixed degradations. Recent\nagentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous\nworkflows to alleviate this issue, yet they suffer from suboptimal results and\ninefficiency due to their resource-intensive finetunings, and ineffective\nsearches and tool execution trials for satisfactory outputs. In this paper, we\npropose MAIR, a novel Multi-Agent approach for complex IR problems. We\nintroduce a real-world degradation prior, categorizing degradations into three\ntypes: (1) scene, (2) imaging, and (3) compression, which are observed to occur\nsequentially in real world, and reverse them in the opposite order. Built upon\nthis three-stage restoration framework, MAIR emulates a team of collaborative\nhuman specialists, including a \"scheduler\" for overall planning and multiple\n\"experts\" dedicated to specific degradations. This design minimizes search\nspace and trial efforts, improving image quality while reducing inference\ncosts. In addition, a registry mechanism is introduced to enable easy\nintegration of new tools. Experiments on both synthetic and real-world datasets\nshow that proposed MAIR achieves competitive performance and improved\nefficiency over the previous agentic IR system. Code and models will be made\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image restoration (IR) is challenging due to the complexity of real-world\ndegradations. While many specialized and all-in-one IR models have been\ndeveloped, they fail to effectively handle complex, mixed degradations. Recent\nagentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous\nworkflows to alleviate this issue, yet they suffer from suboptimal results and\ninefficiency due to their resource-intensive finetunings, and ineffective\nsearches and tool execution trials for satisfactory outputs. In this paper, we\npropose MAIR, a novel Multi-Agent approach for complex IR problems. We\nintroduce a real-world degradation prior, categorizing degradations into three\ntypes: (1) scene, (2) imaging, and (3) compression, which are observed to occur\nsequentially in real world, and reverse them in the opposite order. Built upon\nthis three-stage restoration framework, MAIR emulates a team of collaborative\nhuman specialists, including a \"scheduler\" for overall planning and multiple\n\"experts\" dedicated to specific degradations. This design minimizes search\nspace and trial efforts, improving image quality while reducing inference\ncosts. In addition, a registry mechanism is introduced to enable easy\nintegration of new tools. Experiments on both synthetic and real-world datasets\nshow that proposed MAIR achieves competitive performance and improved\nefficiency over the previous agentic IR system. Code and models will be made\navailable."
                },
                "authors": [
                    {
                        "name": "Xu Jiang"
                    },
                    {
                        "name": "Gehui Li"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09402v1",
                "updated": "2025-03-12T13:53:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    53,
                    30,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:53:30Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    53,
                    30,
                    2,
                    71,
                    0
                ],
                "title": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLog: Video-Language Models by Generative Retrieval of Narration\n  Vocabulary"
                },
                "summary": "Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's efficient similarity search. (ii) A\nhierarchical vocabulary derived from large-scale video narrations using our\nnarration pair encoding algorithm, enabling efficient indexing of specific\nevents (e.g., cutting a tomato) by identifying broader scenarios (e.g.,\nkitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary\nupdate strategy leveraging generative models to extend the vocabulary for novel\nevents encountered during inference. To validate our approach, we introduce\nVidCap-Eval, a development set requiring concise narrations with reasoning\nrelationships (e.g., before and after). Experiments on EgoSchema, COIN, and\nHiREST further demonstrate the effectiveness of VLog, highlighting its ability\nto generate concise, contextually accurate, and efficient narrations, offering\na novel perspective on video understanding. Codes are released at\nhttps://github.com/showlab/VLog.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human daily activities can be concisely narrated as sequences of routine\nevents (e.g., turning off an alarm) in video streams, forming an event\nvocabulary. Motivated by this, we introduce VLog, a novel video understanding\nframework that define video narrations as vocabulary, going beyond the typical\nsubword vocabularies in existing generative video-language models. Built on the\nlightweight language model GPT-2, VLog feature three key innovations: (i) A\ngenerative retrieval model, marrying language model's complex reasoning\ncapabilities with contrastive retrieval's efficient similarity search. (ii) A\nhierarchical vocabulary derived from large-scale video narrations using our\nnarration pair encoding algorithm, enabling efficient indexing of specific\nevents (e.g., cutting a tomato) by identifying broader scenarios (e.g.,\nkitchen) with expressive postfixes (e.g., by the left hand). (iii) A vocabulary\nupdate strategy leveraging generative models to extend the vocabulary for novel\nevents encountered during inference. To validate our approach, we introduce\nVidCap-Eval, a development set requiring concise narrations with reasoning\nrelationships (e.g., before and after). Experiments on EgoSchema, COIN, and\nHiREST further demonstrate the effectiveness of VLog, highlighting its ability\nto generate concise, contextually accurate, and efficient narrations, offering\na novel perspective on video understanding. Codes are released at\nhttps://github.com/showlab/VLog."
                },
                "authors": [
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "arxiv_comment": "Accepted by CVPR 2025. Github: https://github.com/showlab/VLog",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07923v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07923v3",
                "updated": "2025-03-12T13:48:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    48,
                    12,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-10T21:09:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    21,
                    9,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "Asking Again and Again: Exploring LLM Robustness to Repeated Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asking Again and Again: Exploring LLM Robustness to Repeated Questions"
                },
                "summary": "This study investigates whether repeating questions within prompts influences\nthe performance of large language models (LLMs). We hypothesize that\nreiterating a question within a single prompt might enhance the model's focus\non key elements of the query. We evaluate five recent LLMs -- including\nGPT-4o-mini, DeepSeek-V3, and smaller open-source models -- on three reading\ncomprehension datasets under different prompt settings, varying question\nrepetition levels (1, 3, or 5 times per prompt). Our results demonstrate that\nquestion repetition can increase models' accuracy by up to $6\\%$. However,\nacross all models, settings, and datasets, we do not find the result\nstatistically significant. These findings provide insights into prompt design\nand LLM behavior, suggesting that repetition alone does not significantly\nimpact output quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates whether repeating questions within prompts influences\nthe performance of large language models (LLMs). We hypothesize that\nreiterating a question within a single prompt might enhance the model's focus\non key elements of the query. We evaluate five recent LLMs -- including\nGPT-4o-mini, DeepSeek-V3, and smaller open-source models -- on three reading\ncomprehension datasets under different prompt settings, varying question\nrepetition levels (1, 3, or 5 times per prompt). Our results demonstrate that\nquestion repetition can increase models' accuracy by up to $6\\%$. However,\nacross all models, settings, and datasets, we do not find the result\nstatistically significant. These findings provide insights into prompt design\nand LLM behavior, suggesting that repetition alone does not significantly\nimpact output quality."
                },
                "authors": [
                    {
                        "name": "Sagi Shaier"
                    },
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07923v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07923v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06552v2",
                "updated": "2025-03-12T13:42:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    42,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-09T10:48:47Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    48,
                    47,
                    6,
                    68,
                    0
                ],
                "title": "Multimodal Programming in Computer Science with Interactive Assistance\n  Powered by Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Programming in Computer Science with Interactive Assistance\n  Powered by Large Language Model"
                },
                "summary": "LLM chatbot interfaces allow students to get instant, interactive assistance\nwith homework, but doing so carelessly may not advance educational objectives.\nIn this study, an interactive homework help system based on DeepSeek R1 is\ndeveloped and first implemented for students enrolled in a large computer\nscience beginning programming course. In addition to an assist button in a\nwell-known code editor, our assistant also has a feedback option in our\ncommand-line automatic evaluator. It wraps student work in a personalized\nprompt that advances our educational objectives without offering answers\nstraight away. We have discovered that our assistant can recognize students'\nconceptual difficulties and provide ideas, plans, and template code in\npedagogically appropriate ways. However, among other mistakes, it occasionally\nincorrectly labels the correct student code as incorrect or encourages students\nto use correct-but-lesson-inappropriate approaches, which can lead to long and\nfrustrating journeys for the students. After discussing many development and\ndeployment issues, we provide our conclusions and future actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM chatbot interfaces allow students to get instant, interactive assistance\nwith homework, but doing so carelessly may not advance educational objectives.\nIn this study, an interactive homework help system based on DeepSeek R1 is\ndeveloped and first implemented for students enrolled in a large computer\nscience beginning programming course. In addition to an assist button in a\nwell-known code editor, our assistant also has a feedback option in our\ncommand-line automatic evaluator. It wraps student work in a personalized\nprompt that advances our educational objectives without offering answers\nstraight away. We have discovered that our assistant can recognize students'\nconceptual difficulties and provide ideas, plans, and template code in\npedagogically appropriate ways. However, among other mistakes, it occasionally\nincorrectly labels the correct student code as incorrect or encourages students\nto use correct-but-lesson-inappropriate approaches, which can lead to long and\nfrustrating journeys for the students. After discussing many development and\ndeployment issues, we provide our conclusions and future actions."
                },
                "authors": [
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md. Tanzib Hosain"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Salah Uddin Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Salah Uddin Ahmed"
                },
                "author": "Salah Uddin Ahmed",
                "arxiv_comment": "Accepted in Proceedings of the 27th International Conference on.\n  Human-Computer Interaction, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09391v1",
                "updated": "2025-03-12T13:37:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    37,
                    19,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:37:19Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    37,
                    19,
                    2,
                    71,
                    0
                ],
                "title": "Context-aware Constrained Reinforcement Learning Based Energy-Efficient\n  Power Scheduling for Non-stationary XR Data Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware Constrained Reinforcement Learning Based Energy-Efficient\n  Power Scheduling for Non-stationary XR Data Traffic"
                },
                "summary": "In XR downlink transmission, energy-efficient power scheduling (EEPS) is\nessential for conserving power resource while delivering large data packets\nwithin hard-latency constraints. Traditional constrained reinforcement learning\n(CRL) algorithms show promise in EEPS but still struggle with non-convex\nstochastic constraints, non-stationary data traffic, and sparse delayed packet\ndropout feedback (rewards) in XR. To overcome these challenges, this paper\nmodels the EEPS in XR as a dynamic parameter-constrained Markov decision\nprocess (DP-CMDP) with a varying transition function linked to the\nnon-stationary data traffic and solves it by a proposed context-aware\nconstrained reinforcement learning (CACRL) algorithm, which consists of a\ncontext inference (CI) module and a CRL module. The CI module trains an encoder\nand multiple potential networks to characterize the current transition function\nand reshape the packet dropout rewards according to the context, transforming\nthe original DP-CMDP into a general CMDP with immediate dense rewards. The CRL\nmodule employs a policy network to make EEPS decisions under this CMDP and\noptimizes the policy using a constrained stochastic successive convex\napproximation (CSSCA) method, which is better suited for non-convex stochastic\nconstraints. Finally, theoretical analyses provide deep insights into the CADAC\nalgorithm, while extensive simulations demonstrate that it outperforms advanced\nbaselines in both power conservation and satisfying packet dropout constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In XR downlink transmission, energy-efficient power scheduling (EEPS) is\nessential for conserving power resource while delivering large data packets\nwithin hard-latency constraints. Traditional constrained reinforcement learning\n(CRL) algorithms show promise in EEPS but still struggle with non-convex\nstochastic constraints, non-stationary data traffic, and sparse delayed packet\ndropout feedback (rewards) in XR. To overcome these challenges, this paper\nmodels the EEPS in XR as a dynamic parameter-constrained Markov decision\nprocess (DP-CMDP) with a varying transition function linked to the\nnon-stationary data traffic and solves it by a proposed context-aware\nconstrained reinforcement learning (CACRL) algorithm, which consists of a\ncontext inference (CI) module and a CRL module. The CI module trains an encoder\nand multiple potential networks to characterize the current transition function\nand reshape the packet dropout rewards according to the context, transforming\nthe original DP-CMDP into a general CMDP with immediate dense rewards. The CRL\nmodule employs a policy network to make EEPS decisions under this CMDP and\noptimizes the policy using a constrained stochastic successive convex\napproximation (CSSCA) method, which is better suited for non-convex stochastic\nconstraints. Finally, theoretical analyses provide deep insights into the CADAC\nalgorithm, while extensive simulations demonstrate that it outperforms advanced\nbaselines in both power conservation and satisfying packet dropout constraints."
                },
                "authors": [
                    {
                        "name": "Kexuan Wang"
                    },
                    {
                        "name": "An Liu"
                    }
                ],
                "author_detail": {
                    "name": "An Liu"
                },
                "author": "An Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19649v3",
                "updated": "2025-03-12T13:31:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    31,
                    36,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-27T00:40:01Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    0,
                    40,
                    1,
                    3,
                    58,
                    0
                ],
                "title": "Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models"
                },
                "summary": "Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices."
                },
                "authors": [
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09387v1",
                "updated": "2025-03-12T13:30:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    30,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:30:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    30,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "VideoScan: Enabling Efficient Streaming Video Understanding via\n  Frame-level Semantic Carriers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoScan: Enabling Efficient Streaming Video Understanding via\n  Frame-level Semantic Carriers"
                },
                "summary": "This paper introduces VideoScan, an efficient vision-language model (VLM)\ninference framework designed for real-time video interaction that effectively\ncomprehends and retains streamed video inputs while delivering rapid and\naccurate responses. A longstanding challenge in video\nunderstanding--particularly for long-term or real-time applications--stems from\nthe substantial computational overhead caused by the extensive length of visual\ntokens. To address this, VideoScan employs a single semantic carrier token to\nrepresent each frame, progressively reducing computational and memory overhead\nduring its two-phase inference process: prefilling and decoding. The embedding\nof the semantic carrier token is derived from an optimized aggregation of\nframe-level visual features, ensuring compact yet semantically rich\nrepresentations. Critically, the corresponding key-value pairs are trained to\nretain contextual semantics from prior frames, enabling efficient memory\nmanagement without sacrificing temporal coherence. During inference, the visual\ntokens of each frame are processed only once during the prefilling phase and\nsubsequently discarded in the decoding stage, eliminating redundant\ncomputations. This design ensures efficient VLM inference even under stringent\nreal-time constraints. Comprehensive experiments on diverse offline and online\nbenchmarks demonstrate that LLaVA-Video, supported by our method, achieves up\nto $\\sim 5\\times$ and $1.29\\times$ speedups compared to its original version\nand previous efficient streaming video understanding approaches, respectively.\nCrucially, these improvements are attained while maintaining competitive\nperformance and ensuring stable GPU memory consumption (consistently $\\sim\n18$GB, independent of video duration).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces VideoScan, an efficient vision-language model (VLM)\ninference framework designed for real-time video interaction that effectively\ncomprehends and retains streamed video inputs while delivering rapid and\naccurate responses. A longstanding challenge in video\nunderstanding--particularly for long-term or real-time applications--stems from\nthe substantial computational overhead caused by the extensive length of visual\ntokens. To address this, VideoScan employs a single semantic carrier token to\nrepresent each frame, progressively reducing computational and memory overhead\nduring its two-phase inference process: prefilling and decoding. The embedding\nof the semantic carrier token is derived from an optimized aggregation of\nframe-level visual features, ensuring compact yet semantically rich\nrepresentations. Critically, the corresponding key-value pairs are trained to\nretain contextual semantics from prior frames, enabling efficient memory\nmanagement without sacrificing temporal coherence. During inference, the visual\ntokens of each frame are processed only once during the prefilling phase and\nsubsequently discarded in the decoding stage, eliminating redundant\ncomputations. This design ensures efficient VLM inference even under stringent\nreal-time constraints. Comprehensive experiments on diverse offline and online\nbenchmarks demonstrate that LLaVA-Video, supported by our method, achieves up\nto $\\sim 5\\times$ and $1.29\\times$ speedups compared to its original version\nand previous efficient streaming video understanding approaches, respectively.\nCrucially, these improvements are attained while maintaining competitive\nperformance and ensuring stable GPU memory consumption (consistently $\\sim\n18$GB, independent of video duration)."
                },
                "authors": [
                    {
                        "name": "Ruanjun Li"
                    },
                    {
                        "name": "Yuedong Tan"
                    },
                    {
                        "name": "Yuanming Shi"
                    },
                    {
                        "name": "Jiawei Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Shao"
                },
                "author": "Jiawei Shao",
                "arxiv_comment": "11 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09382v1",
                "updated": "2025-03-12T13:28:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    28,
                    23,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:28:23Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    28,
                    23,
                    2,
                    71,
                    0
                ],
                "title": "Towards Next-Generation Recommender Systems: A Benchmark for\n  Personalized Recommendation Assistant with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Next-Generation Recommender Systems: A Benchmark for\n  Personalized Recommendation Assistant with LLMs"
                },
                "summary": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git."
                },
                "authors": [
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Liang-bo Ning"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03749v2",
                "updated": "2025-03-12T13:20:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    20,
                    15,
                    2,
                    71,
                    0
                ],
                "published": "2024-08-07T13:01:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    13,
                    1,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Reduction of the type Ia supernova host galaxy step in the outer regions\n  of galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reduction of the type Ia supernova host galaxy step in the outer regions\n  of galaxies"
                },
                "summary": "Using 1533 type Ia supernovae (SNe Ia) from the five-year sample of the Dark\nEnergy Survey (DES), we investigate the relationship between the projected\ngalactocentric separation of the SNe and their host galaxies and their light\ncurves and standardization. We show, for the first time, that the difference in\nSN Ia post-standardization brightnesses between high and low-mass hosts reduces\nfrom $0.078\\pm0.011$\\,mag in the full sample to $0.036 \\pm 0.018$\\,mag for SNe\nIa located in the outer regions of their host galaxies, while increasing to\n$0.100 \\pm 0.014$\\,mag for SNe in the inner regions. The difference in the size\nof the mass step between inner and outer regions is $0.064\\pm0.023$\\,mag. In\nthese inner regions, the step can be reduced (but not removed) using a model\nwhere the $R_V$ of dust along the line-of-sight to the SN changes as a function\nof galaxy properties. We investigate the remaining difference using the\ndistributions of the SN Ia stretch parameter to test the inferred age of SN\nprogenitors. Comparing red (older) environments only, outer regions have a\nhigher proportion of high-stretch SNe and a more homogeneous stretch\ndistribution. However, this effect cannot explain the reduction in significance\nof any Hubble residual step in outer regions. We conclude that the standardized\ndistances of SNe Ia located in the outer regions of galaxies are less affected\nby their global host galaxy properties than those in the inner regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using 1533 type Ia supernovae (SNe Ia) from the five-year sample of the Dark\nEnergy Survey (DES), we investigate the relationship between the projected\ngalactocentric separation of the SNe and their host galaxies and their light\ncurves and standardization. We show, for the first time, that the difference in\nSN Ia post-standardization brightnesses between high and low-mass hosts reduces\nfrom $0.078\\pm0.011$\\,mag in the full sample to $0.036 \\pm 0.018$\\,mag for SNe\nIa located in the outer regions of their host galaxies, while increasing to\n$0.100 \\pm 0.014$\\,mag for SNe in the inner regions. The difference in the size\nof the mass step between inner and outer regions is $0.064\\pm0.023$\\,mag. In\nthese inner regions, the step can be reduced (but not removed) using a model\nwhere the $R_V$ of dust along the line-of-sight to the SN changes as a function\nof galaxy properties. We investigate the remaining difference using the\ndistributions of the SN Ia stretch parameter to test the inferred age of SN\nprogenitors. Comparing red (older) environments only, outer regions have a\nhigher proportion of high-stretch SNe and a more homogeneous stretch\ndistribution. However, this effect cannot explain the reduction in significance\nof any Hubble residual step in outer regions. We conclude that the standardized\ndistances of SNe Ia located in the outer regions of galaxies are less affected\nby their global host galaxy properties than those in the inner regions."
                },
                "authors": [
                    {
                        "name": "M. Toy"
                    },
                    {
                        "name": "P. Wiseman"
                    },
                    {
                        "name": "M. Sullivan"
                    },
                    {
                        "name": "D. Scolnic"
                    },
                    {
                        "name": "M. Vincenzi"
                    },
                    {
                        "name": "D. Brout"
                    },
                    {
                        "name": "T. M. Davis"
                    },
                    {
                        "name": "C. Frohmaier"
                    },
                    {
                        "name": "L. Galbany"
                    },
                    {
                        "name": "C. Lidman"
                    },
                    {
                        "name": "J. Lee"
                    },
                    {
                        "name": "L. Kelsey"
                    },
                    {
                        "name": "R. Kessler"
                    },
                    {
                        "name": "A. Möller"
                    },
                    {
                        "name": "B. Popovic"
                    },
                    {
                        "name": "B. O. Sánchez"
                    },
                    {
                        "name": "P. Shah"
                    },
                    {
                        "name": "M. Smith"
                    },
                    {
                        "name": "S. Allam"
                    },
                    {
                        "name": "M. Aguena"
                    },
                    {
                        "name": "O. Alves"
                    },
                    {
                        "name": "D. Bacon"
                    },
                    {
                        "name": "D. Brooks"
                    },
                    {
                        "name": "D. L. Burke"
                    },
                    {
                        "name": "A. Carnero Rosell"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "L. N. da Costa"
                    },
                    {
                        "name": "M. E. S. Pereira"
                    },
                    {
                        "name": "S. Desai"
                    },
                    {
                        "name": "H. T. Diehl"
                    },
                    {
                        "name": "P. Doel"
                    },
                    {
                        "name": "A. Drlica-Wagner"
                    },
                    {
                        "name": "S. Everett"
                    },
                    {
                        "name": "I. Ferrero"
                    },
                    {
                        "name": "B. Flaugher"
                    },
                    {
                        "name": "J. Frieman"
                    },
                    {
                        "name": "J. García-Bellido"
                    },
                    {
                        "name": "M. Gatti"
                    },
                    {
                        "name": "E. Gaztanaga"
                    },
                    {
                        "name": "G. Giannini"
                    },
                    {
                        "name": "R. A. Gruendl"
                    },
                    {
                        "name": "G. Gutierrez"
                    },
                    {
                        "name": "S. R. Hinton"
                    },
                    {
                        "name": "D. L. Hollowood"
                    },
                    {
                        "name": "K. Honscheid"
                    },
                    {
                        "name": "D. J. James"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "S. Lee"
                    },
                    {
                        "name": "J. L. Marshall"
                    },
                    {
                        "name": "J. Mena-Fernández"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "A. Palmese"
                    },
                    {
                        "name": "A. Pieres"
                    },
                    {
                        "name": "A. A. Plazas Malagón"
                    },
                    {
                        "name": "A. K. Romer"
                    },
                    {
                        "name": "S. Samuroff"
                    },
                    {
                        "name": "E. Sanchez"
                    },
                    {
                        "name": "D. Sanchez Cid"
                    },
                    {
                        "name": "M. Schubnell"
                    },
                    {
                        "name": "E. Suchyta"
                    },
                    {
                        "name": "M. E. C. Swanson"
                    },
                    {
                        "name": "G. Tarle"
                    },
                    {
                        "name": "D. L. Tucker"
                    },
                    {
                        "name": "V. Vikram"
                    },
                    {
                        "name": "A. R. Walker"
                    },
                    {
                        "name": "N. Weaverdyck"
                    }
                ],
                "author_detail": {
                    "name": "N. Weaverdyck"
                },
                "author": "N. Weaverdyck",
                "arxiv_comment": "17 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07813v2",
                "updated": "2025-03-12T13:17:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    17,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-08T17:19:43Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    17,
                    19,
                    43,
                    5,
                    39,
                    0
                ],
                "title": "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoX : Compositional Reasoning Evaluation of Large Language Models"
                },
                "summary": "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Chaoren Wei"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Chenghao Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Stephen Huang"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zhoufutu Wen"
                },
                "author": "Zhoufutu Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17448v2",
                "updated": "2025-03-12T13:14:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    14,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-22T21:50:52Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    21,
                    50,
                    52,
                    1,
                    296,
                    0
                ],
                "title": "In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language."
                },
                "authors": [
                    {
                        "name": "Samiha Sharlin"
                    },
                    {
                        "name": "Tyler R. Josephson"
                    }
                ],
                "author_detail": {
                    "name": "Tyler R. Josephson"
                },
                "author": "Tyler R. Josephson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09365v1",
                "updated": "2025-03-12T13:09:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    9,
                    43,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:09:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    9,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Membership Inference Attacks fueled by Few-Short Learning to detect\n  privacy leakage tackling data integrity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attacks fueled by Few-Short Learning to detect\n  privacy leakage tackling data integrity"
                },
                "summary": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have an intrinsic privacy issue as they memorize parts\nof their training data, creating a privacy leakage. Membership Inference\nAttacks (MIA) exploit it to obtain confidential information about the data used\nfor training, aiming to steal information. They can be repurposed as a\nmeasurement of data integrity by inferring whether it was used to train a\nmachine learning model. While state-of-the-art attacks achieve a significant\nprivacy leakage, their requirements are not feasible enough, hindering their\nrole as practical tools to assess the magnitude of the privacy risk. Moreover,\nthe most appropriate evaluation metric of MIA, the True Positive Rate at low\nFalse Positive Rate lacks interpretability. We claim that the incorporation of\nFew-Shot Learning techniques to the MIA field and a proper qualitative and\nquantitative privacy evaluation measure should deal with these issues. In this\ncontext, our proposal is twofold. We propose a Few-Shot learning based MIA,\ncoined as the FeS-MIA model, which eases the evaluation of the privacy breach\nof a deep learning model by significantly reducing the number of resources\nrequired for the purpose. Furthermore, we propose an interpretable quantitative\nand qualitative measure of privacy, referred to as Log-MIA measure. Jointly,\nthese proposals provide new tools to assess the privacy leakage and to ease the\nevaluation of the training data integrity of deep learning models, that is, to\nanalyze the privacy breach of a deep learning model. Experiments carried out\nwith MIA over image classification and language modeling tasks and its\ncomparison to the state-of-the-art show that our proposals excel at reporting\nthe privacy leakage of a deep learning model with little extra information."
                },
                "authors": [
                    {
                        "name": "Daniel Jiménez-López"
                    },
                    {
                        "name": "Nuria Rodríguez-Barroso"
                    },
                    {
                        "name": "M. Victoria Luzón"
                    },
                    {
                        "name": "Francisco Herrera"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Herrera"
                },
                "author": "Francisco Herrera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09358v1",
                "updated": "2025-03-12T13:00:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    57,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:00:57Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    57,
                    2,
                    71,
                    0
                ],
                "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image\n  Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image\n  Reports"
                },
                "summary": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B."
                },
                "authors": [
                    {
                        "name": "Jiushen Cai"
                    },
                    {
                        "name": "Weihang Zhang"
                    },
                    {
                        "name": "Hanruo Liu"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Huiqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Huiqi Li"
                },
                "author": "Huiqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09357v1",
                "updated": "2025-03-12T13:00:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    29,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:00:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Automatic Operator-level Parallelism Planning for Distributed Deep\n  Learning -- A Mixed-Integer Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Operator-level Parallelism Planning for Distributed Deep\n  Learning -- A Mixed-Integer Programming Approach"
                },
                "summary": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment."
                },
                "authors": [
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Bowen Pang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12464v3",
                "updated": "2025-03-12T12:50:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    50,
                    0,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-16T11:25:13Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    13,
                    2,
                    290,
                    0
                ],
                "title": "Exploring LLM Cryptocurrency Trading Through Fact-Subjectivity Aware\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM Cryptocurrency Trading Through Fact-Subjectivity Aware\n  Reasoning"
                },
                "summary": "While many studies show that more advanced LLMs excel in tasks such as\nmathematics and coding, we observe that in cryptocurrency trading, stronger\nLLMs sometimes underperform compared to weaker ones. To investigate this\ncounterintuitive phenomenon, we examine how LLMs reason when making trading\ndecisions. Our findings reveal that (1) stronger LLMs show a preference for\nfactual information over subjectivity; (2) separating the reasoning process\ninto factual and subjective components leads to higher profits. Building on\nthese insights, we propose a multi-agent framework, FS-ReasoningAgent, which\nenables LLMs to recognize and learn from both factual and subjective reasoning.\nExtensive experiments demonstrate that this fine-grained reasoning approach\nenhances LLM trading performance in cryptocurrency markets, yielding profit\nimprovements of 7\\% in BTC, 2\\% in ETH, and 10\\% in SOL. Additionally, an\nablation study reveals that relying on subjective news generates higher returns\nin bull markets, while focusing on factual information yields better results in\nbear markets. Code is available at\nhttps://github.com/Persdre/FS-ReasoningAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many studies show that more advanced LLMs excel in tasks such as\nmathematics and coding, we observe that in cryptocurrency trading, stronger\nLLMs sometimes underperform compared to weaker ones. To investigate this\ncounterintuitive phenomenon, we examine how LLMs reason when making trading\ndecisions. Our findings reveal that (1) stronger LLMs show a preference for\nfactual information over subjectivity; (2) separating the reasoning process\ninto factual and subjective components leads to higher profits. Building on\nthese insights, we propose a multi-agent framework, FS-ReasoningAgent, which\nenables LLMs to recognize and learn from both factual and subjective reasoning.\nExtensive experiments demonstrate that this fine-grained reasoning approach\nenhances LLM trading performance in cryptocurrency markets, yielding profit\nimprovements of 7\\% in BTC, 2\\% in ETH, and 10\\% in SOL. Additionally, an\nablation study reveals that relying on subjective news generates higher returns\nin bull markets, while focusing on factual information yields better results in\nbear markets. Code is available at\nhttps://github.com/Persdre/FS-ReasoningAgent."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yuchen Gao"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingqiao Luo"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Accepted at ICLR 2025 Financial AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09347v1",
                "updated": "2025-03-12T12:49:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:49:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "arxiv_comment": "8 pages, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09341v1",
                "updated": "2025-03-12T12:36:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    36,
                    45,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:36:45Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    36,
                    45,
                    2,
                    71,
                    0
                ],
                "title": "An Evaluation of LLMs for Detecting Harmful Computing Terms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation of LLMs for Detecting Harmful Computing Terms"
                },
                "summary": "Detecting harmful and non-inclusive terminology in technical contexts is\ncritical for fostering inclusive environments in computing. This study explores\nthe impact of model architecture on harmful language detection by evaluating a\ncurated database of technical terms, each paired with specific use cases. We\ntested a range of encoder, decoder, and encoder-decoder language models,\nincluding BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0,\nGPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. Each model was\npresented with a standardized prompt to identify harmful and non-inclusive\nlanguage across 64 terms. Results reveal that decoder models, particularly\nGemini Flash 2.0 and Claude AI, excel in nuanced contextual analysis, while\nencoder models like BERT exhibit strong pattern recognition but struggle with\nclassification certainty. We discuss the implications of these findings for\nimproving automated detection tools and highlight model-specific strengths and\nlimitations in fostering inclusive communication in technical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting harmful and non-inclusive terminology in technical contexts is\ncritical for fostering inclusive environments in computing. This study explores\nthe impact of model architecture on harmful language detection by evaluating a\ncurated database of technical terms, each paired with specific use cases. We\ntested a range of encoder, decoder, and encoder-decoder language models,\nincluding BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0,\nGPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. Each model was\npresented with a standardized prompt to identify harmful and non-inclusive\nlanguage across 64 terms. Results reveal that decoder models, particularly\nGemini Flash 2.0 and Claude AI, excel in nuanced contextual analysis, while\nencoder models like BERT exhibit strong pattern recognition but struggle with\nclassification certainty. We discuss the implications of these findings for\nimproving automated detection tools and highlight model-specific strengths and\nlimitations in fostering inclusive communication in technical domains."
                },
                "authors": [
                    {
                        "name": "Joshua Jacas"
                    },
                    {
                        "name": "Hana Winchester"
                    },
                    {
                        "name": "Alicia Boyd"
                    },
                    {
                        "name": "Brittany Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Brittany Johnson"
                },
                "author": "Brittany Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09335v1",
                "updated": "2025-03-12T12:30:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    30,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:30:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    30,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot\n  Interaction via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot\n  Interaction via Large Language Model"
                },
                "summary": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git."
                },
                "authors": [
                    {
                        "name": "Yuzhi Lai"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Youssef Nassar"
                    },
                    {
                        "name": "Mingyu Fan"
                    },
                    {
                        "name": "Thomas Weber"
                    },
                    {
                        "name": "Matthias Rätsch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Rätsch"
                },
                "author": "Matthias Rätsch",
                "arxiv_doi": "10.1016/j.eswa.2024.126360",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2024.126360",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted for publication in ESWA @ 2025 Elsevier.\n  Personal use of this material is permitted. Permission from Elsevier must be\n  obtained for all other uses, including reprinting/redistribution, creating\n  new works, or reuse of any copyrighted components of this work in other media",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09334v1",
                "updated": "2025-03-12T12:29:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    29,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:29:27Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    29,
                    27,
                    2,
                    71,
                    0
                ],
                "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs\n  Using Cyber Security Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs\n  Using Cyber Security Data"
                },
                "summary": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance."
                },
                "authors": [
                    {
                        "name": "Adel ElZemity"
                    },
                    {
                        "name": "Budi Arief"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_comment": "The paper is submitted to \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval\" and is\n  currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09326v1",
                "updated": "2025-03-12T12:20:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    20,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:20:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    20,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models"
                },
                "summary": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Zhuo Cai"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Kun Yu"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09321v1",
                "updated": "2025-03-12T12:12:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    12,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:12:46Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    12,
                    46,
                    2,
                    71,
                    0
                ],
                "title": "DAVE: Diagnostic benchmark for Audio Visual Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAVE: Diagnostic benchmark for Audio Visual Evaluation"
                },
                "summary": "Audio-visual understanding is a rapidly evolving field that seeks to\nintegrate and interpret information from both auditory and visual modalities.\nDespite recent advances in multi-modal learning, existing benchmarks often\nsuffer from strong visual bias -- where answers can be inferred from visual\ndata alone -- and provide only aggregate scores that conflate multiple sources\nof error. This makes it difficult to determine whether models struggle with\nvisual understanding, audio interpretation, or audio-visual alignment. In this\nwork, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark\ndataset designed to systematically evaluate audio-visual models across\ncontrolled challenges. DAVE alleviates existing limitations by (i) ensuring\nboth modalities are necessary to answer correctly and (ii) decoupling\nevaluation into atomic subcategories. Our detailed analysis of state-of-the-art\nmodels reveals specific failure modes and provides targeted insights for\nimprovement. By offering this standardized diagnostic framework, we aim to\nfacilitate more robust development of audio-visual models. The dataset is\nreleased: https://github.com/gorjanradevski/dave",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-visual understanding is a rapidly evolving field that seeks to\nintegrate and interpret information from both auditory and visual modalities.\nDespite recent advances in multi-modal learning, existing benchmarks often\nsuffer from strong visual bias -- where answers can be inferred from visual\ndata alone -- and provide only aggregate scores that conflate multiple sources\nof error. This makes it difficult to determine whether models struggle with\nvisual understanding, audio interpretation, or audio-visual alignment. In this\nwork, we introduce DAVE (Diagnostic Audio Visual Evaluation), a novel benchmark\ndataset designed to systematically evaluate audio-visual models across\ncontrolled challenges. DAVE alleviates existing limitations by (i) ensuring\nboth modalities are necessary to answer correctly and (ii) decoupling\nevaluation into atomic subcategories. Our detailed analysis of state-of-the-art\nmodels reveals specific failure modes and provides targeted insights for\nimprovement. By offering this standardized diagnostic framework, we aim to\nfacilitate more robust development of audio-visual models. The dataset is\nreleased: https://github.com/gorjanradevski/dave"
                },
                "authors": [
                    {
                        "name": "Gorjan Radevski"
                    },
                    {
                        "name": "Teodora Popordanoska"
                    },
                    {
                        "name": "Matthew B. Blaschko"
                    },
                    {
                        "name": "Tinne Tuytelaars"
                    }
                ],
                "author_detail": {
                    "name": "Tinne Tuytelaars"
                },
                "author": "Tinne Tuytelaars",
                "arxiv_comment": "First two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09311v1",
                "updated": "2025-03-12T12:02:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    2,
                    36,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:02:36Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    2,
                    36,
                    2,
                    71,
                    0
                ],
                "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem\n  with simulated user interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive political surveys and GPT-4: Tackling the cold start problem\n  with simulated user interactions"
                },
                "summary": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys."
                },
                "authors": [
                    {
                        "name": "Fynn Bachmann"
                    },
                    {
                        "name": "Daan van der Weijden"
                    },
                    {
                        "name": "Lucien Heitz"
                    },
                    {
                        "name": "Cristina Sarasua"
                    },
                    {
                        "name": "Abraham Bernstein"
                    }
                ],
                "author_detail": {
                    "name": "Abraham Bernstein"
                },
                "author": "Abraham Bernstein",
                "arxiv_comment": "23 pages. Under review at PLOS One",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02358v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02358v4",
                "updated": "2025-03-12T11:59:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    59,
                    19,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-04T14:43:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    43,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionLab: Unified Human Motion Generation and Editing via the\n  Motion-Condition-Motion Paradigm"
                },
                "summary": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion. Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions. In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion generation and editing are key components of computer graphics\nand vision. However, current approaches in this field tend to offer isolated\nsolutions tailored to specific tasks, which can be inefficient and impractical\nfor real-world applications. While some efforts have aimed to unify\nmotion-related tasks, these methods simply use different modalities as\nconditions to guide motion generation. Consequently, they lack editing\ncapabilities, fine-grained control, and fail to facilitate knowledge sharing\nacross tasks. To address these limitations and provide a versatile, unified\nframework capable of handling both human motion generation and editing, we\nintroduce a novel paradigm: Motion-Condition-Motion, which enables the unified\nformulation of diverse tasks with three concepts: source motion, condition, and\ntarget motion. Based on this paradigm, we propose a unified framework,\nMotionLab, which incorporates rectified flows to learn the mapping from source\nmotion to target motion, guided by the specified conditions. In MotionLab, we\nintroduce the 1) MotionFlow Transformer to enhance conditional generation and\nediting without task-specific modules; 2) Aligned Rotational Position Encoding}\nto guarantee the time synchronization between source motion and target motion;\n3) Task Specified Instruction Modulation; and 4) Motion Curriculum Learning for\neffective multi-task learning and knowledge sharing across tasks. Notably, our\nMotionLab demonstrates promising generalization capabilities and inference\nefficiency across multiple benchmarks for human motion. Our code and additional\nvideo results are available at: https://diouo.github.io/motionlab.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziyan Guo"
                    },
                    {
                        "name": "Zeyu Hu"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "De Wen Soh"
                    }
                ],
                "author_detail": {
                    "name": "De Wen Soh"
                },
                "author": "De Wen Soh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02358v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02358v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09304v1",
                "updated": "2025-03-12T11:56:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    56,
                    1,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:56:01Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    56,
                    1,
                    2,
                    71,
                    0
                ],
                "title": "Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Priority-Aware Preemptive Scheduling for Mixed-Priority Workloads in MoE\n  Inference"
                },
                "summary": "Large Language Models have revolutionized natural language processing, yet\nserving them efficiently in data centers remains challenging due to mixed\nworkloads comprising latency-sensitive (LS) and best-effort (BE) jobs. Existing\ninference systems employ iteration-level first-come-first-served scheduling,\ncausing head-of-line blocking when BE jobs delay LS jobs. We introduce QLLM, a\nnovel inference system designed for Mixture of Experts (MoE) models, featuring\na fine-grained, priority-aware preemptive scheduler. QLLM enables expert-level\npreemption, deferring BE job execution while minimizing LS time-to-first-token\n(TTFT). Our approach removes iteration-level scheduling constraints, enabling\nthe scheduler to preempt jobs at any layer based on priority. Evaluations on an\nNvidia A100 GPU show that QLLM significantly improves performance. It reduces\nLS TTFT by an average of $65.5\\times$ and meets the SLO at up to $7$\nrequests/sec, whereas the baseline fails to do so under the tested workload.\nAdditionally, it cuts LS turnaround time by up to $12.8\\times$ without\nimpacting throughput. QLLM is modular, extensible, and seamlessly integrates\nwith Hugging Face MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have revolutionized natural language processing, yet\nserving them efficiently in data centers remains challenging due to mixed\nworkloads comprising latency-sensitive (LS) and best-effort (BE) jobs. Existing\ninference systems employ iteration-level first-come-first-served scheduling,\ncausing head-of-line blocking when BE jobs delay LS jobs. We introduce QLLM, a\nnovel inference system designed for Mixture of Experts (MoE) models, featuring\na fine-grained, priority-aware preemptive scheduler. QLLM enables expert-level\npreemption, deferring BE job execution while minimizing LS time-to-first-token\n(TTFT). Our approach removes iteration-level scheduling constraints, enabling\nthe scheduler to preempt jobs at any layer based on priority. Evaluations on an\nNvidia A100 GPU show that QLLM significantly improves performance. It reduces\nLS TTFT by an average of $65.5\\times$ and meets the SLO at up to $7$\nrequests/sec, whereas the baseline fails to do so under the tested workload.\nAdditionally, it cuts LS turnaround time by up to $12.8\\times$ without\nimpacting throughput. QLLM is modular, extensible, and seamlessly integrates\nwith Hugging Face MoE models."
                },
                "authors": [
                    {
                        "name": "Mohammad Siavashi"
                    },
                    {
                        "name": "Faezeh Keshmiri Dindarloo"
                    },
                    {
                        "name": "Dejan Kostic"
                    },
                    {
                        "name": "Marco Chiesa"
                    }
                ],
                "author_detail": {
                    "name": "Marco Chiesa"
                },
                "author": "Marco Chiesa",
                "arxiv_doi": "10.1145/3721146.3721956",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721956",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09297v1",
                "updated": "2025-03-12T11:46:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    46,
                    29,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:46:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    46,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Gravitational form factors and mechanical properties of the nucleon in a\n  meson dominance approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational form factors and mechanical properties of the nucleon in a\n  meson dominance approach"
                },
                "summary": "We analyze the gravitational form factors and mechanical properties of the\nnucleon, focusing on both some general issues as well as on modeling with meson\ndominance. We show that the lattice QCD results for the nucleon gravitational\nform factors at $m_\\pi=170$~MeV, available for space-like momentum transfer\nsquared up to 2GeV, are explained in a natural way within the meson dominance\napproach. We carry out the proper Raman spin decomposition of the\nenergy-momentum tensor and in each spin channel use a minimum number of\nresonances consistent with the perturbative QCD short-distance constraints.\nThese constraints are related to the super-convergence sum rules, following\nfrom the asymptotic perturbative QCD fall-off of the form factors. The value of\nthe nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the\ntwo-dimensional transverse gravitational densities of the nucleon in the\ntransverse coordinate $b$. With the super-convergence sum rules, we derive new\nsum rules for these densities at the origin and for their derivatives,\ninvolving logarithmic weighting in the corresponding spectral density\nintegrals. From analysis of the threshold behavior in the time-like region and\nthe properties of the $\\pi\\pi \\to N\\bar{N}$ reaction, we infer the behavior of\nthe transverse densities at asymptotically large coordinates. We also carry out\nthe meson dominance analysis of the two- and three-dimensional mechanical\nproperties of the nucleon (the pressure and stress) and explore their\nconnection to the spectral densities via dispersion relations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the gravitational form factors and mechanical properties of the\nnucleon, focusing on both some general issues as well as on modeling with meson\ndominance. We show that the lattice QCD results for the nucleon gravitational\nform factors at $m_\\pi=170$~MeV, available for space-like momentum transfer\nsquared up to 2GeV, are explained in a natural way within the meson dominance\napproach. We carry out the proper Raman spin decomposition of the\nenergy-momentum tensor and in each spin channel use a minimum number of\nresonances consistent with the perturbative QCD short-distance constraints.\nThese constraints are related to the super-convergence sum rules, following\nfrom the asymptotic perturbative QCD fall-off of the form factors. The value of\nthe nucleon $D$-term following from the fits is -3.0(4). Next, we obtain the\ntwo-dimensional transverse gravitational densities of the nucleon in the\ntransverse coordinate $b$. With the super-convergence sum rules, we derive new\nsum rules for these densities at the origin and for their derivatives,\ninvolving logarithmic weighting in the corresponding spectral density\nintegrals. From analysis of the threshold behavior in the time-like region and\nthe properties of the $\\pi\\pi \\to N\\bar{N}$ reaction, we infer the behavior of\nthe transverse densities at asymptotically large coordinates. We also carry out\nthe meson dominance analysis of the two- and three-dimensional mechanical\nproperties of the nucleon (the pressure and stress) and explore their\nconnection to the spectral densities via dispersion relations."
                },
                "authors": [
                    {
                        "name": "Wojciech Broniowski"
                    },
                    {
                        "name": "Enrique Ruiz Arriola"
                    }
                ],
                "author_detail": {
                    "name": "Enrique Ruiz Arriola"
                },
                "author": "Enrique Ruiz Arriola",
                "arxiv_comment": "21 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-lat",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09291v1",
                "updated": "2025-03-12T11:36:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    36,
                    29,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:36:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    36,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Prompt Inference Attack on Distributed Large Language Model Inference\n  Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Inference Attack on Distributed Large Language Model Inference\n  Frameworks"
                },
                "summary": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Xinjian Luo"
                    },
                    {
                        "name": "Ting Yu"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08102v2",
                "updated": "2025-03-12T11:31:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    31,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T07:05:52Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    7,
                    5,
                    52,
                    1,
                    70,
                    0
                ],
                "title": "AI-native Memory 2.0: Second Me",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-native Memory 2.0: Second Me"
                },
                "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me."
                },
                "authors": [
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Tao Gao"
                    },
                    {
                        "name": "Fangyi Bao"
                    },
                    {
                        "name": "Felix Tao"
                    },
                    {
                        "name": "Jingbo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Shang"
                },
                "author": "Jingbo Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09281v1",
                "updated": "2025-03-12T11:27:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    27,
                    4,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:27:04Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    27,
                    4,
                    2,
                    71,
                    0
                ],
                "title": "Crowdsourced Homophily Ties Based Graph Annotation Via Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowdsourced Homophily Ties Based Graph Annotation Via Large Language\n  Model"
                },
                "summary": "Accurate graph annotation typically requires substantial labeled data, which\nis often challenging and resource-intensive to obtain. In this paper, we\npresent Crowdsourced Homophily Ties Based Graph Annotation via Large Language\nModel (CSA-LLM), a novel approach that combines the strengths of crowdsourced\nannotations with the capabilities of large language models (LLMs) to enhance\nthe graph annotation process. CSA-LLM harnesses the structural context of graph\ndata by integrating information from 1-hop and 2-hop neighbors. By emphasizing\nhomophily ties - key connections that signify similarity within the graph -\nCSA-LLM significantly improves the accuracy of annotations. Experimental\nresults demonstrate that this method enhances the performance of Graph Neural\nNetworks (GNNs) by delivering more precise and reliable annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate graph annotation typically requires substantial labeled data, which\nis often challenging and resource-intensive to obtain. In this paper, we\npresent Crowdsourced Homophily Ties Based Graph Annotation via Large Language\nModel (CSA-LLM), a novel approach that combines the strengths of crowdsourced\nannotations with the capabilities of large language models (LLMs) to enhance\nthe graph annotation process. CSA-LLM harnesses the structural context of graph\ndata by integrating information from 1-hop and 2-hop neighbors. By emphasizing\nhomophily ties - key connections that signify similarity within the graph -\nCSA-LLM significantly improves the accuracy of annotations. Experimental\nresults demonstrate that this method enhances the performance of Graph Neural\nNetworks (GNNs) by delivering more precise and reliable annotations."
                },
                "authors": [
                    {
                        "name": "Yu Bu"
                    },
                    {
                        "name": "Yulin Zhu"
                    },
                    {
                        "name": "Kai Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kai Zhou"
                },
                "author": "Kai Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09276v1",
                "updated": "2025-03-12T11:22:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    22,
                    13,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:22:13Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    22,
                    13,
                    2,
                    71,
                    0
                ],
                "title": "Fine-Tuning Large Language Models for Educational Support: Leveraging\n  Gagne's Nine Events of Instruction for Lesson Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Large Language Models for Educational Support: Leveraging\n  Gagne's Nine Events of Instruction for Lesson Planning"
                },
                "summary": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes."
                },
                "authors": [
                    {
                        "name": "Linzhao Jia"
                    },
                    {
                        "name": "Changyong Qi"
                    },
                    {
                        "name": "Yuang Wei"
                    },
                    {
                        "name": "Han Sun"
                    },
                    {
                        "name": "Xiaozhe Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhe Yang"
                },
                "author": "Xiaozhe Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09263v1",
                "updated": "2025-03-12T11:03:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    3,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:03:27Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    3,
                    27,
                    2,
                    71,
                    0
                ],
                "title": "COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation"
                },
                "summary": "With the rapid advancements in Large Language Models (LLMs), an increasing\nnumber of studies have leveraged LLMs as the cognitive core of agents to\naddress complex task decision-making challenges. Specially, recent research has\ndemonstrated the potential of LLM-based agents on automating Windows GUI\noperations. However, existing methodologies exhibit two critical challenges:\n(1) static agent architectures fail to dynamically adapt to the heterogeneous\nrequirements of OS-level tasks, leading to inadequate scenario\ngeneralization;(2) the agent workflows lack fault tolerance mechanism,\nnecessitating complete process re-execution for UI agent decision error. To\naddress these limitations, we introduce \\textit{COLA}, a collaborative\nmulti-agent framework for automating Windows UI operations. In this framework,\na scenario-aware agent Task Scheduler decomposes task requirements into atomic\ncapability units, dynamically selects the optimal agent from a decision agent\npool, effectively responds to the capability requirements of diverse scenarios.\nThe decision agent pool supports plug-and-play expansion for enhanced\nflexibility. In addition, we design a memory unit equipped to all agents for\ntheir self-evolution. Furthermore, we develop an interactive backtracking\nmechanism that enables human to intervene to trigger state rollbacks for\nnon-destructive process repair. Our experimental results on the GAIA benchmark\ndemonstrates that the \\textit{COLA} framework achieves state-of-the-art\nperformance with an average score of 31.89\\%, significantly outperforming\nbaseline approaches without web API integration. Ablation studies further\nvalidate the individual contributions of our dynamic scheduling. The code is\navailable at https://github.com/Alokia/COLA-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements in Large Language Models (LLMs), an increasing\nnumber of studies have leveraged LLMs as the cognitive core of agents to\naddress complex task decision-making challenges. Specially, recent research has\ndemonstrated the potential of LLM-based agents on automating Windows GUI\noperations. However, existing methodologies exhibit two critical challenges:\n(1) static agent architectures fail to dynamically adapt to the heterogeneous\nrequirements of OS-level tasks, leading to inadequate scenario\ngeneralization;(2) the agent workflows lack fault tolerance mechanism,\nnecessitating complete process re-execution for UI agent decision error. To\naddress these limitations, we introduce \\textit{COLA}, a collaborative\nmulti-agent framework for automating Windows UI operations. In this framework,\na scenario-aware agent Task Scheduler decomposes task requirements into atomic\ncapability units, dynamically selects the optimal agent from a decision agent\npool, effectively responds to the capability requirements of diverse scenarios.\nThe decision agent pool supports plug-and-play expansion for enhanced\nflexibility. In addition, we design a memory unit equipped to all agents for\ntheir self-evolution. Furthermore, we develop an interactive backtracking\nmechanism that enables human to intervene to trigger state rollbacks for\nnon-destructive process repair. Our experimental results on the GAIA benchmark\ndemonstrates that the \\textit{COLA} framework achieves state-of-the-art\nperformance with an average score of 31.89\\%, significantly outperforming\nbaseline approaches without web API integration. Ablation studies further\nvalidate the individual contributions of our dynamic scheduling. The code is\navailable at https://github.com/Alokia/COLA-demo."
                },
                "authors": [
                    {
                        "name": "Di Zhao"
                    },
                    {
                        "name": "Longhui Ma"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Miao Wang"
                    },
                    {
                        "name": "Zhao Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Lv"
                },
                "author": "Zhao Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09260v1",
                "updated": "2025-03-12T11:00:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    0,
                    16,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:00:16Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    0,
                    16,
                    2,
                    71,
                    0
                ],
                "title": "Neural Normalized Cut: A Differential and Generalizable Approach for\n  Spectral Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Normalized Cut: A Differential and Generalizable Approach for\n  Spectral Clustering"
                },
                "summary": "Spectral clustering, as a popular tool for data clustering, requires an\neigen-decomposition step on a given affinity to obtain the spectral embedding.\nNevertheless, such a step suffers from the lack of generalizability and\nscalability. Moreover, the obtained spectral embeddings can hardly provide a\ngood approximation to the ground-truth partition and thus a k-means step is\nadopted to quantize the embedding. In this paper, we propose a simple yet\neffective scalable and generalizable approach, called Neural Normalized Cut\n(NeuNcut), to learn the clustering membership for spectral clustering directly.\nIn NeuNcut, we properly reparameterize the unknown cluster membership via a\nneural network, and train the neural network via stochastic gradient descent\nwith a properly relaxed normalized cut loss. As a result, our NeuNcut enjoys a\ndesired generalization ability to directly infer clustering membership for\nout-of-sample unseen data and hence brings us an efficient way to handle\nclustering task with ultra large-scale data. We conduct extensive experiments\non both synthetic data and benchmark datasets and experimental results validate\nthe effectiveness and the superiority of our approach. Our code is available\nat: https://github.com/hewei98/NeuNcut.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral clustering, as a popular tool for data clustering, requires an\neigen-decomposition step on a given affinity to obtain the spectral embedding.\nNevertheless, such a step suffers from the lack of generalizability and\nscalability. Moreover, the obtained spectral embeddings can hardly provide a\ngood approximation to the ground-truth partition and thus a k-means step is\nadopted to quantize the embedding. In this paper, we propose a simple yet\neffective scalable and generalizable approach, called Neural Normalized Cut\n(NeuNcut), to learn the clustering membership for spectral clustering directly.\nIn NeuNcut, we properly reparameterize the unknown cluster membership via a\nneural network, and train the neural network via stochastic gradient descent\nwith a properly relaxed normalized cut loss. As a result, our NeuNcut enjoys a\ndesired generalization ability to directly infer clustering membership for\nout-of-sample unseen data and hence brings us an efficient way to handle\nclustering task with ultra large-scale data. We conduct extensive experiments\non both synthetic data and benchmark datasets and experimental results validate\nthe effectiveness and the superiority of our approach. Our code is available\nat: https://github.com/hewei98/NeuNcut."
                },
                "authors": [
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Shangzhi Zhang"
                    },
                    {
                        "name": "Chun-Guang Li"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    },
                    {
                        "name": "Jun Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Guo"
                },
                "author": "Jun Guo",
                "arxiv_doi": "10.1016/j.patcog.2025.111545",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.patcog.2025.111545",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 figures, 8 tables, accepted by Pattern Recognition (2025-03-11)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01076v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01076v2",
                "updated": "2025-03-12T10:58:26Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    58,
                    26,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-02T03:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    3,
                    29,
                    27,
                    0,
                    337,
                    0
                ],
                "title": "Quantum criticality and universality in stationary state of long-range\n  Kitaev model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum criticality and universality in stationary state of long-range\n  Kitaev model"
                },
                "summary": "We investigate the signature of quantum criticality in the long-time\nstationary state of the long-range Kitaev chain by performing various quench\nprotocols. In this model, the pairing interaction decays with distance\naccording to a power law with exponent $\\alpha$. Using quantum\ninformation-theoretic measures, such as mutual information and logarithmic\nnegativity, we show that, irrespective of the values of $\\alpha$,\ncritical-to-critical quench displays quantum criticality even in the stationary\nstate. Remarkably, in the presence of long-range pairing interactions, where\nfermionic correlators decay algebraically even at non-critical points,\nsignature of quantum criticality persists in the stationary state. Furthermore,\nthe effective central charge, calculated from both mutual information and\nlogarithmic negativity of stationary state following a critical-to-critical\nquench, agrees with the central charge of the corresponding ground states for\nboth $\\alpha = 0$ and $\\alpha = 2$. Therefore, information of the universality\nclass can be inferred from the stationary state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the signature of quantum criticality in the long-time\nstationary state of the long-range Kitaev chain by performing various quench\nprotocols. In this model, the pairing interaction decays with distance\naccording to a power law with exponent $\\alpha$. Using quantum\ninformation-theoretic measures, such as mutual information and logarithmic\nnegativity, we show that, irrespective of the values of $\\alpha$,\ncritical-to-critical quench displays quantum criticality even in the stationary\nstate. Remarkably, in the presence of long-range pairing interactions, where\nfermionic correlators decay algebraically even at non-critical points,\nsignature of quantum criticality persists in the stationary state. Furthermore,\nthe effective central charge, calculated from both mutual information and\nlogarithmic negativity of stationary state following a critical-to-critical\nquench, agrees with the central charge of the corresponding ground states for\nboth $\\alpha = 0$ and $\\alpha = 2$. Therefore, information of the universality\nclass can be inferred from the stationary state."
                },
                "authors": [
                    {
                        "name": "Akash Mitra"
                    },
                    {
                        "name": "Sanku Paul"
                    },
                    {
                        "name": "Shashi C. L. Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Shashi C. L. Srivastava"
                },
                "author": "Shashi C. L. Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01076v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01076v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09248v1",
                "updated": "2025-03-12T10:42:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    42,
                    11,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:42:11Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    42,
                    11,
                    2,
                    71,
                    0
                ],
                "title": "Bayesian Test-Time Adaptation for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Test-Time Adaptation for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models, such as CLIP,\naims to adapt the model to new, potentially out-of-distribution test data.\nExisting methods calculate the similarity between visual embedding and\nlearnable class embeddings, which are initialized by text embeddings, for\nzero-shot image classification. In this work, we first analyze this process\nbased on Bayes theorem, and observe that the core factors influencing the final\nprediction are the likelihood and the prior. However, existing methods\nessentially focus on adapting class embeddings to adapt likelihood, but they\noften ignore the importance of prior. To address this gap, we propose a novel\napproach, \\textbf{B}ayesian \\textbf{C}lass \\textbf{A}daptation (BCA), which in\naddition to continuously updating class embeddings to adapt likelihood, also\nuses the posterior of incoming samples to continuously update the prior for\neach class embedding. This dual updating mechanism allows the model to better\nadapt to distribution shifts and achieve higher prediction accuracy. Our method\nnot only surpasses existing approaches in terms of performance metrics but also\nmaintains superior inference rates and memory usage, making it highly efficient\nand practical for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models, such as CLIP,\naims to adapt the model to new, potentially out-of-distribution test data.\nExisting methods calculate the similarity between visual embedding and\nlearnable class embeddings, which are initialized by text embeddings, for\nzero-shot image classification. In this work, we first analyze this process\nbased on Bayes theorem, and observe that the core factors influencing the final\nprediction are the likelihood and the prior. However, existing methods\nessentially focus on adapting class embeddings to adapt likelihood, but they\noften ignore the importance of prior. To address this gap, we propose a novel\napproach, \\textbf{B}ayesian \\textbf{C}lass \\textbf{A}daptation (BCA), which in\naddition to continuously updating class embeddings to adapt likelihood, also\nuses the posterior of incoming samples to continuously update the prior for\neach class embedding. This dual updating mechanism allows the model to better\nadapt to distribution shifts and achieve higher prediction accuracy. Our method\nnot only surpasses existing approaches in terms of performance metrics but also\nmaintains superior inference rates and memory usage, making it highly efficient\nand practical for real-world applications."
                },
                "authors": [
                    {
                        "name": "Lihua Zhou"
                    },
                    {
                        "name": "Mao Ye"
                    },
                    {
                        "name": "Shuaifeng Li"
                    },
                    {
                        "name": "Nianxin Li"
                    },
                    {
                        "name": "Xiatian Zhu"
                    },
                    {
                        "name": "Lei Deng"
                    },
                    {
                        "name": "Hongbin Liu"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05003v2",
                "updated": "2025-03-12T10:40:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    40,
                    48,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-06T12:58:58Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    58,
                    58,
                    4,
                    341,
                    0
                ],
                "title": "SLayR: Scene Layout Generation with Rectified Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLayR: Scene Layout Generation with Rectified Flow"
                },
                "summary": "We introduce SLayR, Scene Layout Generation with Rectified flow, a novel\ntransformer-based model for text-to-layout generation which can then be paired\nwith existing layout-to-image models to produce images. SLayR addresses a\ndomain in which current text-to-image pipelines struggle: generating scene\nlayouts that are of significant variety and plausibility, when the given prompt\nis ambiguous and does not provide constraints on the scene. SLayR surpasses\nexisting baselines including LLMs in unconstrained generation, and can generate\nlayouts from an open caption set. To accurately evaluate the layout generation,\nwe introduce a new benchmark suite, including numerical metrics and a carefully\ndesigned repeatable human-evaluation procedure that assesses the plausibility\nand variety of generated images. We show that our method sets a new state of\nthe art for achieving both at the same time, while being at least 3x times\nsmaller in the number of parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SLayR, Scene Layout Generation with Rectified flow, a novel\ntransformer-based model for text-to-layout generation which can then be paired\nwith existing layout-to-image models to produce images. SLayR addresses a\ndomain in which current text-to-image pipelines struggle: generating scene\nlayouts that are of significant variety and plausibility, when the given prompt\nis ambiguous and does not provide constraints on the scene. SLayR surpasses\nexisting baselines including LLMs in unconstrained generation, and can generate\nlayouts from an open caption set. To accurately evaluate the layout generation,\nwe introduce a new benchmark suite, including numerical metrics and a carefully\ndesigned repeatable human-evaluation procedure that assesses the plausibility\nand variety of generated images. We show that our method sets a new state of\nthe art for achieving both at the same time, while being at least 3x times\nsmaller in the number of parameters."
                },
                "authors": [
                    {
                        "name": "Cameron Braunstein"
                    },
                    {
                        "name": "Hevra Petekkaya"
                    },
                    {
                        "name": "Jan Eric Lenssen"
                    },
                    {
                        "name": "Mariya Toneva"
                    },
                    {
                        "name": "Eddy Ilg"
                    }
                ],
                "author_detail": {
                    "name": "Eddy Ilg"
                },
                "author": "Eddy Ilg",
                "arxiv_comment": "43 pages, 29 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09244v1",
                "updated": "2025-03-12T10:39:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    39,
                    14,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:39:14Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    39,
                    14,
                    2,
                    71,
                    0
                ],
                "title": "How To Make Your Cell Tracker Say \"I dunno!\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How To Make Your Cell Tracker Say \"I dunno!\""
                },
                "summary": "Cell tracking is a key computational task in live-cell microscopy, but fully\nautomated analysis of high-throughput imaging requires reliable and, thus,\nuncertainty-aware data analysis tools, as the amount of data recorded within a\nsingle experiment exceeds what humans are able to overlook. We here propose and\nbenchmark various methods to reason about and quantify uncertainty in linear\nassignment-based cell tracking algorithms. Our methods take inspiration from\nstatistics and machine learning, leveraging two perspectives on the cell\ntracking problem explored throughout this work: Considering it as a Bayesian\ninference problem and as a classification problem. Our methods admit a\nframework-like character in that they equip any frame-to-frame tracking method\nwith uncertainty quantification. We demonstrate this by applying it to various\nexisting tracking algorithms including the recently presented Transformer-based\ntrackers. We demonstrate empirically that our methods yield useful and\nwell-calibrated tracking uncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell tracking is a key computational task in live-cell microscopy, but fully\nautomated analysis of high-throughput imaging requires reliable and, thus,\nuncertainty-aware data analysis tools, as the amount of data recorded within a\nsingle experiment exceeds what humans are able to overlook. We here propose and\nbenchmark various methods to reason about and quantify uncertainty in linear\nassignment-based cell tracking algorithms. Our methods take inspiration from\nstatistics and machine learning, leveraging two perspectives on the cell\ntracking problem explored throughout this work: Considering it as a Bayesian\ninference problem and as a classification problem. Our methods admit a\nframework-like character in that they equip any frame-to-frame tracking method\nwith uncertainty quantification. We demonstrate this by applying it to various\nexisting tracking algorithms including the recently presented Transformer-based\ntrackers. We demonstrate empirically that our methods yield useful and\nwell-calibrated tracking uncertainties."
                },
                "authors": [
                    {
                        "name": "Richard D. Paul"
                    },
                    {
                        "name": "Johannes Seiffarth"
                    },
                    {
                        "name": "David Rügamer"
                    },
                    {
                        "name": "Hanno Scharr"
                    },
                    {
                        "name": "Katharina Nöh"
                    }
                ],
                "author_detail": {
                    "name": "Katharina Nöh"
                },
                "author": "Katharina Nöh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09242v1",
                "updated": "2025-03-12T10:38:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    38,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:38:58Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    38,
                    58,
                    2,
                    71,
                    0
                ],
                "title": "NAMI: Efficient Image Generation via Progressive Rectified Flow\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAMI: Efficient Image Generation via Progressive Rectified Flow\n  Transformers"
                },
                "summary": "Flow-based transformer models for image generation have achieved\nstate-of-the-art performance with larger model parameters, but their inference\ndeployment cost remains high. To enhance inference performance while\nmaintaining generation quality, we propose progressive rectified flow\ntransformers. We divide the rectified flow into different stages according to\nresolution, using fewer transformer layers at the low-resolution stages to\ngenerate image layouts and concept contours, and progressively adding more\nlayers as the resolution increases. Experiments demonstrate that our approach\nachieves fast convergence and reduces inference time while ensuring generation\nquality. The main contributions of this paper are summarized as follows: (1) We\nintroduce progressive rectified flow transformers that enable multi-resolution\ntraining, accelerating model convergence; (2) NAMI leverages piecewise flow and\nspatial cascading of Diffusion Transformer (DiT) to rapidly generate images,\nreducing inference time by 40% to generate a 1024 resolution image; (3) We\npropose NAMI-1K benchmark to evaluate human preference performance, aiming to\nmitigate distributional bias and prevent data leakage from open-source\nbenchmarks. The results show that our model is competitive with\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-based transformer models for image generation have achieved\nstate-of-the-art performance with larger model parameters, but their inference\ndeployment cost remains high. To enhance inference performance while\nmaintaining generation quality, we propose progressive rectified flow\ntransformers. We divide the rectified flow into different stages according to\nresolution, using fewer transformer layers at the low-resolution stages to\ngenerate image layouts and concept contours, and progressively adding more\nlayers as the resolution increases. Experiments demonstrate that our approach\nachieves fast convergence and reduces inference time while ensuring generation\nquality. The main contributions of this paper are summarized as follows: (1) We\nintroduce progressive rectified flow transformers that enable multi-resolution\ntraining, accelerating model convergence; (2) NAMI leverages piecewise flow and\nspatial cascading of Diffusion Transformer (DiT) to rapidly generate images,\nreducing inference time by 40% to generate a 1024 resolution image; (3) We\npropose NAMI-1K benchmark to evaluate human preference performance, aiming to\nmitigate distributional bias and prevent data leakage from open-source\nbenchmarks. The results show that our model is competitive with\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09224v1",
                "updated": "2025-03-12T10:10:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    10,
                    55,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:10:55Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    10,
                    55,
                    2,
                    71,
                    0
                ],
                "title": "X-ray spectral fitting with Monte Carlo Dropout Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray spectral fitting with Monte Carlo Dropout Neural Networks"
                },
                "summary": "We present a novel approach using neural networks to recover X-ray spectral\nmodel parameters and quantify uncertainties, balancing accuracy and\ncomputational efficiency against traditional frequentist and Bayesian methods.\nFrequentist techniques often fall into local minima, compromising parameter\nestimation, while Bayesian methods, though more reliable, suffer from high\ncomputational costs. To address these challenges, we apply Monte Carlo Dropout\nwithin various neural network architectures trained on simulated spectra\ngenerated from a multiparameter emission model convolved with an instrument\nresponse. The model parameters are sampled from a predefined prior, and our\nproof of concept is illustrated using simulated data based on the NICER\nresponse matrix for simple emission models with up to five parameters. Our\nmethod delivers well-defined posterior distributions comparable to Bayesian\ninference, achieves accuracy akin to conventional spectral fitting, and is\nsignificantly less prone to local minima, thereby reducing the risk of\nselecting parameter outliers. Moreover, the approach improves computational\nspeed by roughly an order of magnitude compared to traditional Bayesian\ntechniques. This work demonstrates the potential of neural network-based\nmethods as a robust alternative for X-ray spectral analysis, particularly in\nthe context of future astronomical missions expected to generate extensive\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach using neural networks to recover X-ray spectral\nmodel parameters and quantify uncertainties, balancing accuracy and\ncomputational efficiency against traditional frequentist and Bayesian methods.\nFrequentist techniques often fall into local minima, compromising parameter\nestimation, while Bayesian methods, though more reliable, suffer from high\ncomputational costs. To address these challenges, we apply Monte Carlo Dropout\nwithin various neural network architectures trained on simulated spectra\ngenerated from a multiparameter emission model convolved with an instrument\nresponse. The model parameters are sampled from a predefined prior, and our\nproof of concept is illustrated using simulated data based on the NICER\nresponse matrix for simple emission models with up to five parameters. Our\nmethod delivers well-defined posterior distributions comparable to Bayesian\ninference, achieves accuracy akin to conventional spectral fitting, and is\nsignificantly less prone to local minima, thereby reducing the risk of\nselecting parameter outliers. Moreover, the approach improves computational\nspeed by roughly an order of magnitude compared to traditional Bayesian\ntechniques. This work demonstrates the potential of neural network-based\nmethods as a robust alternative for X-ray spectral analysis, particularly in\nthe context of future astronomical missions expected to generate extensive\ndatasets."
                },
                "authors": [
                    {
                        "name": "A. Tutone"
                    },
                    {
                        "name": "A. Anitra"
                    },
                    {
                        "name": "E. Ambrosi"
                    },
                    {
                        "name": "R. La Placa"
                    },
                    {
                        "name": "A. D'Aì"
                    },
                    {
                        "name": "C. Pinto"
                    },
                    {
                        "name": "M. Del Santo"
                    },
                    {
                        "name": "F. Pintore"
                    },
                    {
                        "name": "A. Pagliaro"
                    },
                    {
                        "name": "A. Anzalone"
                    },
                    {
                        "name": "T. Di Salvo"
                    },
                    {
                        "name": "R. Iaria"
                    },
                    {
                        "name": "L. Burderi"
                    },
                    {
                        "name": "A. Sanna"
                    }
                ],
                "author_detail": {
                    "name": "A. Sanna"
                },
                "author": "A. Sanna",
                "arxiv_comment": "11 pages, 10 figures, accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09223v1",
                "updated": "2025-03-12T10:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    10,
                    30,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:10:30Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    10,
                    30,
                    2,
                    71,
                    0
                ],
                "title": "LREF: A Novel LLM-based Relevance Framework for E-commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LREF: A Novel LLM-based Relevance Framework for E-commerce"
                },
                "summary": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits."
                },
                "authors": [
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Zhixing Tian"
                    },
                    {
                        "name": "Zhenyu Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Haiqing Hu"
                    },
                    {
                        "name": "Guoyu Tang"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sulong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Sulong Xu"
                },
                "author": "Sulong Xu",
                "arxiv_doi": "10.1145/3701716.3715246",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715246",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16965v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16965v3",
                "updated": "2025-03-12T10:09:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    9,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T08:44:01Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    8,
                    44,
                    1,
                    0,
                    55,
                    0
                ],
                "title": "Autoregressive Image Generation with Vision Full-view Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Vision Full-view Prompt"
                },
                "summary": "In autoregressive (AR) image generation, models based on the 'next-token\nprediction' paradigm of LLMs have shown comparable performance to diffusion\nmodels by reducing inductive biases. However, directly applying LLMs to complex\nimage generation can struggle with reconstructing the image's structure and\ndetails, impacting the generation's accuracy and stability. Additionally, the\n'next-token prediction' paradigm in the AR model does not align with the\ncontextual scanning and logical reasoning processes involved in human visual\nperception, limiting effective image generation. Prompt engineering, as a key\ntechnique for guiding LLMs, leverages specifically designed prompts to improve\nmodel performance on complex natural language processing (NLP) tasks, enhancing\naccuracy and stability of generation while maintaining contextual coherence and\nlogical consistency, similar to human reasoning. Inspired by prompt engineering\nfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) to\nenhance autoregressive image generation. Specifically, we design specialized\nimage-related VF prompts for AR image generation to simulate the process of\nhuman image creation. This enhances contextual logic ability by allowing the\nmodel to first perceive overall distribution information before generating the\nimage, and improve generation stability by increasing the inference steps.\nCompared to the AR method without VF prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autoregressive (AR) image generation, models based on the 'next-token\nprediction' paradigm of LLMs have shown comparable performance to diffusion\nmodels by reducing inductive biases. However, directly applying LLMs to complex\nimage generation can struggle with reconstructing the image's structure and\ndetails, impacting the generation's accuracy and stability. Additionally, the\n'next-token prediction' paradigm in the AR model does not align with the\ncontextual scanning and logical reasoning processes involved in human visual\nperception, limiting effective image generation. Prompt engineering, as a key\ntechnique for guiding LLMs, leverages specifically designed prompts to improve\nmodel performance on complex natural language processing (NLP) tasks, enhancing\naccuracy and stability of generation while maintaining contextual coherence and\nlogical consistency, similar to human reasoning. Inspired by prompt engineering\nfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) to\nenhance autoregressive image generation. Specifically, we design specialized\nimage-related VF prompts for AR image generation to simulate the process of\nhuman image creation. This enhances contextual logic ability by allowing the\nmodel to first perceive overall distribution information before generating the\nimage, and improve generation stability by increasing the inference steps.\nCompared to the AR method without VF prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%."
                },
                "authors": [
                    {
                        "name": "Miaomiao Cai"
                    },
                    {
                        "name": "Guanjie Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zhijun Tu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16965v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16965v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23746v3",
                "updated": "2025-03-12T10:08:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    8,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-31T09:01:25Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    1,
                    25,
                    3,
                    305,
                    0
                ],
                "title": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios"
                },
                "summary": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL."
                },
                "authors": [
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Yulin Yuan"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09219v1",
                "updated": "2025-03-12T10:06:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    6,
                    3,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:06:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    6,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Rethinking Prompt-based Debiasing in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Prompt-based Debiasing in Large Language Models"
                },
                "summary": "Investigating bias in large language models (LLMs) is crucial for developing\ntrustworthy AI. While prompt-based through prompt engineering is common, its\neffectiveness relies on the assumption that models inherently understand\nbiases. Our study systematically analyzed this assumption using the BBQ and\nStereoSet benchmarks on both open-source models as well as commercial GPT\nmodel. Experimental results indicate that prompt-based is often superficial;\nfor instance, the Llama2-7B-Chat model misclassified over 90% of unbiased\ncontent as biased, despite achieving high accuracy in identifying bias issues\non the BBQ dataset. Additionally, specific evaluation and question settings in\nbias benchmarks often lead LLMs to choose \"evasive answers\", disregarding the\ncore of the question and the relevance of the response to the context.\nMoreover, the apparent success of previous methods may stem from flawed\nevaluation metrics. Our research highlights a potential \"false prosperity\" in\nprompt-base efforts and emphasizes the need to rethink bias metrics to ensure\ntruly trustworthy AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating bias in large language models (LLMs) is crucial for developing\ntrustworthy AI. While prompt-based through prompt engineering is common, its\neffectiveness relies on the assumption that models inherently understand\nbiases. Our study systematically analyzed this assumption using the BBQ and\nStereoSet benchmarks on both open-source models as well as commercial GPT\nmodel. Experimental results indicate that prompt-based is often superficial;\nfor instance, the Llama2-7B-Chat model misclassified over 90% of unbiased\ncontent as biased, despite achieving high accuracy in identifying bias issues\non the BBQ dataset. Additionally, specific evaluation and question settings in\nbias benchmarks often lead LLMs to choose \"evasive answers\", disregarding the\ncore of the question and the relevance of the response to the context.\nMoreover, the apparent success of previous methods may stem from flawed\nevaluation metrics. Our research highlights a potential \"false prosperity\" in\nprompt-base efforts and emphasizes the need to rethink bias metrics to ensure\ntruly trustworthy AI."
                },
                "authors": [
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10153v3",
                "updated": "2025-03-12T10:05:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    5,
                    37,
                    2,
                    71,
                    0
                ],
                "published": "2024-11-15T12:52:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    12,
                    52,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "A unifying framework for generalised Bayesian online learning in\n  non-stationary environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A unifying framework for generalised Bayesian online learning in\n  non-stationary environments"
                },
                "summary": "We propose a unifying framework for methods that perform probabilistic online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for generalised (B)ayesian (O)nline learning in (N)on-stationary\n(E)nvironments. BONE provides a common structure to tackle a variety of\nproblems, including online continual learning, prequential forecasting, and\ncontextual bandits. The framework requires specifying three modelling choices:\n(i) a model for measurements (e.g., a neural network), (ii) an auxiliary\nprocess to model non-stationarity (e.g., the time since the last changepoint),\nand (iii) a conditional prior over model parameters (e.g., a multivariate\nGaussian). The framework also requires two algorithmic choices, which we use to\ncarry out approximate inference under this framework: (i) an algorithm to\nestimate beliefs (posterior distribution) about the model parameters given the\nauxiliary variable, and (ii) an algorithm to estimate beliefs about the\nauxiliary variable. We show how the modularity of our framework allows for many\nexisting methods to be reinterpreted as instances of BONE, and it allows us to\npropose new methods. We compare experimentally existing methods with our\nproposed new method on several datasets, providing insights into the situations\nthat make each method more suitable for a specific task. We provide a Jax open\nsource library to facilitate the adoption of this framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a unifying framework for methods that perform probabilistic online\nlearning in non-stationary environments. We call the framework BONE, which\nstands for generalised (B)ayesian (O)nline learning in (N)on-stationary\n(E)nvironments. BONE provides a common structure to tackle a variety of\nproblems, including online continual learning, prequential forecasting, and\ncontextual bandits. The framework requires specifying three modelling choices:\n(i) a model for measurements (e.g., a neural network), (ii) an auxiliary\nprocess to model non-stationarity (e.g., the time since the last changepoint),\nand (iii) a conditional prior over model parameters (e.g., a multivariate\nGaussian). The framework also requires two algorithmic choices, which we use to\ncarry out approximate inference under this framework: (i) an algorithm to\nestimate beliefs (posterior distribution) about the model parameters given the\nauxiliary variable, and (ii) an algorithm to estimate beliefs about the\nauxiliary variable. We show how the modularity of our framework allows for many\nexisting methods to be reinterpreted as instances of BONE, and it allows us to\npropose new methods. We compare experimentally existing methods with our\nproposed new method on several datasets, providing insights into the situations\nthat make each method more suitable for a specific task. We provide a Jax open\nsource library to facilitate the adoption of this framework."
                },
                "authors": [
                    {
                        "name": "Gerardo Duran-Martin"
                    },
                    {
                        "name": "Leandro Sánchez-Betancourt"
                    },
                    {
                        "name": "Alexander Y. Shestopaloff"
                    },
                    {
                        "name": "Kevin Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Murphy"
                },
                "author": "Kevin Murphy",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (03/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09217v1",
                "updated": "2025-03-12T10:03:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    3,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:03:58Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    3,
                    58,
                    2,
                    71,
                    0
                ],
                "title": "Evaluating the Generalizability of LLMs in Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Generalizability of LLMs in Automated Program Repair"
                },
                "summary": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research."
                },
                "authors": [
                    {
                        "name": "Fengjie Li"
                    },
                    {
                        "name": "Jiajun Jiang"
                    },
                    {
                        "name": "Jiajun Sun"
                    },
                    {
                        "name": "Hongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Zhang"
                },
                "author": "Hongyu Zhang",
                "arxiv_comment": "5 pages, 1 figure, to be published in ICSE2025-NIER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09211v1",
                "updated": "2025-03-12T10:00:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    0,
                    9,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:00:09Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    0,
                    9,
                    2,
                    71,
                    0
                ],
                "title": "Why LLMs Cannot Think and How to Fix It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why LLMs Cannot Think and How to Fix It"
                },
                "summary": "This paper elucidates that current state-of-the-art Large Language Models\n(LLMs) are fundamentally incapable of making decisions or developing \"thoughts\"\nwithin the feature space due to their architectural constraints. We establish a\ndefinition of \"thought\" that encompasses traditional understandings of that\nterm and adapt it for application to LLMs. We demonstrate that the\narchitectural design and language modeling training methodology of contemporary\nLLMs inherently preclude them from engaging in genuine thought processes. Our\nprimary focus is on this theoretical realization rather than practical insights\nderived from experimental data. Finally, we propose solutions to enable thought\nprocesses within the feature space and discuss the broader implications of\nthese architectural modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper elucidates that current state-of-the-art Large Language Models\n(LLMs) are fundamentally incapable of making decisions or developing \"thoughts\"\nwithin the feature space due to their architectural constraints. We establish a\ndefinition of \"thought\" that encompasses traditional understandings of that\nterm and adapt it for application to LLMs. We demonstrate that the\narchitectural design and language modeling training methodology of contemporary\nLLMs inherently preclude them from engaging in genuine thought processes. Our\nprimary focus is on this theoretical realization rather than practical insights\nderived from experimental data. Finally, we propose solutions to enable thought\nprocesses within the feature space and discuss the broader implications of\nthese architectural modifications."
                },
                "authors": [
                    {
                        "name": "Marius Jahrens"
                    },
                    {
                        "name": "Thomas Martinetz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Martinetz"
                },
                "author": "Thomas Martinetz",
                "arxiv_comment": "Original conference submission for neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07450v2",
                "updated": "2025-03-12T09:57:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    57,
                    19,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-10T15:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper"
                },
                "summary": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted."
                },
                "authors": [
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Asifa Mehmood Qureshi"
                    },
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Roisin Loughran"
                    },
                    {
                        "name": "Subramaniam Kazhuparambil"
                    },
                    {
                        "name": "Andrew Shaw"
                    },
                    {
                        "name": "Mohammed Sabry"
                    },
                    {
                        "name": "Niamh St John Lynch"
                    },
                    {
                        "name": ". Nikhil Singh"
                    },
                    {
                        "name": "Padraic O'Hara"
                    },
                    {
                        "name": "Pranay Jaiswal"
                    },
                    {
                        "name": "Roshan Chandru"
                    },
                    {
                        "name": "David Lillis"
                    }
                ],
                "author_detail": {
                    "name": "David Lillis"
                },
                "arxiv_affiliation": "School of Computer Science, University College Dublin",
                "author": "David Lillis",
                "arxiv_comment": "The project is partially supported by the DkIT Postgraduate\n  Scholarship, Research Ireland under Grant number 13/RC/2094_2, and Grant\n  number 21/FFP-A/925",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01824v2",
                "updated": "2025-03-12T09:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    55,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2024-09-16T16:03:08Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    3,
                    8,
                    0,
                    260,
                    0
                ],
                "title": "AI Conversational Interviewing: Transforming Surveys with LLMs as\n  Adaptive Interviewers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Conversational Interviewing: Transforming Surveys with LLMs as\n  Adaptive Interviewers"
                },
                "summary": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation."
                },
                "authors": [
                    {
                        "name": "Alexander Wuttke"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    },
                    {
                        "name": "Christopher Klamm"
                    },
                    {
                        "name": "Max M. Lang"
                    },
                    {
                        "name": "Quirin Würschinger"
                    },
                    {
                        "name": "Frauke Kreuter"
                    }
                ],
                "author_detail": {
                    "name": "Frauke Kreuter"
                },
                "author": "Frauke Kreuter",
                "arxiv_journal_ref": "LaTeCH-CLfL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09205v1",
                "updated": "2025-03-12T09:48:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    48,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    48,
                    38,
                    2,
                    71,
                    0
                ],
                "title": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model"
                },
                "summary": "Integrating audio and visual data for training multimodal foundational models\nremains challenging. We present Audio-Video Vector Alignment (AVVA), which\naligns audiovisual (AV) scene content beyond mere temporal synchronization via\na Large Language Model (LLM)-based data curation pipeline. Specifically, AVVA\nscores and selects high-quality training clips using Whisper (speech-based\naudio foundation model) for audio and DINOv2 for video within a dual-encoder\ncontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSound\ndemonstrate that this approach can achieve significant accuracy gains with\nsubstantially less curated data. For instance, AVVA yields a 7.6% improvement\nin top-1 accuracy for audio-to-video retrieval on VGGSound compared to\nImageBind, despite training on only 192 hours of carefully filtered data (vs.\n5800+ hours). Moreover, an ablation study highlights that trading data quantity\nfor data quality improves performance, yielding respective top-3 accuracy\nincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, and\nVGGSound over uncurated baselines. While these results underscore AVVA's data\nefficiency, we also discuss the overhead of LLM-driven curation and how it may\nbe scaled or approximated in larger domains. Overall, AVVA provides a viable\npath toward more robust, text-free audiovisual learning with improved retrieval\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio and visual data for training multimodal foundational models\nremains challenging. We present Audio-Video Vector Alignment (AVVA), which\naligns audiovisual (AV) scene content beyond mere temporal synchronization via\na Large Language Model (LLM)-based data curation pipeline. Specifically, AVVA\nscores and selects high-quality training clips using Whisper (speech-based\naudio foundation model) for audio and DINOv2 for video within a dual-encoder\ncontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSound\ndemonstrate that this approach can achieve significant accuracy gains with\nsubstantially less curated data. For instance, AVVA yields a 7.6% improvement\nin top-1 accuracy for audio-to-video retrieval on VGGSound compared to\nImageBind, despite training on only 192 hours of carefully filtered data (vs.\n5800+ hours). Moreover, an ablation study highlights that trading data quantity\nfor data quality improves performance, yielding respective top-3 accuracy\nincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, and\nVGGSound over uncurated baselines. While these results underscore AVVA's data\nefficiency, we also discuss the overhead of LLM-driven curation and how it may\nbe scaled or approximated in larger domains. Overall, AVVA provides a viable\npath toward more robust, text-free audiovisual learning with improved retrieval\naccuracy."
                },
                "authors": [
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Dimitra Emmanouilidou"
                    },
                    {
                        "name": "Hannes Gamper"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Gamper"
                },
                "author": "Hannes Gamper",
                "arxiv_comment": "5 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T, 68T45, 68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09202v1",
                "updated": "2025-03-12T09:46:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    46,
                    59,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:46:59Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    46,
                    59,
                    2,
                    71,
                    0
                ],
                "title": "Token Weighting for Long-Range Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Weighting for Long-Range Language Modeling"
                },
                "summary": "Many applications of large language models (LLMs) require long-context\nunderstanding, but models continue to struggle with such tasks. We hypothesize\nthat conventional next-token prediction training could contribute to this,\nbecause each token is assigned equal weight. Yet, intuitively, the amount of\ncontext needed to predict the next token accurately varies greatly across\ndifferent data. To reflect this, we propose various novel token-weighting\nschemes that assign different weights to each training token in the loss,\nthereby generalizing existing works. For this, we categorize token-weighting\nmethods using a two-step framework which compares the confidences of a\nlong-context and short-context model to score tokens. We evaluate all methods\non multiple long-context understanding tasks and show that non-uniform loss\nweights are helpful to improve the long-context abilities of LLMs. Different\nshort-context models can be used effectively for token scoring, including\nmodels that are much smaller than the long-context model that is trained. All\nin all, this work contributes to a better understanding of the trade-offs\nlong-context language modeling faces and provides guidelines for model steering\nvia loss-weighting based on empirical evidence. The code can be found on\nGithub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications of large language models (LLMs) require long-context\nunderstanding, but models continue to struggle with such tasks. We hypothesize\nthat conventional next-token prediction training could contribute to this,\nbecause each token is assigned equal weight. Yet, intuitively, the amount of\ncontext needed to predict the next token accurately varies greatly across\ndifferent data. To reflect this, we propose various novel token-weighting\nschemes that assign different weights to each training token in the loss,\nthereby generalizing existing works. For this, we categorize token-weighting\nmethods using a two-step framework which compares the confidences of a\nlong-context and short-context model to score tokens. We evaluate all methods\non multiple long-context understanding tasks and show that non-uniform loss\nweights are helpful to improve the long-context abilities of LLMs. Different\nshort-context models can be used effectively for token scoring, including\nmodels that are much smaller than the long-context model that is trained. All\nin all, this work contributes to a better understanding of the trade-offs\nlong-context language modeling faces and provides guidelines for model steering\nvia loss-weighting based on empirical evidence. The code can be found on\nGithub."
                },
                "authors": [
                    {
                        "name": "Falko Helm"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted to NAACL 2025 (Findings). For the code, see\n  https://github.com/UKPLab/naacl2025-token-weighting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09194v1",
                "updated": "2025-03-12T09:38:40Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    38,
                    40,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:38:40Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    38,
                    40,
                    2,
                    71,
                    0
                ],
                "title": "Addressing pitfalls in implicit unobserved confounding synthesis using\n  explicit block hierarchical ancestral sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing pitfalls in implicit unobserved confounding synthesis using\n  explicit block hierarchical ancestral sampling"
                },
                "summary": "Unbiased data synthesis is crucial for evaluating causal discovery algorithms\nin the presence of unobserved confounding, given the scarcity of real-world\ndatasets. A common approach, implicit parameterization, encodes unobserved\nconfounding by modifying the off-diagonal entries of the idiosyncratic\ncovariance matrix while preserving positive definiteness. Within this approach,\nstate-of-the-art protocols have two distinct issues that hinder unbiased\nsampling from the complete space of causal models: first, the use of diagonally\ndominant constructions, which restrict the spectrum of partial correlation\nmatrices; and second, the restriction of possible graphical structures when\nsampling bidirected edges, unnecessarily ruling out valid causal models. To\naddress these limitations, we propose an improved explicit modeling approach\nfor unobserved confounding, leveraging block-hierarchical ancestral generation\nof ground truth causal graphs. Algorithms for converting the ground truth DAG\ninto ancestral graph is provided so that the output of causal discovery\nalgorithms could be compared with. We prove that our approach fully covers the\nspace of causal models, including those generated by the implicit\nparameterization, thus enabling more robust evaluation of methods for causal\ndiscovery and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased data synthesis is crucial for evaluating causal discovery algorithms\nin the presence of unobserved confounding, given the scarcity of real-world\ndatasets. A common approach, implicit parameterization, encodes unobserved\nconfounding by modifying the off-diagonal entries of the idiosyncratic\ncovariance matrix while preserving positive definiteness. Within this approach,\nstate-of-the-art protocols have two distinct issues that hinder unbiased\nsampling from the complete space of causal models: first, the use of diagonally\ndominant constructions, which restrict the spectrum of partial correlation\nmatrices; and second, the restriction of possible graphical structures when\nsampling bidirected edges, unnecessarily ruling out valid causal models. To\naddress these limitations, we propose an improved explicit modeling approach\nfor unobserved confounding, leveraging block-hierarchical ancestral generation\nof ground truth causal graphs. Algorithms for converting the ground truth DAG\ninto ancestral graph is provided so that the output of causal discovery\nalgorithms could be compared with. We prove that our approach fully covers the\nspace of causal models, including those generated by the implicit\nparameterization, thus enabling more robust evaluation of methods for causal\ndiscovery and inference."
                },
                "authors": [
                    {
                        "name": "Xudong Sun"
                    },
                    {
                        "name": "Alex Markham"
                    },
                    {
                        "name": "Pratik Misra"
                    },
                    {
                        "name": "Carsten Marr"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Marr"
                },
                "author": "Carsten Marr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10815v2",
                "updated": "2025-03-12T09:16:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    16,
                    59,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-14T17:59:46Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    46,
                    0,
                    288,
                    0
                ],
                "title": "Depth Any Video with Scalable Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth Any Video with Scalable Synthetic Data"
                },
                "summary": "Video depth estimation has long been hindered by the scarcity of consistent\nand scalable ground truth data, leading to inconsistent and unreliable results.\nIn this paper, we introduce Depth Any Video, a model that tackles the challenge\nthrough two key innovations. First, we develop a scalable synthetic data\npipeline, capturing real-time video depth data from diverse virtual\nenvironments, yielding 40,000 video clips of 5-second duration, each with\nprecise depth annotations. Second, we leverage the powerful priors of\ngenerative video diffusion models to handle real-world videos effectively,\nintegrating advanced techniques such as rotary position encoding and flow\nmatching to further enhance flexibility and efficiency. Unlike previous models,\nwhich are limited to fixed-length video sequences, our approach introduces a\nnovel mixed-duration training strategy that handles videos of varying lengths\nand performs robustly across different frame rates-even on single frames. At\ninference, we propose a depth interpolation method that enables our model to\ninfer high-resolution video depth across sequences of up to 150 frames. Our\nmodel outperforms all previous generative depth models in terms of spatial\naccuracy and temporal consistency. The code and model weights are open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video depth estimation has long been hindered by the scarcity of consistent\nand scalable ground truth data, leading to inconsistent and unreliable results.\nIn this paper, we introduce Depth Any Video, a model that tackles the challenge\nthrough two key innovations. First, we develop a scalable synthetic data\npipeline, capturing real-time video depth data from diverse virtual\nenvironments, yielding 40,000 video clips of 5-second duration, each with\nprecise depth annotations. Second, we leverage the powerful priors of\ngenerative video diffusion models to handle real-world videos effectively,\nintegrating advanced techniques such as rotary position encoding and flow\nmatching to further enhance flexibility and efficiency. Unlike previous models,\nwhich are limited to fixed-length video sequences, our approach introduces a\nnovel mixed-duration training strategy that handles videos of varying lengths\nand performs robustly across different frame rates-even on single frames. At\ninference, we propose a depth interpolation method that enables our model to\ninfer high-resolution video depth across sequences of up to 150 frames. Our\nmodel outperforms all previous generative depth models in terms of spatial\naccuracy and temporal consistency. The code and model weights are open-sourced."
                },
                "authors": [
                    {
                        "name": "Honghui Yang"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Wei Yin"
                    },
                    {
                        "name": "Chunhua Shen"
                    },
                    {
                        "name": "Haifeng Liu"
                    },
                    {
                        "name": "Xiaofei He"
                    },
                    {
                        "name": "Binbin Lin"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Tong He"
                    }
                ],
                "author_detail": {
                    "name": "Tong He"
                },
                "author": "Tong He",
                "arxiv_comment": "Project Page: https://depthanyvideo.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01156v2",
                "updated": "2025-03-12T09:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    11,
                    37,
                    2,
                    71,
                    0
                ],
                "published": "2024-09-02T10:42:30Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    42,
                    30,
                    0,
                    246,
                    0
                ],
                "title": "TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval"
                },
                "summary": "Most text-video retrieval methods utilize the text-image pre-trained models\nlike CLIP as a backbone. These methods process each sampled frame independently\nby the image encoder, resulting in high computational overhead and limiting\npractical deployment. Addressing this, we focus on efficient text-video\nretrieval by tackling two key challenges: 1. From the perspective of trainable\nparameters, current parameter-efficient fine-tuning methods incur high\ninference costs; 2. From the perspective of model complexity, current token\ncompression methods are mainly designed for images to reduce spatial redundancy\nbut overlook temporal redundancy in consecutive frames of a video. To tackle\nthese challenges, we propose Temporal Token Merging (TempMe), a\nparameter-efficient and training-inference efficient text-video retrieval\narchitecture that minimizes trainable parameters and model complexity.\nSpecifically, we introduce a progressive multi-granularity framework. By\ngradually combining neighboring clips, we reduce spatio-temporal redundancy and\nenhance temporal modeling across different frames, leading to improved\nefficiency and performance. Extensive experiments validate the superiority of\nour TempMe. Compared to previous parameter-efficient text-video retrieval\nmethods, TempMe achieves superior performance with just 0.50M trainable\nparameters. It significantly reduces output tokens by 95% and GFLOPs by 51%,\nwhile achieving a 1.8X speedup and a 4.4% R-Sum improvement. With full\nfine-tuning, TempMe achieves a significant 7.9% R-Sum improvement, trains 1.57X\nfaster, and utilizes 75.2% GPU memory usage. The code is available at\nhttps://github.com/LunarShen/TempMe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most text-video retrieval methods utilize the text-image pre-trained models\nlike CLIP as a backbone. These methods process each sampled frame independently\nby the image encoder, resulting in high computational overhead and limiting\npractical deployment. Addressing this, we focus on efficient text-video\nretrieval by tackling two key challenges: 1. From the perspective of trainable\nparameters, current parameter-efficient fine-tuning methods incur high\ninference costs; 2. From the perspective of model complexity, current token\ncompression methods are mainly designed for images to reduce spatial redundancy\nbut overlook temporal redundancy in consecutive frames of a video. To tackle\nthese challenges, we propose Temporal Token Merging (TempMe), a\nparameter-efficient and training-inference efficient text-video retrieval\narchitecture that minimizes trainable parameters and model complexity.\nSpecifically, we introduce a progressive multi-granularity framework. By\ngradually combining neighboring clips, we reduce spatio-temporal redundancy and\nenhance temporal modeling across different frames, leading to improved\nefficiency and performance. Extensive experiments validate the superiority of\nour TempMe. Compared to previous parameter-efficient text-video retrieval\nmethods, TempMe achieves superior performance with just 0.50M trainable\nparameters. It significantly reduces output tokens by 95% and GFLOPs by 51%,\nwhile achieving a 1.8X speedup and a 4.4% R-Sum improvement. With full\nfine-tuning, TempMe achieves a significant 7.9% R-Sum improvement, trains 1.57X\nfaster, and utilizes 75.2% GPU memory usage. The code is available at\nhttps://github.com/LunarShen/TempMe."
                },
                "authors": [
                    {
                        "name": "Leqi Shen"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Pengzhang Liu"
                    },
                    {
                        "name": "Yongjun Bao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02542v2",
                "updated": "2025-03-12T09:02:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    2,
                    44,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-03T16:34:49Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    34,
                    49,
                    1,
                    338,
                    0
                ],
                "title": "Unveiling Concept Attribution in Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Concept Attribution in Diffusion Models"
                },
                "summary": "Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains largely\nblack-box; little do we know about the roles of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize knowledge-storing layers in generative models without showing how\nother layers contribute to the target concept. In this work, we approach\ndiffusion models' interpretability problem from a more general perspective and\npose a question: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. To answer this question, we decompose diffusion models using\ncomponent attribution, systematically unveiling the importance of each\ncomponent (specifically the model parameter) in generating a concept. The\nproposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for\n\\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing\n(positive) components, while interestingly uncovers another type of components\nthat contribute negatively to generating a concept, which is missing in the\nprevious knowledge localization work. Based on this holistic understanding of\ndiffusion models, we introduce two fast, inference-time model editing\nalgorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure\nand CAD-Amplify allows amplification of a generated concept by ablating the\npositive and negative components, respectively, while retaining knowledge of\nother concepts. Extensive experimental results validate the significance of\nboth positive and negative components pinpointed by our framework,\ndemonstrating the potential of providing a complete view of interpreting\ngenerative models. Our code is available\n\\href{https://github.com/mail-research/CAD-attribution4diffusion}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable abilities in generating realistic and\nhigh-quality images from text prompts. However, a trained model remains largely\nblack-box; little do we know about the roles of its components in exhibiting a\nconcept such as objects or styles. Recent works employ causal tracing to\nlocalize knowledge-storing layers in generative models without showing how\nother layers contribute to the target concept. In this work, we approach\ndiffusion models' interpretability problem from a more general perspective and\npose a question: \\textit{``How do model components work jointly to demonstrate\nknowledge?''}. To answer this question, we decompose diffusion models using\ncomponent attribution, systematically unveiling the importance of each\ncomponent (specifically the model parameter) in generating a concept. The\nproposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for\n\\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing\n(positive) components, while interestingly uncovers another type of components\nthat contribute negatively to generating a concept, which is missing in the\nprevious knowledge localization work. Based on this holistic understanding of\ndiffusion models, we introduce two fast, inference-time model editing\nalgorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure\nand CAD-Amplify allows amplification of a generated concept by ablating the\npositive and negative components, respectively, while retaining knowledge of\nother concepts. Extensive experimental results validate the significance of\nboth positive and negative components pinpointed by our framework,\ndemonstrating the potential of providing a complete view of interpreting\ngenerative models. Our code is available\n\\href{https://github.com/mail-research/CAD-attribution4diffusion}{here}."
                },
                "authors": [
                    {
                        "name": "Quang H. Nguyen"
                    },
                    {
                        "name": "Hoang Phan"
                    },
                    {
                        "name": "Khoa D. Doan"
                    }
                ],
                "author_detail": {
                    "name": "Khoa D. Doan"
                },
                "author": "Khoa D. Doan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07641v3",
                "updated": "2025-03-12T08:58:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    58,
                    29,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-11T15:33:06Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    33,
                    6,
                    1,
                    42,
                    0
                ],
                "title": "Distributional Instrumental Variable Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Instrumental Variable Method"
                },
                "summary": "The instrumental variable (IV) approach is commonly used to infer causal\neffects in the presence of unmeasured confounding. Existing methods typically\naim to estimate the mean causal effects, whereas a few other methods focus on\nquantile treatment effects. The aim of this work is to estimate the entire\ninterventional distribution. We propose a method called Distributional\nInstrumental Variable (DIV), which uses generative modelling in a nonlinear IV\nsetting. We establish identifiability of the interventional distribution under\ngeneral assumptions and demonstrate an 'under-identified' case, where DIV can\nidentify the causal effects while two-step least squares fails to. Our\nempirical results show that the DIV method performs well for a broad range of\nsimulated data, exhibiting advantages over existing IV approaches in terms of\nthe identifiability and estimation error of the mean or quantile treatment\neffects. Furthermore, we apply DIV to an economic data set to examine the\ncausal relation between institutional quality and economic development and our\nresults align well with the original study. We also apply DIV to a single-cell\ndata set, where we study the generalizability and stability in predicting gene\nexpression under unseen interventions. The software implementations of DIV are\navailable in R and Python.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The instrumental variable (IV) approach is commonly used to infer causal\neffects in the presence of unmeasured confounding. Existing methods typically\naim to estimate the mean causal effects, whereas a few other methods focus on\nquantile treatment effects. The aim of this work is to estimate the entire\ninterventional distribution. We propose a method called Distributional\nInstrumental Variable (DIV), which uses generative modelling in a nonlinear IV\nsetting. We establish identifiability of the interventional distribution under\ngeneral assumptions and demonstrate an 'under-identified' case, where DIV can\nidentify the causal effects while two-step least squares fails to. Our\nempirical results show that the DIV method performs well for a broad range of\nsimulated data, exhibiting advantages over existing IV approaches in terms of\nthe identifiability and estimation error of the mean or quantile treatment\neffects. Furthermore, we apply DIV to an economic data set to examine the\ncausal relation between institutional quality and economic development and our\nresults align well with the original study. We also apply DIV to a single-cell\ndata set, where we study the generalizability and stability in predicting gene\nexpression under unseen interventions. The software implementations of DIV are\navailable in R and Python."
                },
                "authors": [
                    {
                        "name": "Anastasiia Holovchak"
                    },
                    {
                        "name": "Sorawit Saengkyongam"
                    },
                    {
                        "name": "Nicolai Meinshausen"
                    },
                    {
                        "name": "Xinwei Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xinwei Shen"
                },
                "author": "Xinwei Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06485v2",
                "updated": "2025-03-12T08:57:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    57,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-09T13:35:28Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    35,
                    28,
                    0,
                    344,
                    0
                ],
                "title": "Fourier-enhanced reduced-order surrogate modeling for uncertainty\n  quantification in electric machine design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier-enhanced reduced-order surrogate modeling for uncertainty\n  quantification in electric machine design"
                },
                "summary": "This work proposes a data-driven surrogate modeling framework for\ncost-effectively inferring the torque of a permanent magnet synchronous machine\nunder geometric design variations. The framework is separated into a\nreduced-order modeling and an inference part. Given a dataset of torque\nsignals, each corresponding to a different set of design parameters, torque\ndimension is first reduced by post-processing a discrete Fourier transform and\nkeeping a reduced number of frequency components. This allows to take advantage\nof torque periodicity and preserve physical information contained in the\nfrequency components. Next, a response surface model is computed by means of\nmachine learning regression, which maps the design parameters to the reduced\nfrequency components. The response surface models of choice are polynomial\nchaos expansions, feedforward neural networks, and Gaussian processes. Torque\ninference is performed by evaluating the response surface model for new design\nparameters and then inverting the dimension reduction. Numerical results show\nthat the resulting surrogate models lead to sufficiently accurate torque\npredictions for previously unseen design configurations. The framework is found\nto be significantly advantageous compared to approximating the original (not\nreduced) torque signal directly, as well as slightly advantageous compared to\nusing principal component analysis for dimension reduction. The combination of\ndiscrete Fourier transform-based dimension reduction with Gaussian\nprocess-based response surfaces yields the best-in-class surrogate model for\nthis use case. The surrogate models replace the original, high-fidelity model\nin Monte Carlo-based uncertainty quantification studies, where they provide\naccurate torque statistics estimates at significantly reduced computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work proposes a data-driven surrogate modeling framework for\ncost-effectively inferring the torque of a permanent magnet synchronous machine\nunder geometric design variations. The framework is separated into a\nreduced-order modeling and an inference part. Given a dataset of torque\nsignals, each corresponding to a different set of design parameters, torque\ndimension is first reduced by post-processing a discrete Fourier transform and\nkeeping a reduced number of frequency components. This allows to take advantage\nof torque periodicity and preserve physical information contained in the\nfrequency components. Next, a response surface model is computed by means of\nmachine learning regression, which maps the design parameters to the reduced\nfrequency components. The response surface models of choice are polynomial\nchaos expansions, feedforward neural networks, and Gaussian processes. Torque\ninference is performed by evaluating the response surface model for new design\nparameters and then inverting the dimension reduction. Numerical results show\nthat the resulting surrogate models lead to sufficiently accurate torque\npredictions for previously unseen design configurations. The framework is found\nto be significantly advantageous compared to approximating the original (not\nreduced) torque signal directly, as well as slightly advantageous compared to\nusing principal component analysis for dimension reduction. The combination of\ndiscrete Fourier transform-based dimension reduction with Gaussian\nprocess-based response surfaces yields the best-in-class surrogate model for\nthis use case. The surrogate models replace the original, high-fidelity model\nin Monte Carlo-based uncertainty quantification studies, where they provide\naccurate torque statistics estimates at significantly reduced computational\ncost."
                },
                "authors": [
                    {
                        "name": "Aylar Partovizadeh"
                    },
                    {
                        "name": "Sebastian Schöps"
                    },
                    {
                        "name": "Dimitrios Loukrezis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Loukrezis"
                },
                "author": "Dimitrios Loukrezis",
                "arxiv_doi": "10.1007/s00366-025-02123-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00366-025-02123-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.06485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19844v3",
                "updated": "2025-03-12T08:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    56,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-27T07:39:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    7,
                    39,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "ProAPO: Progressively Automatic Prompt Optimization for Visual\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProAPO: Progressively Automatic Prompt Optimization for Visual\n  Classification"
                },
                "summary": "Vision-language models (VLMs) have made significant progress in image\nclassification by training with large-scale paired image-text data. Their\nperformances largely depend on the prompt quality. While recent methods show\nthat visual descriptions generated by large language models (LLMs) enhance the\ngeneralization of VLMs, class-specific prompts may be inaccurate or lack\ndiscrimination due to the hallucination in LLMs. In this paper, we aim to find\nvisually discriminative prompts for fine-grained categories with minimal\nsupervision and no human-in-the-loop. An evolution-based algorithm is proposed\nto progressively optimize language prompts from task-specific templates to\nclass-specific descriptions. Unlike optimizing templates, the search space\nshows an explosion in class-specific candidate prompts. This increases prompt\ngeneration costs, iterative times, and the overfitting problem. To this end, we\nfirst introduce several simple yet effective edit-based and evolution-based\noperations to generate diverse candidate prompts by one-time query of LLMs.\nThen, two sampling strategies are proposed to find a better initial search\npoint and reduce traversed categories, saving iteration costs. Moreover, we\napply a novel fitness score with entropy constraints to mitigate overfitting.\nIn a challenging one-shot image classification setting, our method outperforms\nexisting textual prompt-based methods and improves LLM-generated description\nmethods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts\nimprove adapter-based methods and transfer effectively across different\nbackbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have made significant progress in image\nclassification by training with large-scale paired image-text data. Their\nperformances largely depend on the prompt quality. While recent methods show\nthat visual descriptions generated by large language models (LLMs) enhance the\ngeneralization of VLMs, class-specific prompts may be inaccurate or lack\ndiscrimination due to the hallucination in LLMs. In this paper, we aim to find\nvisually discriminative prompts for fine-grained categories with minimal\nsupervision and no human-in-the-loop. An evolution-based algorithm is proposed\nto progressively optimize language prompts from task-specific templates to\nclass-specific descriptions. Unlike optimizing templates, the search space\nshows an explosion in class-specific candidate prompts. This increases prompt\ngeneration costs, iterative times, and the overfitting problem. To this end, we\nfirst introduce several simple yet effective edit-based and evolution-based\noperations to generate diverse candidate prompts by one-time query of LLMs.\nThen, two sampling strategies are proposed to find a better initial search\npoint and reduce traversed categories, saving iteration costs. Moreover, we\napply a novel fitness score with entropy constraints to mitigate overfitting.\nIn a challenging one-shot image classification setting, our method outperforms\nexisting textual prompt-based methods and improves LLM-generated description\nmethods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts\nimprove adapter-based methods and transfer effectively across different\nbackbones."
                },
                "authors": [
                    {
                        "name": "Xiangyan Qu"
                    },
                    {
                        "name": "Gaopeng Gou"
                    },
                    {
                        "name": "Jiamin Zhuang"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Kun Song"
                    },
                    {
                        "name": "Qihao Wang"
                    },
                    {
                        "name": "Yili Li"
                    },
                    {
                        "name": "Gang Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Gang Xiong"
                },
                "author": "Gang Xiong",
                "arxiv_comment": "Accepted to the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01478v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01478v4",
                "updated": "2025-03-12T08:49:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    49,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-03T12:37:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    37,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction"
                },
                "summary": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios."
                },
                "authors": [
                    {
                        "name": "Lu Dai"
                    },
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Jinhui Ye"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01478v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01478v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v2",
                "updated": "2025-03-12T08:48:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    48,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "48 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08179v2",
                "updated": "2025-03-12T08:46:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    46,
                    33,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T08:43:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    43,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models"
                },
                "summary": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks."
                },
                "authors": [
                    {
                        "name": "Zicheng Ma"
                    },
                    {
                        "name": "Chuanliu Fan"
                    },
                    {
                        "name": "Zhicong Wang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Xiaohan Lin"
                    },
                    {
                        "name": "Yanheng Li"
                    },
                    {
                        "name": "Shihao Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Yi Qin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Qin Gao"
                },
                "author": "Yi Qin Gao",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.09600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09600v1",
                "updated": "2025-03-12T17:59:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:59:42Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    42,
                    2,
                    71,
                    0
                ],
                "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented\n  Generation System"
                },
                "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system."
                },
                "authors": [
                    {
                        "name": "Jihao Zhao"
                    },
                    {
                        "name": "Zhiyuan Ji"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Hanyu Wang"
                    },
                    {
                        "name": "Simin Niu"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Feiyu Xiong"
                    },
                    {
                        "name": "Zhiyu Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyu Li"
                },
                "author": "Zhiyu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v2",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. Current solutions require\nlarge compute budgets, training data, model weight access, or complex\ntask-specific designs. We introduce PRISM, which processes information as a\nstream of chunks while maintaining a structured in-context memory specified\nwith a typed hierarchical schema. PRISM outperforms baselines on diverse tasks\nwhile using at least 4x shorter contexts than long-context models. This\napproach is token-efficient, producing concise outputs and efficiently\nleveraging key-value (KV) caches to reduce costs by up to 54% compared to\nalternative short-context methods. PRISM scales down to tiny chunks (<500\ntokens) without increasing encoding costs or sacrificing quality, and\ngeneralizes to new tasks with minimal effort by automatically generating\nschemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "28 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09598v1",
                "updated": "2025-03-12T17:59:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:59:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    59,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Protect Yourself from 5G Radiation? Investigating LLM Responses\n  to Implicit Misinformation"
                },
                "summary": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are widely deployed in diverse scenarios, the\nextent to which they could tacitly spread misinformation emerges as a critical\nsafety concern. Current research primarily evaluates LLMs on explicit false\nstatements, overlooking how misinformation often manifests subtly as\nunchallenged premises in real-world user interactions. We curated ECHOMIST, the\nfirst comprehensive benchmark for implicit misinformation, where the\nmisinformed assumptions are embedded in a user query to LLMs. ECHOMIST is based\non rigorous selection criteria and carefully curated data from diverse sources,\nincluding real-world human-AI conversations and social media interactions. We\nalso introduce a new evaluation metric to measure whether LLMs can recognize\nand counter false information rather than amplify users' misconceptions.\nThrough an extensive empirical study on a wide range of LLMs, including GPT-4,\nClaude, and Llama, we find that current models perform alarmingly poorly on\nthis task, often failing to detect false premises and generating misleading\nexplanations. Our findings underscore the critical need for an increased focus\non implicit misinformation in LLM safety research."
                },
                "authors": [
                    {
                        "name": "Ruohao Guo"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Alan Ritter"
                    }
                ],
                "author_detail": {
                    "name": "Alan Ritter"
                },
                "author": "Alan Ritter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09594v1",
                "updated": "2025-03-12T17:58:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    58,
                    6,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:58:06Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    58,
                    6,
                    2,
                    71,
                    0
                ],
                "title": "SimLingo: Vision-Only Closed-Loop Autonomous Driving with\n  Language-Action Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLingo: Vision-Only Closed-Loop Autonomous Driving with\n  Language-Action Alignment"
                },
                "summary": "Integrating large language models (LLMs) into autonomous driving has\nattracted significant attention with the hope of improving generalization and\nexplainability. However, existing methods often focus on either driving or\nvision-language understanding but achieving both high driving performance and\nextensive language understanding remains challenging. In addition, the dominant\napproach to tackle vision-language understanding is using visual question\nanswering. However, for autonomous driving, this is only useful if it is\naligned with the action space. Otherwise, the model's answers could be\ninconsistent with its behavior. Therefore, we propose a model that can handle\nthree different tasks: (1) closed-loop driving, (2) vision-language\nunderstanding, and (3) language-action alignment. Our model SimLingo is based\non a vision language model (VLM) and works using only camera, excluding\nexpensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on\nthe widely used CARLA simulator on the Bench2Drive benchmark and is the winning\nentry at the CARLA challenge 2024. Additionally, we achieve strong results in a\nwide variety of language-related tasks while maintaining high driving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) into autonomous driving has\nattracted significant attention with the hope of improving generalization and\nexplainability. However, existing methods often focus on either driving or\nvision-language understanding but achieving both high driving performance and\nextensive language understanding remains challenging. In addition, the dominant\napproach to tackle vision-language understanding is using visual question\nanswering. However, for autonomous driving, this is only useful if it is\naligned with the action space. Otherwise, the model's answers could be\ninconsistent with its behavior. Therefore, we propose a model that can handle\nthree different tasks: (1) closed-loop driving, (2) vision-language\nunderstanding, and (3) language-action alignment. Our model SimLingo is based\non a vision language model (VLM) and works using only camera, excluding\nexpensive sensors like LiDAR. SimLingo obtains state-of-the-art performance on\nthe widely used CARLA simulator on the Bench2Drive benchmark and is the winning\nentry at the CARLA challenge 2024. Additionally, we achieve strong results in a\nwide variety of language-related tasks while maintaining high driving\nperformance."
                },
                "authors": [
                    {
                        "name": "Katrin Renz"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Elahe Arani"
                    },
                    {
                        "name": "Oleg Sinavski"
                    }
                ],
                "author_detail": {
                    "name": "Oleg Sinavski"
                },
                "author": "Oleg Sinavski",
                "arxiv_comment": "CVPR 2025. 1st Place @ CARLA Challenge 2024. Challenge tech report\n  (preliminary version of SimLingo): arXiv:2406.10165",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09590v1",
                "updated": "2025-03-12T17:57:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:57:32Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    57,
                    32,
                    2,
                    71,
                    0
                ],
                "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BIMBA: Selective-Scan Compression for Long-Range Video Question\n  Answering"
                },
                "summary": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (VQA) in long videos poses the key challenge of\nextracting relevant information and modeling long-range dependencies from many\nredundant frames. The self-attention mechanism provides a general solution for\nsequence modeling, but it has a prohibitive cost when applied to a massive\nnumber of spatiotemporal tokens in long videos. Most prior methods rely on\ncompression strategies to lower the computational cost, such as reducing the\ninput length via sparse frame sampling or compressing the output sequence\npassed to the large language model (LLM) via space-time pooling. However, these\nnaive approaches over-represent redundant information and often miss salient\nevents or fast-occurring space-time patterns. In this work, we introduce BIMBA,\nan efficient state-space model to handle long-form videos. Our model leverages\nthe selective scan algorithm to learn to effectively select critical\ninformation from high-dimensional video and transform it into a reduced token\nsequence for efficient LLM processing. Extensive experiments demonstrate that\nBIMBA achieves state-of-the-art accuracy on multiple long-form VQA benchmarks,\nincluding PerceptionTest, NExT-QA, EgoSchema, VNBench, LongVideoBench, and\nVideo-MME. Code, and models are publicly available at\nhttps://sites.google.com/view/bimba-mllm."
                },
                "authors": [
                    {
                        "name": "Md Mohaiminul Islam"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Huiyu Wang"
                    },
                    {
                        "name": "Gedas Bertasius"
                    },
                    {
                        "name": "Lorenzo Torresani"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Torresani"
                },
                "author": "Lorenzo Torresani",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09579v1",
                "updated": "2025-03-12T17:50:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    50,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:50:42Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    50,
                    42,
                    2,
                    71,
                    0
                ],
                "title": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Optimal Grouped-Query Attention for Long-Context LLMs"
                },
                "summary": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building effective and efficient Transformer-based large language models\n(LLMs) has recently become a research focus, requiring maximizing model\nlanguage capabilities and minimizing training and deployment costs. Existing\nefforts have primarily described complex relationships among model performance,\nparameter size, and data size, as well as searched for the optimal compute\nallocation to train LLMs. However, they overlook the impacts of context length\nand attention head configuration (the number of query and key-value heads in\ngrouped-query attention) on training and inference. In this paper, we\nsystematically compare models with different parameter sizes, context lengths,\nand attention head configurations in terms of model performance, computational\ncost, and memory cost. Then, we extend the existing scaling methods, which are\nbased solely on parameter size and training compute, to guide the construction\nof cost-optimal LLMs during both training and inference. Our quantitative\nscaling studies show that, when processing sufficiently long sequences, a\nlarger model with fewer attention heads can achieve a lower loss while\nincurring lower computational and memory costs. Our findings provide valuable\ninsights for developing practical LLMs, especially in long-context processing\nscenarios. We will publicly release our code and data."
                },
                "authors": [
                    {
                        "name": "Yingfa Chen"
                    },
                    {
                        "name": "Yutong Wu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "16 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09572v1",
                "updated": "2025-03-12T17:40:52Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    40,
                    52,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T17:40:52Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    40,
                    52,
                    2,
                    71,
                    0
                ],
                "title": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plan-and-Act: Improving Planning of Agents for Long-Horizon Tasks"
                },
                "summary": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advancements in enabling\nlanguage agents to tackle simple tasks. However, applying them for complex,\nmulti-step, long-horizon tasks remains a challenge. Recent work have found\nsuccess by separating high-level planning from low-level execution, which\nenables the model to effectively balance high-level planning objectives and\nlow-level execution details. However, generating accurate plans remains\ndifficult since LLMs are not inherently trained for this task. To address this,\nwe propose Plan-and-Act, a novel framework that incorporates explicit planning\ninto LLM-based agents and introduces a scalable method to enhance plan\ngeneration through a novel synthetic data generation method. Plan-and-Act\nconsists of a Planner model which generates structured, high-level plans to\nachieve user goals, and an Executor model that translates these plans into\nenvironment-specific actions. To train the Planner effectively, we introduce a\nsynthetic data generation method that annotates ground-truth trajectories with\nfeasible plans, augmented with diverse and extensive examples to enhance\ngeneralization. We evaluate Plan-and-Act using web navigation as a\nrepresentative long-horizon planning environment, demonstrating a state-of\nthe-art 54% success rate on the WebArena-Lite benchmark."
                },
                "authors": [
                    {
                        "name": "Lutfi Eren Erdogan"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Hiroki Furuta"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00575v2",
                "updated": "2025-03-12T17:21:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    21,
                    45,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-01T21:59:40Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    21,
                    59,
                    40,
                    5,
                    32,
                    0
                ],
                "title": "DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D\n  Visual-Inertial Navigation based on IMU-Vision-Net",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepUKF-VIN: Adaptively-tuned Deep Unscented Kalman Filter for 3D\n  Visual-Inertial Navigation based on IMU-Vision-Net"
                },
                "summary": "This paper addresses the challenge of estimating the orientation, position,\nand velocity of a vehicle operating in three-dimensional (3D) space with six\ndegrees of freedom (6-DoF). A Deep Learning-based Adaptation Mechanism (DLAM)\nis proposed to adaptively tune the noise covariance matrices of Kalman-type\nfilters for the Visual-Inertial Navigation (VIN) problem, leveraging\nIMU-Vision-Net. Subsequently, an adaptively tuned Deep Learning Unscented\nKalman Filter for 3D VIN (DeepUKF-VIN) is introduced to utilize the proposed\nDLAM, thereby robustly estimating key navigation components, including\norientation, position, and linear velocity. The proposed DeepUKF-VIN integrates\ndata from onboard sensors, specifically an inertial measurement unit (IMU) and\nvisual feature points extracted from a camera, and is applicable for GPS-denied\nnavigation. Its quaternion-based design effectively captures navigation\nnonlinearities and avoids the singularities commonly encountered with\nEuler-angle-based filters. Implemented in discrete space, the DeepUKF-VIN\nfacilitates practical filter deployment. The filter's performance is evaluated\nusing real-world data collected from an IMU and a stereo camera at low sampling\nrates. The results demonstrate filter stability and rapid attenuation of\nestimation errors, highlighting its high estimation accuracy. Furthermore,\ncomparative testing against the standard Unscented Kalman Filter (UKF) in two\nscenarios consistently shows superior performance across all navigation\ncomponents, thereby validating the efficacy and robustness of the proposed\nDeepUKF-VIN. Keywords: Deep Learning, Unscented Kalman Filter, Adaptive tuning,\nEstimation, Navigation, Unmanned Aerial Vehicle, Sensor-fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of estimating the orientation, position,\nand velocity of a vehicle operating in three-dimensional (3D) space with six\ndegrees of freedom (6-DoF). A Deep Learning-based Adaptation Mechanism (DLAM)\nis proposed to adaptively tune the noise covariance matrices of Kalman-type\nfilters for the Visual-Inertial Navigation (VIN) problem, leveraging\nIMU-Vision-Net. Subsequently, an adaptively tuned Deep Learning Unscented\nKalman Filter for 3D VIN (DeepUKF-VIN) is introduced to utilize the proposed\nDLAM, thereby robustly estimating key navigation components, including\norientation, position, and linear velocity. The proposed DeepUKF-VIN integrates\ndata from onboard sensors, specifically an inertial measurement unit (IMU) and\nvisual feature points extracted from a camera, and is applicable for GPS-denied\nnavigation. Its quaternion-based design effectively captures navigation\nnonlinearities and avoids the singularities commonly encountered with\nEuler-angle-based filters. Implemented in discrete space, the DeepUKF-VIN\nfacilitates practical filter deployment. The filter's performance is evaluated\nusing real-world data collected from an IMU and a stereo camera at low sampling\nrates. The results demonstrate filter stability and rapid attenuation of\nestimation errors, highlighting its high estimation accuracy. Furthermore,\ncomparative testing against the standard Unscented Kalman Filter (UKF) in two\nscenarios consistently shows superior performance across all navigation\ncomponents, thereby validating the efficacy and robustness of the proposed\nDeepUKF-VIN. Keywords: Deep Learning, Unscented Kalman Filter, Adaptive tuning,\nEstimation, Navigation, Unmanned Aerial Vehicle, Sensor-fusion."
                },
                "authors": [
                    {
                        "name": "Khashayar Ghanizadegan"
                    },
                    {
                        "name": "Hashim A. Hashim"
                    }
                ],
                "author_detail": {
                    "name": "Hashim A. Hashim"
                },
                "author": "Hashim A. Hashim",
                "arxiv_doi": "10.1016/j.eswa.2025.126656",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2025.126656",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.00575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08677v2",
                "updated": "2025-03-12T17:05:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    5,
                    47,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T17:55:27Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    17,
                    55,
                    27,
                    1,
                    70,
                    0
                ],
                "title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniPaint: Mastering Object-Oriented Editing via Disentangled\n  Insertion-Removal Inpainting"
                },
                "summary": "Diffusion-based generative models have revolutionized object-oriented image\nediting, yet their deployment in realistic object removal and insertion remains\nhampered by challenges such as the intricate interplay of physical effects and\ninsufficient paired training data. In this work, we introduce OmniPaint, a\nunified framework that re-conceptualizes object removal and insertion as\ninterdependent processes rather than isolated tasks. Leveraging a pre-trained\ndiffusion prior along with a progressive training pipeline comprising initial\npaired sample optimization and subsequent large-scale unpaired refinement via\nCycleFlow, OmniPaint achieves precise foreground elimination and seamless\nobject insertion while faithfully preserving scene geometry and intrinsic\nproperties. Furthermore, our novel CFD metric offers a robust, reference-free\nevaluation of context consistency and object hallucination, establishing a new\nbenchmark for high-fidelity image editing. Project page:\nhttps://yeates.github.io/OmniPaint-Page/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based generative models have revolutionized object-oriented image\nediting, yet their deployment in realistic object removal and insertion remains\nhampered by challenges such as the intricate interplay of physical effects and\ninsufficient paired training data. In this work, we introduce OmniPaint, a\nunified framework that re-conceptualizes object removal and insertion as\ninterdependent processes rather than isolated tasks. Leveraging a pre-trained\ndiffusion prior along with a progressive training pipeline comprising initial\npaired sample optimization and subsequent large-scale unpaired refinement via\nCycleFlow, OmniPaint achieves precise foreground elimination and seamless\nobject insertion while faithfully preserving scene geometry and intrinsic\nproperties. Furthermore, our novel CFD metric offers a robust, reference-free\nevaluation of context consistency and object hallucination, establishing a new\nbenchmark for high-fidelity image editing. Project page:\nhttps://yeates.github.io/OmniPaint-Page/"
                },
                "authors": [
                    {
                        "name": "Yongsheng Yu"
                    },
                    {
                        "name": "Ziyun Zeng"
                    },
                    {
                        "name": "Haitian Zheng"
                    },
                    {
                        "name": "Jiebo Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jiebo Luo"
                },
                "author": "Jiebo Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09533v2",
                "updated": "2025-03-13T05:54:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    54,
                    22,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T16:49:56Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    49,
                    56,
                    2,
                    71,
                    0
                ],
                "title": "Large Language Models for Multi-Facility Location Mechanism Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Multi-Facility Location Mechanism Design"
                },
                "summary": "Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing strategyproof mechanisms for multi-facility location that optimize\nsocial costs based on agent preferences had been challenging due to the\nextensive domain knowledge required and poor worst-case guarantees. Recently,\ndeep learning models have been proposed as alternatives. However, these models\nrequire some domain knowledge and extensive hyperparameter tuning as well as\nlacking interpretability, which is crucial in practice when transparency of the\nlearned mechanisms is mandatory. In this paper, we introduce a novel approach,\nnamed LLMMech, that addresses these limitations by incorporating large language\nmodels (LLMs) into an evolutionary framework for generating interpretable,\nhyperparameter-free, empirically strategyproof, and nearly optimal mechanisms.\nOur experimental results, evaluated on various problem settings where the\nsocial cost is arbitrarily weighted across agents and the agent preferences may\nnot be uniformly distributed, demonstrate that the LLM-generated mechanisms\ngenerally outperform existing handcrafted baselines and deep learning models.\nFurthermore, the mechanisms exhibit impressive generalizability to\nout-of-distribution agent preferences and to larger instances with more agents."
                },
                "authors": [
                    {
                        "name": "Nguyen Thach"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Houyu Zhou"
                    },
                    {
                        "name": "Hau Chan"
                    }
                ],
                "author_detail": {
                    "name": "Hau Chan"
                },
                "author": "Hau Chan",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01228v2",
                "updated": "2025-03-12T16:29:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    29,
                    28,
                    2,
                    71,
                    0
                ],
                "published": "2024-11-02T12:32:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    12,
                    32,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "The Interaction Layer: An Exploration for Co-Designing User-LLM\n  Interactions in Parental Wellbeing Support Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Interaction Layer: An Exploration for Co-Designing User-LLM\n  Interactions in Parental Wellbeing Support Systems"
                },
                "summary": "Parenting brings emotional and physical challenges, from balancing work,\nchildcare, and finances to coping with exhaustion and limited personal time.\nYet, one in three parents never seek support. AI systems potentially offer\nstigma-free, accessible, and affordable solutions. Yet, user adoption often\nfails due to issues with explainability and reliability. To see if these issues\ncould be solved using a co-design approach, we developed and tested NurtureBot,\na wellbeing support assistant for new parents. 32 parents co-designed the\nsystem through Asynchronous Remote Communities method, identifying the key\nchallenge as achieving a \"successful chat.\" As part of co-design, parents\nrole-played as NurtureBot, rewriting its dialogues to improve user\nunderstanding, control, and outcomes. The refined prototype, featuring an\nInteraction Layer, was evaluated by 32 initial and 46 new parents, showing\nimproved user experience and usability, with final CUQ score of 91.3/100,\ndemonstrating successful interaction patterns. Our process revealed useful\ninteraction design lessons for effective AI parenting support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parenting brings emotional and physical challenges, from balancing work,\nchildcare, and finances to coping with exhaustion and limited personal time.\nYet, one in three parents never seek support. AI systems potentially offer\nstigma-free, accessible, and affordable solutions. Yet, user adoption often\nfails due to issues with explainability and reliability. To see if these issues\ncould be solved using a co-design approach, we developed and tested NurtureBot,\na wellbeing support assistant for new parents. 32 parents co-designed the\nsystem through Asynchronous Remote Communities method, identifying the key\nchallenge as achieving a \"successful chat.\" As part of co-design, parents\nrole-played as NurtureBot, rewriting its dialogues to improve user\nunderstanding, control, and outcomes. The refined prototype, featuring an\nInteraction Layer, was evaluated by 32 initial and 46 new parents, showing\nimproved user experience and usability, with final CUQ score of 91.3/100,\ndemonstrating successful interaction patterns. Our process revealed useful\ninteraction design lessons for effective AI parenting support."
                },
                "authors": [
                    {
                        "name": "Sruthi Viswanathan"
                    },
                    {
                        "name": "Seray Ibrahim"
                    },
                    {
                        "name": "Ravi Shankar"
                    },
                    {
                        "name": "Reuben Binns"
                    },
                    {
                        "name": "Max Van Kleek"
                    },
                    {
                        "name": "Petr Slovak"
                    }
                ],
                "author_detail": {
                    "name": "Petr Slovak"
                },
                "author": "Petr Slovak",
                "arxiv_doi": "10.1145/3706598.3714088",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3714088",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18798v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18798v3",
                "updated": "2025-03-12T16:27:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    27,
                    59,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-26T04:10:18Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    4,
                    10,
                    18,
                    2,
                    57,
                    0
                ],
                "title": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ANPMI: Assessing the True Comprehension Capabilities of LLMs for\n  Multiple Choice Questions"
                },
                "summary": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice benchmarks, consisting of various prompts and choices, are\namong the most widely used methods to assess a language model's natural\nlanguage understanding capability. Given a specific prompt, we typically\ncompute $P(Choice|Prompt)$ to evaluate how likely a language model is to\ngenerate the correct choice compared to incorrect ones. However, we observe\nthat performance measured using this approach reflects not only the model's\ncomprehension of the prompt but also its inherent biases for certain choices\nregardless of the prompt. This issue makes it challenging to accurately measure\na model's natural language understanding, as models may select the answer\nwithout fully understanding the prompt. To address this limitation, we propose\na novel metric called ANPMI, which normalizes Pointwise Mutual Information\n(PMI) by $-\\log P(Choice)$. ANPMI provides a more accurate assessment of the\nmodel's natural language understanding by ensuring that it is challenging to\nanswer a question without properly understanding the prompt."
                },
                "authors": [
                    {
                        "name": "Gyeongje Cho"
                    },
                    {
                        "name": "Yeonkyoung So"
                    },
                    {
                        "name": "Jaejin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaejin Lee"
                },
                "author": "Jaejin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18798v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18798v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09516v1",
                "updated": "2025-03-12T16:26:39Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    26,
                    39,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:26:39Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    26,
                    39,
                    2,
                    71,
                    0
                ],
                "title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-R1: Training LLMs to Reason and Leverage Search Engines with\n  Reinforcement Learning"
                },
                "summary": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently acquiring external knowledge and up-to-date information is\nessential for effective reasoning and text generation in large language models\n(LLMs). Retrieval augmentation and tool-use training approaches where a search\nengine is treated as a tool lack complex multi-turn retrieval flexibility or\nrequire large-scale supervised data. Prompting advanced LLMs with reasoning\ncapabilities during inference to use search engines is not optimal, since the\nLLM does not learn how to optimally interact with the search engine. This paper\nintroduces Search-R1, an extension of the DeepSeek-R1 model where the LLM\nlearns -- solely through reinforcement learning (RL) -- to autonomously\ngenerate (multiple) search queries during step-by-step reasoning with real-time\nretrieval. Search-R1 optimizes LLM rollouts with multi-turn search\ninteractions, leveraging retrieved token masking for stable RL training and a\nsimple outcome-based reward function. Experiments on seven question-answering\ndatasets show that Search-R1 improves performance by 26% (Qwen2.5-7B), 21%\n(Qwen2.5-3B), and 10% (LLaMA3.2-3B) over SOTA baselines. This paper further\nprovides empirical insights into RL optimization methods, LLM choices, and\nresponse length dynamics in retrieval-augmented reasoning. The code and model\ncheckpoints are available at https://github.com/PeterGriffinJin/Search-R1."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Hansi Zeng"
                    },
                    {
                        "name": "Zhenrui Yue"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09509v1",
                "updated": "2025-03-12T16:18:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    18,
                    45,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:18:45Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    18,
                    45,
                    2,
                    71,
                    0
                ],
                "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba"
                },
                "summary": "Visual Mamba networks (ViMs) extend the selective space state model (Mamba)\nto various vision tasks and demonstrate significant potential. Vector\nquantization (VQ), on the other hand, decomposes network weights into codebooks\nand assignments, significantly reducing memory usage and computational latency\nto enable ViMs deployment on edge devices. Although existing VQ methods have\nachieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in\nconvolutional neural networks and Transformer-based networks, directly applying\nthese methods to ViMs results in unsatisfactory accuracy. We identify several\nkey challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous\noutliers, significantly amplifying quantization errors. 2) When applied to\nViMs, the latest VQ methods suffer from excessive memory consumption, lengthy\ncalibration procedures, and suboptimal performance in the search for optimal\ncodewords. In this paper, we propose ViM-VQ, an efficient post-training vector\nquantization method tailored for ViMs. ViM-VQ consists of two innovative\ncomponents: 1) a fast convex combination optimization algorithm that\nefficiently updates both the convex combinations and the convex hulls to search\nfor optimal codewords, and 2) an incremental vector quantization strategy that\nincrementally confirms optimal codewords to mitigate truncation errors.\nExperimental results demonstrate that ViM-VQ achieves state-of-the-art\nperformance in low-bit quantization across various visual tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Mamba networks (ViMs) extend the selective space state model (Mamba)\nto various vision tasks and demonstrate significant potential. Vector\nquantization (VQ), on the other hand, decomposes network weights into codebooks\nand assignments, significantly reducing memory usage and computational latency\nto enable ViMs deployment on edge devices. Although existing VQ methods have\nachieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in\nconvolutional neural networks and Transformer-based networks, directly applying\nthese methods to ViMs results in unsatisfactory accuracy. We identify several\nkey challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous\noutliers, significantly amplifying quantization errors. 2) When applied to\nViMs, the latest VQ methods suffer from excessive memory consumption, lengthy\ncalibration procedures, and suboptimal performance in the search for optimal\ncodewords. In this paper, we propose ViM-VQ, an efficient post-training vector\nquantization method tailored for ViMs. ViM-VQ consists of two innovative\ncomponents: 1) a fast convex combination optimization algorithm that\nefficiently updates both the convex combinations and the convex hulls to search\nfor optimal codewords, and 2) an incremental vector quantization strategy that\nincrementally confirms optimal codewords to mitigate truncation errors.\nExperimental results demonstrate that ViM-VQ achieves state-of-the-art\nperformance in low-bit quantization across various visual tasks."
                },
                "authors": [
                    {
                        "name": "Juncan Deng"
                    },
                    {
                        "name": "Shuaiting Li"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Kedong Xu"
                    },
                    {
                        "name": "Hong Gu"
                    },
                    {
                        "name": "Kejie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kejie Huang"
                },
                "author": "Kejie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09501v1",
                "updated": "2025-03-12T16:05:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    5,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T16:05:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    16,
                    5,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement\n  Learning"
                },
                "summary": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on Reasoning of Large Language Models (LLMs) has sought to\nfurther enhance their performance by integrating meta-thinking -- enabling\nmodels to monitor, evaluate, and control their reasoning processes for more\nadaptive and effective problem-solving. However, current single-agent work\nlacks a specialized design for acquiring meta-thinking, resulting in low\nefficacy. To address this challenge, we introduce Reinforced Meta-thinking\nAgents (ReMA), a novel framework that leverages Multi-Agent Reinforcement\nLearning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think\nabout thinking. ReMA decouples the reasoning process into two hierarchical\nagents: a high-level meta-thinking agent responsible for generating strategic\noversight and plans, and a low-level reasoning agent for detailed executions.\nThrough iterative reinforcement learning with aligned objectives, these agents\nexplore and learn collaboration, leading to improved generalization and\nrobustness. Experimental results demonstrate that ReMA outperforms single-agent\nRL baselines on complex reasoning tasks, including competitive-level\nmathematical benchmarks and LLM-as-a-Judge benchmarks. Comprehensive ablation\nstudies further illustrate the evolving dynamics of each distinct agent,\nproviding valuable insights into how the meta-thinking reasoning process\nenhances the reasoning capabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Ziyu Wan"
                    },
                    {
                        "name": "Yunxiang Li"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Hanjing Wang"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Mark Schmidt"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Ying Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wen"
                },
                "author": "Ying Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20129v2",
                "updated": "2025-03-12T15:47:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    47,
                    8,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-27T14:24:51Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    14,
                    24,
                    51,
                    3,
                    58,
                    0
                ],
                "title": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite State Automata Inside Transformers with Chain-of-Thought: A\n  Mechanistic Study on State Tracking"
                },
                "summary": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) significantly enhances the performance of large\nlanguage models (LLMs) across a wide range of tasks, and prior research shows\nthat CoT can theoretically increase expressiveness. However, there is limited\nmechanistic understanding of the algorithms that Transformer+CoT can learn. In\nthis work, we (1) evaluate the state tracking capabilities of Transformer+CoT\nand its variants, confirming the effectiveness of CoT. (2) Next, we identify\nthe circuit, a subset of model components, responsible for tracking the world\nstate, finding that late-layer MLP neurons play a key role. We propose two\nmetrics, compression and distinction, and show that the neuron sets for each\nstate achieve nearly 100% accuracy, providing evidence of an implicit finite\nstate automaton (FSA) embedded within the model. (3) Additionally, we explore\nthree realistic settings: skipping intermediate steps, introducing data noise,\nand testing length generalization. Our results demonstrate that Transformer+CoT\nlearns robust algorithms (FSA), highlighting its resilience in challenging\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Dongming Jin"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09481v1",
                "updated": "2025-03-12T15:36:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    36,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T15:36:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    36,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "BAMBI: Developing Baby Language Models for Italian",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BAMBI: Developing Baby Language Models for Italian"
                },
                "summary": "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents BAMBI (BAby language Models Boostrapped for Italian), a\nseries of Baby Language Models (BabyLMs) trained on data that mimic the\nlinguistic input received by a five-year-old Italian-speaking child. The BAMBI\nmodels are tested using a benchmark specifically designed to evaluate language\nmodels, which takes into account the amount of training input the models\nreceived. The BAMBI models are compared against a large language model (LLM)\nand a multimodal language model (VLM) to study the contribution of\nextralinguistic information for language acquisition. The results of our\nevaluation align with the existing literature on English language models,\nconfirming that while reduced training data support the development of\nrelatively robust syntactic competence, they are insufficient for fostering\nsemantic understanding. However, the gap between the training resources (data\nand computation) of the BAMBI models and the LLMs is not fully reflected in\ntheir performance: despite LLMs' massive training, their performance is not\nmuch better than that of BAMBI models. This suggests that strategies beyond\nscaling training resources, such as data curation, inclusion of multimodal\ninput, and other training strategies such as curriculum learning, could play a\ncrucial role in shaping model performance."
                },
                "authors": [
                    {
                        "name": "Alice Suozzi"
                    },
                    {
                        "name": "Luca Capone"
                    },
                    {
                        "name": "Gianluca E. Lebani"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_comment": "20 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05891v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05891v3",
                "updated": "2025-03-12T15:02:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    2,
                    43,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-07T19:24:59Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    19,
                    24,
                    59,
                    4,
                    66,
                    0
                ],
                "title": "MastermindEval: A Simple But Scalable Reasoning Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MastermindEval: A Simple But Scalable Reasoning Benchmark"
                },
                "summary": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have led to remarkable\nperformance across a wide range of language understanding and mathematical\ntasks. As a result, increasing attention has been given to assessing the true\nreasoning capabilities of LLMs, driving research into commonsense, numerical,\nlogical, and qualitative reasoning. However, with the rapid progress of\nreasoning-focused models such as OpenAI's o1 and DeepSeek's R1, there has been\na growing demand for reasoning benchmarks that can keep pace with ongoing model\ndevelopments. In this paper, we introduce MastermindEval, a simple, scalable,\nand interpretable deductive reasoning benchmark inspired by the board game\nMastermind. Our benchmark supports two evaluation paradigms: (1) agentic\nevaluation, in which the model autonomously plays the game, and (2) deductive\nreasoning evaluation, in which the model is given a pre-played game state with\nonly one possible valid code to infer. In our experimental results we (1) find\nthat even easy Mastermind instances are difficult for current models and (2)\ndemonstrate that the benchmark is scalable to possibly more advanced models in\nthe future Furthermore, we investigate possible reasons why models cannot\ndeduce the final solution and find that current models are limited in deducing\nthe concealed code as the number of statement to combine information from is\nincreasing."
                },
                "authors": [
                    {
                        "name": "Jonas Golde"
                    },
                    {
                        "name": "Patrick Haller"
                    },
                    {
                        "name": "Fabio Barth"
                    },
                    {
                        "name": "Alan Akbik"
                    }
                ],
                "author_detail": {
                    "name": "Alan Akbik"
                },
                "author": "Alan Akbik",
                "arxiv_comment": "9 pages, 2 figures, 4 tables. In: ICLR 2025 Workshop on Reasoning and\n  Planning for Large Language Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05891v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05891v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09454v1",
                "updated": "2025-03-12T14:57:08Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:57:08Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    57,
                    8,
                    2,
                    71,
                    0
                ],
                "title": "Explicit Learning and the LLM in Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explicit Learning and the LLM in Machine Translation"
                },
                "summary": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the capacity of large language models (LLMs) for explicit\nlearning, a process involving the assimilation of metalinguistic explanations\nto carry out language tasks. Using constructed languages generated by\ncryptographic means as controlled test environments, we designed experiments to\nassess an LLM's ability to explicitly learn and apply grammar rules. Our\nresults demonstrate that while LLMs possess a measurable capacity for explicit\nlearning, this ability diminishes as the complexity of the linguistic phenomena\nat hand increases. Supervised fine-tuning on chains of thought significantly\nenhances LLM performance but struggles to generalize to typologically novel or\nmore complex linguistic features. These findings point to the need for more\ndiverse training sets and alternative fine-tuning strategies to further improve\nexplicit learning by LLMs."
                },
                "authors": [
                    {
                        "name": "Malik Marmonier"
                    },
                    {
                        "name": "Rachel Bawden"
                    },
                    {
                        "name": "Benoît Sagot"
                    }
                ],
                "author_detail": {
                    "name": "Benoît Sagot"
                },
                "author": "Benoît Sagot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04789v2",
                "updated": "2025-03-12T14:42:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    42,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-28T06:46:53Z",
                "published_parsed": [
                    2025,
                    2,
                    28,
                    6,
                    46,
                    53,
                    4,
                    59,
                    0
                ],
                "title": "Ext2Gen: Alignment through Unified Extraction and Generation for Robust\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ext2Gen: Alignment through Unified Extraction and Generation for Robust\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances LLMs by integrating external\nknowledge, but generation remains fragile due to the uncertain placement of\nrelevant chunks and retrieval-induced information overload, leading to\nhallucinations. We propose Ext2Gen, a novel extract-then-generate model that\nenhances RAG robustness by first extracting query-relevant sentences before\ngenerating answers. To optimize this model, we employ preference alignment\nthrough pairwise feedback learning, enabling the model to generate robust\nanswers regardless of variations in retrieval results. Extensive experiments\ndemonstrate that Ext2Gen effectively identifies query-relevant sentences with\nhigh precision and recall, leading to highly reliable answers. Furthermore,\ndeploying our model in a RAG environment reveals that it not only boosts the\nperformance of the base LLM but also synergizes with advanced retrieval\nstrategies like query expansion. The model is available at\nhttps://huggingface.co/DISLab/Ext2Gen-8B-R2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances LLMs by integrating external\nknowledge, but generation remains fragile due to the uncertain placement of\nrelevant chunks and retrieval-induced information overload, leading to\nhallucinations. We propose Ext2Gen, a novel extract-then-generate model that\nenhances RAG robustness by first extracting query-relevant sentences before\ngenerating answers. To optimize this model, we employ preference alignment\nthrough pairwise feedback learning, enabling the model to generate robust\nanswers regardless of variations in retrieval results. Extensive experiments\ndemonstrate that Ext2Gen effectively identifies query-relevant sentences with\nhigh precision and recall, leading to highly reliable answers. Furthermore,\ndeploying our model in a RAG environment reveals that it not only boosts the\nperformance of the base LLM but also synergizes with advanced retrieval\nstrategies like query expansion. The model is available at\nhttps://huggingface.co/DISLab/Ext2Gen-8B-R2."
                },
                "authors": [
                    {
                        "name": "Hwanjun Song"
                    },
                    {
                        "name": "Jeonghwan Choi"
                    },
                    {
                        "name": "Minseok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Minseok Kim"
                },
                "author": "Minseok Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09436v1",
                "updated": "2025-03-12T14:31:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    31,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:31:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    31,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "PromptMap: An Alternative Interaction Style for AI-Based Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMap: An Alternative Interaction Style for AI-Based Image\n  Generation"
                },
                "summary": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technological advances popularized the use of image generation among\nthe general public. Crafting effective prompts can, however, be difficult for\nnovice users. To tackle this challenge, we developed PromptMap, a new\ninteraction style for text-to-image AI that allows users to freely explore a\nvast collection of synthetic prompts through a map-like view with semantic\nzoom. PromptMap groups images visually by their semantic similarity, allowing\nusers to discover relevant examples. We evaluated PromptMap in a\nbetween-subject online study ($n=60$) and a qualitative within-subject study\n($n=12$). We found that PromptMap supported users in crafting prompts by\nproviding them with examples. We also demonstrated the feasibility of using\nLLMs to create vast example collections. Our work contributes a new interaction\nstyle that supports users unfamiliar with prompting in achieving a satisfactory\nimage output."
                },
                "authors": [
                    {
                        "name": "Krzysztof Adamkiewicz"
                    },
                    {
                        "name": "Paweł W. Woźniak"
                    },
                    {
                        "name": "Julia Dominiak"
                    },
                    {
                        "name": "Andrzej Romanowski"
                    },
                    {
                        "name": "Jakob Karolus"
                    },
                    {
                        "name": "Stanislav Frolov"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Frolov"
                },
                "author": "Stanislav Frolov",
                "arxiv_doi": "10.1145/3708359.3712150",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708359.3712150",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To be published in the proceedings of 30th International Conference\n  on Intelligent User Interfaces (IUI '25), March 24-27, 2025, Cagliari, Italy",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09433v1",
                "updated": "2025-03-12T14:30:05Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T14:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    30,
                    5,
                    2,
                    71,
                    0
                ],
                "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards\n  CWE Detection"
                },
                "summary": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying vulnerabilities in source code is crucial, especially in critical\nsoftware components. Existing methods such as static analysis, dynamic\nanalysis, formal verification, and recently Large Language Models are widely\nused to detect security flaws. This paper introduces CASTLE (CWE Automated\nSecurity Testing and Low-Level Evaluation), a benchmarking framework for\nevaluating the vulnerability detection capabilities of different methods. We\nassess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using\na hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs.\nWe propose the CASTLE Score, a novel evaluation metric to ensure fair\ncomparison. Our results reveal key differences: ESBMC (a formal verification\ntool) minimizes false positives but struggles with vulnerabilities beyond model\nchecking, such as weak cryptography or SQL injection. Static analyzers suffer\nfrom high false positives, increasing manual validation efforts for developers.\nLLMs perform exceptionally well in the CASTLE dataset when identifying\nvulnerabilities in small code snippets. However, their accuracy declines, and\nhallucinations increase as the code size grows. These results suggest that LLMs\ncould play a pivotal role in future security solutions, particularly within\ncode completion frameworks, where they can provide real-time guidance to\nprevent vulnerabilities. The dataset is accessible at\nhttps://github.com/CASTLE-Benchmark."
                },
                "authors": [
                    {
                        "name": "Richard A. Dubniczky"
                    },
                    {
                        "name": "Krisztofer Zoltán Horvát"
                    },
                    {
                        "name": "Tamás Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    },
                    {
                        "name": "Norbert Tihanyi"
                    }
                ],
                "author_detail": {
                    "name": "Norbert Tihanyi"
                },
                "author": "Norbert Tihanyi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19482v2",
                "updated": "2025-03-12T14:25:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    14,
                    25,
                    10,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-25T11:37:04Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    11,
                    37,
                    4,
                    4,
                    299,
                    0
                ],
                "title": "Measuring memorization in language models via probabilistic extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring memorization in language models via probabilistic extraction"
                },
                "summary": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to memorizing training data,\nraising concerns about the potential extraction of sensitive information at\ngeneration time. Discoverable extraction is the most common method for\nmeasuring this issue: split a training example into a prefix and suffix, then\nprompt the LLM with the prefix, and deem the example extractable if the LLM\ngenerates the matching suffix using greedy sampling. This definition yields a\nyes-or-no determination of whether extraction was successful with respect to a\nsingle query. Though efficient to compute, we show that this definition is\nunreliable because it does not account for non-determinism present in more\nrealistic (non-greedy) sampling schemes, for which LLMs produce a range of\noutputs for the same prompt. We introduce probabilistic discoverable\nextraction, which, without additional cost, relaxes discoverable extraction by\nconsidering multiple queries to quantify the probability of extracting a target\nsequence. We evaluate our probabilistic measure across different models,\nsampling schemes, and training-data repetitions, and find that this measure\nprovides more nuanced information about extraction risk compared to traditional\ndiscoverable extraction."
                },
                "authors": [
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Marika Swanberg"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Itay Yona"
                    },
                    {
                        "name": "Ilia Shumailov"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "A. Feder Cooper"
                    }
                ],
                "author_detail": {
                    "name": "A. Feder Cooper"
                },
                "author": "A. Feder Cooper",
                "arxiv_comment": "NAACL 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09407v1",
                "updated": "2025-03-12T13:58:43Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    58,
                    43,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:58:43Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    58,
                    43,
                    2,
                    71,
                    0
                ],
                "title": "Got Compute, but No Data: Lessons From Post-training a Finnish LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Got Compute, but No Data: Lessons From Post-training a Finnish LLM"
                },
                "summary": "As LLMs gain more popularity as chatbots and general assistants, methods have\nbeen developed to enable LLMs to follow instructions and align with human\npreferences. These methods have found success in the field, but their\neffectiveness has not been demonstrated outside of high-resource languages. In\nthis work, we discuss our experiences in post-training an LLM for\ninstruction-following for English and Finnish. We use a multilingual LLM to\ntranslate instruction and preference datasets from English to Finnish. We\nperform instruction tuning and preference optimization in English and Finnish\nand evaluate the instruction-following capabilities of the model in both\nlanguages. Our results show that with a few hundred Finnish instruction samples\nwe can obtain competitive performance in Finnish instruction-following. We also\nfound that although preference optimization in English offers some\ncross-lingual benefits, we obtain our best results by using preference data\nfrom both languages. We release our model, datasets, and recipes under open\nlicenses at https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs gain more popularity as chatbots and general assistants, methods have\nbeen developed to enable LLMs to follow instructions and align with human\npreferences. These methods have found success in the field, but their\neffectiveness has not been demonstrated outside of high-resource languages. In\nthis work, we discuss our experiences in post-training an LLM for\ninstruction-following for English and Finnish. We use a multilingual LLM to\ntranslate instruction and preference datasets from English to Finnish. We\nperform instruction tuning and preference optimization in English and Finnish\nand evaluate the instruction-following capabilities of the model in both\nlanguages. Our results show that with a few hundred Finnish instruction samples\nwe can obtain competitive performance in Finnish instruction-following. We also\nfound that although preference optimization in English offers some\ncross-lingual benefits, we obtain our best results by using preference data\nfrom both languages. We release our model, datasets, and recipes under open\nlicenses at https://huggingface.co/LumiOpen/Poro-34B-chat-OpenAssistant"
                },
                "authors": [
                    {
                        "name": "Elaine Zosa"
                    },
                    {
                        "name": "Ville Komulainen"
                    },
                    {
                        "name": "Sampo Pyysalo"
                    }
                ],
                "author_detail": {
                    "name": "Sampo Pyysalo"
                },
                "author": "Sampo Pyysalo",
                "arxiv_comment": "7 pages",
                "arxiv_journal_ref": "Proceedings of the Joint 25th Nordic Conference on Computational\n  Linguistics and 11th Baltic Conference on Human Language Technologies\n  (NoDaLiDa/Baltic-HLT 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07923v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07923v3",
                "updated": "2025-03-12T13:48:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    48,
                    12,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-10T21:09:12Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    21,
                    9,
                    12,
                    1,
                    345,
                    0
                ],
                "title": "Asking Again and Again: Exploring LLM Robustness to Repeated Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asking Again and Again: Exploring LLM Robustness to Repeated Questions"
                },
                "summary": "This study investigates whether repeating questions within prompts influences\nthe performance of large language models (LLMs). We hypothesize that\nreiterating a question within a single prompt might enhance the model's focus\non key elements of the query. We evaluate five recent LLMs -- including\nGPT-4o-mini, DeepSeek-V3, and smaller open-source models -- on three reading\ncomprehension datasets under different prompt settings, varying question\nrepetition levels (1, 3, or 5 times per prompt). Our results demonstrate that\nquestion repetition can increase models' accuracy by up to $6\\%$. However,\nacross all models, settings, and datasets, we do not find the result\nstatistically significant. These findings provide insights into prompt design\nand LLM behavior, suggesting that repetition alone does not significantly\nimpact output quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates whether repeating questions within prompts influences\nthe performance of large language models (LLMs). We hypothesize that\nreiterating a question within a single prompt might enhance the model's focus\non key elements of the query. We evaluate five recent LLMs -- including\nGPT-4o-mini, DeepSeek-V3, and smaller open-source models -- on three reading\ncomprehension datasets under different prompt settings, varying question\nrepetition levels (1, 3, or 5 times per prompt). Our results demonstrate that\nquestion repetition can increase models' accuracy by up to $6\\%$. However,\nacross all models, settings, and datasets, we do not find the result\nstatistically significant. These findings provide insights into prompt design\nand LLM behavior, suggesting that repetition alone does not significantly\nimpact output quality."
                },
                "authors": [
                    {
                        "name": "Sagi Shaier"
                    },
                    {
                        "name": "Mario Sanz-Guerrero"
                    },
                    {
                        "name": "Katharina von der Wense"
                    }
                ],
                "author_detail": {
                    "name": "Katharina von der Wense"
                },
                "author": "Katharina von der Wense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07923v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07923v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06552v2",
                "updated": "2025-03-12T13:42:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    42,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-09T10:48:47Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    10,
                    48,
                    47,
                    6,
                    68,
                    0
                ],
                "title": "Multimodal Programming in Computer Science with Interactive Assistance\n  Powered by Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Programming in Computer Science with Interactive Assistance\n  Powered by Large Language Model"
                },
                "summary": "LLM chatbot interfaces allow students to get instant, interactive assistance\nwith homework, but doing so carelessly may not advance educational objectives.\nIn this study, an interactive homework help system based on DeepSeek R1 is\ndeveloped and first implemented for students enrolled in a large computer\nscience beginning programming course. In addition to an assist button in a\nwell-known code editor, our assistant also has a feedback option in our\ncommand-line automatic evaluator. It wraps student work in a personalized\nprompt that advances our educational objectives without offering answers\nstraight away. We have discovered that our assistant can recognize students'\nconceptual difficulties and provide ideas, plans, and template code in\npedagogically appropriate ways. However, among other mistakes, it occasionally\nincorrectly labels the correct student code as incorrect or encourages students\nto use correct-but-lesson-inappropriate approaches, which can lead to long and\nfrustrating journeys for the students. After discussing many development and\ndeployment issues, we provide our conclusions and future actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM chatbot interfaces allow students to get instant, interactive assistance\nwith homework, but doing so carelessly may not advance educational objectives.\nIn this study, an interactive homework help system based on DeepSeek R1 is\ndeveloped and first implemented for students enrolled in a large computer\nscience beginning programming course. In addition to an assist button in a\nwell-known code editor, our assistant also has a feedback option in our\ncommand-line automatic evaluator. It wraps student work in a personalized\nprompt that advances our educational objectives without offering answers\nstraight away. We have discovered that our assistant can recognize students'\nconceptual difficulties and provide ideas, plans, and template code in\npedagogically appropriate ways. However, among other mistakes, it occasionally\nincorrectly labels the correct student code as incorrect or encourages students\nto use correct-but-lesson-inappropriate approaches, which can lead to long and\nfrustrating journeys for the students. After discussing many development and\ndeployment issues, we provide our conclusions and future actions."
                },
                "authors": [
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md. Tanzib Hosain"
                    },
                    {
                        "name": "M. F. Mridha"
                    },
                    {
                        "name": "Salah Uddin Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Salah Uddin Ahmed"
                },
                "author": "Salah Uddin Ahmed",
                "arxiv_comment": "Accepted in Proceedings of the 27th International Conference on.\n  Human-Computer Interaction, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09388v1",
                "updated": "2025-03-12T13:33:07Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    33,
                    7,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:33:07Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    33,
                    7,
                    2,
                    71,
                    0
                ],
                "title": "Evaluating Reinforcement Learning Safety and Trustworthiness in\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Reinforcement Learning Safety and Trustworthiness in\n  Cyber-Physical Systems"
                },
                "summary": "Cyber-Physical Systems (CPS) often leverage Reinforcement Learning (RL)\ntechniques to adapt dynamically to changing environments and optimize\nperformance. However, it is challenging to construct safety cases for RL\ncomponents. We therefore propose the SAFE-RL (Safety and Accountability\nFramework for Evaluating Reinforcement Learning) for supporting the\ndevelopment, validation, and safe deployment of RL-based CPS. We adopt a design\nscience approach to construct the framework and demonstrate its use in three RL\napplications in small Uncrewed Aerial systems (sUAS)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-Physical Systems (CPS) often leverage Reinforcement Learning (RL)\ntechniques to adapt dynamically to changing environments and optimize\nperformance. However, it is challenging to construct safety cases for RL\ncomponents. We therefore propose the SAFE-RL (Safety and Accountability\nFramework for Evaluating Reinforcement Learning) for supporting the\ndevelopment, validation, and safe deployment of RL-based CPS. We adopt a design\nscience approach to construct the framework and demonstrate its use in three RL\napplications in small Uncrewed Aerial systems (sUAS)"
                },
                "authors": [
                    {
                        "name": "Katherine Dearstyne"
                    },
                    {
                        "name": "Pedro"
                    },
                    {
                        "name": "Alarcon Granadeno"
                    },
                    {
                        "name": "Theodore Chambers"
                    },
                    {
                        "name": "Jane Cleland-Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jane Cleland-Huang"
                },
                "arxiv_affiliation": "Tony",
                "author": "Jane Cleland-Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19649v3",
                "updated": "2025-03-12T13:31:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    31,
                    36,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-27T00:40:01Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    0,
                    40,
                    1,
                    3,
                    58,
                    0
                ],
                "title": "Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models"
                },
                "summary": "Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices."
                },
                "authors": [
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Daniel Tan"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Mario Fritz"
                    }
                ],
                "author_detail": {
                    "name": "Mario Fritz"
                },
                "author": "Mario Fritz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09382v1",
                "updated": "2025-03-12T13:28:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    28,
                    23,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:28:23Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    28,
                    23,
                    2,
                    71,
                    0
                ],
                "title": "Towards Next-Generation Recommender Systems: A Benchmark for\n  Personalized Recommendation Assistant with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Next-Generation Recommender Systems: A Benchmark for\n  Personalized Recommendation Assistant with LLMs"
                },
                "summary": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RecSys) are widely used across various modern digital\nplatforms and have garnered significant attention. Traditional recommender\nsystems usually focus only on fixed and simple recommendation scenarios, making\nit difficult to generalize to new and unseen recommendation tasks in an\ninteractive paradigm. Recently, the advancement of large language models (LLMs)\nhas revolutionized the foundational architecture of RecSys, driving their\nevolution into more intelligent and interactive personalized recommendation\nassistants. However, most existing studies rely on fixed task-specific prompt\ntemplates to generate recommendations and evaluate the performance of\npersonalized assistants, which limits the comprehensive assessments of their\ncapabilities. This is because commonly used datasets lack high-quality textual\nuser queries that reflect real-world recommendation scenarios, making them\nunsuitable for evaluating LLM-based personalized recommendation assistants. To\naddress this gap, we introduce RecBench+, a new dataset benchmark designed to\naccess LLMs' ability to handle intricate user recommendation needs in the era\nof LLMs. RecBench+ encompasses a diverse set of queries that span both hard\nconditions and soft preferences, with varying difficulty levels. We evaluated\ncommonly used LLMs on RecBench+ and uncovered below findings: 1) LLMs\ndemonstrate preliminary abilities to act as recommendation assistants, 2) LLMs\nare better at handling queries with explicitly stated conditions, while facing\nchallenges with queries that require reasoning or contain misleading\ninformation. Our dataset has been released at\nhttps://github.com/jiani-huang/RecBench.git."
                },
                "authors": [
                    {
                        "name": "Jiani Huang"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Liang-bo Ning"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07813v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07813v2",
                "updated": "2025-03-12T13:17:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    17,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-08T17:19:43Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    17,
                    19,
                    43,
                    5,
                    39,
                    0
                ],
                "title": "CryptoX : Compositional Reasoning Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CryptoX : Compositional Reasoning Evaluation of Large Language Models"
                },
                "summary": "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The compositional reasoning capacity has long been regarded as critical to\nthe generalization and intelligence emergence of large language models LLMs.\nHowever, despite numerous reasoning-related benchmarks, the compositional\nreasoning capacity of LLMs is rarely studied or quantified in the existing\nbenchmarks. In this paper, we introduce CryptoX, an evaluation framework that,\nfor the first time, combines existing benchmarks and cryptographic, to quantify\nthe compositional reasoning capacity of LLMs. Building upon CryptoX, we\nconstruct CryptoBench, which integrates these principles into several\nbenchmarks for systematic evaluation. We conduct detailed experiments on widely\nused open-source and closed-source LLMs using CryptoBench, revealing a huge gap\nbetween open-source and closed-source LLMs. We further conduct thorough\nmechanical interpretability experiments to reveal the inner mechanism of LLMs'\ncompositional reasoning, involving subproblem decomposition, subproblem\ninference, and summarizing subproblem conclusions. Through analysis based on\nCryptoBench, we highlight the value of independently studying compositional\nreasoning and emphasize the need to enhance the compositional reasoning\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Jiajun Shi"
                    },
                    {
                        "name": "Chaoren Wei"
                    },
                    {
                        "name": "Liqun Yang"
                    },
                    {
                        "name": "Zekun Moore Wang"
                    },
                    {
                        "name": "Chenghao Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Stephen Huang"
                    },
                    {
                        "name": "Tao Peng"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    }
                ],
                "author_detail": {
                    "name": "Zhoufutu Wen"
                },
                "author": "Zhoufutu Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07813v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07813v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09369v1",
                "updated": "2025-03-12T13:16:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    16,
                    0,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:16:00Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    16,
                    0,
                    2,
                    71,
                    0
                ],
                "title": "Task Allocation for Multi-agent Systems via Unequal-dimensional Optimal\n  Transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Allocation for Multi-agent Systems via Unequal-dimensional Optimal\n  Transport"
                },
                "summary": "We consider a probabilistic model for large-scale task allocation problems\nfor multi-agent systems, aiming to determine an optimal deployment strategy\nthat minimizes the overall transport cost. Specifically, we assign\ntransportation agents to delivery tasks with given pick-up and drop-off\nlocations, pairing the spatial distribution of transport resources with the\njoint distribution of task origins and destinations. This aligns with the\noptimal mass transport framework where the problem and is in the\nunequal-dimensional setting. The task allocation problem can be thus seen as a\nlinear programming problem that minimizes a quadratic transport cost\nfunctional, optimizing the energy of all transport units. The problem is\nmotivated by time-sensitive medical deliveries using drones, such as emergency\nequipment and blood transport. In this paper, we establish the existence,\nuniqueness, and smoothness of the optimal solution, and illustrate its\nproperties through numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a probabilistic model for large-scale task allocation problems\nfor multi-agent systems, aiming to determine an optimal deployment strategy\nthat minimizes the overall transport cost. Specifically, we assign\ntransportation agents to delivery tasks with given pick-up and drop-off\nlocations, pairing the spatial distribution of transport resources with the\njoint distribution of task origins and destinations. This aligns with the\noptimal mass transport framework where the problem and is in the\nunequal-dimensional setting. The task allocation problem can be thus seen as a\nlinear programming problem that minimizes a quadratic transport cost\nfunctional, optimizing the energy of all transport units. The problem is\nmotivated by time-sensitive medical deliveries using drones, such as emergency\nequipment and blood transport. In this paper, we establish the existence,\nuniqueness, and smoothness of the optimal solution, and illustrate its\nproperties through numerical simulations."
                },
                "authors": [
                    {
                        "name": "Anqi Dong"
                    },
                    {
                        "name": "Karl H. Johansson"
                    },
                    {
                        "name": "Johan Karlsson"
                    }
                ],
                "author_detail": {
                    "name": "Johan Karlsson"
                },
                "author": "Johan Karlsson",
                "arxiv_comment": "6 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91B32, 93A16, 91B68, 90B06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17448v2",
                "updated": "2025-03-12T13:14:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    14,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-22T21:50:52Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    21,
                    50,
                    52,
                    1,
                    296,
                    0
                ],
                "title": "In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Context Learning and Reasoning for Symbolic Regression with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transformer-based machine learning models\nthat have shown remarkable performance in tasks for which they were not\nexplicitly trained. Here, we explore the potential of LLMs to perform symbolic\nregression -- a machine-learning method for finding simple and accurate\nequations from datasets. We prompt GPT-4 to suggest expressions from data,\nwhich are then optimized and evaluated using external Python tools. These\nresults are fed back to GPT-4, which proposes improved expressions while\noptimizing for complexity and loss. Using chain-of-thought prompting, we\ninstruct GPT-4 to analyze the data, prior expressions, and the scientific\ncontext (expressed in natural language) for each problem before generating new\nexpressions. We evaluated the workflow in rediscovery of five well-known\nscientific equations from experimental data, and on an additional dataset\nwithout a known equation. GPT-4 successfully rediscovered all five equations,\nand in general, performed better when prompted to use a scratchpad and consider\nscientific context. We demonstrate how strategic prompting improves the model's\nperformance and how the natural language interface simplifies integrating\ntheory with data. We also observe how theory can sometimes offset noisy data\nand, in other cases, data can make up for poor context. Although this approach\ndoes not outperform established SR programs where target equations are more\ncomplex, LLMs can nonetheless iterate toward improved solutions while following\ninstructions and incorporating scientific context in natural language."
                },
                "authors": [
                    {
                        "name": "Samiha Sharlin"
                    },
                    {
                        "name": "Tyler R. Josephson"
                    }
                ],
                "author_detail": {
                    "name": "Tyler R. Josephson"
                },
                "author": "Tyler R. Josephson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09358v1",
                "updated": "2025-03-12T13:00:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    57,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:00:57Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    57,
                    2,
                    71,
                    0
                ],
                "title": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image\n  Reports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image\n  Reports"
                },
                "summary": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardization of clinical reports is crucial for improving the quality of\nhealthcare and facilitating data integration. The lack of unified standards,\nincluding format, terminology, and style, is a great challenge in clinical\nfundus diagnostic reports, which increases the difficulty for large language\nmodels (LLMs) to understand the data. To address this, we construct a bilingual\nstandard terminology, containing fundus clinical terms and commonly used\ndescriptions in clinical diagnosis. Then, we establish two models,\nRetSTA-7B-Zero and RetSTA-7B. RetSTA-7B-Zero, fine-tuned on an augmented\ndataset simulating clinical scenarios, demonstrates powerful standardization\nbehaviors. However, it encounters a challenge of limitation to cover a wider\nrange of diseases. To further enhance standardization performance, we build\nRetSTA-7B, which integrates a substantial amount of standardized data generated\nby RetSTA-7B-Zero along with corresponding English data, covering diverse\ncomplex clinical scenarios and achieving report-level standardization for the\nfirst time. Experimental results demonstrate that RetSTA-7B outperforms other\ncompared LLMs in bilingual standardization task, which validates its superior\nperformance and generalizability. The checkpoints are available at\nhttps://github.com/AB-Story/RetSTA-7B."
                },
                "authors": [
                    {
                        "name": "Jiushen Cai"
                    },
                    {
                        "name": "Weihang Zhang"
                    },
                    {
                        "name": "Hanruo Liu"
                    },
                    {
                        "name": "Ningli Wang"
                    },
                    {
                        "name": "Huiqi Li"
                    }
                ],
                "author_detail": {
                    "name": "Huiqi Li"
                },
                "author": "Huiqi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09357v1",
                "updated": "2025-03-12T13:00:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    29,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T13:00:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    0,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Automatic Operator-level Parallelism Planning for Distributed Deep\n  Learning -- A Mixed-Integer Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Operator-level Parallelism Planning for Distributed Deep\n  Learning -- A Mixed-Integer Programming Approach"
                },
                "summary": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the artificial intelligence community advances into the era of large\nmodels with billions of parameters, distributed training and inference have\nbecome essential. While various parallelism strategies-data, model, sequence,\nand pipeline-have been successfully implemented for popular neural networks on\nmain-stream hardware, optimizing the distributed deployment schedule requires\nextensive expertise and manual effort. Further more, while existing frameworks\nwith most simple chain-like structures, they struggle with complex non-linear\narchitectures. Mixture-of-experts and multi-modal models feature intricate MIMO\nand branch-rich topologies that require fine-grained operator-level\nparallelization beyond the capabilities of existing frameworks. We propose\nformulating parallelism planning as a scheduling optimization problem using\nmixed-integer programming. We propose a bi-level solution framework balancing\noptimality with computational efficiency, automatically generating effective\ndistributed plans that capture both the heterogeneous structure of modern\nneural networks and the underlying hardware constraints. In experiments\ncomparing against expert-designed strategies like DeepSeek's DualPipe, our\nframework achieves comparable or superior performance, reducing computational\nbubbles by half under the same memory constraints. The framework's versatility\nextends beyond throughput optimization to incorporate hardware utilization\nmaximization, memory capacity constraints, and other considerations or\npotential strategies. Such capabilities position our solution as both a\nvaluable research tool for exploring optimal parallelization strategies and a\npractical industrial solution for large-scale AI deployment."
                },
                "authors": [
                    {
                        "name": "Ruifeng She"
                    },
                    {
                        "name": "Bowen Pang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zehua Liu"
                    },
                    {
                        "name": "Tao Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhong"
                },
                "author": "Tao Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12464v3",
                "updated": "2025-03-12T12:50:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    50,
                    0,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-16T11:25:13Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    25,
                    13,
                    2,
                    290,
                    0
                ],
                "title": "Exploring LLM Cryptocurrency Trading Through Fact-Subjectivity Aware\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLM Cryptocurrency Trading Through Fact-Subjectivity Aware\n  Reasoning"
                },
                "summary": "While many studies show that more advanced LLMs excel in tasks such as\nmathematics and coding, we observe that in cryptocurrency trading, stronger\nLLMs sometimes underperform compared to weaker ones. To investigate this\ncounterintuitive phenomenon, we examine how LLMs reason when making trading\ndecisions. Our findings reveal that (1) stronger LLMs show a preference for\nfactual information over subjectivity; (2) separating the reasoning process\ninto factual and subjective components leads to higher profits. Building on\nthese insights, we propose a multi-agent framework, FS-ReasoningAgent, which\nenables LLMs to recognize and learn from both factual and subjective reasoning.\nExtensive experiments demonstrate that this fine-grained reasoning approach\nenhances LLM trading performance in cryptocurrency markets, yielding profit\nimprovements of 7\\% in BTC, 2\\% in ETH, and 10\\% in SOL. Additionally, an\nablation study reveals that relying on subjective news generates higher returns\nin bull markets, while focusing on factual information yields better results in\nbear markets. Code is available at\nhttps://github.com/Persdre/FS-ReasoningAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many studies show that more advanced LLMs excel in tasks such as\nmathematics and coding, we observe that in cryptocurrency trading, stronger\nLLMs sometimes underperform compared to weaker ones. To investigate this\ncounterintuitive phenomenon, we examine how LLMs reason when making trading\ndecisions. Our findings reveal that (1) stronger LLMs show a preference for\nfactual information over subjectivity; (2) separating the reasoning process\ninto factual and subjective components leads to higher profits. Building on\nthese insights, we propose a multi-agent framework, FS-ReasoningAgent, which\nenables LLMs to recognize and learn from both factual and subjective reasoning.\nExtensive experiments demonstrate that this fine-grained reasoning approach\nenhances LLM trading performance in cryptocurrency markets, yielding profit\nimprovements of 7\\% in BTC, 2\\% in ETH, and 10\\% in SOL. Additionally, an\nablation study reveals that relying on subjective news generates higher returns\nin bull markets, while focusing on factual information yields better results in\nbear markets. Code is available at\nhttps://github.com/Persdre/FS-ReasoningAgent."
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yuchen Gao"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Bingqiao Luo"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Accepted at ICLR 2025 Financial AI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09347v1",
                "updated": "2025-03-12T12:49:02Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:49:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "arxiv_comment": "8 pages, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09341v1",
                "updated": "2025-03-12T12:36:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    36,
                    45,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:36:45Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    36,
                    45,
                    2,
                    71,
                    0
                ],
                "title": "An Evaluation of LLMs for Detecting Harmful Computing Terms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Evaluation of LLMs for Detecting Harmful Computing Terms"
                },
                "summary": "Detecting harmful and non-inclusive terminology in technical contexts is\ncritical for fostering inclusive environments in computing. This study explores\nthe impact of model architecture on harmful language detection by evaluating a\ncurated database of technical terms, each paired with specific use cases. We\ntested a range of encoder, decoder, and encoder-decoder language models,\nincluding BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0,\nGPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. Each model was\npresented with a standardized prompt to identify harmful and non-inclusive\nlanguage across 64 terms. Results reveal that decoder models, particularly\nGemini Flash 2.0 and Claude AI, excel in nuanced contextual analysis, while\nencoder models like BERT exhibit strong pattern recognition but struggle with\nclassification certainty. We discuss the implications of these findings for\nimproving automated detection tools and highlight model-specific strengths and\nlimitations in fostering inclusive communication in technical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting harmful and non-inclusive terminology in technical contexts is\ncritical for fostering inclusive environments in computing. This study explores\nthe impact of model architecture on harmful language detection by evaluating a\ncurated database of technical terms, each paired with specific use cases. We\ntested a range of encoder, decoder, and encoder-decoder language models,\nincluding BERT-base-uncased, RoBERTa large-mnli, Gemini Flash 1.5 and 2.0,\nGPT-4, Claude AI Sonnet 3.5, T5-large, and BART-large-mnli. Each model was\npresented with a standardized prompt to identify harmful and non-inclusive\nlanguage across 64 terms. Results reveal that decoder models, particularly\nGemini Flash 2.0 and Claude AI, excel in nuanced contextual analysis, while\nencoder models like BERT exhibit strong pattern recognition but struggle with\nclassification certainty. We discuss the implications of these findings for\nimproving automated detection tools and highlight model-specific strengths and\nlimitations in fostering inclusive communication in technical domains."
                },
                "authors": [
                    {
                        "name": "Joshua Jacas"
                    },
                    {
                        "name": "Hana Winchester"
                    },
                    {
                        "name": "Alicia Boyd"
                    },
                    {
                        "name": "Brittany Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Brittany Johnson"
                },
                "author": "Brittany Johnson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09335v1",
                "updated": "2025-03-12T12:30:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    30,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:30:18Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    30,
                    18,
                    2,
                    71,
                    0
                ],
                "title": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot\n  Interaction via Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVP-HRI: Zero Shot Natural Voice and Posture-based Human-Robot\n  Interaction via Large Language Model"
                },
                "summary": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Human-Robot Interaction (HRI) is crucial for future service robots\nin aging societies. Existing solutions are biased toward only well-trained\nobjects, creating a gap when dealing with new objects. Currently, HRI systems\nusing predefined gestures or language tokens for pretrained objects pose\nchallenges for all individuals, especially elderly ones. These challenges\ninclude difficulties in recalling commands, memorizing hand gestures, and\nlearning new names. This paper introduces NVP-HRI, an intuitive multi-modal HRI\nparadigm that combines voice commands and deictic posture. NVP-HRI utilizes the\nSegment Anything Model (SAM) to analyze visual cues and depth data, enabling\nprecise structural object representation. Through a pre-trained SAM network,\nNVP-HRI allows interaction with new objects via zero-shot prediction, even\nwithout prior knowledge. NVP-HRI also integrates with a large language model\n(LLM) for multimodal commands, coordinating them with object selection and\nscene distribution in real time for collision-free trajectory solutions. We\nalso regulate the action sequence with the essential control syntax to reduce\nLLM hallucination risks. The evaluation of diverse real-world tasks using a\nUniversal Robot showcased up to 59.2\\% efficiency improvement over traditional\ngesture control, as illustrated in the video https://youtu.be/EbC7al2wiAc. Our\ncode and design will be openly available at\nhttps://github.com/laiyuzhi/NVP-HRI.git."
                },
                "authors": [
                    {
                        "name": "Yuzhi Lai"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Youssef Nassar"
                    },
                    {
                        "name": "Mingyu Fan"
                    },
                    {
                        "name": "Thomas Weber"
                    },
                    {
                        "name": "Matthias Rätsch"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Rätsch"
                },
                "author": "Matthias Rätsch",
                "arxiv_doi": "10.1016/j.eswa.2024.126360",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.eswa.2024.126360",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted for publication in ESWA @ 2025 Elsevier.\n  Personal use of this material is permitted. Permission from Elsevier must be\n  obtained for all other uses, including reprinting/redistribution, creating\n  new works, or reuse of any copyrighted components of this work in other media",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09334v1",
                "updated": "2025-03-12T12:29:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    29,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:29:27Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    29,
                    27,
                    2,
                    71,
                    0
                ],
                "title": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs\n  Using Cyber Security Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs\n  Using Cyber Security Data"
                },
                "summary": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of large language models (LLMs) into cyber security\napplications presents significant opportunities, such as enhancing threat\nanalysis and malware detection, but can also introduce critical risks and\nsafety concerns, including personal data leakage and automated generation of\nnew malware. To address these challenges, we developed CyberLLMInstruct, a\ndataset of 54,928 instruction-response pairs spanning cyber security tasks such\nas malware analysis, phishing simulations, and zero-day vulnerabilities. The\ndataset was constructed through a multi-stage process. This involved sourcing\ndata from multiple resources, filtering and structuring it into\ninstruction-response pairs, and aligning it with real-world scenarios to\nenhance its applicability. Seven open-source LLMs were chosen to test the\nusefulness of CyberLLMInstruct: Phi 3 Mini 3.8B, Mistral 7B, Qwen 2.5 7B, Llama\n3 8B, Llama 3.1 8B, Gemma 2 9B, and Llama 2 70B. In our primary example, we\nrigorously assess the safety of fine-tuned models using the OWASP top 10\nframework, finding that fine-tuning reduces safety resilience across all tested\nLLMs and every adversarial attack (e.g., the security score of Llama 3.1 8B\nagainst prompt injection drops from 0.95 to 0.15). In our second example, we\nshow that these same fine-tuned models can also achieve up to 92.50 percent\naccuracy on the CyberMetric benchmark. These findings highlight a trade-off\nbetween performance and safety, showing the importance of adversarial testing\nand further research into fine-tuning methodologies that can mitigate safety\nrisks while still improving performance across diverse datasets and domains.\nAll scripts required to reproduce the dataset, along with examples and relevant\nresources for replicating our results, will be made available upon the paper's\nacceptance."
                },
                "authors": [
                    {
                        "name": "Adel ElZemity"
                    },
                    {
                        "name": "Budi Arief"
                    },
                    {
                        "name": "Shujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Shujun Li"
                },
                "author": "Shujun Li",
                "arxiv_comment": "The paper is submitted to \"The 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval\" and is\n  currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06674v2",
                "updated": "2025-03-12T12:25:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    25,
                    18,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-09T15:53:49Z",
                "published_parsed": [
                    2025,
                    3,
                    9,
                    15,
                    53,
                    49,
                    6,
                    68,
                    0
                ],
                "title": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Few-Step Diffusion Models by Trajectory Distribution Matching"
                },
                "summary": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-$\\alpha$, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-$\\alpha$ into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating diffusion model sampling is crucial for efficient AIGC\ndeployment. While diffusion distillation methods -- based on distribution\nmatching and trajectory matching -- reduce sampling to as few as one step, they\nfall short on complex tasks like text-to-image generation. Few-step generation\noffers a better balance between speed and quality, but existing approaches face\na persistent trade-off: distribution matching lacks flexibility for multi-step\nsampling, while trajectory matching often yields suboptimal image quality. To\nbridge this gap, we propose learning few-step diffusion models by Trajectory\nDistribution Matching (TDM), a unified distillation paradigm that combines the\nstrengths of distribution and trajectory matching. Our method introduces a\ndata-free score distillation objective, aligning the student's trajectory with\nthe teacher's at the distribution level. Further, we develop a\nsampling-steps-aware objective that decouples learning targets across different\nsteps, enabling more adjustable sampling. This approach supports both\ndeterministic sampling for superior image quality and flexible multi-step\nadaptation, achieving state-of-the-art performance with remarkable efficiency.\nOur model, TDM, outperforms existing methods on various backbones, such as SDXL\nand PixArt-$\\alpha$, delivering superior quality and significantly reduced\ntraining costs. In particular, our method distills PixArt-$\\alpha$ into a\n4-step generator that outperforms its teacher on real user preference at 1024\nresolution. This is accomplished with 500 iterations and 2 A800 hours -- a mere\n0.01% of the teacher's training cost. In addition, our proposed TDM can be\nextended to accelerate text-to-video diffusion. Notably, TDM can outperform its\nteacher model (CogVideoX-2B) by using only 4 NFE on VBench, improving the total\nscore from 80.91 to 81.65. Project page: https://tdm-t2x.github.io/"
                },
                "authors": [
                    {
                        "name": "Yihong Luo"
                    },
                    {
                        "name": "Tianyang Hu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Yujun Cai"
                    },
                    {
                        "name": "Jing Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Tang"
                },
                "author": "Jing Tang",
                "arxiv_comment": "Project page: https://tdm-t2x.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09326v1",
                "updated": "2025-03-12T12:20:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    20,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:20:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    20,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models"
                },
                "summary": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown remarkable performance in\nlanguage tasks and beyond. However, due to their limited inherent causal\nreasoning ability, LLMs still face challenges in handling tasks that require\nrobust causal reasoning ability, such as health-care and economic analysis. As\na result, a growing body of research has focused on enhancing the causal\nreasoning ability of LLMs. Despite the booming research, there lacks a survey\nto well review the challenges, progress and future directions in this area. To\nbridge this significant gap, we systematically review literature on how to\nstrengthen LLMs' causal reasoning ability in this paper. We start from the\nintroduction of background and motivations of this topic, followed by the\nsummarisation of key challenges in this area. Thereafter, we propose a novel\ntaxonomy to systematically categorise existing methods, together with detailed\ncomparisons within and between classes of methods. Furthermore, we summarise\nexisting benchmarks and evaluation metrics for assessing LLMs' causal reasoning\nability. Finally, we outline future research directions for this emerging\nfield, offering insights and inspiration to researchers and practitioners in\nthe area."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Zhuo Cai"
                    },
                    {
                        "name": "Shoujin Wang"
                    },
                    {
                        "name": "Kun Yu"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20077v2",
                "updated": "2025-03-12T12:07:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    7,
                    19,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-28T08:18:40Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    8,
                    18,
                    40,
                    5,
                    363,
                    0
                ],
                "title": "A Time-Triggered Communication Method Based on Urgency-Based Scheduler\n  in Time-Sensitive Networking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Time-Triggered Communication Method Based on Urgency-Based Scheduler\n  in Time-Sensitive Networking"
                },
                "summary": "The development of the automotive industry and automation has led to a\ngrowing demand for time-critical systems to have low latency and jitter for\ncritical traffic. To address this issue, the IEEE 802.1 Time-Sensitive\nNetworking (TSN) task group proposed the Time-Aware Shaper (TAS) to implement\nTime-Triggered (TT) communication, enabling deterministic transmission by\nassigning specific time windows to each stream. While Fixed Routing and\nWaiting-Allowed (FR-WA) scheduling algorithms offer flexibility, they suffer\nfrom inefficiencies in solution time and scalability. This study analyzes TAS\nimplementation challenges, emphasizing how network scale expansion increases\ncomputational constraints. We propose an Urgency-Based Scheduler method\n(TT-UBS) to address these limitations to enhance deterministic transmission and\ncomputational efficiency under anomalies. A novel scheduling algorithm for\nTT-UBS parameter determination is developed, alongside simulations and\ncomparative evaluations. Results show that TT-UBS guarantees deterministic\ntraffic delivery while reducing solution time by 98.22% in test scenarios\ncompared to traditional approaches. The methodology is extended to other\nscheduling algorithms to assess efficiency improvements. This advancement\nsupports TSN's application in mission-critical systems by optimizing\ntime-triggered communication performance and enabling reliable network\ndeployment. The framework demonstrates significant potential for real-time\nin-vehicle networks requiring latency and jitter control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of the automotive industry and automation has led to a\ngrowing demand for time-critical systems to have low latency and jitter for\ncritical traffic. To address this issue, the IEEE 802.1 Time-Sensitive\nNetworking (TSN) task group proposed the Time-Aware Shaper (TAS) to implement\nTime-Triggered (TT) communication, enabling deterministic transmission by\nassigning specific time windows to each stream. While Fixed Routing and\nWaiting-Allowed (FR-WA) scheduling algorithms offer flexibility, they suffer\nfrom inefficiencies in solution time and scalability. This study analyzes TAS\nimplementation challenges, emphasizing how network scale expansion increases\ncomputational constraints. We propose an Urgency-Based Scheduler method\n(TT-UBS) to address these limitations to enhance deterministic transmission and\ncomputational efficiency under anomalies. A novel scheduling algorithm for\nTT-UBS parameter determination is developed, alongside simulations and\ncomparative evaluations. Results show that TT-UBS guarantees deterministic\ntraffic delivery while reducing solution time by 98.22% in test scenarios\ncompared to traditional approaches. The methodology is extended to other\nscheduling algorithms to assess efficiency improvements. This advancement\nsupports TSN's application in mission-critical systems by optimizing\ntime-triggered communication performance and enabling reliable network\ndeployment. The framework demonstrates significant potential for real-time\nin-vehicle networks requiring latency and jitter control."
                },
                "authors": [
                    {
                        "name": "Zitong Wang"
                    },
                    {
                        "name": "Feng Luo"
                    },
                    {
                        "name": "Yi Ren"
                    },
                    {
                        "name": "Dengcheng Liu"
                    },
                    {
                        "name": "Chuan Wan"
                    },
                    {
                        "name": "Zhouping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhouping Zhang"
                },
                "author": "Zhouping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09311v1",
                "updated": "2025-03-12T12:02:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    2,
                    36,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T12:02:36Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    2,
                    36,
                    2,
                    71,
                    0
                ],
                "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem\n  with simulated user interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive political surveys and GPT-4: Tackling the cold start problem\n  with simulated user interactions"
                },
                "summary": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive questionnaires dynamically select the next question for a survey\nparticipant based on their previous answers. Due to digitalisation, they have\nbecome a viable alternative to traditional surveys in application areas such as\npolitical science. One limitation, however, is their dependency on data to\ntrain the model for question selection. Often, such training data (i.e., user\ninteractions) are unavailable a priori. To address this problem, we (i) test\nwhether Large Language Models (LLM) can accurately generate such interaction\ndata and (ii) explore if these synthetic data can be used to pre-train the\nstatistical model of an adaptive political survey. To evaluate this approach,\nwe utilise existing data from the Swiss Voting Advice Application (VAA)\nSmartvote in two ways: First, we compare the distribution of LLM-generated\nsynthetic data to the real distribution to assess its similarity. Second, we\ncompare the performance of an adaptive questionnaire that is randomly\ninitialised with one pre-trained on synthetic data to assess their suitability\nfor training. We benchmark these results against an \"oracle\" questionnaire with\nperfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately\ngenerates answers to the Smartvote questionnaire from the perspective of\ndifferent Swiss parties. Furthermore, we demonstrate that initialising the\nstatistical model with synthetic data can (i) significantly reduce the error in\npredicting user responses and (ii) increase the candidate recommendation\naccuracy of the VAA. Our work emphasises the considerable potential of LLMs to\ncreate training data to improve the data collection process in adaptive\nquestionnaires in LLM-affine areas such as political surveys."
                },
                "authors": [
                    {
                        "name": "Fynn Bachmann"
                    },
                    {
                        "name": "Daan van der Weijden"
                    },
                    {
                        "name": "Lucien Heitz"
                    },
                    {
                        "name": "Cristina Sarasua"
                    },
                    {
                        "name": "Abraham Bernstein"
                    }
                ],
                "author_detail": {
                    "name": "Abraham Bernstein"
                },
                "author": "Abraham Bernstein",
                "arxiv_comment": "23 pages. Under review at PLOS One",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09291v1",
                "updated": "2025-03-12T11:36:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    36,
                    29,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:36:29Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    36,
                    29,
                    2,
                    71,
                    0
                ],
                "title": "Prompt Inference Attack on Distributed Large Language Model Inference\n  Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Inference Attack on Distributed Large Language Model Inference\n  Frameworks"
                },
                "summary": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference process of modern large language models (LLMs) demands\nprohibitive computational resources, rendering them infeasible for deployment\non consumer-grade devices. To address this limitation, recent studies propose\ndistributed LLM inference frameworks, which employ split learning principles to\nenable collaborative LLM inference on resource-constrained hardware. However,\ndistributing LLM layers across participants requires the transmission of\nintermediate outputs, which may introduce privacy risks to the original input\nprompts - a critical issue that has yet to be thoroughly explored in the\nliterature.\n  In this paper, we rigorously examine the privacy vulnerabilities of\ndistributed LLM inference frameworks by designing and evaluating three prompt\ninference attacks aimed at reconstructing input prompts from intermediate LLM\noutputs. These attacks are developed under various query and data constraints\nto reflect diverse real-world LLM service scenarios. Specifically, the first\nattack assumes an unlimited query budget and access to an auxiliary dataset\nsharing the same distribution as the target prompts. The second attack also\nleverages unlimited queries but uses an auxiliary dataset with a distribution\ndiffering from the target prompts. The third attack operates under the most\nrestrictive scenario, with limited query budgets and no auxiliary dataset\navailable. We evaluate these attacks on a range of LLMs, including\nstate-of-the-art models such as Llama-3.2 and Phi-3.5, as well as widely-used\nmodels like GPT-2 and BERT for comparative analysis. Our experiments show that\nthe first two attacks achieve reconstruction accuracies exceeding 90%, while\nthe third achieves accuracies typically above 50%, even under stringent\nconstraints. These findings highlight privacy risks in distributed LLM\ninference frameworks, issuing a strong alert on their deployment in real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Xinjian Luo"
                    },
                    {
                        "name": "Ting Yu"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08102v2",
                "updated": "2025-03-12T11:31:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    31,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T07:05:52Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    7,
                    5,
                    52,
                    1,
                    70,
                    0
                ],
                "title": "AI-native Memory 2.0: Second Me",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-native Memory 2.0: Second Me"
                },
                "summary": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human interaction with the external world fundamentally involves the exchange\nof personal memory, whether with other individuals, websites, applications, or,\nin the future, AI agents. A significant portion of this interaction is\nredundant, requiring users to repeatedly provide the same information across\ndifferent contexts. Existing solutions, such as browser-stored credentials,\nautofill mechanisms, and unified authentication systems, have aimed to mitigate\nthis redundancy by serving as intermediaries that store and retrieve commonly\nused user data. The advent of large language models (LLMs) presents an\nopportunity to redefine memory management through an AI-native paradigm: SECOND\nME. SECOND ME acts as an intelligent, persistent memory offload system that\nretains, organizes, and dynamically utilizes user-specific knowledge. By\nserving as an intermediary in user interactions, it can autonomously generate\ncontext-aware responses, prefill required information, and facilitate seamless\ncommunication with external systems, significantly reducing cognitive load and\ninteraction friction. Unlike traditional memory storage solutions, SECOND ME\nextends beyond static data retention by leveraging LLM-based memory\nparameterization. This enables structured organization, contextual reasoning,\nand adaptive knowledge retrieval, facilitating a more systematic and\nintelligent approach to memory management. As AI-driven personal agents like\nSECOND ME become increasingly integrated into digital ecosystems, SECOND ME\nfurther represents a critical step toward augmenting human-world interaction\nwith persistent, contextually aware, and self-optimizing memory systems. We\nhave open-sourced the fully localizable deployment system at GitHub:\nhttps://github.com/Mindverse/Second-Me."
                },
                "authors": [
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Tao Gao"
                    },
                    {
                        "name": "Fangyi Bao"
                    },
                    {
                        "name": "Felix Tao"
                    },
                    {
                        "name": "Jingbo Shang"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Shang"
                },
                "author": "Jingbo Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09281v1",
                "updated": "2025-03-12T11:27:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    27,
                    4,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:27:04Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    27,
                    4,
                    2,
                    71,
                    0
                ],
                "title": "Crowdsourced Homophily Ties Based Graph Annotation Via Large Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowdsourced Homophily Ties Based Graph Annotation Via Large Language\n  Model"
                },
                "summary": "Accurate graph annotation typically requires substantial labeled data, which\nis often challenging and resource-intensive to obtain. In this paper, we\npresent Crowdsourced Homophily Ties Based Graph Annotation via Large Language\nModel (CSA-LLM), a novel approach that combines the strengths of crowdsourced\nannotations with the capabilities of large language models (LLMs) to enhance\nthe graph annotation process. CSA-LLM harnesses the structural context of graph\ndata by integrating information from 1-hop and 2-hop neighbors. By emphasizing\nhomophily ties - key connections that signify similarity within the graph -\nCSA-LLM significantly improves the accuracy of annotations. Experimental\nresults demonstrate that this method enhances the performance of Graph Neural\nNetworks (GNNs) by delivering more precise and reliable annotations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate graph annotation typically requires substantial labeled data, which\nis often challenging and resource-intensive to obtain. In this paper, we\npresent Crowdsourced Homophily Ties Based Graph Annotation via Large Language\nModel (CSA-LLM), a novel approach that combines the strengths of crowdsourced\nannotations with the capabilities of large language models (LLMs) to enhance\nthe graph annotation process. CSA-LLM harnesses the structural context of graph\ndata by integrating information from 1-hop and 2-hop neighbors. By emphasizing\nhomophily ties - key connections that signify similarity within the graph -\nCSA-LLM significantly improves the accuracy of annotations. Experimental\nresults demonstrate that this method enhances the performance of Graph Neural\nNetworks (GNNs) by delivering more precise and reliable annotations."
                },
                "authors": [
                    {
                        "name": "Yu Bu"
                    },
                    {
                        "name": "Yulin Zhu"
                    },
                    {
                        "name": "Kai Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kai Zhou"
                },
                "author": "Kai Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09276v1",
                "updated": "2025-03-12T11:22:13Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    22,
                    13,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:22:13Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    22,
                    13,
                    2,
                    71,
                    0
                ],
                "title": "Fine-Tuning Large Language Models for Educational Support: Leveraging\n  Gagne's Nine Events of Instruction for Lesson Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Large Language Models for Educational Support: Leveraging\n  Gagne's Nine Events of Instruction for Lesson Planning"
                },
                "summary": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective lesson planning is crucial in education process, serving as the\ncornerstone for high-quality teaching and the cultivation of a conducive\nlearning atmosphere. This study investigates how large language models (LLMs)\ncan enhance teacher preparation by incorporating them with Gagne's Nine Events\nof Instruction, especially in the field of mathematics education in compulsory\neducation. It investigates two distinct methodologies: the development of Chain\nof Thought (CoT) prompts to direct LLMs in generating content that aligns with\ninstructional events, and the application of fine-tuning approaches like\nLow-Rank Adaptation (LoRA) to enhance model performance. This research starts\nwith creating a comprehensive dataset based on math curriculum standards and\nGagne's instructional events. The first method involves crafting CoT-optimized\nprompts to generate detailed, logically coherent responses from LLMs, improving\ntheir ability to create educationally relevant content. The second method uses\nspecialized datasets to fine-tune open-source models, enhancing their\neducational content generation and analysis capabilities. This study\ncontributes to the evolving dialogue on the integration of AI in education,\nillustrating innovative strategies for leveraging LLMs to bolster teaching and\nlearning processes."
                },
                "authors": [
                    {
                        "name": "Linzhao Jia"
                    },
                    {
                        "name": "Changyong Qi"
                    },
                    {
                        "name": "Yuang Wei"
                    },
                    {
                        "name": "Han Sun"
                    },
                    {
                        "name": "Xiaozhe Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhe Yang"
                },
                "author": "Xiaozhe Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09263v1",
                "updated": "2025-03-12T11:03:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    3,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T11:03:27Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    11,
                    3,
                    27,
                    2,
                    71,
                    0
                ],
                "title": "COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COLA: A Scalable Multi-Agent Framework For Windows UI Task Automation"
                },
                "summary": "With the rapid advancements in Large Language Models (LLMs), an increasing\nnumber of studies have leveraged LLMs as the cognitive core of agents to\naddress complex task decision-making challenges. Specially, recent research has\ndemonstrated the potential of LLM-based agents on automating Windows GUI\noperations. However, existing methodologies exhibit two critical challenges:\n(1) static agent architectures fail to dynamically adapt to the heterogeneous\nrequirements of OS-level tasks, leading to inadequate scenario\ngeneralization;(2) the agent workflows lack fault tolerance mechanism,\nnecessitating complete process re-execution for UI agent decision error. To\naddress these limitations, we introduce \\textit{COLA}, a collaborative\nmulti-agent framework for automating Windows UI operations. In this framework,\na scenario-aware agent Task Scheduler decomposes task requirements into atomic\ncapability units, dynamically selects the optimal agent from a decision agent\npool, effectively responds to the capability requirements of diverse scenarios.\nThe decision agent pool supports plug-and-play expansion for enhanced\nflexibility. In addition, we design a memory unit equipped to all agents for\ntheir self-evolution. Furthermore, we develop an interactive backtracking\nmechanism that enables human to intervene to trigger state rollbacks for\nnon-destructive process repair. Our experimental results on the GAIA benchmark\ndemonstrates that the \\textit{COLA} framework achieves state-of-the-art\nperformance with an average score of 31.89\\%, significantly outperforming\nbaseline approaches without web API integration. Ablation studies further\nvalidate the individual contributions of our dynamic scheduling. The code is\navailable at https://github.com/Alokia/COLA-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements in Large Language Models (LLMs), an increasing\nnumber of studies have leveraged LLMs as the cognitive core of agents to\naddress complex task decision-making challenges. Specially, recent research has\ndemonstrated the potential of LLM-based agents on automating Windows GUI\noperations. However, existing methodologies exhibit two critical challenges:\n(1) static agent architectures fail to dynamically adapt to the heterogeneous\nrequirements of OS-level tasks, leading to inadequate scenario\ngeneralization;(2) the agent workflows lack fault tolerance mechanism,\nnecessitating complete process re-execution for UI agent decision error. To\naddress these limitations, we introduce \\textit{COLA}, a collaborative\nmulti-agent framework for automating Windows UI operations. In this framework,\na scenario-aware agent Task Scheduler decomposes task requirements into atomic\ncapability units, dynamically selects the optimal agent from a decision agent\npool, effectively responds to the capability requirements of diverse scenarios.\nThe decision agent pool supports plug-and-play expansion for enhanced\nflexibility. In addition, we design a memory unit equipped to all agents for\ntheir self-evolution. Furthermore, we develop an interactive backtracking\nmechanism that enables human to intervene to trigger state rollbacks for\nnon-destructive process repair. Our experimental results on the GAIA benchmark\ndemonstrates that the \\textit{COLA} framework achieves state-of-the-art\nperformance with an average score of 31.89\\%, significantly outperforming\nbaseline approaches without web API integration. Ablation studies further\nvalidate the individual contributions of our dynamic scheduling. The code is\navailable at https://github.com/Alokia/COLA-demo."
                },
                "authors": [
                    {
                        "name": "Di Zhao"
                    },
                    {
                        "name": "Longhui Ma"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Miao Wang"
                    },
                    {
                        "name": "Zhao Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Lv"
                },
                "author": "Zhao Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05003v2",
                "updated": "2025-03-12T10:40:48Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    40,
                    48,
                    2,
                    71,
                    0
                ],
                "published": "2024-12-06T12:58:58Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    12,
                    58,
                    58,
                    4,
                    341,
                    0
                ],
                "title": "SLayR: Scene Layout Generation with Rectified Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLayR: Scene Layout Generation with Rectified Flow"
                },
                "summary": "We introduce SLayR, Scene Layout Generation with Rectified flow, a novel\ntransformer-based model for text-to-layout generation which can then be paired\nwith existing layout-to-image models to produce images. SLayR addresses a\ndomain in which current text-to-image pipelines struggle: generating scene\nlayouts that are of significant variety and plausibility, when the given prompt\nis ambiguous and does not provide constraints on the scene. SLayR surpasses\nexisting baselines including LLMs in unconstrained generation, and can generate\nlayouts from an open caption set. To accurately evaluate the layout generation,\nwe introduce a new benchmark suite, including numerical metrics and a carefully\ndesigned repeatable human-evaluation procedure that assesses the plausibility\nand variety of generated images. We show that our method sets a new state of\nthe art for achieving both at the same time, while being at least 3x times\nsmaller in the number of parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SLayR, Scene Layout Generation with Rectified flow, a novel\ntransformer-based model for text-to-layout generation which can then be paired\nwith existing layout-to-image models to produce images. SLayR addresses a\ndomain in which current text-to-image pipelines struggle: generating scene\nlayouts that are of significant variety and plausibility, when the given prompt\nis ambiguous and does not provide constraints on the scene. SLayR surpasses\nexisting baselines including LLMs in unconstrained generation, and can generate\nlayouts from an open caption set. To accurately evaluate the layout generation,\nwe introduce a new benchmark suite, including numerical metrics and a carefully\ndesigned repeatable human-evaluation procedure that assesses the plausibility\nand variety of generated images. We show that our method sets a new state of\nthe art for achieving both at the same time, while being at least 3x times\nsmaller in the number of parameters."
                },
                "authors": [
                    {
                        "name": "Cameron Braunstein"
                    },
                    {
                        "name": "Hevra Petekkaya"
                    },
                    {
                        "name": "Jan Eric Lenssen"
                    },
                    {
                        "name": "Mariya Toneva"
                    },
                    {
                        "name": "Eddy Ilg"
                    }
                ],
                "author_detail": {
                    "name": "Eddy Ilg"
                },
                "author": "Eddy Ilg",
                "arxiv_comment": "43 pages, 29 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09242v1",
                "updated": "2025-03-12T10:38:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    38,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:38:58Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    38,
                    58,
                    2,
                    71,
                    0
                ],
                "title": "NAMI: Efficient Image Generation via Progressive Rectified Flow\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NAMI: Efficient Image Generation via Progressive Rectified Flow\n  Transformers"
                },
                "summary": "Flow-based transformer models for image generation have achieved\nstate-of-the-art performance with larger model parameters, but their inference\ndeployment cost remains high. To enhance inference performance while\nmaintaining generation quality, we propose progressive rectified flow\ntransformers. We divide the rectified flow into different stages according to\nresolution, using fewer transformer layers at the low-resolution stages to\ngenerate image layouts and concept contours, and progressively adding more\nlayers as the resolution increases. Experiments demonstrate that our approach\nachieves fast convergence and reduces inference time while ensuring generation\nquality. The main contributions of this paper are summarized as follows: (1) We\nintroduce progressive rectified flow transformers that enable multi-resolution\ntraining, accelerating model convergence; (2) NAMI leverages piecewise flow and\nspatial cascading of Diffusion Transformer (DiT) to rapidly generate images,\nreducing inference time by 40% to generate a 1024 resolution image; (3) We\npropose NAMI-1K benchmark to evaluate human preference performance, aiming to\nmitigate distributional bias and prevent data leakage from open-source\nbenchmarks. The results show that our model is competitive with\nstate-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-based transformer models for image generation have achieved\nstate-of-the-art performance with larger model parameters, but their inference\ndeployment cost remains high. To enhance inference performance while\nmaintaining generation quality, we propose progressive rectified flow\ntransformers. We divide the rectified flow into different stages according to\nresolution, using fewer transformer layers at the low-resolution stages to\ngenerate image layouts and concept contours, and progressively adding more\nlayers as the resolution increases. Experiments demonstrate that our approach\nachieves fast convergence and reduces inference time while ensuring generation\nquality. The main contributions of this paper are summarized as follows: (1) We\nintroduce progressive rectified flow transformers that enable multi-resolution\ntraining, accelerating model convergence; (2) NAMI leverages piecewise flow and\nspatial cascading of Diffusion Transformer (DiT) to rapidly generate images,\nreducing inference time by 40% to generate a 1024 resolution image; (3) We\npropose NAMI-1K benchmark to evaluate human preference performance, aiming to\nmitigate distributional bias and prevent data leakage from open-source\nbenchmarks. The results show that our model is competitive with\nstate-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Ao Ma"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09223v1",
                "updated": "2025-03-12T10:10:30Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    10,
                    30,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:10:30Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    10,
                    30,
                    2,
                    71,
                    0
                ],
                "title": "LREF: A Novel LLM-based Relevance Framework for E-commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LREF: A Novel LLM-based Relevance Framework for E-commerce"
                },
                "summary": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query and product relevance prediction is a critical component for ensuring a\nsmooth user experience in e-commerce search. Traditional studies mainly focus\non BERT-based models to assess the semantic relevance between queries and\nproducts. However, the discriminative paradigm and limited knowledge capacity\nof these approaches restrict their ability to comprehend the relevance between\nqueries and products fully. With the rapid advancement of Large Language Models\n(LLMs), recent research has begun to explore their application to industrial\nsearch systems, as LLMs provide extensive world knowledge and flexible\noptimization for reasoning processes. Nonetheless, directly leveraging LLMs for\nrelevance prediction tasks introduces new challenges, including a high demand\nfor data quality, the necessity for meticulous optimization of reasoning\nprocesses, and an optimistic bias that can result in over-recall. To overcome\nthe above problems, this paper proposes a novel framework called the LLM-based\nRElevance Framework (LREF) aimed at enhancing e-commerce search relevance. The\nframework comprises three main stages: supervised fine-tuning (SFT) with Data\nSelection, Multiple Chain of Thought (Multi-CoT) tuning, and Direct Preference\nOptimization (DPO) for de-biasing. We evaluate the performance of the framework\nthrough a series of offline experiments on large-scale real-world datasets, as\nwell as online A/B testing. The results indicate significant improvements in\nboth offline and online metrics. Ultimately, the model was deployed in a\nwell-known e-commerce application, yielding substantial commercial benefits."
                },
                "authors": [
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Zhixing Tian"
                    },
                    {
                        "name": "Zhenyu Zhu"
                    },
                    {
                        "name": "Chenyang Wang"
                    },
                    {
                        "name": "Haiqing Hu"
                    },
                    {
                        "name": "Guoyu Tang"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sulong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Sulong Xu"
                },
                "author": "Sulong Xu",
                "arxiv_doi": "10.1145/3701716.3715246",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715246",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16965v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16965v3",
                "updated": "2025-03-12T10:09:21Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    9,
                    21,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-24T08:44:01Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    8,
                    44,
                    1,
                    0,
                    55,
                    0
                ],
                "title": "Autoregressive Image Generation with Vision Full-view Prompt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Vision Full-view Prompt"
                },
                "summary": "In autoregressive (AR) image generation, models based on the 'next-token\nprediction' paradigm of LLMs have shown comparable performance to diffusion\nmodels by reducing inductive biases. However, directly applying LLMs to complex\nimage generation can struggle with reconstructing the image's structure and\ndetails, impacting the generation's accuracy and stability. Additionally, the\n'next-token prediction' paradigm in the AR model does not align with the\ncontextual scanning and logical reasoning processes involved in human visual\nperception, limiting effective image generation. Prompt engineering, as a key\ntechnique for guiding LLMs, leverages specifically designed prompts to improve\nmodel performance on complex natural language processing (NLP) tasks, enhancing\naccuracy and stability of generation while maintaining contextual coherence and\nlogical consistency, similar to human reasoning. Inspired by prompt engineering\nfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) to\nenhance autoregressive image generation. Specifically, we design specialized\nimage-related VF prompts for AR image generation to simulate the process of\nhuman image creation. This enhances contextual logic ability by allowing the\nmodel to first perceive overall distribution information before generating the\nimage, and improve generation stability by increasing the inference steps.\nCompared to the AR method without VF prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autoregressive (AR) image generation, models based on the 'next-token\nprediction' paradigm of LLMs have shown comparable performance to diffusion\nmodels by reducing inductive biases. However, directly applying LLMs to complex\nimage generation can struggle with reconstructing the image's structure and\ndetails, impacting the generation's accuracy and stability. Additionally, the\n'next-token prediction' paradigm in the AR model does not align with the\ncontextual scanning and logical reasoning processes involved in human visual\nperception, limiting effective image generation. Prompt engineering, as a key\ntechnique for guiding LLMs, leverages specifically designed prompts to improve\nmodel performance on complex natural language processing (NLP) tasks, enhancing\naccuracy and stability of generation while maintaining contextual coherence and\nlogical consistency, similar to human reasoning. Inspired by prompt engineering\nfrom the field of NLP, we propose Vision Full-view prompt (VF prompt) to\nenhance autoregressive image generation. Specifically, we design specialized\nimage-related VF prompts for AR image generation to simulate the process of\nhuman image creation. This enhances contextual logic ability by allowing the\nmodel to first perceive overall distribution information before generating the\nimage, and improve generation stability by increasing the inference steps.\nCompared to the AR method without VF prompts, our method shows outstanding\nperformance and achieves an approximate improvement of 20%."
                },
                "authors": [
                    {
                        "name": "Miaomiao Cai"
                    },
                    {
                        "name": "Guanjie Wang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Zhijun Tu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Shaohui Lin"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16965v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16965v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23746v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23746v3",
                "updated": "2025-03-12T10:08:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    8,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-31T09:01:25Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    9,
                    1,
                    25,
                    3,
                    305,
                    0
                ],
                "title": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DetectRL: Benchmarking LLM-Generated Text Detection in Real-World\n  Scenarios"
                },
                "summary": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting text generated by large language models (LLMs) is of great recent\ninterest. With zero-shot methods like DetectGPT, detection capabilities have\nreached impressive levels. However, the reliability of existing detectors in\nreal-world applications remains underexplored. In this study, we present a new\nbenchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection\ntechniques still underperformed in this task. We collected human-written\ndatasets from domains where LLMs are particularly prone to misuse. Using\npopular LLMs, we generated data that better aligns with real-world\napplications. Unlike previous studies, we employed heuristic rules to create\nadversarial LLM-generated text, simulating various prompts usages, human\nrevisions like word substitutions, and writing noises like spelling mistakes.\nOur development of DetectRL reveals the strengths and limitations of current\nSOTA detectors. More importantly, we analyzed the potential impact of writing\nstyles, model types, attack methods, the text lengths, and real-world human\nwriting factors on different types of detectors. We believe DetectRL could\nserve as an effective benchmark for assessing detectors in real-world\nscenarios, evolving with advanced attack methods, thus providing more stressful\nevaluation to drive the development of more efficient detectors. Data and code\nare publicly available at: https://github.com/NLP2CT/DetectRL."
                },
                "authors": [
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Yulin Yuan"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "arxiv_comment": "Accepted to NeurIPS 2024 Datasets and Benchmarks Track (Camera-Ready)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23746v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23746v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09219v1",
                "updated": "2025-03-12T10:06:03Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    6,
                    3,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:06:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    6,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Rethinking Prompt-based Debiasing in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Prompt-based Debiasing in Large Language Models"
                },
                "summary": "Investigating bias in large language models (LLMs) is crucial for developing\ntrustworthy AI. While prompt-based through prompt engineering is common, its\neffectiveness relies on the assumption that models inherently understand\nbiases. Our study systematically analyzed this assumption using the BBQ and\nStereoSet benchmarks on both open-source models as well as commercial GPT\nmodel. Experimental results indicate that prompt-based is often superficial;\nfor instance, the Llama2-7B-Chat model misclassified over 90% of unbiased\ncontent as biased, despite achieving high accuracy in identifying bias issues\non the BBQ dataset. Additionally, specific evaluation and question settings in\nbias benchmarks often lead LLMs to choose \"evasive answers\", disregarding the\ncore of the question and the relevance of the response to the context.\nMoreover, the apparent success of previous methods may stem from flawed\nevaluation metrics. Our research highlights a potential \"false prosperity\" in\nprompt-base efforts and emphasizes the need to rethink bias metrics to ensure\ntruly trustworthy AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating bias in large language models (LLMs) is crucial for developing\ntrustworthy AI. While prompt-based through prompt engineering is common, its\neffectiveness relies on the assumption that models inherently understand\nbiases. Our study systematically analyzed this assumption using the BBQ and\nStereoSet benchmarks on both open-source models as well as commercial GPT\nmodel. Experimental results indicate that prompt-based is often superficial;\nfor instance, the Llama2-7B-Chat model misclassified over 90% of unbiased\ncontent as biased, despite achieving high accuracy in identifying bias issues\non the BBQ dataset. Additionally, specific evaluation and question settings in\nbias benchmarks often lead LLMs to choose \"evasive answers\", disregarding the\ncore of the question and the relevance of the response to the context.\nMoreover, the apparent success of previous methods may stem from flawed\nevaluation metrics. Our research highlights a potential \"false prosperity\" in\nprompt-base efforts and emphasizes the need to rethink bias metrics to ensure\ntruly trustworthy AI."
                },
                "authors": [
                    {
                        "name": "Xinyi Yang"
                    },
                    {
                        "name": "Runzhe Zhan"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Junchao Wu"
                    },
                    {
                        "name": "Lidia S. Chao"
                    }
                ],
                "author_detail": {
                    "name": "Lidia S. Chao"
                },
                "author": "Lidia S. Chao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09217v1",
                "updated": "2025-03-12T10:03:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    3,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:03:58Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    3,
                    58,
                    2,
                    71,
                    0
                ],
                "title": "Evaluating the Generalizability of LLMs in Automated Program Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Generalizability of LLMs in Automated Program Repair"
                },
                "summary": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based automated program repair methods have attracted significant\nattention for their state-of-the-art performance. However, they were primarily\nevaluated on a few well known datasets like Defects4J, raising questions about\ntheir effectiveness on new datasets. In this study, we evaluate 11\ntop-performing LLMs on DEFECTS4J-TRANS, a new dataset derived from transforming\nDefects4J while maintaining the original semantics. Results from experiments on\nboth Defects4J and DEFECTS4J-TRANS show that all studied LLMs have limited\ngeneralizability in APR tasks, with the average number of correct and plausible\npatches decreasing by 49.48% and 42.90%, respectively, on DEFECTS4J-TRANS.\nFurther investigation into incorporating additional repair-relevant information\nin repair prompts reveals that, although this information significantly\nenhances the LLMs' capabilities (increasing the number of correct and plausible\npatches by up to 136.67% and 121.82%, respectively), performance still falls\nshort of their original results. This indicates that prompt engineering alone\nis insufficient to substantially enhance LLMs' repair capabilities. Based on\nour study, we also offer several recommendations for future research."
                },
                "authors": [
                    {
                        "name": "Fengjie Li"
                    },
                    {
                        "name": "Jiajun Jiang"
                    },
                    {
                        "name": "Jiajun Sun"
                    },
                    {
                        "name": "Hongyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyu Zhang"
                },
                "author": "Hongyu Zhang",
                "arxiv_comment": "5 pages, 1 figure, to be published in ICSE2025-NIER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09211v1",
                "updated": "2025-03-12T10:00:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    0,
                    9,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T10:00:09Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    10,
                    0,
                    9,
                    2,
                    71,
                    0
                ],
                "title": "Why LLMs Cannot Think and How to Fix It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why LLMs Cannot Think and How to Fix It"
                },
                "summary": "This paper elucidates that current state-of-the-art Large Language Models\n(LLMs) are fundamentally incapable of making decisions or developing \"thoughts\"\nwithin the feature space due to their architectural constraints. We establish a\ndefinition of \"thought\" that encompasses traditional understandings of that\nterm and adapt it for application to LLMs. We demonstrate that the\narchitectural design and language modeling training methodology of contemporary\nLLMs inherently preclude them from engaging in genuine thought processes. Our\nprimary focus is on this theoretical realization rather than practical insights\nderived from experimental data. Finally, we propose solutions to enable thought\nprocesses within the feature space and discuss the broader implications of\nthese architectural modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper elucidates that current state-of-the-art Large Language Models\n(LLMs) are fundamentally incapable of making decisions or developing \"thoughts\"\nwithin the feature space due to their architectural constraints. We establish a\ndefinition of \"thought\" that encompasses traditional understandings of that\nterm and adapt it for application to LLMs. We demonstrate that the\narchitectural design and language modeling training methodology of contemporary\nLLMs inherently preclude them from engaging in genuine thought processes. Our\nprimary focus is on this theoretical realization rather than practical insights\nderived from experimental data. Finally, we propose solutions to enable thought\nprocesses within the feature space and discuss the broader implications of\nthese architectural modifications."
                },
                "authors": [
                    {
                        "name": "Marius Jahrens"
                    },
                    {
                        "name": "Thomas Martinetz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Martinetz"
                },
                "author": "Thomas Martinetz",
                "arxiv_comment": "Original conference submission for neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07450v2",
                "updated": "2025-03-12T09:57:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    57,
                    19,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-10T15:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper"
                },
                "summary": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted."
                },
                "authors": [
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Asifa Mehmood Qureshi"
                    },
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Roisin Loughran"
                    },
                    {
                        "name": "Subramaniam Kazhuparambil"
                    },
                    {
                        "name": "Andrew Shaw"
                    },
                    {
                        "name": "Mohammed Sabry"
                    },
                    {
                        "name": "Niamh St John Lynch"
                    },
                    {
                        "name": ". Nikhil Singh"
                    },
                    {
                        "name": "Padraic O'Hara"
                    },
                    {
                        "name": "Pranay Jaiswal"
                    },
                    {
                        "name": "Roshan Chandru"
                    },
                    {
                        "name": "David Lillis"
                    }
                ],
                "author_detail": {
                    "name": "David Lillis"
                },
                "arxiv_affiliation": "School of Computer Science, University College Dublin",
                "author": "David Lillis",
                "arxiv_comment": "The project is partially supported by the DkIT Postgraduate\n  Scholarship, Research Ireland under Grant number 13/RC/2094_2, and Grant\n  number 21/FFP-A/925",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01824v2",
                "updated": "2025-03-12T09:55:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    55,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2024-09-16T16:03:08Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    16,
                    3,
                    8,
                    0,
                    260,
                    0
                ],
                "title": "AI Conversational Interviewing: Transforming Surveys with LLMs as\n  Adaptive Interviewers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Conversational Interviewing: Transforming Surveys with LLMs as\n  Adaptive Interviewers"
                },
                "summary": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for eliciting people's opinions face a trade-off between\ndepth and scale: structured surveys enable large-scale data collection but\nlimit respondents' ability to voice their opinions in their own words, while\nconversational interviews provide deeper insights but are resource-intensive.\nThis study explores the potential of replacing human interviewers with large\nlanguage models (LLMs) to conduct scalable conversational interviews. Our goal\nis to assess the performance of AI Conversational Interviewing and to identify\nopportunities for improvement in a controlled environment. We conducted a\nsmall-scale, in-depth study with university students who were randomly assigned\nto a conversational interview by either AI or human interviewers, both\nemploying identical questionnaires on political topics. Various quantitative\nand qualitative measures assessed interviewer adherence to guidelines, response\nquality, participant engagement, and overall interview efficacy. The findings\nindicate the viability of AI Conversational Interviewing in producing quality\ndata comparable to traditional methods, with the added benefit of scalability.\nWe publish our data and materials for re-use and present specific\nrecommendations for effective implementation."
                },
                "authors": [
                    {
                        "name": "Alexander Wuttke"
                    },
                    {
                        "name": "Matthias Aßenmacher"
                    },
                    {
                        "name": "Christopher Klamm"
                    },
                    {
                        "name": "Max M. Lang"
                    },
                    {
                        "name": "Quirin Würschinger"
                    },
                    {
                        "name": "Frauke Kreuter"
                    }
                ],
                "author_detail": {
                    "name": "Frauke Kreuter"
                },
                "author": "Frauke Kreuter",
                "arxiv_journal_ref": "LaTeCH-CLfL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09206v1",
                "updated": "2025-03-12T09:52:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    52,
                    4,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:52:04Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    52,
                    4,
                    2,
                    71,
                    0
                ],
                "title": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted\n  Clients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Asymmetric Heterogeneous Federated Learning with Corrupted\n  Clients"
                },
                "summary": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a challenging robust federated learning task with model\nheterogeneous and data corrupted clients, where the clients have different\nlocal model structures. Data corruption is unavoidable due to factors such as\nrandom noise, compression artifacts, or environmental conditions in real-world\ndeployment, drastically crippling the entire federated system. To address these\nissues, this paper introduces a novel Robust Asymmetric Heterogeneous Federated\nLearning (RAHFL) framework. We propose a Diversity-enhanced supervised\nContrastive Learning technique to enhance the resilience and adaptability of\nlocal models on various data corruption patterns. Its basic idea is to utilize\ncomplex augmented samples obtained by the mixed-data augmentation strategy for\nsupervised contrastive learning, thereby enhancing the ability of the model to\nlearn robust and diverse feature representations. Furthermore, we design an\nAsymmetric Heterogeneous Federated Learning strategy to resist corrupt feedback\nfrom external clients. The strategy allows clients to perform selective one-way\nlearning during collaborative learning phase, enabling clients to refrain from\nincorporating lower-quality information from less robust or underperforming\ncollaborators. Extensive experimental results demonstrate the effectiveness and\nrobustness of our approach in diverse, challenging federated learning\nenvironments. Our code and models are public available at\nhttps://github.com/FangXiuwen/RAHFL."
                },
                "authors": [
                    {
                        "name": "Xiuwen Fang"
                    },
                    {
                        "name": "Mang Ye"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du",
                "arxiv_doi": "10.1109/TPAMI.2025.3527137",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3527137",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.09206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 47, issue. 4, pp. 2693-2705, April 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09205v1",
                "updated": "2025-03-12T09:48:38Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    48,
                    38,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    48,
                    38,
                    2,
                    71,
                    0
                ],
                "title": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quality Over Quantity? LLM-Based Curation for a Data-Efficient\n  Audio-Video Foundation Model"
                },
                "summary": "Integrating audio and visual data for training multimodal foundational models\nremains challenging. We present Audio-Video Vector Alignment (AVVA), which\naligns audiovisual (AV) scene content beyond mere temporal synchronization via\na Large Language Model (LLM)-based data curation pipeline. Specifically, AVVA\nscores and selects high-quality training clips using Whisper (speech-based\naudio foundation model) for audio and DINOv2 for video within a dual-encoder\ncontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSound\ndemonstrate that this approach can achieve significant accuracy gains with\nsubstantially less curated data. For instance, AVVA yields a 7.6% improvement\nin top-1 accuracy for audio-to-video retrieval on VGGSound compared to\nImageBind, despite training on only 192 hours of carefully filtered data (vs.\n5800+ hours). Moreover, an ablation study highlights that trading data quantity\nfor data quality improves performance, yielding respective top-3 accuracy\nincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, and\nVGGSound over uncurated baselines. While these results underscore AVVA's data\nefficiency, we also discuss the overhead of LLM-driven curation and how it may\nbe scaled or approximated in larger domains. Overall, AVVA provides a viable\npath toward more robust, text-free audiovisual learning with improved retrieval\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating audio and visual data for training multimodal foundational models\nremains challenging. We present Audio-Video Vector Alignment (AVVA), which\naligns audiovisual (AV) scene content beyond mere temporal synchronization via\na Large Language Model (LLM)-based data curation pipeline. Specifically, AVVA\nscores and selects high-quality training clips using Whisper (speech-based\naudio foundation model) for audio and DINOv2 for video within a dual-encoder\ncontrastive learning framework. Evaluations on AudioCaps, VALOR, and VGGSound\ndemonstrate that this approach can achieve significant accuracy gains with\nsubstantially less curated data. For instance, AVVA yields a 7.6% improvement\nin top-1 accuracy for audio-to-video retrieval on VGGSound compared to\nImageBind, despite training on only 192 hours of carefully filtered data (vs.\n5800+ hours). Moreover, an ablation study highlights that trading data quantity\nfor data quality improves performance, yielding respective top-3 accuracy\nincreases of 47.8, 48.4, and 58.0 percentage points on AudioCaps, VALOR, and\nVGGSound over uncurated baselines. While these results underscore AVVA's data\nefficiency, we also discuss the overhead of LLM-driven curation and how it may\nbe scaled or approximated in larger domains. Overall, AVVA provides a viable\npath toward more robust, text-free audiovisual learning with improved retrieval\naccuracy."
                },
                "authors": [
                    {
                        "name": "Ali Vosoughi"
                    },
                    {
                        "name": "Dimitra Emmanouilidou"
                    },
                    {
                        "name": "Hannes Gamper"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Gamper"
                },
                "author": "Hannes Gamper",
                "arxiv_comment": "5 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T, 68T45, 68T10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09202v1",
                "updated": "2025-03-12T09:46:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    46,
                    59,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:46:59Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    46,
                    59,
                    2,
                    71,
                    0
                ],
                "title": "Token Weighting for Long-Range Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Weighting for Long-Range Language Modeling"
                },
                "summary": "Many applications of large language models (LLMs) require long-context\nunderstanding, but models continue to struggle with such tasks. We hypothesize\nthat conventional next-token prediction training could contribute to this,\nbecause each token is assigned equal weight. Yet, intuitively, the amount of\ncontext needed to predict the next token accurately varies greatly across\ndifferent data. To reflect this, we propose various novel token-weighting\nschemes that assign different weights to each training token in the loss,\nthereby generalizing existing works. For this, we categorize token-weighting\nmethods using a two-step framework which compares the confidences of a\nlong-context and short-context model to score tokens. We evaluate all methods\non multiple long-context understanding tasks and show that non-uniform loss\nweights are helpful to improve the long-context abilities of LLMs. Different\nshort-context models can be used effectively for token scoring, including\nmodels that are much smaller than the long-context model that is trained. All\nin all, this work contributes to a better understanding of the trade-offs\nlong-context language modeling faces and provides guidelines for model steering\nvia loss-weighting based on empirical evidence. The code can be found on\nGithub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications of large language models (LLMs) require long-context\nunderstanding, but models continue to struggle with such tasks. We hypothesize\nthat conventional next-token prediction training could contribute to this,\nbecause each token is assigned equal weight. Yet, intuitively, the amount of\ncontext needed to predict the next token accurately varies greatly across\ndifferent data. To reflect this, we propose various novel token-weighting\nschemes that assign different weights to each training token in the loss,\nthereby generalizing existing works. For this, we categorize token-weighting\nmethods using a two-step framework which compares the confidences of a\nlong-context and short-context model to score tokens. We evaluate all methods\non multiple long-context understanding tasks and show that non-uniform loss\nweights are helpful to improve the long-context abilities of LLMs. Different\nshort-context models can be used effectively for token scoring, including\nmodels that are much smaller than the long-context model that is trained. All\nin all, this work contributes to a better understanding of the trade-offs\nlong-context language modeling faces and provides guidelines for model steering\nvia loss-weighting based on empirical evidence. The code can be found on\nGithub."
                },
                "authors": [
                    {
                        "name": "Falko Helm"
                    },
                    {
                        "name": "Nico Daheim"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "Accepted to NAACL 2025 (Findings). For the code, see\n  https://github.com/UKPLab/naacl2025-token-weighting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09200v1",
                "updated": "2025-03-12T09:44:15Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    44,
                    15,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:44:15Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    44,
                    15,
                    2,
                    71,
                    0
                ],
                "title": "Time-EAPCR: A Deep Learning-Based Novel Approach for Anomaly Detection\n  Applied to the Environmental Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-EAPCR: A Deep Learning-Based Novel Approach for Anomaly Detection\n  Applied to the Environmental Field"
                },
                "summary": "As human activities intensify, environmental systems such as aquatic\necosystems and water treatment systems face increasingly complex pressures,\nimpacting ecological balance, public health, and sustainable development,\nmaking intelligent anomaly monitoring essential. However, traditional\nmonitoring methods suffer from delayed responses, insufficient data processing\ncapabilities, and weak generalisation, making them unsuitable for complex\nenvironmental monitoring needs.In recent years, machine learning has been\nwidely applied to anomaly detection, but the multi-dimensional features and\nspatiotemporal dynamics of environmental ecological data, especially the\nlong-term dependencies and strong variability in the time dimension, limit the\neffectiveness of traditional methods.Deep learning, with its ability to\nautomatically learn features, captures complex nonlinear relationships,\nimproving detection performance. However, its application in environmental\nmonitoring is still in its early stages and requires further exploration.This\npaper introduces a new deep learning method, Time-EAPCR\n(Time-Embedding-Attention-Permutated CNN-Residual), and applies it to\nenvironmental science. The method uncovers feature correlations, captures\ntemporal evolution patterns, and enables precise anomaly detection in\nenvironmental systems.We validated Time-EAPCR's high accuracy and robustness\nacross four publicly available environmental datasets. Experimental results\nshow that the method efficiently handles multi-source data, improves detection\naccuracy, and excels across various scenarios with strong adaptability and\ngeneralisation. Additionally, a real-world river monitoring dataset confirmed\nthe feasibility of its deployment, providing reliable technical support for\nenvironmental monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As human activities intensify, environmental systems such as aquatic\necosystems and water treatment systems face increasingly complex pressures,\nimpacting ecological balance, public health, and sustainable development,\nmaking intelligent anomaly monitoring essential. However, traditional\nmonitoring methods suffer from delayed responses, insufficient data processing\ncapabilities, and weak generalisation, making them unsuitable for complex\nenvironmental monitoring needs.In recent years, machine learning has been\nwidely applied to anomaly detection, but the multi-dimensional features and\nspatiotemporal dynamics of environmental ecological data, especially the\nlong-term dependencies and strong variability in the time dimension, limit the\neffectiveness of traditional methods.Deep learning, with its ability to\nautomatically learn features, captures complex nonlinear relationships,\nimproving detection performance. However, its application in environmental\nmonitoring is still in its early stages and requires further exploration.This\npaper introduces a new deep learning method, Time-EAPCR\n(Time-Embedding-Attention-Permutated CNN-Residual), and applies it to\nenvironmental science. The method uncovers feature correlations, captures\ntemporal evolution patterns, and enables precise anomaly detection in\nenvironmental systems.We validated Time-EAPCR's high accuracy and robustness\nacross four publicly available environmental datasets. Experimental results\nshow that the method efficiently handles multi-source data, improves detection\naccuracy, and excels across various scenarios with strong adaptability and\ngeneralisation. Additionally, a real-world river monitoring dataset confirmed\nthe feasibility of its deployment, providing reliable technical support for\nenvironmental monitoring."
                },
                "authors": [
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Yuchao Lu"
                    },
                    {
                        "name": "Ling An"
                    },
                    {
                        "name": "Huajie Liang"
                    },
                    {
                        "name": "Chichun Zhou"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Zhang"
                },
                "author": "Zhenyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09184v1",
                "updated": "2025-03-12T09:24:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    24,
                    31,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T09:24:31Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    24,
                    31,
                    2,
                    71,
                    0
                ],
                "title": "Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Unstructured Sparsity in Fully Homomorphic Encrypted DNNs"
                },
                "summary": "The deployment of deep neural networks (DNNs) in privacy-sensitive\nenvironments is constrained by computational overheads in fully homomorphic\nencryption (FHE). This paper explores unstructured sparsity in FHE matrix\nmultiplication schemes as a means of reducing this burden while maintaining\nmodel accuracy requirements. We demonstrate that sparsity can be exploited in\narbitrary matrix multiplication, providing runtime benefits compared to a\nbaseline naive algorithm at all sparsity levels. This is a notable departure\nfrom the plaintext domain, where there is a trade-off between sparsity and the\noverhead of the sparse multiplication algorithm. In addition, we propose three\nsparse multiplication schemes in FHE based on common plaintext sparse\nencodings. We demonstrate the performance gain is scheme-invariant; however,\nsome sparse schemes vastly reduce the memory storage requirements of the\nencrypted matrix at high sparsity values. Our proposed sparse schemes yield an\naverage performance gain of 2.5x at 50% unstructured sparsity, with our\nmulti-threading scheme providing a 32.5x performance increase over the\nequivalent single-threaded sparse computation when utilizing 64 cores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of deep neural networks (DNNs) in privacy-sensitive\nenvironments is constrained by computational overheads in fully homomorphic\nencryption (FHE). This paper explores unstructured sparsity in FHE matrix\nmultiplication schemes as a means of reducing this burden while maintaining\nmodel accuracy requirements. We demonstrate that sparsity can be exploited in\narbitrary matrix multiplication, providing runtime benefits compared to a\nbaseline naive algorithm at all sparsity levels. This is a notable departure\nfrom the plaintext domain, where there is a trade-off between sparsity and the\noverhead of the sparse multiplication algorithm. In addition, we propose three\nsparse multiplication schemes in FHE based on common plaintext sparse\nencodings. We demonstrate the performance gain is scheme-invariant; however,\nsome sparse schemes vastly reduce the memory storage requirements of the\nencrypted matrix at high sparsity values. Our proposed sparse schemes yield an\naverage performance gain of 2.5x at 50% unstructured sparsity, with our\nmulti-threading scheme providing a 32.5x performance increase over the\nequivalent single-threaded sparse computation when utilizing 64 cores."
                },
                "authors": [
                    {
                        "name": "Aidan Ferguson"
                    },
                    {
                        "name": "Perry Gibson"
                    },
                    {
                        "name": "Lara D'Agata"
                    },
                    {
                        "name": "Parker McLeod"
                    },
                    {
                        "name": "Ferhat Yaman"
                    },
                    {
                        "name": "Amitabh Das"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "José Cano"
                    }
                ],
                "author_detail": {
                    "name": "José Cano"
                },
                "author": "José Cano",
                "arxiv_comment": "Accepted to 5th Workshop on Machine Learning and Systems (EuroMLSys)\n  co-located with EuroSys '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01156v2",
                "updated": "2025-03-12T09:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    9,
                    11,
                    37,
                    2,
                    71,
                    0
                ],
                "published": "2024-09-02T10:42:30Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    10,
                    42,
                    30,
                    0,
                    246,
                    0
                ],
                "title": "TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval"
                },
                "summary": "Most text-video retrieval methods utilize the text-image pre-trained models\nlike CLIP as a backbone. These methods process each sampled frame independently\nby the image encoder, resulting in high computational overhead and limiting\npractical deployment. Addressing this, we focus on efficient text-video\nretrieval by tackling two key challenges: 1. From the perspective of trainable\nparameters, current parameter-efficient fine-tuning methods incur high\ninference costs; 2. From the perspective of model complexity, current token\ncompression methods are mainly designed for images to reduce spatial redundancy\nbut overlook temporal redundancy in consecutive frames of a video. To tackle\nthese challenges, we propose Temporal Token Merging (TempMe), a\nparameter-efficient and training-inference efficient text-video retrieval\narchitecture that minimizes trainable parameters and model complexity.\nSpecifically, we introduce a progressive multi-granularity framework. By\ngradually combining neighboring clips, we reduce spatio-temporal redundancy and\nenhance temporal modeling across different frames, leading to improved\nefficiency and performance. Extensive experiments validate the superiority of\nour TempMe. Compared to previous parameter-efficient text-video retrieval\nmethods, TempMe achieves superior performance with just 0.50M trainable\nparameters. It significantly reduces output tokens by 95% and GFLOPs by 51%,\nwhile achieving a 1.8X speedup and a 4.4% R-Sum improvement. With full\nfine-tuning, TempMe achieves a significant 7.9% R-Sum improvement, trains 1.57X\nfaster, and utilizes 75.2% GPU memory usage. The code is available at\nhttps://github.com/LunarShen/TempMe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most text-video retrieval methods utilize the text-image pre-trained models\nlike CLIP as a backbone. These methods process each sampled frame independently\nby the image encoder, resulting in high computational overhead and limiting\npractical deployment. Addressing this, we focus on efficient text-video\nretrieval by tackling two key challenges: 1. From the perspective of trainable\nparameters, current parameter-efficient fine-tuning methods incur high\ninference costs; 2. From the perspective of model complexity, current token\ncompression methods are mainly designed for images to reduce spatial redundancy\nbut overlook temporal redundancy in consecutive frames of a video. To tackle\nthese challenges, we propose Temporal Token Merging (TempMe), a\nparameter-efficient and training-inference efficient text-video retrieval\narchitecture that minimizes trainable parameters and model complexity.\nSpecifically, we introduce a progressive multi-granularity framework. By\ngradually combining neighboring clips, we reduce spatio-temporal redundancy and\nenhance temporal modeling across different frames, leading to improved\nefficiency and performance. Extensive experiments validate the superiority of\nour TempMe. Compared to previous parameter-efficient text-video retrieval\nmethods, TempMe achieves superior performance with just 0.50M trainable\nparameters. It significantly reduces output tokens by 95% and GFLOPs by 51%,\nwhile achieving a 1.8X speedup and a 4.4% R-Sum improvement. With full\nfine-tuning, TempMe achieves a significant 7.9% R-Sum improvement, trains 1.57X\nfaster, and utilizes 75.2% GPU memory usage. The code is available at\nhttps://github.com/LunarShen/TempMe."
                },
                "authors": [
                    {
                        "name": "Leqi Shen"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Tao He"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Yifeng Zhang"
                    },
                    {
                        "name": "Pengzhang Liu"
                    },
                    {
                        "name": "Yongjun Bao"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19844v3",
                "updated": "2025-03-12T08:56:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    56,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-27T07:39:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    7,
                    39,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "ProAPO: Progressively Automatic Prompt Optimization for Visual\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProAPO: Progressively Automatic Prompt Optimization for Visual\n  Classification"
                },
                "summary": "Vision-language models (VLMs) have made significant progress in image\nclassification by training with large-scale paired image-text data. Their\nperformances largely depend on the prompt quality. While recent methods show\nthat visual descriptions generated by large language models (LLMs) enhance the\ngeneralization of VLMs, class-specific prompts may be inaccurate or lack\ndiscrimination due to the hallucination in LLMs. In this paper, we aim to find\nvisually discriminative prompts for fine-grained categories with minimal\nsupervision and no human-in-the-loop. An evolution-based algorithm is proposed\nto progressively optimize language prompts from task-specific templates to\nclass-specific descriptions. Unlike optimizing templates, the search space\nshows an explosion in class-specific candidate prompts. This increases prompt\ngeneration costs, iterative times, and the overfitting problem. To this end, we\nfirst introduce several simple yet effective edit-based and evolution-based\noperations to generate diverse candidate prompts by one-time query of LLMs.\nThen, two sampling strategies are proposed to find a better initial search\npoint and reduce traversed categories, saving iteration costs. Moreover, we\napply a novel fitness score with entropy constraints to mitigate overfitting.\nIn a challenging one-shot image classification setting, our method outperforms\nexisting textual prompt-based methods and improves LLM-generated description\nmethods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts\nimprove adapter-based methods and transfer effectively across different\nbackbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) have made significant progress in image\nclassification by training with large-scale paired image-text data. Their\nperformances largely depend on the prompt quality. While recent methods show\nthat visual descriptions generated by large language models (LLMs) enhance the\ngeneralization of VLMs, class-specific prompts may be inaccurate or lack\ndiscrimination due to the hallucination in LLMs. In this paper, we aim to find\nvisually discriminative prompts for fine-grained categories with minimal\nsupervision and no human-in-the-loop. An evolution-based algorithm is proposed\nto progressively optimize language prompts from task-specific templates to\nclass-specific descriptions. Unlike optimizing templates, the search space\nshows an explosion in class-specific candidate prompts. This increases prompt\ngeneration costs, iterative times, and the overfitting problem. To this end, we\nfirst introduce several simple yet effective edit-based and evolution-based\noperations to generate diverse candidate prompts by one-time query of LLMs.\nThen, two sampling strategies are proposed to find a better initial search\npoint and reduce traversed categories, saving iteration costs. Moreover, we\napply a novel fitness score with entropy constraints to mitigate overfitting.\nIn a challenging one-shot image classification setting, our method outperforms\nexisting textual prompt-based methods and improves LLM-generated description\nmethods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts\nimprove adapter-based methods and transfer effectively across different\nbackbones."
                },
                "authors": [
                    {
                        "name": "Xiangyan Qu"
                    },
                    {
                        "name": "Gaopeng Gou"
                    },
                    {
                        "name": "Jiamin Zhuang"
                    },
                    {
                        "name": "Jing Yu"
                    },
                    {
                        "name": "Kun Song"
                    },
                    {
                        "name": "Qihao Wang"
                    },
                    {
                        "name": "Yili Li"
                    },
                    {
                        "name": "Gang Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Gang Xiong"
                },
                "author": "Gang Xiong",
                "arxiv_comment": "Accepted to the IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01478v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01478v4",
                "updated": "2025-03-12T08:49:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    49,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-03T12:37:34Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    12,
                    37,
                    34,
                    0,
                    62,
                    0
                ],
                "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction"
                },
                "summary": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios."
                },
                "authors": [
                    {
                        "name": "Lu Dai"
                    },
                    {
                        "name": "Yijie Xu"
                    },
                    {
                        "name": "Jinhui Ye"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "ICLR 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01478v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01478v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v2",
                "updated": "2025-03-12T08:48:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    48,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "48 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08179v2",
                "updated": "2025-03-12T08:46:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    46,
                    33,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T08:43:05Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    43,
                    5,
                    1,
                    70,
                    0
                ],
                "title": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProtTeX: Structure-In-Context Reasoning and Editing of Proteins with\n  Large Language Models"
                },
                "summary": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have made remarkable progress in the field of molecular\nscience, particularly in understanding and generating functional small\nmolecules. This success is largely attributed to the effectiveness of molecular\ntokenization strategies. In protein science, the amino acid sequence serves as\nthe sole tokenizer for LLMs. However, many fundamental challenges in protein\nscience are inherently structure-dependent. The absence of structure-aware\ntokens significantly limits the capabilities of LLMs for comprehensive\nbiomolecular comprehension and multimodal generation. To address these\nchallenges, we introduce a novel framework, ProtTeX, which tokenizes the\nprotein sequences, structures, and textual information into a unified discrete\nspace. This innovative approach enables joint training of the LLM exclusively\nthrough the Next-Token Prediction paradigm, facilitating multimodal protein\nreasoning and generation. ProtTeX enables general LLMs to perceive and process\nprotein structures through sequential text input, leverage structural\ninformation as intermediate reasoning components, and generate or manipulate\nstructures via sequential text output. Experiments demonstrate that our model\nachieves significant improvements in protein function prediction, outperforming\nthe state-of-the-art domain expert model with a twofold increase in accuracy.\nOur framework enables high-quality conformational generation and customizable\nprotein design. For the first time, we demonstrate that by adopting the\nstandard training and inference pipelines from the LLM domain, ProtTeX empowers\ndecoder-only LLMs to effectively address diverse spectrum of protein-related\ntasks."
                },
                "authors": [
                    {
                        "name": "Zicheng Ma"
                    },
                    {
                        "name": "Chuanliu Fan"
                    },
                    {
                        "name": "Zhicong Wang"
                    },
                    {
                        "name": "Zhenyu Chen"
                    },
                    {
                        "name": "Xiaohan Lin"
                    },
                    {
                        "name": "Yanheng Li"
                    },
                    {
                        "name": "Shihao Feng"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Ziqiang Cao"
                    },
                    {
                        "name": "Yi Qin Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yi Qin Gao"
                },
                "author": "Yi Qin Gao",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05712v2",
                "updated": "2025-03-12T08:45:28Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    45,
                    28,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-10T05:07:27Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    5,
                    7,
                    27,
                    4,
                    10,
                    0
                ],
                "title": "Multi-Step Reasoning in Korean and the Emergent Mirage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Step Reasoning in Korean and the Emergent Mirage"
                },
                "summary": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce HRMCR (HAE-RAE Multi-Step Commonsense Reasoning), a benchmark\ndesigned to evaluate large language models' ability to perform multi-step\nreasoning in culturally specific contexts, focusing on Korean. The questions\nare automatically generated via templates and algorithms, requiring LLMs to\nintegrate Korean cultural knowledge into sequential reasoning steps. Consistent\nwith prior observations on emergent abilities, our experiments reveal that\nmodels trained on fewer than \\(2 \\cdot 10^{25}\\) training FLOPs struggle to\nsolve any questions, showing near-zero performance. Beyond this threshold,\nperformance improves sharply. State-of-the-art models (e.g., O1) still score\nunder 50\\%, underscoring the difficulty of our tasks. Notably, stepwise\nanalysis suggests the observed emergent behavior may stem from compounding\nerrors across multiple steps rather than reflecting a genuinely new capability.\nWe publicly release the benchmark and commit to regularly updating the dataset\nto prevent contamination."
                },
                "authors": [
                    {
                        "name": "Guijin Son"
                    },
                    {
                        "name": "Hyunwoo Ko"
                    },
                    {
                        "name": "Dasol Choi"
                    }
                ],
                "author_detail": {
                    "name": "Dasol Choi"
                },
                "author": "Dasol Choi",
                "arxiv_comment": "C3NLP @ NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09158v1",
                "updated": "2025-03-12T08:33:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    33,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T08:33:46Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    33,
                    46,
                    2,
                    71,
                    0
                ],
                "title": "FaVChat: Unlocking Fine-Grained Facail Video Understanding with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaVChat: Unlocking Fine-Grained Facail Video Understanding with\n  Multimodal Large Language Models"
                },
                "summary": "Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (VMLLMs) have demonstrated\nremarkable potential in cross-modal video understanding. However, their\nabilities in fine-grained face comprehension remain largely underexplored.\nGiven its pivotal role in human-centric intelligence, developing VMLLMs for\nfacial understanding holds a fundamental problem. To address this gap, we\npropose FaVChat, the first VMLLM specifically designed for fine-grained facial\nvideo understanding. To facilitate its training, we construct a large-scale\nfacial video dataset comprising over 60k videos, with the majority annotated\nwith 83 fine-grained facial attributes. These attributes are incorporated to\nenrich GPT-4o-generated captions, yielding 60k high-quality video-summary pairs\nand an additional 170k fine-grained question-answering (QA) pairs. To\neffectively capture rich facial clues, we propose a hybrid model architecture\ncomposed of a general visual encoder, a dedicated facial encoder, and a\nmixture-of-experts-enhanced adapter for adaptive fusion of multi-source visual\nfeatures. To mitigate information loss during feature transformation, we\nextract multi-granularity representations from the facial encoder and integrate\nthem into the subsequent LLM. This design enhances the model's ability to\ncomprehend and respond to questions involving diverse levels of visual details.\nWe employ a progressive training paradigm, transitioning from video\nsummarization to a high-quality subset of video QA, gradually increasing task\ncomplexity to enhance the model's fine-grained visual perception. We conduct\nextensive zero-shot evaluation on a couple of public benchmarks, demonstrating\nthat FaVChat consistently surpasses existing VMLLMs across multiple tasks."
                },
                "authors": [
                    {
                        "name": "Fufangchen Zhao"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Linrui Xu"
                    },
                    {
                        "name": "Wenhao Jiang"
                    },
                    {
                        "name": "Jian Gao"
                    },
                    {
                        "name": "Danfeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Danfeng Yan"
                },
                "author": "Danfeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09153v1",
                "updated": "2025-03-12T08:29:59Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    29,
                    59,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T08:29:59Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    29,
                    59,
                    2,
                    71,
                    0
                ],
                "title": "Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News\n  Detection"
                },
                "summary": "The questionable responses caused by knowledge hallucination may lead to\nLLMs' unstable ability in decision-making. However, it has never been\ninvestigated whether the LLMs' hallucination is possibly usable to generate\nnegative reasoning for facilitating the detection of fake news. This study\nproposes a novel supervised self-reinforced reasoning rectification approach -\nSR$^3$ that yields both common reasonable reasoning and wrong understandings\n(negative reasoning) for news via LLMs reflection for semantic consistency\nlearning. Upon that, we construct a negative reasoning-based news learning\nmodel called - \\emph{NRFE}, which leverages positive or negative news-reasoning\npairs for learning the semantic consistency between them. To avoid the impact\nof label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that\nonly takes news content as input to inspect the performance of our method by\ndistilling the knowledge from \\emph{NRFE}. The experimental results verified on\nthree popular fake news datasets demonstrate the superiority of our method\ncompared with three kinds of baselines including prompting on LLMs, fine-tuning\non pre-trained SLMs, and other representative fake news detection methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The questionable responses caused by knowledge hallucination may lead to\nLLMs' unstable ability in decision-making. However, it has never been\ninvestigated whether the LLMs' hallucination is possibly usable to generate\nnegative reasoning for facilitating the detection of fake news. This study\nproposes a novel supervised self-reinforced reasoning rectification approach -\nSR$^3$ that yields both common reasonable reasoning and wrong understandings\n(negative reasoning) for news via LLMs reflection for semantic consistency\nlearning. Upon that, we construct a negative reasoning-based news learning\nmodel called - \\emph{NRFE}, which leverages positive or negative news-reasoning\npairs for learning the semantic consistency between them. To avoid the impact\nof label-implicated reasoning, we deploy a student model - \\emph{NRFE-D} that\nonly takes news content as input to inspect the performance of our method by\ndistilling the knowledge from \\emph{NRFE}. The experimental results verified on\nthree popular fake news datasets demonstrate the superiority of our method\ncompared with three kinds of baselines including prompting on LLMs, fine-tuning\non pre-trained SLMs, and other representative fake news detection methods."
                },
                "authors": [
                    {
                        "name": "Chaowei Zhang"
                    },
                    {
                        "name": "Zongling Feng"
                    },
                    {
                        "name": "Zewei Zhang"
                    },
                    {
                        "name": "Jipeng Qiang"
                    },
                    {
                        "name": "Guandong Xu"
                    },
                    {
                        "name": "Yun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yun Li"
                },
                "author": "Yun Li",
                "arxiv_comment": "9 pages, 12 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09150v1",
                "updated": "2025-03-12T08:25:58Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    25,
                    58,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T08:25:58Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    25,
                    58,
                    2,
                    71,
                    0
                ],
                "title": "AdaptAI: A Personalized Solution to Sense Your Stress, Fix Your Mess,\n  and Boost Productivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptAI: A Personalized Solution to Sense Your Stress, Fix Your Mess,\n  and Boost Productivity"
                },
                "summary": "Personalization is a critical yet often overlooked factor in boosting\nproductivity and wellbeing in knowledge-intensive workplaces to better address\nindividual preferences. Existing tools typically offer uniform guidance whether\nauto-generating email responses or prompting break reminders without accounting\nfor individual behavioral patterns or stress triggers. We introduce AdaptAI, a\nmultimodal AI solution combining egocentric vision and audio, heart and motion\nactivities, and the agentic workflow of Large Language Models LLMs to deliver\nhighly personalized productivity support and context-aware well-being\ninterventions. AdaptAI not only automates peripheral tasks (e.g. drafting\nsuccinct document summaries, replying to emails etc.) but also continuously\nmonitors the users unique physiological and situational indicators to\ndynamically tailor interventions such as micro-break suggestions or exercise\nprompts, at the exact point of need. In a preliminary study with 15\nparticipants, AdaptAI demonstrated significant improvements in task throughput\nand user satisfaction by anticipating user stressors and streamlining daily\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization is a critical yet often overlooked factor in boosting\nproductivity and wellbeing in knowledge-intensive workplaces to better address\nindividual preferences. Existing tools typically offer uniform guidance whether\nauto-generating email responses or prompting break reminders without accounting\nfor individual behavioral patterns or stress triggers. We introduce AdaptAI, a\nmultimodal AI solution combining egocentric vision and audio, heart and motion\nactivities, and the agentic workflow of Large Language Models LLMs to deliver\nhighly personalized productivity support and context-aware well-being\ninterventions. AdaptAI not only automates peripheral tasks (e.g. drafting\nsuccinct document summaries, replying to emails etc.) but also continuously\nmonitors the users unique physiological and situational indicators to\ndynamically tailor interventions such as micro-break suggestions or exercise\nprompts, at the exact point of need. In a preliminary study with 15\nparticipants, AdaptAI demonstrated significant improvements in task throughput\nand user satisfaction by anticipating user stressors and streamlining daily\nworkflows."
                },
                "authors": [
                    {
                        "name": "Rushiraj Gadhvi"
                    },
                    {
                        "name": "Soham Petkar"
                    },
                    {
                        "name": "Priyansh Desai"
                    },
                    {
                        "name": "Shreyas Ramachandran"
                    },
                    {
                        "name": "Siddharth Siddharth"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Siddharth"
                },
                "author": "Siddharth Siddharth",
                "arxiv_comment": "Accepted for publication at the ACM Conference on Human Factors in\n  Computing Systems (CHI) Late Breaking Work 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02233v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02233v2",
                "updated": "2025-03-12T07:42:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    42,
                    4,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-04T03:16:02Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    16,
                    2,
                    1,
                    63,
                    0
                ],
                "title": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling"
                },
                "summary": "Large language models (LLMs) frequently hallucinate due to misaligned\nself-awareness, generating erroneous outputs when addressing queries beyond\ntheir knowledge boundaries. While existing approaches mitigate hallucinations\nvia uncertainty estimation or query rejection, they suffer from computational\ninefficiency or sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate use of high-confidence outputs. For uncertain predictions, a\nslow refinement model conducts targeted reasoning to improve accuracy. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. Our work establishes a scalable paradigm for advancing LLM\nreliability and balancing accuracy and practical utility in error-sensitive\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently hallucinate due to misaligned\nself-awareness, generating erroneous outputs when addressing queries beyond\ntheir knowledge boundaries. While existing approaches mitigate hallucinations\nvia uncertainty estimation or query rejection, they suffer from computational\ninefficiency or sacrificed helpfulness. To address these issues, we propose the\nExplicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and\nslow reasoning systems to harmonize reliability and usability. The framework\nfirst employs a fast-thinking model to generate confidence-labeled responses,\nenabling immediate use of high-confidence outputs. For uncertain predictions, a\nslow refinement model conducts targeted reasoning to improve accuracy. To align\nmodel behavior with our proposed object, we propose a hybrid training pipeline,\nenhancing self-awareness without degrading task performance. Evaluations on\ndialogue state tracking tasks demonstrate that EKBM achieves superior model\nreliability over uncertainty-based baselines. Further analysis reveals that\nrefinement substantially boosts accuracy while maintaining low computational\noverhead. Our work establishes a scalable paradigm for advancing LLM\nreliability and balancing accuracy and practical utility in error-sensitive\napplications."
                },
                "authors": [
                    {
                        "name": "Hang Zheng"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Yuncong Liu"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Pascale Fung"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02233v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02233v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09117v1",
                "updated": "2025-03-12T07:08:54Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    8,
                    54,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T07:08:54Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    8,
                    54,
                    2,
                    71,
                    0
                ],
                "title": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRU: Mitigating the Trade-off between Unlearning and Retention for Large\n  Language Models"
                },
                "summary": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. In examining\nthe update process for unlearning dynamically, we find gradients hold essential\ninformation for revealing this trade-off. In particular, we look at the varying\nrelationship between retention performance and directional disparities between\ngradients during unlearning. It motivates the sculpting of an update mechanism\nderived from gradients from two sources, i.e., harmful for retention and useful\nfor unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an\nenhanced unlearning framework controlling the updating gradients in a\ngeometry-focused and optimization-driven manner such that their side impacts on\nother, unrelated responses can be minimized. Specifically, GRU derives a\nclosed-form solution to project the unlearning gradient onto the orthogonal\nspace of that gradient harmful for retention, ensuring minimal deviation from\nits original direction under the condition that overall performance is\nretained. Comprehensive experiments are conducted to demonstrate that GRU, as a\ngeneral framework, is straightforward to implement and efficiently enhances a\nrange of baseline methods through its adaptable and compatible characteristics.\nAdditionally, experimental results show its broad effectiveness across a\ndiverse set of benchmarks for LLM unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) unlearning has demonstrated its essential role in\nremoving privacy and copyright-related responses, crucial for their legal and\nsafe applications. However, the pursuit of complete unlearning often comes with\nsubstantial costs due to its compromises in their general functionality,\nleading to a notorious trade-off between unlearning and retention. In examining\nthe update process for unlearning dynamically, we find gradients hold essential\ninformation for revealing this trade-off. In particular, we look at the varying\nrelationship between retention performance and directional disparities between\ngradients during unlearning. It motivates the sculpting of an update mechanism\nderived from gradients from two sources, i.e., harmful for retention and useful\nfor unlearning. Accordingly, we propose Gradient Rectified Unlearning (GRU), an\nenhanced unlearning framework controlling the updating gradients in a\ngeometry-focused and optimization-driven manner such that their side impacts on\nother, unrelated responses can be minimized. Specifically, GRU derives a\nclosed-form solution to project the unlearning gradient onto the orthogonal\nspace of that gradient harmful for retention, ensuring minimal deviation from\nits original direction under the condition that overall performance is\nretained. Comprehensive experiments are conducted to demonstrate that GRU, as a\ngeneral framework, is straightforward to implement and efficiently enhances a\nrange of baseline methods through its adaptable and compatible characteristics.\nAdditionally, experimental results show its broad effectiveness across a\ndiverse set of benchmarks for LLM unlearning."
                },
                "authors": [
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Qizhou Wang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yali Du"
                    },
                    {
                        "name": "Xiaojiang Du"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09114v1",
                "updated": "2025-03-12T07:01:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    1,
                    34,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T07:01:34Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    7,
                    1,
                    34,
                    2,
                    71,
                    0
                ],
                "title": "Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of\n  Language Model Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sometimes Painful but Certainly Promising: Feasibility and Trade-offs of\n  Language Model Inference at the Edge"
                },
                "summary": "The rapid rise of Language Models (LMs) has expanded the capabilities of\nnatural language processing, powering applications from text generation to\ncomplex decision-making. While state-of-the-art LMs often boast hundreds of\nbillions of parameters and are primarily deployed in data centers, recent\ntrends show a growing focus on compact models-typically under 10 billion\nparameters-enabled by techniques such as quantization and other model\ncompression techniques. This shift paves the way for LMs on edge devices,\noffering potential benefits such as enhanced privacy, reduced latency, and\nimproved data sovereignty. However, the inherent complexity of even these\nsmaller models, combined with the limited computing resources of edge hardware,\nraises critical questions about the practical trade-offs in executing LM\ninference outside the cloud. To address these challenges, we present a\ncomprehensive evaluation of generative LM inference on representative CPU-based\nand GPU-accelerated edge devices. Our study measures key performance\nindicators-including memory usage, inference speed, and energy\nconsumption-across various device configurations. Additionally, we examine\nthroughput-energy trade-offs, cost considerations, and usability, alongside an\nassessment of qualitative model performance. While quantization helps mitigate\nmemory overhead, it does not fully eliminate resource bottlenecks, especially\nfor larger models. Our findings quantify the memory and energy constraints that\nmust be considered for practical real-world deployments, offering concrete\ninsights into the trade-offs between model size, inference performance, and\nefficiency. The exploration of LMs at the edge is still in its early stages. We\nhope this study provides a foundation for future research, guiding the\nrefinement of models, the enhancement of inference efficiency, and the\nadvancement of edge-centric AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Language Models (LMs) has expanded the capabilities of\nnatural language processing, powering applications from text generation to\ncomplex decision-making. While state-of-the-art LMs often boast hundreds of\nbillions of parameters and are primarily deployed in data centers, recent\ntrends show a growing focus on compact models-typically under 10 billion\nparameters-enabled by techniques such as quantization and other model\ncompression techniques. This shift paves the way for LMs on edge devices,\noffering potential benefits such as enhanced privacy, reduced latency, and\nimproved data sovereignty. However, the inherent complexity of even these\nsmaller models, combined with the limited computing resources of edge hardware,\nraises critical questions about the practical trade-offs in executing LM\ninference outside the cloud. To address these challenges, we present a\ncomprehensive evaluation of generative LM inference on representative CPU-based\nand GPU-accelerated edge devices. Our study measures key performance\nindicators-including memory usage, inference speed, and energy\nconsumption-across various device configurations. Additionally, we examine\nthroughput-energy trade-offs, cost considerations, and usability, alongside an\nassessment of qualitative model performance. While quantization helps mitigate\nmemory overhead, it does not fully eliminate resource bottlenecks, especially\nfor larger models. Our findings quantify the memory and energy constraints that\nmust be considered for practical real-world deployments, offering concrete\ninsights into the trade-offs between model size, inference performance, and\nefficiency. The exploration of LMs at the edge is still in its early stages. We\nhope this study provides a foundation for future research, guiding the\nrefinement of models, the enhancement of inference efficiency, and the\nadvancement of edge-centric AI systems."
                },
                "authors": [
                    {
                        "name": "Maximilian Abstreiter"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    },
                    {
                        "name": "Roberto Morabito"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Morabito"
                },
                "author": "Roberto Morabito",
                "arxiv_comment": "This paper is currently under review for publication in an ACM\n  journal. If accepted, the copyright will be transferred to ACM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09103v1",
                "updated": "2025-03-12T06:43:25Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    43,
                    25,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T06:43:25Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    43,
                    25,
                    2,
                    71,
                    0
                ],
                "title": "VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for\n  Detecting LLM-Generated Vaccine Misinformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VaxGuard: A Multi-Generator, Multi-Type, and Multi-Role Dataset for\n  Detecting LLM-Generated Vaccine Misinformation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved text generation capabilities. However, they also present challenges,\nparticularly in generating vaccine-related misinformation, which poses risks to\npublic health. Despite research on human-authored misinformation, a notable gap\nremains in understanding how LLMs contribute to vaccine misinformation and how\nbest to detect it. Existing benchmarks often overlook vaccine-specific\nmisinformation and the diverse roles of misinformation spreaders. This paper\nintroduces VaxGuard, a novel dataset designed to address these challenges.\nVaxGuard includes vaccine-related misinformation generated by multiple LLMs and\nprovides a comprehensive framework for detecting misinformation across various\nroles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other\nLLMs in detecting misinformation, especially when dealing with subtle or\nemotionally charged narratives. On the other hand, PHI3 and Mistral show lower\nperformance, struggling with precision and recall in fear-driven contexts.\nAdditionally, detection performance tends to decline as input text length\nincreases, indicating the need for improved methods to handle larger content.\nThese results highlight the importance of role-specific detection strategies\nand suggest that VaxGuard can serve as a key resource for improving the\ndetection of LLM-generated vaccine misinformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved text generation capabilities. However, they also present challenges,\nparticularly in generating vaccine-related misinformation, which poses risks to\npublic health. Despite research on human-authored misinformation, a notable gap\nremains in understanding how LLMs contribute to vaccine misinformation and how\nbest to detect it. Existing benchmarks often overlook vaccine-specific\nmisinformation and the diverse roles of misinformation spreaders. This paper\nintroduces VaxGuard, a novel dataset designed to address these challenges.\nVaxGuard includes vaccine-related misinformation generated by multiple LLMs and\nprovides a comprehensive framework for detecting misinformation across various\nroles. Our findings show that GPT-3.5 and GPT-4o consistently outperform other\nLLMs in detecting misinformation, especially when dealing with subtle or\nemotionally charged narratives. On the other hand, PHI3 and Mistral show lower\nperformance, struggling with precision and recall in fear-driven contexts.\nAdditionally, detection performance tends to decline as input text length\nincreases, indicating the need for improved methods to handle larger content.\nThese results highlight the importance of role-specific detection strategies\nand suggest that VaxGuard can serve as a key resource for improving the\ndetection of LLM-generated vaccine misinformation."
                },
                "authors": [
                    {
                        "name": "Syed Talal Ahmad"
                    },
                    {
                        "name": "Haohui Lu"
                    },
                    {
                        "name": "Sidong Liu"
                    },
                    {
                        "name": "Annie Lau"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15131v3",
                "updated": "2025-03-12T06:15:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    15,
                    34,
                    2,
                    71,
                    0
                ],
                "published": "2024-02-23T06:32:18Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    6,
                    32,
                    18,
                    4,
                    54,
                    0
                ],
                "title": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question\n  Answering with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question\n  Answering with Large Language Models"
                },
                "summary": "This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the realm of knowledge base question answering (KBQA).\nKBQA is considered a challenging task, particularly in parsing intricate\nquestions into executable logical forms. Traditional semantic parsing\n(SP)-based methods require extensive data annotations, which result in\nsignificant costs. Recently, the advent of few-shot in-context learning,\npowered by large language models (LLMs), has showcased promising capabilities.\nHowever, fully leveraging LLMs to parse questions into logical forms in\nlow-resource scenarios poses a substantial challenge. To tackle these hurdles,\nwe introduce Interactive-KBQA, a framework designed to generate logical forms\nthrough direct interaction with knowledge bases (KBs). Within this framework,\nwe have developed three generic APIs for KB interaction. For each category of\ncomplex question, we devised exemplars to guide LLMs through the reasoning\nprocesses. Our method achieves competitive results on the WebQuestionsSP,\nComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of\nexamples (shots). Importantly, our approach supports manual intervention,\nallowing for the iterative refinement of LLM outputs. By annotating a dataset\nwith step-wise reasoning processes, we showcase our model's adaptability and\nhighlight its potential for contributing significant enhancements to the field."
                },
                "authors": [
                    {
                        "name": "Guanming Xiong"
                    },
                    {
                        "name": "Junwei Bao"
                    },
                    {
                        "name": "Wen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhao"
                },
                "author": "Wen Zhao",
                "arxiv_doi": "10.18653/v1/2024.acl-long.569",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.acl-long.569",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.15131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This work has been accepted by the ACL 2024 main conference. Code and\n  data are available at: https://github.com/JimXiongGM/Interactive-KBQA",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08161v2",
                "updated": "2025-03-12T06:04:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    4,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T08:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    8,
                    26,
                    37,
                    1,
                    70,
                    0
                ],
                "title": "OASIS: Order-Augmented Strategy for Improved Code Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS: Order-Augmented Strategy for Improved Code Search"
                },
                "summary": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code embeddings capture the semantic representations of code and are crucial\nfor various code-related large language model (LLM) applications, such as code\nsearch. Previous training primarily relies on optimizing the InfoNCE loss by\ncomparing positive natural language (NL)-code pairs with in-batch negatives.\nHowever, due to the sparse nature of code contexts, training solely by\ncomparing the major differences between positive and negative pairs may fail to\ncapture deeper semantic nuances. To address this issue, we propose a novel\norder-augmented strategy for improved code search (OASIS). It leverages\norder-based similarity labels to train models to capture subtle differences in\nsimilarity among negative pairs. Extensive benchmark evaluations demonstrate\nthat our OASIS model significantly outperforms previous state-of-the-art models\nfocusing solely on major positive-negative differences. It underscores the\nvalue of exploiting subtle differences among negative pairs with order labels\nfor effective code embedding training."
                },
                "authors": [
                    {
                        "name": "Zuchen Gao"
                    },
                    {
                        "name": "Zizheng Zhan"
                    },
                    {
                        "name": "Xianming Li"
                    },
                    {
                        "name": "Erxin Yu"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yuqun Zhang"
                    },
                    {
                        "name": "Jing Li"
                    }
                ],
                "author_detail": {
                    "name": "Jing Li"
                },
                "author": "Jing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09091v1",
                "updated": "2025-03-12T06:03:33Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    3,
                    33,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T06:03:33Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    3,
                    33,
                    2,
                    71,
                    0
                ],
                "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Modal Foundation Models for Computational Pathology: A Survey"
                },
                "summary": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have emerged as a powerful paradigm in computational\npathology (CPath), enabling scalable and generalizable analysis of\nhistopathological images. While early developments centered on uni-modal models\ntrained solely on visual data, recent advances have highlighted the promise of\nmulti-modal foundation models that integrate heterogeneous data sources such as\ntextual reports, structured domain knowledge, and molecular profiles. In this\nsurvey, we provide a comprehensive and up-to-date review of multi-modal\nfoundation models in CPath, with a particular focus on models built upon\nhematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level\nrepresentations. We categorize 32 state-of-the-art multi-modal foundation\nmodels into three major paradigms: vision-language, vision-knowledge graph, and\nvision-gene expression. We further divide vision-language models into\nnon-LLM-based and LLM-based approaches. Additionally, we analyze 28 available\nmulti-modal datasets tailored for pathology, grouped into image-text pairs,\ninstruction datasets, and image-other modality pairs. Our survey also presents\na taxonomy of downstream tasks, highlights training and evaluation strategies,\nand identifies key challenges and future directions. We aim for this survey to\nserve as a valuable resource for researchers and practitioners working at the\nintersection of pathology and AI."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Guihong Wan"
                    },
                    {
                        "name": "Xintao Wu"
                    },
                    {
                        "name": "Xinyu Wu"
                    },
                    {
                        "name": "Xiaohui Chen"
                    },
                    {
                        "name": "Yi He"
                    },
                    {
                        "name": "Christine G. Lian"
                    },
                    {
                        "name": "Peter K. Sorger"
                    },
                    {
                        "name": "Yevgeniy R. Semenov"
                    },
                    {
                        "name": "Chen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhao"
                },
                "author": "Chen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08301v2",
                "updated": "2025-03-12T06:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    0,
                    27,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-11T11:13:11Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    11,
                    13,
                    11,
                    1,
                    70,
                    0
                ],
                "title": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task\n  Optimization: A Proof-of-Principle Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model as Meta-Surrogate for Data-Driven Many-Task\n  Optimization: A Proof-of-Principle Study"
                },
                "summary": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many-task optimization scenarios, surrogate models are valuable for\nmitigating the computational burden of repeated fitness evaluations across\ntasks. This study proposes a novel meta-surrogate framework to assist many-task\noptimization, by leveraging the knowledge transfer strengths and emergent\ncapabilities of large language models (LLMs). We formulate a unified framework\nfor many-task fitness prediction, by defining a universal model with metadata\nto fit a group of problems. Fitness prediction is performed on metadata and\ndecision variables, enabling efficient knowledge sharing across tasks and\nadaptability to new tasks. The LLM-based meta-surrogate treats fitness\nprediction as conditional probability estimation, employing a unified token\nsequence representation for task metadata, inputs, and outputs. This approach\nfacilitates efficient inter-task knowledge sharing through shared token\nembeddings and captures complex task dependencies via multi-task model\ntraining. Experimental results demonstrate the model's emergent generalization\nability, including zero-shot performance on problems with unseen dimensions.\nWhen integrated into evolutionary transfer optimization (ETO), our framework\nsupports dual-level knowledge transfer -- at both the surrogate and individual\nlevels -- enhancing optimization efficiency and robustness. This work\nestablishes a novel foundation for applying LLMs in surrogate modeling,\noffering a versatile solution for many-task optimization."
                },
                "authors": [
                    {
                        "name": "Xian-Rong Zhang"
                    },
                    {
                        "name": "Yue-Jiao Gong"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09090v1",
                "updated": "2025-03-12T06:00:16Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    0,
                    16,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T06:00:16Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    6,
                    0,
                    16,
                    2,
                    71,
                    0
                ],
                "title": "Data-Driven Inverse Optimal Control for Continuous-Time Nonlinear\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Inverse Optimal Control for Continuous-Time Nonlinear\n  Systems"
                },
                "summary": "This paper introduces a novel model-free and a partially model-free algorithm\nfor inverse optimal control (IOC), also known as inverse reinforcement learning\n(IRL), aimed at estimating the cost function of continuous-time nonlinear\ndeterministic systems. Using the input-state trajectories of an expert agent,\nthe proposed algorithms separately utilize control policy information and the\nHamilton-Jacobi-Bellman equation to estimate different sets of cost function\nparameters. This approach allows the algorithms to achieve broader\napplicability while maintaining a model-free framework. Also, the model-free\nalgorithm reduces complexity compared to existing methods, as it requires\nsolving a forward optimal control problem only once during initialization.\nFurthermore, in our partially model-free algorithm, this step can be bypassed\nentirely for systems with known input dynamics. Simulation results demonstrate\nthe effectiveness and efficiency of our algorithms, highlighting their\npotential for real-world deployment in autonomous systems and robotics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel model-free and a partially model-free algorithm\nfor inverse optimal control (IOC), also known as inverse reinforcement learning\n(IRL), aimed at estimating the cost function of continuous-time nonlinear\ndeterministic systems. Using the input-state trajectories of an expert agent,\nthe proposed algorithms separately utilize control policy information and the\nHamilton-Jacobi-Bellman equation to estimate different sets of cost function\nparameters. This approach allows the algorithms to achieve broader\napplicability while maintaining a model-free framework. Also, the model-free\nalgorithm reduces complexity compared to existing methods, as it requires\nsolving a forward optimal control problem only once during initialization.\nFurthermore, in our partially model-free algorithm, this step can be bypassed\nentirely for systems with known input dynamics. Simulation results demonstrate\nthe effectiveness and efficiency of our algorithms, highlighting their\npotential for real-world deployment in autonomous systems and robotics."
                },
                "authors": [
                    {
                        "name": "Hamed Jabbari Asl"
                    },
                    {
                        "name": "Eiji Uchibe"
                    }
                ],
                "author_detail": {
                    "name": "Eiji Uchibe"
                },
                "author": "Eiji Uchibe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09089v1",
                "updated": "2025-03-12T05:55:01Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    55,
                    1,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T05:55:01Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    55,
                    1,
                    2,
                    71,
                    0
                ],
                "title": "LocAgent: Graph-Guided LLM Agents for Code Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LocAgent: Graph-Guided LLM Agents for Code Localization"
                },
                "summary": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code localization--identifying precisely where in a codebase changes need to\nbe made--is a fundamental yet challenging task in software maintenance.\nExisting approaches struggle to efficiently navigate complex codebases when\nidentifying relevant code sections. The challenge lies in bridging natural\nlanguage problem descriptions with the appropriate code elements, often\nrequiring reasoning across hierarchical structures and multiple dependencies.\nWe introduce LocAgent, a framework that addresses code localization through\ngraph-based representation. By parsing codebases into directed heterogeneous\ngraphs, LocAgent creates a lightweight representation that captures code\nstructures (files, classes, functions) and their dependencies (imports,\ninvocations, inheritance), enabling LLM agents to effectively search and locate\nrelevant entities through powerful multi-hop reasoning. Experimental results on\nreal-world benchmarks demonstrate that our approach significantly enhances\naccuracy in code localization. Notably, our method with the fine-tuned\nQwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA\nproprietary models at greatly reduced cost (approximately 86% reduction),\nreaching up to 92.7% accuracy on file-level localization while improving\ndownstream GitHub issue resolution success rates by 12% for multiple attempts\n(Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent."
                },
                "authors": [
                    {
                        "name": "Zhaoling Chen"
                    },
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhiwei Jiang"
                    },
                    {
                        "name": "Viktor Prasanna"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xingyao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingyao Wang"
                },
                "author": "Xingyao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05628v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05628v5",
                "updated": "2025-03-12T05:54:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    54,
                    44,
                    2,
                    71,
                    0
                ],
                "published": "2024-10-08T02:23:53Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    2,
                    23,
                    53,
                    1,
                    282,
                    0
                ],
                "title": "A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Motion Reasoning and Generation in Human\n  Interaction"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural and contextually relevant text,\nenabling more human-like AI interactions. However, generating and understanding\ninteractive human-like motion, where multiple individuals engage in coordinated\nmovements, remains challenging due to the complexity of modeling these\ninteractions. Additionally, a unified and versatile model is needed to handle\ndiverse interactive scenarios, such as chat systems that dynamically adapt to\nuser instructions and assigned roles. To address these challenges, we introduce\nVIM, the Versatile Interactive Motion-language model, which integrates both\nlanguage and motion modalities to effectively understand, generate, and control\ninteractive motions in multi-turn conversational contexts. Unlike previous\nstudies that primarily focus on uni-directional tasks such as text-to-motion or\nmotion-to-text, VIM employs a unified architecture capable of simultaneously\nunderstanding and generating both motion and text modalities. Given the absence\nof an appropriate dataset to support this task, we introduce Inter-MT2, a\nlarge-scale instruction-tuning dataset containing 82.7K multi-turn interactive\nmotion instructions, covering 153K interactive motion samples. Inter-MT2 spans\ndiverse instructional scenarios, including motion editing, question answering,\nand story generation, leveraging off-the-shelf large language models and motion\ndiffusion models to construct a broad set of interactive motion instructions.\nWe extensively evaluate the versatility of VIM across multiple interactive\nmotion-related tasks, including motion-to-text, text-to-motion, reaction\ngeneration, motion editing, and reasoning about motion sequences."
                },
                "authors": [
                    {
                        "name": "Jeongeun Park"
                    },
                    {
                        "name": "Sungjoon Choi"
                    },
                    {
                        "name": "Sangdoo Yun"
                    }
                ],
                "author_detail": {
                    "name": "Sangdoo Yun"
                },
                "author": "Sangdoo Yun",
                "arxiv_comment": "https://vim-motion-language.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05628v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05628v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00936v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00936v4",
                "updated": "2025-03-12T05:48:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    48,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2024-07-01T03:37:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    3,
                    37,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Enhanced Knowledge Representation Learning: A\n  Survey"
                },
                "summary": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Representation Learning (KRL) is crucial for enabling applications\nof symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by\nprojecting knowledge facts into vector spaces. Despite their effectiveness in\nmodeling KG structural information, KRL methods are suffering from the\nsparseness of KGs. The rise of Large Language Models (LLMs) built on the\nTransformer architecture presents promising opportunities for enhancing KRL by\nincorporating textual information to address information sparsity in KGs.\nLLM-enhanced KRL methods, including three key approaches, encoder-based methods\nthat leverage detailed contextual information, encoder-decoder-based methods\nthat utilize a unified Seq2Seq model for comprehensive encoding and decoding,\nand decoder-based methods that utilize extensive knowledge from large corpora,\nhave significantly advanced the effectiveness and generalization of KRL in\naddressing a wide range of downstream tasks. This work provides a broad\noverview of downstream tasks while simultaneously identifying emerging research\ndirections in these evolving domains."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Haofen Wang"
                    },
                    {
                        "name": "Leong Hou U"
                    },
                    {
                        "name": "Zhao Li"
                    },
                    {
                        "name": "Wenbin Guo"
                    }
                ],
                "author_detail": {
                    "name": "Wenbin Guo"
                },
                "author": "Wenbin Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00936v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00936v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09078v1",
                "updated": "2025-03-12T05:22:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    22,
                    32,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T05:22:32Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    22,
                    32,
                    2,
                    71,
                    0
                ],
                "title": "Sequential Multi-Object Grasping with One Dexterous Hand",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Multi-Object Grasping with One Dexterous Hand"
                },
                "summary": "Sequentially grasping multiple objects with multi-fingered hands is common in\ndaily life, where humans can fully leverage the dexterity of their hands to\nenclose multiple objects. However, the diversity of object geometries and the\ncomplex contact interactions required for high-DOF hands to grasp one object\nwhile enclosing another make sequential multi-object grasping challenging for\nrobots. In this paper, we propose SeqMultiGrasp, a system for sequentially\ngrasping objects with a four-fingered Allegro Hand. We focus on sequentially\ngrasping two objects, ensuring that the hand fully encloses one object before\nlifting it and then grasps the second object without dropping the first. Our\nsystem first synthesizes single-object grasp candidates, where each grasp is\nconstrained to use only a subset of the hand's links. These grasps are then\nvalidated in a physics simulator to ensure stability and feasibility. Next, we\nmerge the validated single-object grasp poses to construct multi-object grasp\nconfigurations. For real-world deployment, we train a diffusion model\nconditioned on point clouds to propose grasp poses, followed by a\nheuristic-based execution strategy. We test our system using $8 \\times 8$\nobject combinations in simulation and $6 \\times 3$ object combinations in real.\nOur diffusion-based grasp model obtains an average success rate of 65.8% over\n1600 simulation trials and 56.7% over 90 real-world trials, suggesting that it\nis a promising approach for sequential multi-object grasping with\nmulti-fingered hands. Supplementary material is available on our project\nwebsite: https://hesic73.github.io/SeqMultiGrasp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequentially grasping multiple objects with multi-fingered hands is common in\ndaily life, where humans can fully leverage the dexterity of their hands to\nenclose multiple objects. However, the diversity of object geometries and the\ncomplex contact interactions required for high-DOF hands to grasp one object\nwhile enclosing another make sequential multi-object grasping challenging for\nrobots. In this paper, we propose SeqMultiGrasp, a system for sequentially\ngrasping objects with a four-fingered Allegro Hand. We focus on sequentially\ngrasping two objects, ensuring that the hand fully encloses one object before\nlifting it and then grasps the second object without dropping the first. Our\nsystem first synthesizes single-object grasp candidates, where each grasp is\nconstrained to use only a subset of the hand's links. These grasps are then\nvalidated in a physics simulator to ensure stability and feasibility. Next, we\nmerge the validated single-object grasp poses to construct multi-object grasp\nconfigurations. For real-world deployment, we train a diffusion model\nconditioned on point clouds to propose grasp poses, followed by a\nheuristic-based execution strategy. We test our system using $8 \\times 8$\nobject combinations in simulation and $6 \\times 3$ object combinations in real.\nOur diffusion-based grasp model obtains an average success rate of 65.8% over\n1600 simulation trials and 56.7% over 90 real-world trials, suggesting that it\nis a promising approach for sequential multi-object grasping with\nmulti-fingered hands. Supplementary material is available on our project\nwebsite: https://hesic73.github.io/SeqMultiGrasp."
                },
                "authors": [
                    {
                        "name": "Sicheng He"
                    },
                    {
                        "name": "Zeyu Shangguan"
                    },
                    {
                        "name": "Kuanning Wang"
                    },
                    {
                        "name": "Yongchong Gu"
                    },
                    {
                        "name": "Yuqian Fu"
                    },
                    {
                        "name": "Yanwei Fu"
                    },
                    {
                        "name": "Daniel Seita"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Seita"
                },
                "author": "Daniel Seita",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19555v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19555v2",
                "updated": "2025-03-12T05:12:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    12,
                    12,
                    2,
                    71,
                    0
                ],
                "published": "2025-02-26T20:49:40Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    20,
                    49,
                    40,
                    2,
                    57,
                    0
                ],
                "title": "Real-Time Active Learning for optimised spectroscopic follow-up:\n  Enhancing early SN Ia classification with the Fink broker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Active Learning for optimised spectroscopic follow-up:\n  Enhancing early SN Ia classification with the Fink broker"
                },
                "summary": "Current and future surveys rely on machine learning classification to obtain\nlarge and complete samples of transients. Many of these algorithms are\nrestricted by training samples that contain a limited number of\nspectroscopically confirmed events. Here, we present the first real-time\napplication of Active Learning to optimise spectroscopic follow-up with the\ngoal of improving training sets of early type Ia supernovae (SNe Ia)\nclassifiers. Using a photometric classifier for early SN Ia, we apply an Active\nLearning strategy for follow-up optimisation using the real-time FINK broker\nprocessing of the ZTF public stream. We perform follow-up observations at the\nANU 2.3m telescope in Australia and obtain 92 spectroscopic classified events\nthat are incorporated in our training set. We show that our follow-up strategy\nyields a training set that, with 25% less spectra, improves classification\nmetrics when compared to publicly reported spectra. Our strategy selects in\naverage fainter events and, not only supernovae types, but also microlensing\nevents and flaring stars which are usually not incorporated on training sets.\nOur results confirm the effectiveness of active learning strategies to\nconstruct optimal training samples for astronomical classifiers. With the Rubin\nObservatory LSST soon online, we propose improvements to obtain earlier\ncandidates and optimise follow-up. This work paves the way to the deployment of\nreal-time AL follow-up strategies in the era of large surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current and future surveys rely on machine learning classification to obtain\nlarge and complete samples of transients. Many of these algorithms are\nrestricted by training samples that contain a limited number of\nspectroscopically confirmed events. Here, we present the first real-time\napplication of Active Learning to optimise spectroscopic follow-up with the\ngoal of improving training sets of early type Ia supernovae (SNe Ia)\nclassifiers. Using a photometric classifier for early SN Ia, we apply an Active\nLearning strategy for follow-up optimisation using the real-time FINK broker\nprocessing of the ZTF public stream. We perform follow-up observations at the\nANU 2.3m telescope in Australia and obtain 92 spectroscopic classified events\nthat are incorporated in our training set. We show that our follow-up strategy\nyields a training set that, with 25% less spectra, improves classification\nmetrics when compared to publicly reported spectra. Our strategy selects in\naverage fainter events and, not only supernovae types, but also microlensing\nevents and flaring stars which are usually not incorporated on training sets.\nOur results confirm the effectiveness of active learning strategies to\nconstruct optimal training samples for astronomical classifiers. With the Rubin\nObservatory LSST soon online, we propose improvements to obtain earlier\ncandidates and optimise follow-up. This work paves the way to the deployment of\nreal-time AL follow-up strategies in the era of large surveys."
                },
                "authors": [
                    {
                        "name": "A. Möller"
                    },
                    {
                        "name": "E. E. O. Ishida"
                    },
                    {
                        "name": "J. Peloton"
                    },
                    {
                        "name": "O. Vidal Velázquez"
                    },
                    {
                        "name": "J. Soon"
                    },
                    {
                        "name": "B. Martin"
                    },
                    {
                        "name": "M. Cluver"
                    },
                    {
                        "name": "M. Leoni"
                    },
                    {
                        "name": "E. Taylor"
                    }
                ],
                "author_detail": {
                    "name": "E. Taylor"
                },
                "author": "E. Taylor",
                "arxiv_comment": "Accepted for publication in PASA. 14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19555v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19555v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11551v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11551v4",
                "updated": "2025-03-12T05:09:10Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    5,
                    9,
                    10,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-20T15:39:39Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    15,
                    39,
                    39,
                    0,
                    20,
                    0
                ],
                "title": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIKE-RAG: sPecIalized KnowledgE and Rationale Augmented Generation"
                },
                "summary": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Retrieval-Augmented Generation (RAG) systems\nthat expand large language model (LLM) capabilities through external retrieval,\nthese systems often struggle to meet the complex and diverse needs of\nreal-world industrial applications. The reliance on retrieval alone proves\ninsufficient for extracting deep, domain-specific knowledge performing in\nlogical reasoning from specialized corpora. To address this, we introduce\nsPecIalized KnowledgE and Rationale Augmentation Generation (PIKE-RAG),\nfocusing on extracting, understanding, and applying specialized knowledge,\nwhile constructing coherent rationale to incrementally steer LLMs toward\naccurate responses. Recognizing the diverse challenges of industrial tasks, we\nintroduce a new paradigm that classifies tasks based on their complexity in\nknowledge extraction and application, allowing for a systematic evaluation of\nRAG systems' problem-solving capabilities. This strategic approach offers a\nroadmap for the phased development and enhancement of RAG systems, tailored to\nmeet the evolving demands of industrial applications. Furthermore, we propose\nknowledge atomizing and knowledge-aware task decomposition to effectively\nextract multifaceted knowledge from the data chunks and iteratively construct\nthe rationale based on original query and the accumulated knowledge,\nrespectively, showcasing exceptional performance across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Jingjing Fu"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "38 pages, 18 figures, technique report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11551v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11551v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09066v1",
                "updated": "2025-03-12T04:59:22Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    59,
                    22,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T04:59:22Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    59,
                    22,
                    2,
                    71,
                    0
                ],
                "title": "Probing Latent Subspaces in LLM for AI Security: Identifying and\n  Manipulating Adversarial States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Latent Subspaces in LLM for AI Security: Identifying and\n  Manipulating Adversarial States"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they remain vulnerable to adversarial manipulations such as\njailbreaking via prompt injection attacks. These attacks bypass safety\nmechanisms to generate restricted or harmful content. In this study, we\ninvestigated the underlying latent subspaces of safe and jailbroken states by\nextracting hidden activations from a LLM. Inspired by attractor dynamics in\nneuroscience, we hypothesized that LLM activations settle into semi stable\nstates that can be identified and perturbed to induce state transitions. Using\ndimensionality reduction techniques, we projected activations from safe and\njailbroken responses to reveal latent subspaces in lower dimensional spaces. We\nthen derived a perturbation vector that when applied to safe representations,\nshifted the model towards a jailbreak state. Our results demonstrate that this\ncausal intervention results in statistically significant jailbreak responses in\na subset of prompts. Next, we probed how these perturbations propagate through\nthe model's layers, testing whether the induced state change remains localized\nor cascades throughout the network. Our findings indicate that targeted\nperturbations induced distinct shifts in activations and model responses. Our\napproach paves the way for potential proactive defenses, shifting from\ntraditional guardrail based methods to preemptive, model agnostic techniques\nthat neutralize adversarial states at the representation level."
                },
                "authors": [
                    {
                        "name": "Xin Wei Chia"
                    },
                    {
                        "name": "Jonathan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Pan"
                },
                "author": "Jonathan Pan",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04776v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04776v3",
                "updated": "2025-03-12T04:56:46Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    56,
                    46,
                    2,
                    71,
                    0
                ],
                "published": "2024-05-08T02:48:28Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    2,
                    48,
                    28,
                    2,
                    129,
                    0
                ],
                "title": "Chain of Thoughtlessness? An Analysis of CoT in Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thoughtlessness? An Analysis of CoT in Planning"
                },
                "summary": "Large language model (LLM) performance on reasoning problems typically does\nnot generalize out of distribution. Previous work has claimed that this can be\nmitigated with chain of thought prompting-a method of demonstrating solution\nprocedures-with the intuition that it is possible to in-context teach an LLM an\nalgorithm for solving the problem. This paper presents a case study of chain of\nthought on problems from Blocksworld, a classical planning domain, and examines\nthe performance of two state-of-the-art LLMs across two axes: generality of\nexamples given in prompt, and complexity of problems queried with each prompt.\nWhile our problems are very simple, we only find meaningful performance\nimprovements from chain of thought prompts when those prompts are exceedingly\nspecific to their problem class, and that those improvements quickly\ndeteriorate as the size n of the query-specified stack grows past the size of\nstacks shown in the examples. We also create scalable variants of three domains\ncommonly studied in previous CoT papers and demonstrate the existence of\nsimilar failure modes. Our results hint that, contrary to previous claims in\nthe literature, CoT's performance improvements do not stem from the model\nlearning general algorithmic procedures via demonstrations but depend on\ncarefully engineering highly problem specific prompts. This spotlights\ndrawbacks of chain of thought, especially the sharp tradeoff between possible\nperformance gains and the amount of human labor necessary to generate examples\nwith correct reasoning traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) performance on reasoning problems typically does\nnot generalize out of distribution. Previous work has claimed that this can be\nmitigated with chain of thought prompting-a method of demonstrating solution\nprocedures-with the intuition that it is possible to in-context teach an LLM an\nalgorithm for solving the problem. This paper presents a case study of chain of\nthought on problems from Blocksworld, a classical planning domain, and examines\nthe performance of two state-of-the-art LLMs across two axes: generality of\nexamples given in prompt, and complexity of problems queried with each prompt.\nWhile our problems are very simple, we only find meaningful performance\nimprovements from chain of thought prompts when those prompts are exceedingly\nspecific to their problem class, and that those improvements quickly\ndeteriorate as the size n of the query-specified stack grows past the size of\nstacks shown in the examples. We also create scalable variants of three domains\ncommonly studied in previous CoT papers and demonstrate the existence of\nsimilar failure modes. Our results hint that, contrary to previous claims in\nthe literature, CoT's performance improvements do not stem from the model\nlearning general algorithmic procedures via demonstrations but depend on\ncarefully engineering highly problem specific prompts. This spotlights\ndrawbacks of chain of thought, especially the sharp tradeoff between possible\nperformance gains and the amount of human labor necessary to generate examples\nwith correct reasoning traces."
                },
                "authors": [
                    {
                        "name": "Kaya Stechly"
                    },
                    {
                        "name": "Karthik Valmeekam"
                    },
                    {
                        "name": "Subbarao Kambhampati"
                    }
                ],
                "author_detail": {
                    "name": "Subbarao Kambhampati"
                },
                "author": "Subbarao Kambhampati",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04776v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04776v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08952v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08952v5",
                "updated": "2025-03-12T04:46:47Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    46,
                    47,
                    2,
                    71,
                    0
                ],
                "published": "2024-07-12T03:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    3,
                    15,
                    1,
                    4,
                    194,
                    0
                ],
                "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection"
                },
                "summary": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Jiajun Zhu"
                    },
                    {
                        "name": "Xukai Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Yanghai Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08952v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08952v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11402v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11402v3",
                "updated": "2025-03-12T04:37:42Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    4,
                    37,
                    42,
                    2,
                    71,
                    0
                ],
                "published": "2024-06-17T10:45:36Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    10,
                    45,
                    36,
                    0,
                    169,
                    0
                ],
                "title": "Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Small Language Models Ready to Compete with Large Language Models\n  for Practical Applications?"
                },
                "summary": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs do\nnot perform well universally. This work tries to bridge this gap by proposing a\nframework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains, and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify the best LM and prompt style depending on specific application\nrequirements using the proposed framework. We also show that if selected\nappropriately, they can outperform SOTA LLMs like DeepSeek-v2, GPT-4o,\nGPT-4o-mini, Gemini-1.5-Pro, and even compete with GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs do\nnot perform well universally. This work tries to bridge this gap by proposing a\nframework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains, and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify the best LM and prompt style depending on specific application\nrequirements using the proposed framework. We also show that if selected\nappropriately, they can outperform SOTA LLMs like DeepSeek-v2, GPT-4o,\nGPT-4o-mini, Gemini-1.5-Pro, and even compete with GPT-4o."
                },
                "authors": [
                    {
                        "name": "Neelabh Sinha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "arxiv_comment": "Accepted at The Fifth Workshop on Trustworthy Natural Language\n  Processing (TrustNLP 2025) in Annual Conference of the Nations of the\n  Americas Chapter of the Association for Computational Linguistics (NAACL),\n  2025. 8 pages + references + Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11402v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11402v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15723v2",
                "updated": "2025-03-12T03:53:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    53,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-01-28T06:06:28Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    6,
                    6,
                    28,
                    1,
                    28,
                    0
                ],
                "title": "Balancing Content Size in RAG-Text2SQL System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Content Size in RAG-Text2SQL System"
                },
                "summary": "Large Language Models (LLMs) have emerged as a promising solution for\nconverting natural language queries into SQL commands, enabling seamless\ndatabase interaction. However, these Text-to-SQL (Text2SQL) systems face\ninherent limitations, hallucinations, outdated knowledge, and untraceable\nreasoning. To address these challenges, the integration of retrieval-augmented\ngeneration (RAG) with Text2SQL models has gained traction. RAG serves as a\nretrieval mechanism, providing essential contextual information, such as table\nschemas and metadata, to enhance the query generation process. Despite their\npotential, RAG + Text2SQL systems are susceptible to the quality and size of\nretrieved documents. While richer document content can improve schema relevance\nand retrieval accuracy, it also introduces noise, increasing the risk of\nhallucinations and reducing query fidelity as the prompt size of the Text2SQL\nmodel increases. This research investigates the nuanced trade-off between\ndocument size and quality, aiming to strike a balance that optimizes system\nperformance. Key thresholds are identified where performance degradation\noccurs, along with actionable strategies to mitigate these challenges.\nAdditionally, we explore the phenomenon of hallucinations in Text2SQL models,\nemphasizing the critical role of curated document presentation in minimizing\nerrors. Our findings provide a roadmap for enhancing the robustness of RAG +\nText2SQL systems, offering practical insights for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have emerged as a promising solution for\nconverting natural language queries into SQL commands, enabling seamless\ndatabase interaction. However, these Text-to-SQL (Text2SQL) systems face\ninherent limitations, hallucinations, outdated knowledge, and untraceable\nreasoning. To address these challenges, the integration of retrieval-augmented\ngeneration (RAG) with Text2SQL models has gained traction. RAG serves as a\nretrieval mechanism, providing essential contextual information, such as table\nschemas and metadata, to enhance the query generation process. Despite their\npotential, RAG + Text2SQL systems are susceptible to the quality and size of\nretrieved documents. While richer document content can improve schema relevance\nand retrieval accuracy, it also introduces noise, increasing the risk of\nhallucinations and reducing query fidelity as the prompt size of the Text2SQL\nmodel increases. This research investigates the nuanced trade-off between\ndocument size and quality, aiming to strike a balance that optimizes system\nperformance. Key thresholds are identified where performance degradation\noccurs, along with actionable strategies to mitigate these challenges.\nAdditionally, we explore the phenomenon of hallucinations in Text2SQL models,\nemphasizing the critical role of curated document presentation in minimizing\nerrors. Our findings provide a roadmap for enhancing the robustness of RAG +\nText2SQL systems, offering practical insights for real-world applications."
                },
                "authors": [
                    {
                        "name": "Prakhar Gurawa"
                    },
                    {
                        "name": "Anjali Dharmik"
                    }
                ],
                "author_detail": {
                    "name": "Anjali Dharmik"
                },
                "author": "Anjali Dharmik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09035v1",
                "updated": "2025-03-12T03:51:41Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    51,
                    41,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T03:51:41Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    51,
                    41,
                    2,
                    71,
                    0
                ],
                "title": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManeuverGPT Agentic Control for Safe Autonomous Stunt Maneuvers"
                },
                "summary": "The next generation of active safety features in autonomous vehicles should\nbe capable of safely executing evasive hazard-avoidance maneuvers akin to those\nperformed by professional stunt drivers to achieve high-agility motion at the\nlimits of vehicle handling. This paper presents a novel framework, ManeuverGPT,\nfor generating and executing high-dynamic stunt maneuvers in autonomous\nvehicles using large language model (LLM)-based agents as controllers. We\ntarget aggressive maneuvers, such as J-turns, within the CARLA simulation\nenvironment and demonstrate an iterative, prompt-based approach to refine\nvehicle control parameters, starting tabula rasa without retraining model\nweights. We propose an agentic architecture comprised of three specialized\nagents (1) a Query Enricher Agent for contextualizing user commands, (2) a\nDriver Agent for generating maneuver parameters, and (3) a Parameter Validator\nAgent that enforces physics-based and safety constraints. Experimental results\ndemonstrate successful J-turn execution across multiple vehicle models through\ntextual prompts that adapt to differing vehicle dynamics. We evaluate\nperformance via established success criteria and discuss limitations regarding\nnumeric precision and scenario complexity. Our findings underscore the\npotential of LLM-driven control for flexible, high-dynamic maneuvers, while\nhighlighting the importance of hybrid approaches that combine language-based\nreasoning with algorithmic validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next generation of active safety features in autonomous vehicles should\nbe capable of safely executing evasive hazard-avoidance maneuvers akin to those\nperformed by professional stunt drivers to achieve high-agility motion at the\nlimits of vehicle handling. This paper presents a novel framework, ManeuverGPT,\nfor generating and executing high-dynamic stunt maneuvers in autonomous\nvehicles using large language model (LLM)-based agents as controllers. We\ntarget aggressive maneuvers, such as J-turns, within the CARLA simulation\nenvironment and demonstrate an iterative, prompt-based approach to refine\nvehicle control parameters, starting tabula rasa without retraining model\nweights. We propose an agentic architecture comprised of three specialized\nagents (1) a Query Enricher Agent for contextualizing user commands, (2) a\nDriver Agent for generating maneuver parameters, and (3) a Parameter Validator\nAgent that enforces physics-based and safety constraints. Experimental results\ndemonstrate successful J-turn execution across multiple vehicle models through\ntextual prompts that adapt to differing vehicle dynamics. We evaluate\nperformance via established success criteria and discuss limitations regarding\nnumeric precision and scenario complexity. Our findings underscore the\npotential of LLM-driven control for flexible, high-dynamic maneuvers, while\nhighlighting the importance of hybrid approaches that combine language-based\nreasoning with algorithmic validation."
                },
                "authors": [
                    {
                        "name": "Shawn Azdam"
                    },
                    {
                        "name": "Pranav Doma"
                    },
                    {
                        "name": "Aliasghar Moj Arab"
                    }
                ],
                "author_detail": {
                    "name": "Aliasghar Moj Arab"
                },
                "author": "Aliasghar Moj Arab",
                "arxiv_comment": "6 Pages, Submitted to IROS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09032v1",
                "updated": "2025-03-12T03:45:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    45,
                    53,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T03:45:53Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    45,
                    53,
                    2,
                    71,
                    0
                ],
                "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs How to Learn with Contextual Fine-Tuning"
                },
                "summary": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains."
                },
                "authors": [
                    {
                        "name": "Younwoo Choi"
                    },
                    {
                        "name": "Muhammad Adil Asif"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "John Willes"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G. Krishnan"
                },
                "author": "Rahul G. Krishnan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09029v1",
                "updated": "2025-03-12T03:36:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    36,
                    45,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T03:36:45Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    36,
                    45,
                    2,
                    71,
                    0
                ],
                "title": "DAST: Difficulty-Aware Self-Training on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAST: Difficulty-Aware Self-Training on Large Language Models"
                },
                "summary": "Present Large Language Models (LLM) self-training methods always under-sample\non challenging queries, leading to inadequate learning on difficult problems\nwhich limits LLMs' ability. Therefore, this work proposes a difficulty-aware\nself-training (DAST) framework that focuses on improving both the quantity and\nquality of self-generated responses on challenging queries during\nself-training. DAST is specified in three components: 1) sampling-based\ndifficulty level estimation, 2) difficulty-aware data augmentation, and 3) the\nself-training algorithm using SFT and DPO respectively. Experiments on\nmathematical tasks demonstrate the effectiveness and generalization of DAST,\nhighlighting the critical role of difficulty-aware strategies in advancing LLM\nself-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Present Large Language Models (LLM) self-training methods always under-sample\non challenging queries, leading to inadequate learning on difficult problems\nwhich limits LLMs' ability. Therefore, this work proposes a difficulty-aware\nself-training (DAST) framework that focuses on improving both the quantity and\nquality of self-generated responses on challenging queries during\nself-training. DAST is specified in three components: 1) sampling-based\ndifficulty level estimation, 2) difficulty-aware data augmentation, and 3) the\nself-training algorithm using SFT and DPO respectively. Experiments on\nmathematical tasks demonstrate the effectiveness and generalization of DAST,\nhighlighting the critical role of difficulty-aware strategies in advancing LLM\nself-training."
                },
                "authors": [
                    {
                        "name": "Boyang Xue"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Hongling Xu"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Kam-Fai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kam-Fai Wong"
                },
                "author": "Kam-Fai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09027v1",
                "updated": "2025-03-12T03:33:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    33,
                    50,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T03:33:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    33,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "Measure Twice, Cut Once: Grasping Video Structures and Event Semantics\n  with LLMs for Video Temporal Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measure Twice, Cut Once: Grasping Video Structures and Event Semantics\n  with LLMs for Video Temporal Localization"
                },
                "summary": "Localizing user-queried events through natural language is crucial for video\nunderstanding models. Recent methods predominantly adapt Video LLMs to generate\nevent boundary timestamps to handle temporal localization tasks, which struggle\nto leverage LLMs' powerful semantic understanding. In this work, we introduce\nMeCo, a novel timestamp-free framework that enables video LLMs to fully harness\ntheir intrinsic semantic capabilities for temporal localization tasks. Rather\nthan outputting boundary timestamps, MeCo partitions videos into holistic event\nand transition segments based on the proposed structural token generation and\ngrounding pipeline, derived from video LLMs' temporal structure understanding\ncapability. We further propose a query-focused captioning task that compels the\nLLM to extract fine-grained, event-specific details, bridging the gap between\nlocalization and higher-level semantics and enhancing localization performance.\nExtensive experiments on diverse temporal localization tasks show that MeCo\nconsistently outperforms boundary-centric methods, underscoring the benefits of\na semantic-driven approach for temporal localization with video LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing user-queried events through natural language is crucial for video\nunderstanding models. Recent methods predominantly adapt Video LLMs to generate\nevent boundary timestamps to handle temporal localization tasks, which struggle\nto leverage LLMs' powerful semantic understanding. In this work, we introduce\nMeCo, a novel timestamp-free framework that enables video LLMs to fully harness\ntheir intrinsic semantic capabilities for temporal localization tasks. Rather\nthan outputting boundary timestamps, MeCo partitions videos into holistic event\nand transition segments based on the proposed structural token generation and\ngrounding pipeline, derived from video LLMs' temporal structure understanding\ncapability. We further propose a query-focused captioning task that compels the\nLLM to extract fine-grained, event-specific details, bridging the gap between\nlocalization and higher-level semantics and enhancing localization performance.\nExtensive experiments on diverse temporal localization tasks show that MeCo\nconsistently outperforms boundary-centric methods, underscoring the benefits of\na semantic-driven approach for temporal localization with video LLMs."
                },
                "authors": [
                    {
                        "name": "Zongshang Pang"
                    },
                    {
                        "name": "Mayu Otani"
                    },
                    {
                        "name": "Yuta Nakashima"
                    }
                ],
                "author_detail": {
                    "name": "Yuta Nakashima"
                },
                "author": "Yuta Nakashima",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15232v2",
                "updated": "2025-03-12T03:28:09Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    28,
                    9,
                    2,
                    71,
                    0
                ],
                "published": "2024-11-21T19:13:04Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    19,
                    13,
                    4,
                    3,
                    326,
                    0
                ],
                "title": "BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models"
                },
                "summary": "Recent advancements in vision-language models (VLMs), such as CLIP, have\ndemonstrated substantial success in self-supervised representation learning for\nvision tasks. However, effectively adapting VLMs to downstream applications\nremains challenging, as their accuracy often depends on time-intensive and\nexpertise-demanding prompt engineering, while full model fine-tuning is costly.\nThis is particularly true for biomedical images, which, unlike natural images,\ntypically suffer from limited annotated datasets, unintuitive image contrasts,\nand nuanced visual features. Recent prompt learning techniques, such as Context\nOptimization (CoOp) intend to tackle these issues, but still fall short in\ngeneralizability. Meanwhile, explorations in prompt learning for biomedical\nimage analysis are still highly limited. In this work, we propose BiomedCoOp, a\nnovel prompt learning framework that enables efficient adaptation of BiomedCLIP\nfor accurate and highly generalizable few-shot biomedical image classification.\nOur approach achieves effective prompt context learning by leveraging semantic\nconsistency with average prompt ensembles from Large Language Models (LLMs) and\nknowledge distillation with a statistics-based prompt selection strategy. We\nconducted comprehensive validation of our proposed framework on 11 medical\ndatasets across 9 modalities and 10 organs against existing state-of-the-art\nmethods, demonstrating significant improvements in both accuracy and\ngeneralizability. The code is publicly available at\nhttps://github.com/HealthX-Lab/BiomedCoOp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in vision-language models (VLMs), such as CLIP, have\ndemonstrated substantial success in self-supervised representation learning for\nvision tasks. However, effectively adapting VLMs to downstream applications\nremains challenging, as their accuracy often depends on time-intensive and\nexpertise-demanding prompt engineering, while full model fine-tuning is costly.\nThis is particularly true for biomedical images, which, unlike natural images,\ntypically suffer from limited annotated datasets, unintuitive image contrasts,\nand nuanced visual features. Recent prompt learning techniques, such as Context\nOptimization (CoOp) intend to tackle these issues, but still fall short in\ngeneralizability. Meanwhile, explorations in prompt learning for biomedical\nimage analysis are still highly limited. In this work, we propose BiomedCoOp, a\nnovel prompt learning framework that enables efficient adaptation of BiomedCLIP\nfor accurate and highly generalizable few-shot biomedical image classification.\nOur approach achieves effective prompt context learning by leveraging semantic\nconsistency with average prompt ensembles from Large Language Models (LLMs) and\nknowledge distillation with a statistics-based prompt selection strategy. We\nconducted comprehensive validation of our proposed framework on 11 medical\ndatasets across 9 modalities and 10 organs against existing state-of-the-art\nmethods, demonstrating significant improvements in both accuracy and\ngeneralizability. The code is publicly available at\nhttps://github.com/HealthX-Lab/BiomedCoOp."
                },
                "authors": [
                    {
                        "name": "Taha Koleilat"
                    },
                    {
                        "name": "Hojat Asgariandehkordi"
                    },
                    {
                        "name": "Hassan Rivaz"
                    },
                    {
                        "name": "Yiming Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Yiming Xiao"
                },
                "author": "Yiming Xiao",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09025v1",
                "updated": "2025-03-12T03:24:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    24,
                    44,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-12T03:24:44Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    24,
                    44,
                    2,
                    71,
                    0
                ],
                "title": "Aligning to What? Limits to RLHF Based Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning to What? Limits to RLHF Based Alignment"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nalign large language models (LLMs) with human preferences. However, the\neffectiveness of RLHF in addressing underlying biases remains unclear. This\nstudy investigates the relationship between RLHF and both covert and overt\nbiases in LLMs, particularly focusing on biases against African Americans. We\napplied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and\nevaluated the covert and overt biases of the resulting models using\nmatched-guise probing and explicit bias testing. We performed additional tests\nwith DPO on different base models and datasets; among several implications, we\nfound that SFT before RLHF calcifies model biases. Additionally, we extend the\ntools for measuring biases to multi-modal models. Through our experiments we\ncollect evidence that indicates that current alignment techniques are\ninadequate for nebulous tasks such as mitigating covert biases, highlighting\nthe need for capable datasets, data curating techniques, or alignment tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is increasingly used to\nalign large language models (LLMs) with human preferences. However, the\neffectiveness of RLHF in addressing underlying biases remains unclear. This\nstudy investigates the relationship between RLHF and both covert and overt\nbiases in LLMs, particularly focusing on biases against African Americans. We\napplied various RLHF techniques (DPO, ORPO, and RLOO) to Llama 3 8B and\nevaluated the covert and overt biases of the resulting models using\nmatched-guise probing and explicit bias testing. We performed additional tests\nwith DPO on different base models and datasets; among several implications, we\nfound that SFT before RLHF calcifies model biases. Additionally, we extend the\ntools for measuring biases to multi-modal models. Through our experiments we\ncollect evidence that indicates that current alignment techniques are\ninadequate for nebulous tasks such as mitigating covert biases, highlighting\nthe need for capable datasets, data curating techniques, or alignment tools."
                },
                "authors": [
                    {
                        "name": "Logan Barnhart"
                    },
                    {
                        "name": "Reza Akbarian Bafghi"
                    },
                    {
                        "name": "Stephen Becker"
                    },
                    {
                        "name": "Maziar Raissi"
                    }
                ],
                "author_detail": {
                    "name": "Maziar Raissi"
                },
                "author": "Maziar Raissi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09022v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09022v2",
                "updated": "2025-03-13T05:55:55Z",
                "updated_parsed": [
                    2025,
                    3,
                    13,
                    5,
                    55,
                    55,
                    3,
                    72,
                    0
                ],
                "published": "2025-03-12T03:20:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    20,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Inversion Attack against Collaborative Inference of Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely applied for their remarkable\ncapability of content generation. However, the practical use of open-source\nLLMs is hindered by high resource requirements, making deployment expensive and\nlimiting widespread development. The collaborative inference is a promising\nsolution for this problem, in which users collaborate by each hosting a subset\nof layers and transmitting intermediate activation. Many companies are building\ncollaborative inference platforms to reduce LLM serving costs, leveraging\nusers' underutilized GPUs. Despite widespread interest in collaborative\ninference within academia and industry, the privacy risks associated with LLM\ncollaborative inference have not been well studied. This is largely because of\nthe challenge posed by inverting LLM activation due to its strong\nnon-linearity.\n  In this paper, to validate the severity of privacy threats in LLM\ncollaborative inference, we introduce the concept of prompt inversion attack\n(PIA), where a malicious participant intends to recover the input prompt\nthrough the activation transmitted by its previous participant. Extensive\nexperiments show that our PIA method substantially outperforms existing\nbaselines. For example, our method achieves an 88.4\\% token accuracy on the\nSkytrax dataset with the Llama-65B model when inverting the maximum number of\ntransformer layers, while the best baseline method only achieves 22.8\\%\naccuracy. The results verify the effectiveness of our PIA attack and highlights\nits practical threat to LLM collaborative inference systems."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Tingsong Xiao"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "arxiv_comment": "To appear at IEEE Symposium on Security and Privacy 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09022v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09022v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07513v2",
                "updated": "2025-03-12T03:18:36Z",
                "updated_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    18,
                    36,
                    2,
                    71,
                    0
                ],
                "published": "2025-03-10T16:33:14Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    16,
                    33,
                    14,
                    0,
                    69,
                    0
                ],
                "title": "Language Models Fail to Introspect About Their Knowledge of Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Fail to Introspect About Their Knowledge of Language"
                },
                "summary": "There has been recent interest in whether large language models (LLMs) can\nintrospect about their own internal states. Such abilities would make LLMs more\ninterpretable, and also validate the use of standard introspective methods in\nlinguistics to evaluate grammatical knowledge in models (e.g., asking \"Is this\nsentence grammatical?\"). We systematically investigate emergent introspection\nacross 21 open-source LLMs, in two domains where introspection is of\ntheoretical interest: grammatical knowledge and word prediction. Crucially, in\nboth domains, a model's internal linguistic knowledge can be theoretically\ngrounded in direct measurements of string probability. We then evaluate whether\nmodels' responses to metalinguistic prompts faithfully reflect their internal\nknowledge. We propose a new measure of introspection: the degree to which a\nmodel's prompted responses predict its own string probabilities, beyond what\nwould be predicted by another model with nearly identical internal knowledge.\nWhile both metalinguistic prompting and probability comparisons lead to high\ntask accuracy, we do not find evidence that LLMs have privileged \"self-access\".\nOur findings complicate recent results suggesting that models can introspect,\nand add new evidence to the argument that prompted responses should not be\nconflated with models' linguistic generalizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been recent interest in whether large language models (LLMs) can\nintrospect about their own internal states. Such abilities would make LLMs more\ninterpretable, and also validate the use of standard introspective methods in\nlinguistics to evaluate grammatical knowledge in models (e.g., asking \"Is this\nsentence grammatical?\"). We systematically investigate emergent introspection\nacross 21 open-source LLMs, in two domains where introspection is of\ntheoretical interest: grammatical knowledge and word prediction. Crucially, in\nboth domains, a model's internal linguistic knowledge can be theoretically\ngrounded in direct measurements of string probability. We then evaluate whether\nmodels' responses to metalinguistic prompts faithfully reflect their internal\nknowledge. We propose a new measure of introspection: the degree to which a\nmodel's prompted responses predict its own string probabilities, beyond what\nwould be predicted by another model with nearly identical internal knowledge.\nWhile both metalinguistic prompting and probability comparisons lead to high\ntask accuracy, we do not find evidence that LLMs have privileged \"self-access\".\nOur findings complicate recent results suggesting that models can introspect,\nand add new evidence to the argument that prompted responses should not be\nconflated with models' linguistic generalizations."
                },
                "authors": [
                    {
                        "name": "Siyuan Song"
                    },
                    {
                        "name": "Jennifer Hu"
                    },
                    {
                        "name": "Kyle Mahowald"
                    }
                ],
                "author_detail": {
                    "name": "Kyle Mahowald"
                },
                "author": "Kyle Mahowald",
                "arxiv_comment": "Corrected Fig 5a and removed unused figures from source files",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]