[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v2",
                "updated": "2025-08-26T04:02:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    4,
                    2,
                    11,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v1",
                "updated": "2025-08-25T17:41:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstädtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v2",
                "updated": "2025-08-21T22:45:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    22,
                    45,
                    6,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "Added link to code repository. Fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15545v1",
                "updated": "2025-08-21T13:24:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "title": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation"
                },
                "summary": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value."
                },
                "authors": [
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Haorui Yang"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Desheng Kong"
                    },
                    {
                        "name": "Ji Du"
                    },
                    {
                        "name": "Yulong Fu"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v2",
                "updated": "2025-08-21T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    52,
                    11,
                    3,
                    233,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://lukashoel.github.io/3DGS-LM, Video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, Code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14204v3",
                "updated": "2025-08-21T11:43:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    43,
                    48,
                    3,
                    233,
                    0
                ],
                "published": "2024-04-22T14:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    13,
                    36,
                    0,
                    113,
                    0
                ],
                "title": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading"
                },
                "summary": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "18 pages, 13 figures. Part of this work has been accepted by ICDCS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15290v1",
                "updated": "2025-08-21T06:26:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T06:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "title": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search"
                },
                "summary": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%."
                },
                "authors": [
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Xiaolu Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v1",
                "updated": "2025-08-21T03:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15036v1",
                "updated": "2025-08-20T20:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T20:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs"
                },
                "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
                },
                "authors": [
                    {
                        "name": "Ruyi Ding"
                    },
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "This paper will appear in CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15033v1",
                "updated": "2025-08-20T19:54:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T19:54:41Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "title": "Rethinking the Potential of Layer Freezing for Efficient DNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Layer Freezing for Efficient DNN Training"
                },
                "summary": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training."
                },
                "authors": [
                    {
                        "name": "Chence Yang"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Jinzhen Wang"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16653v1",
                "updated": "2025-08-20T03:42:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel Máté"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel Máté"
                },
                "author": "Tejfel Máté",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. León"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10069v2",
                "updated": "2025-08-15T04:27:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    4,
                    27,
                    30,
                    4,
                    227,
                    0
                ],
                "published": "2025-07-14T08:53:48Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    8,
                    53,
                    48,
                    0,
                    195,
                    0
                ],
                "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal\n  Parallelism"
                },
                "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we introduce Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs)."
                },
                "authors": [
                    {
                        "name": "Zedong Liu"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Guangming Tan"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Dingwen Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dingwen Tao"
                },
                "author": "Dingwen Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10875v1",
                "updated": "2025-08-14T17:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T17:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    17,
                    47,
                    22,
                    3,
                    226,
                    0
                ],
                "title": "A Survey on Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Diffusion Language Models"
                },
                "summary": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Language Models (DLMs) are rapidly emerging as a powerful and\npromising alternative to the dominant autoregressive (AR) paradigm. By\ngenerating tokens in parallel through an iterative denoising process, DLMs\npossess inherent advantages in reducing inference latency and capturing\nbidirectional context, thereby enabling fine-grained control over the\ngeneration process. While achieving a several-fold speed-up, recent\nadvancements have allowed DLMs to show performance comparable to their\nautoregressive counterparts, making them a compelling choice for various\nnatural language processing tasks. In this survey, we provide a holistic\noverview of the current DLM landscape. We trace its evolution and relationship\nwith other paradigms, such as autoregressive and masked language models, and\ncover both foundational principles and state-of-the-art models. Our work offers\nan up-to-date, comprehensive taxonomy and an in-depth analysis of current\ntechniques, from pre-training strategies to advanced post-training methods.\nAnother contribution of this survey is a thorough review of DLM inference\nstrategies and optimizations, including improvements in decoding parallelism,\ncaching mechanisms, and generation quality. We also highlight the latest\napproaches to multimodal extensions of DLMs and delineate their applications\nacross various practical scenarios. Furthermore, our discussion addresses the\nlimitations and challenges of DLMs, including efficiency, long-sequence\nhandling, and infrastructure requirements, while outlining future research\ndirections to sustain progress in this rapidly evolving field. Project GitHub\nis available at https://github.com/VILA-Lab/Awesome-DLMs."
                },
                "authors": [
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Mingda Chen"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13109v2",
                "updated": "2025-08-14T16:12:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    12,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18773v2",
                "updated": "2025-08-14T15:37:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    15,
                    37,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-03-24T15:22:41Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    15,
                    22,
                    41,
                    0,
                    83,
                    0
                ],
                "title": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit\n  KV Cache"
                },
                "summary": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of long-context Large Language Models (LLMs) amplifies memory and\nbandwidth demands during autoregressive decoding, as the Key-Value (KV) cache\ngrows with each generated token. Low-bit KV-cache quantization (e.g., 4-bit or\n2-bit) can reduce memory footprint while preserving accuracy, but existing\nsystems suffer from slow decoding due to their exclusive reliance on CUDA\ncores, neglecting Tensor Cores (the primary source of compute on modern GPUs).\nWe present BitDecoding, a new long-context LLM inference system with a low-bit\nKV cache. BitDecoding enables efficient low-bit KV-cache decoding by\ncooperatively leveraging CUDA cores and Tensor Cores. It introduces methods for\nautomatically inducing optimized layouts to exploit Tensor Cores, along with\nwarp-level parallelization strategies for dequantization. For unified system\nsupport, BitDecoding includes a query transformation module supporting diverse\nattention variants, a quantization kernel that supports both tensor-wise and\nchannel-wise scaling used in various quantization algorithms with high\nperformance, and a dequantization kernel with a software-defined pipeline to\ncoordinate CUDA and Tensor Cores execution for mixed-precision operations.\nEvaluated on RTX 4090, A100, and H100, BitDecoding accelerates decoding by up\nto 7.5x, 4.8x, and 8.9x, respectively, over FP16 FlashDecoding-v2, and\nsurpasses the state-of-the-art low-bit system QServe by up to 4.3x. On\nLLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding\nlatency by 3x, showing substantial improvements for long-context generation.\nThe code is available at https://github.com/DD-DuDa/BitDecoding."
                },
                "authors": [
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyi Cheng"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10963v1",
                "updated": "2025-08-14T14:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    11,
                    48,
                    3,
                    226,
                    0
                ],
                "title": "EVCtrl: Efficient Control Adapter for Visual Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVCtrl: Efficient Control Adapter for Visual Generation"
                },
                "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials."
                },
                "authors": [
                    {
                        "name": "Zixiang Yang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Yinhan Zhang"
                    },
                    {
                        "name": "Shanhui Mo"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15806v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15806v1",
                "updated": "2025-08-14T14:08:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    8,
                    58,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T14:08:58Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    14,
                    8,
                    58,
                    3,
                    226,
                    0
                ],
                "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need\n  for Robust KV Cache Compression"
                },
                "summary": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations"
                },
                "authors": [
                    {
                        "name": "Mengjie Li"
                    },
                    {
                        "name": "William J. Song"
                    }
                ],
                "author_detail": {
                    "name": "William J. Song"
                },
                "author": "William J. Song",
                "arxiv_comment": "18 pages, 9 tables, 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15806v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15806v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10613v1",
                "updated": "2025-08-14T13:10:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T13:10:43Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    13,
                    10,
                    43,
                    3,
                    226,
                    0
                ],
                "title": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Routing and Wavelength Assignment with Minimal Attack Radius for QKD\n  Networks"
                },
                "summary": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Key Distribution (QKD) can distribute keys with guaranteed security\nbut remains susceptible to key exchange interruption due to physical-layer\nthreats, such as high-power jamming attacks. To address this challenge, we\nfirst introduce a novel metric, namely Maximum Number of Affected Requests\n(maxNAR), to quantify the worst-case impact of a single physical-layer attack,\nand then we investigate a new problem of Routing and Wavelength Assignment with\nMinimal Attack Radius (RWA-MAR). We formulate the problem using an Integer\nLinear Programming (ILP) model and propose a scalable heuristic to efficiently\nminimize maxNAR. Our approach incorporates key caching through Quantum Key\nPools (QKPs) to enhance resilience and optimize resource utilization. Moreover,\nwe model the impact of different QKD network architectures, employing Optical\nBypass (OB) for optical switching of quantum channels and Trusted Relay (TR)\nfor secure key forwarding. Moreover, a tunable parameter is designed in the\nheuristic to guide the preference for OB or TR, offering enhanced adaptability\nand dynamic control in diverse network scenarios. Simulation results confirm\nthat our method significantly outperforms the baseline in terms of security and\nscalability."
                },
                "authors": [
                    {
                        "name": "Mengyao Li"
                    },
                    {
                        "name": "Qiaolun Zhang"
                    },
                    {
                        "name": "Zongshuai Yang"
                    },
                    {
                        "name": "Stefano Bregni"
                    },
                    {
                        "name": "Alberto Gatto"
                    },
                    {
                        "name": "Raouf Boutaba"
                    },
                    {
                        "name": "Massimo Tornatore"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Tornatore"
                },
                "author": "Massimo Tornatore",
                "arxiv_comment": "6 pages, this paper has been successfully accepted by GLOBECOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08601v3",
                "updated": "2025-08-14T10:26:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    10,
                    26,
                    51,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T03:34:21Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    3,
                    34,
                    21,
                    1,
                    224,
                    0
                ],
                "title": "Yan: Foundational Interactive Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yan: Foundational Interactive Video Generation"
                },
                "summary": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Yan, a foundational framework for interactive video generation,\ncovering the entire pipeline from simulation and generation to editing.\nSpecifically, Yan comprises three core modules. AAA-level Simulation: We design\na highly-compressed, low-latency 3D-VAE coupled with a KV-cache-based\nshift-window denoising inference process, achieving real-time 1080P/60FPS\ninteractive simulation. Multi-Modal Generation: We introduce a hierarchical\nautoregressive caption method that injects game-specific knowledge into\nopen-domain multi-modal video diffusion models (VDMs), then transforming the\nVDM into a frame-wise, action-controllable, real-time infinite interactive\nvideo generator. Notably, when the textual and visual prompts are sourced from\ndifferent domains, the model demonstrates strong generalization, allowing it to\nblend and compose the style and mechanics across domains flexibly according to\nuser prompts. Multi-Granularity Editing: We propose a hybrid model that\nexplicitly disentangles interactive mechanics simulation from visual rendering,\nenabling multi-granularity video content editing during interaction through\ntext. Collectively, Yan offers an integration of these modules, pushing\ninteractive video generation beyond isolated capabilities toward a\ncomprehensive AI-driven interactive creation paradigm, paving the way for the\nnext generation of creative tools, media, and entertainment. The project page\nis: https://greatx3.github.io/Yan/."
                },
                "authors": [
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Fangyun Zhou"
                    },
                    {
                        "name": "Jiacheng Lv"
                    },
                    {
                        "name": "Jianqi Ma"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Junyan Lv"
                    },
                    {
                        "name": "Junyou Li"
                    },
                    {
                        "name": "Minwen Deng"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Wenkai Lv"
                    },
                    {
                        "name": "Yangbin Yu"
                    },
                    {
                        "name": "Yewen Wang"
                    },
                    {
                        "name": "Yonghang Guan"
                    },
                    {
                        "name": "Zhihao Hu"
                    },
                    {
                        "name": "Zhongbin Fang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhongqian Sun"
                },
                "author": "Zhongqian Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08895v2",
                "updated": "2025-08-14T09:04:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    9,
                    4,
                    56,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-12T12:35:55Z",
                "published_parsed": [
                    2025,
                    8,
                    12,
                    12,
                    35,
                    55,
                    1,
                    224,
                    0
                ],
                "title": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic\n  Parallelism in LLMs"
                },
                "summary": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing scale and complexity of large language models (LLMs) pose\nsignificant inference latency challenges, primarily due to their autoregressive\ndecoding paradigm characterized by the sequential nature of next-token\nprediction. By re-examining the outputs of autoregressive models, we observed\nthat some segments exhibit parallelizable structures, which we term intrinsic\nparallelism. Decoding each parallelizable branch simultaneously (i.e. parallel\ndecoding) can significantly improve the overall inference speed of LLMs. In\nthis paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which\naddresses two core challenges: automated construction of parallelizable data\nand efficient parallel decoding mechanism. More specifically, we introduce a\nnon-invasive pipeline that automatically extracts and validates parallelizable\nstructures from the responses of autoregressive models. To empower efficient\nadaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which\nenables seamless transitions between serial and parallel decoding modes while\nmaintaining a reusable KV cache, maximizing computational efficiency. Extensive\nevaluations across General Tasks, Retrieval-Augmented Generation, Mathematical\nReasoning, demonstrate that ASPD achieves unprecedented performance in both\neffectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up\nto 3.19x speedup (1.85x on average) while maintaining response quality within\n1% difference compared to autoregressive models, realizing significant\nacceleration without compromising generation quality. Our framework sets a\ngroundbreaking benchmark for efficient LLM parallel inference, paving the way\nfor its deployment in latency-sensitive applications such as AI-powered\ncustomer service bots and answer retrieval engines."
                },
                "authors": [
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Zhifeng Shen"
                    },
                    {
                        "name": "Daohai Yu"
                    },
                    {
                        "name": "Haoqian Wu"
                    },
                    {
                        "name": "Wei Wen"
                    },
                    {
                        "name": "Jianfeng He"
                    },
                    {
                        "name": "Ruizhi Qiao"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "arxiv_comment": "20 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10424v1",
                "updated": "2025-08-14T07:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T07:54:44Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    7,
                    54,
                    44,
                    3,
                    226,
                    0
                ],
                "title": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoControl: A Lightweight Framework for Precise and Efficient Control\n  in Diffusion Transformer"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional capabilities in\ntext-to-image synthesis. However, in the domain of controllable text-to-image\ngeneration using DiTs, most existing methods still rely on the ControlNet\nparadigm originally designed for UNet-based diffusion models. This paradigm\nintroduces significant parameter overhead and increased computational costs. To\naddress these challenges, we propose the Nano Control Diffusion Transformer\n(NanoControl), which employs Flux as the backbone network. Our model achieves\nstate-of-the-art controllable text-to-image generation performance while\nincurring only a 0.024\\% increase in parameter count and a 0.029\\% increase in\nGFLOPs, thus enabling highly efficient controllable generation. Specifically,\nrather than duplicating the DiT backbone for control, we design a LoRA-style\n(low-rank adaptation) control module that directly learns control signals from\nraw conditioning inputs. Furthermore, we introduce a KV-Context Augmentation\nmechanism that integrates condition-specific key-value information into the\nbackbone in a simple yet highly effective manner, facilitating deep fusion of\nconditional features. Extensive benchmark experiments demonstrate that\nNanoControl significantly reduces computational overhead compared to\nconventional control approaches, while maintaining superior generation quality\nand achieving improved controllability."
                },
                "authors": [
                    {
                        "name": "Shanyuan Liu"
                    },
                    {
                        "name": "Jian Zhu"
                    },
                    {
                        "name": "Junda Lu"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Liuzhuozheng Li"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Yuhang Ma"
                    },
                    {
                        "name": "Liebucha Wu"
                    },
                    {
                        "name": "Xiaoyu Wu"
                    },
                    {
                        "name": "Dawei Leng"
                    },
                    {
                        "name": "Yuhui Yin"
                    }
                ],
                "author_detail": {
                    "name": "Yuhui Yin"
                },
                "author": "Yuhui Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10395v1",
                "updated": "2025-08-14T06:52:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "published": "2025-08-14T06:52:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    6,
                    52,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Breaking the Memory Wall for LLM Inference with KV Cache\n  Rematerialization"
                },
                "summary": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM inference has emerged as a critical workload for many downstream\napplications, efficiently inferring LLMs is challenging due to the substantial\nmemory footprint and bandwidth requirements. In parallel, compute capabilities\nhave steadily outpaced both memory capacity and bandwidth over the last few\ndecades, a trend that remains evident in modern GPU hardware and exacerbates\nthe challenge of LLM inference. As such, new algorithms are emerging that trade\nincreased computation for reduced memory operations. To that end, we present\nXQuant, which takes advantage of this trend, enabling an order-of-magnitude\nreduction in memory consumption through low-bit quantization with substantial\naccuracy benefits relative to state-of-the-art KV cache quantization methods.\nWe accomplish this by quantizing and caching the layer input activations X,\ninstead of using standard KV caching, and then rematerializing the Keys and\nValues on-the-fly during inference. This results in an immediate 2$\\times$\nmemory savings compared to KV caching. By applying XQuant, we achieve up to\n$\\sim 7.7\\times$ memory savings with $<0.1$ perplexity degradation compared to\nthe FP16 baseline. Furthermore, our approach leverages the fact that X values\nare similar across layers. Building on this observation, we introduce\nXQuant-CL, which exploits the cross-layer similarity in the X embeddings for\nextreme compression. Across different models, XQuant-CL attains up to\n10$\\times$ memory savings relative to the FP16 baseline with only 0.01\nperplexity degradation, and 12.5$\\times$ memory savings with only $0.1$\nperplexity degradation. XQuant exploits the rapidly increasing compute\ncapabilities of hardware platforms to eliminate the memory bottleneck, while\nsurpassing state-of-the-art KV cache quantization methods and achieving\nnear-FP16 accuracy across a wide range of models."
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v3",
                "updated": "2025-08-13T17:55:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    17,
                    55,
                    58,
                    2,
                    225,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme. The\nsource code is available here: https://github.com/NVlabs/RocketKV."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v4",
                "updated": "2025-08-13T06:13:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    6,
                    13,
                    36,
                    2,
                    225,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09500v2",
                "updated": "2025-08-13T04:24:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    24,
                    56,
                    2,
                    225,
                    0
                ],
                "published": "2025-07-13T05:37:33Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    5,
                    37,
                    33,
                    6,
                    194,
                    0
                ],
                "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under\n  Visual Variations"
                },
                "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under real-world distribution shifts. Code:\nhttps://github.com/Evelyn1ywliang/ReTA."
                },
                "authors": [
                    {
                        "name": "Yiwen Liang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Zihan Zhou"
                    },
                    {
                        "name": "Mengyao Lyu"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Shuaicheng Niu"
                    },
                    {
                        "name": "Sicheng Zhao"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v4",
                "updated": "2025-08-13T04:03:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    4,
                    3,
                    10,
                    2,
                    225,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09442v1",
                "updated": "2025-08-13T02:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T02:48:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    2,
                    48,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache\n  in LLM Inference"
                },
                "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment."
                },
                "authors": [
                    {
                        "name": "Zhifan Luo"
                    },
                    {
                        "name": "Shuo Shao"
                    },
                    {
                        "name": "Su Zhang"
                    },
                    {
                        "name": "Lijing Zhou"
                    },
                    {
                        "name": "Yuke Hu"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Zhihao Liu"
                    },
                    {
                        "name": "Zhan Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zhan Qin"
                },
                "author": "Zhan Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09419v1",
                "updated": "2025-08-13T01:39:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "published": "2025-08-13T01:39:09Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    1,
                    39,
                    9,
                    2,
                    225,
                    0
                ],
                "title": "Design and Simulation of 6T SRAM Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Simulation of 6T SRAM Array"
                },
                "summary": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional 6T SRAM is used in microprocessors in the cache memory design.\nThe basic 6T SRAM cell and a 6 bit memory array layout are designed in LEdit.\nThe design and analysis of key SRAM components, sense amplifiers, decoders,\nwrite drivers and precharge circuits are also provided. The pulse voltage\nwaveforms generated for read and write operations as well as Q and Qbar nodes\nare simulated in LTSpice. Parasitic capacitances are extracted and their impact\non the waveforms analyzed. Static noise margin, propagation delays, and power\ndissipation are calculated. Comparison of SRAM read and write operational\nperformance using CMOS transistors is made with edge-triggered D flip flops. If\ncertain size area and ratio constraints are satisfied, the 6T cell with CMOS\ntransistors will possess stability, speed, and power efficiency. Both\ntheoretical and simulated results are given."
                },
                "authors": [
                    {
                        "name": "Justin London"
                    }
                ],
                "author_detail": {
                    "name": "Justin London"
                },
                "author": "Justin London",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.14940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14940v2",
                "updated": "2025-08-26T17:59:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-20T02:59:39Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    2,
                    59,
                    39,
                    2,
                    232,
                    0
                ],
                "title": "Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using\n  a Retrieval-Augmented Model Selection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using\n  a Retrieval-Augmented Model Selection Framework"
                },
                "summary": "Accurate lung cancer risk prediction remains challenging due to substantial\nvariability across patient populations and clinical settings -- no single model\nperforms best for all cohorts. To address this, we propose a personalized lung\ncancer risk prediction agent that dynamically selects the most appropriate\nmodel for each patient by combining cohort-specific knowledge with modern\nretrieval and reasoning techniques. Given a patient's CT scan and structured\nmetadata -- including demographic, clinical, and nodule-level features -- the\nagent first performs cohort retrieval using FAISS-based similarity search\nacross nine diverse real-world cohorts to identify the most relevant patient\npopulation from a multi-institutional database. Second, a Large Language Model\n(LLM) is prompted with the retrieved cohort and its associated performance\nmetrics to recommend the optimal prediction algorithm from a pool of eight\nrepresentative models, including classical linear risk models (e.g., Mayo,\nBrock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computer\nvision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent\npipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,\ncohort-aware risk prediction personalized to each patient's profile. Building\non this architecture, the agent supports flexible and cohort-driven model\nselection across diverse clinical populations, offering a practical path toward\nindividualized risk assessment in real-world lung cancer screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate lung cancer risk prediction remains challenging due to substantial\nvariability across patient populations and clinical settings -- no single model\nperforms best for all cohorts. To address this, we propose a personalized lung\ncancer risk prediction agent that dynamically selects the most appropriate\nmodel for each patient by combining cohort-specific knowledge with modern\nretrieval and reasoning techniques. Given a patient's CT scan and structured\nmetadata -- including demographic, clinical, and nodule-level features -- the\nagent first performs cohort retrieval using FAISS-based similarity search\nacross nine diverse real-world cohorts to identify the most relevant patient\npopulation from a multi-institutional database. Second, a Large Language Model\n(LLM) is prompted with the retrieved cohort and its associated performance\nmetrics to recommend the optimal prediction algorithm from a pool of eight\nrepresentative models, including classical linear risk models (e.g., Mayo,\nBrock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computer\nvision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent\npipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,\ncohort-aware risk prediction personalized to each patient's profile. Building\non this architecture, the agent supports flexible and cohort-driven model\nselection across diverse clinical populations, offering a practical path toward\nindividualized risk assessment in real-world lung cancer screening."
                },
                "authors": [
                    {
                        "name": "Chongyu Qu"
                    },
                    {
                        "name": "Allen J. Luna"
                    },
                    {
                        "name": "Thomas Z. Li"
                    },
                    {
                        "name": "Junchao Zhu"
                    },
                    {
                        "name": "Junlin Guo"
                    },
                    {
                        "name": "Juming Xiong"
                    },
                    {
                        "name": "Kim L. Sandler"
                    },
                    {
                        "name": "Bennett A. Landman"
                    },
                    {
                        "name": "Yuankai Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yuankai Huo"
                },
                "author": "Yuankai Huo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19229v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19229v1",
                "updated": "2025-08-26T17:45:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    45,
                    5,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:45:05Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    45,
                    5,
                    1,
                    238,
                    0
                ],
                "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StepWiser: Stepwise Generative Judges for Wiser Reasoning"
                },
                "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search."
                },
                "authors": [
                    {
                        "name": "Wei Xiong"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Weizhe Yuan"
                    },
                    {
                        "name": "Olga Golovneva"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    }
                ],
                "author_detail": {
                    "name": "Sainbayar Sukhbaatar"
                },
                "author": "Sainbayar Sukhbaatar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19229v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19227v1",
                "updated": "2025-08-26T17:43:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    43,
                    20,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:43:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    43,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "Generative Interfaces for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Interfaces for Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Yijia Shao"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14252v2",
                "updated": "2025-08-26T17:22:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    22,
                    52,
                    1,
                    238,
                    0
                ],
                "published": "2024-11-21T15:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification"
                },
                "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "arxiv_comment": "Accepted to Proceedings of CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19208v1",
                "updated": "2025-08-26T17:13:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    13,
                    20,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:13:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    13,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "Astrophysics informed Gaussian processes for gravitational-wave\n  populations: Evidence for the onset of the pair-instability supernova mass\n  gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astrophysics informed Gaussian processes for gravitational-wave\n  populations: Evidence for the onset of the pair-instability supernova mass\n  gap"
                },
                "summary": "We analyze binary black hole (BBH) mergers from the latest Gravitational Wave\nTransient Catalog (GWTC-3) using a flexible, non-parametric framework to infer\nthe underlying black hole mass distribution. Our model employs Gaussian\nProcesses (GPs) with astrophysically motivated priors to represent the mass\ndistribution, assuming both component masses are independently drawn from a\ncommon mass function. This approach enables us to capture complex features in\nthe population without relying on rigid parametric forms. Motivated by\npredictions from binary stellar evolution, we focus on the presence of a mass\ngap in the range $40-120~ M_\\odot$, attributed to the pair-instability\nsupernova (PISN) and pulsational PISN (PPISN) processes. Using our GP-based\nmodel, we find for the first time, strong evidence for the onset of this mass\ngap, locating its lower edge at approximately $45-60 ~M_\\odot$. We observe a\nsuppression in the BBH merger rate when comparing the latest constraints\nagainst our GP model, by a factor of $10-60$ at $\\sim60~M_\\odot$, corresponding\nto the minimum of the mass gap. Additionally, we find evidence of a\nsubpopulation of mergers populating the mass gap around $70 ~M_\\odot$, which we\nargue is due to hierarchical mergers, as well as of a feature in the $40-50~\nM_\\odot$ range, albeit with low significance, which we attribute to the\npredicted PPISN build up. We discuss the astrophysical implications of this\nresult in light of GW231123, a recently reported BBH merger with a total mass\npotentially above the PISN gap, suggesting the need for revised models of\nmassive stellar evolution or alternative formation channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze binary black hole (BBH) mergers from the latest Gravitational Wave\nTransient Catalog (GWTC-3) using a flexible, non-parametric framework to infer\nthe underlying black hole mass distribution. Our model employs Gaussian\nProcesses (GPs) with astrophysically motivated priors to represent the mass\ndistribution, assuming both component masses are independently drawn from a\ncommon mass function. This approach enables us to capture complex features in\nthe population without relying on rigid parametric forms. Motivated by\npredictions from binary stellar evolution, we focus on the presence of a mass\ngap in the range $40-120~ M_\\odot$, attributed to the pair-instability\nsupernova (PISN) and pulsational PISN (PPISN) processes. Using our GP-based\nmodel, we find for the first time, strong evidence for the onset of this mass\ngap, locating its lower edge at approximately $45-60 ~M_\\odot$. We observe a\nsuppression in the BBH merger rate when comparing the latest constraints\nagainst our GP model, by a factor of $10-60$ at $\\sim60~M_\\odot$, corresponding\nto the minimum of the mass gap. Additionally, we find evidence of a\nsubpopulation of mergers populating the mass gap around $70 ~M_\\odot$, which we\nargue is due to hierarchical mergers, as well as of a feature in the $40-50~\nM_\\odot$ range, albeit with low significance, which we attribute to the\npredicted PPISN build up. We discuss the astrophysical implications of this\nresult in light of GW231123, a recently reported BBH merger with a total mass\npotentially above the PISN gap, suggesting the need for revised models of\nmassive stellar evolution or alternative formation channels."
                },
                "authors": [
                    {
                        "name": "Ignacio Magaña Hernandez"
                    },
                    {
                        "name": "Antonella Palmese"
                    }
                ],
                "author_detail": {
                    "name": "Antonella Palmese"
                },
                "author": "Antonella Palmese",
                "arxiv_comment": "Comments are welcomed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13358v3",
                "updated": "2025-08-26T17:11:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    11,
                    42,
                    1,
                    238,
                    0
                ],
                "published": "2025-02-19T01:41:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    1,
                    41,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text\n  Modifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text\n  Modifications"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating strong capabilities in tasks such as text generation,\nsummarization, and reasoning. Recently, their potential for automating precise\ntext editing tasks across specialized domains, such as programming code, LaTeX,\nand structured database languages, has gained attention. However, current\nstate-of-the-art LLMs still struggle with executing precise, instruction-driven\nedits, particularly when structural accuracy and strict adherence to domain\nconventions are required. To address these challenges, we introduce\nInstrEditBench, an automated benchmark dataset comprising over 30,000\nstructured editing tasks spanning diverse domains, including Wikipedia\narticles, LaTeX documents, source code, and database languages. Using this\nbenchmark, we develop FineEdit, a specialized editing model explicitly trained\nfor accurate, context-aware text modifications. Experimental evaluations\ndemonstrate that FineEdit outperforms state-of-the-art models, achieving\nimprovements of approximately 10\\% over Gemini models on single-turn edits, up\nto 30\\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by\nover 40\\% on direct editing tasks. FineEdit also effectively generalizes to\nrealistic multi-turn editing scenarios, highlighting its practical\napplicability. To facilitate further research and reproducibility, we release\nFineEdit at https://github.com/StuRinDQB/FineEdit} and\nhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating strong capabilities in tasks such as text generation,\nsummarization, and reasoning. Recently, their potential for automating precise\ntext editing tasks across specialized domains, such as programming code, LaTeX,\nand structured database languages, has gained attention. However, current\nstate-of-the-art LLMs still struggle with executing precise, instruction-driven\nedits, particularly when structural accuracy and strict adherence to domain\nconventions are required. To address these challenges, we introduce\nInstrEditBench, an automated benchmark dataset comprising over 30,000\nstructured editing tasks spanning diverse domains, including Wikipedia\narticles, LaTeX documents, source code, and database languages. Using this\nbenchmark, we develop FineEdit, a specialized editing model explicitly trained\nfor accurate, context-aware text modifications. Experimental evaluations\ndemonstrate that FineEdit outperforms state-of-the-art models, achieving\nimprovements of approximately 10\\% over Gemini models on single-turn edits, up\nto 30\\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by\nover 40\\% on direct editing tasks. FineEdit also effectively generalizes to\nrealistic multi-turn editing scenarios, highlighting its practical\napplicability. To facilitate further research and reproducibility, we release\nFineEdit at https://github.com/StuRinDQB/FineEdit} and\nhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench."
                },
                "authors": [
                    {
                        "name": "Yiming Zeng"
                    },
                    {
                        "name": "Wanhao Yu"
                    },
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Tao Ren"
                    },
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Jinghan Cao"
                    },
                    {
                        "name": "Xiyan Chen"
                    },
                    {
                        "name": "Tingting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yu"
                },
                "author": "Tingting Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19202v1",
                "updated": "2025-08-26T17:04:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    4,
                    23,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:04:23Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    4,
                    23,
                    1,
                    238,
                    0
                ],
                "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning"
                },
                "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning."
                },
                "authors": [
                    {
                        "name": "Alan Li"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Arpan Sarkar"
                    },
                    {
                        "name": "Doug Downey"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "28 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19201v1",
                "updated": "2025-08-26T17:03:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    46,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:03:46Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    46,
                    1,
                    238,
                    0
                ],
                "title": "Understanding Tool-Integrated Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Tool-Integrated Reasoning"
                },
                "summary": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning."
                },
                "authors": [
                    {
                        "name": "Heng Lin"
                    },
                    {
                        "name": "Zhongwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongwen Xu"
                },
                "author": "Zhongwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19200v1",
                "updated": "2025-08-26T17:03:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:03:43Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ramon Llull's Thinking Machine for Automated Ideation"
                },
                "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Runlong Zhou"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16369v2",
                "updated": "2025-08-26T17:02:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    2,
                    0,
                    1,
                    238,
                    0
                ],
                "published": "2025-06-19T14:45:46Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    14,
                    45,
                    46,
                    3,
                    170,
                    0
                ],
                "title": "Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based Dynamic Token Pruning for Efficient Segmentation of Medical\n  Images"
                },
                "summary": "The high computational demands of Vision Transformers (ViTs) in processing a\nlarge number of tokens often constrain their practical application in analyzing\nmedical images. This research proposes a Prompt-driven Adaptive Token ({\\it\nPrATo}) pruning method to selectively reduce the processing of irrelevant\ntokens in the segmentation pipeline. The prompt-based spatial prior helps to\nrank the tokens according to their relevance. Tokens with low-relevance scores\nare down-weighted, ensuring that only the relevant ones are propagated for\nprocessing across subsequent stages. This data-driven pruning strategy improves\nsegmentation accuracy and inference speed by allocating computational resources\nto essential regions. The proposed framework is integrated with several\nstate-of-the-art models to facilitate the elimination of irrelevant tokens,\nthereby enhancing computational efficiency while preserving segmentation\naccuracy. The experimental results show a reduction of $\\sim$ 35-55% tokens;\nthus reducing the computational costs relative to baselines. Cost-effective\nmedical image processing, using our framework, facilitates real-time diagnosis\nby expanding its applicability in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational demands of Vision Transformers (ViTs) in processing a\nlarge number of tokens often constrain their practical application in analyzing\nmedical images. This research proposes a Prompt-driven Adaptive Token ({\\it\nPrATo}) pruning method to selectively reduce the processing of irrelevant\ntokens in the segmentation pipeline. The prompt-based spatial prior helps to\nrank the tokens according to their relevance. Tokens with low-relevance scores\nare down-weighted, ensuring that only the relevant ones are propagated for\nprocessing across subsequent stages. This data-driven pruning strategy improves\nsegmentation accuracy and inference speed by allocating computational resources\nto essential regions. The proposed framework is integrated with several\nstate-of-the-art models to facilitate the elimination of irrelevant tokens,\nthereby enhancing computational efficiency while preserving segmentation\naccuracy. The experimental results show a reduction of $\\sim$ 35-55% tokens;\nthus reducing the computational costs relative to baselines. Cost-effective\nmedical image processing, using our framework, facilitates real-time diagnosis\nby expanding its applicability in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Pallabi Dutta"
                    },
                    {
                        "name": "Anubhab Maity"
                    },
                    {
                        "name": "Sushmita Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Sushmita Mitra"
                },
                "author": "Sushmita Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19197v1",
                "updated": "2025-08-26T17:01:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    1,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    1,
                    34,
                    1,
                    238,
                    0
                ],
                "title": "Unraveling the temporal dependence of ecological interaction measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the temporal dependence of ecological interaction measures"
                },
                "summary": "Species interactions (ranging from direct predator prey relationships to\nindirect effects mediated by the environment) are central to ecosystem balance\nand biodiversity. While empirical methods for measuring these interactions\nexist, their interpretability and limitations remain unclear. Here we examine\nthe empirical matrix of pairwise interactions, a widely used tool, and analyze\nits temporal variability. We show that apparent fluctuations in interaction\nstrength (and even shifts in interaction signs, often interpreted as\ntransitions between competition and facilitation) can arise intrinsically from\npopulation dynamics with fixed ecological roles. Experimental protocols further\nshape these estimates: the duration of observation and the type of setup in\nmicrobial growth studies (e.g., chemostats, batch cultures, or resource\nconditions) systematically affect measured interactions. Considering\ninteractions across timescales enhances interpretability: short-term\nmeasurements primarily capture direct species couplings, whereas long-term\nobservations increasingly reflect indirect community feedback. Taken together,\nthese results establish short duration inferences, obtained either directly or\nextrapolated, as a principled way to disentangle direct from indirect\ninteractions. Building on this insight, we propose a model inference approach\nthat leverages multiple short time series rather than extended longitudinal\ndatasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Species interactions (ranging from direct predator prey relationships to\nindirect effects mediated by the environment) are central to ecosystem balance\nand biodiversity. While empirical methods for measuring these interactions\nexist, their interpretability and limitations remain unclear. Here we examine\nthe empirical matrix of pairwise interactions, a widely used tool, and analyze\nits temporal variability. We show that apparent fluctuations in interaction\nstrength (and even shifts in interaction signs, often interpreted as\ntransitions between competition and facilitation) can arise intrinsically from\npopulation dynamics with fixed ecological roles. Experimental protocols further\nshape these estimates: the duration of observation and the type of setup in\nmicrobial growth studies (e.g., chemostats, batch cultures, or resource\nconditions) systematically affect measured interactions. Considering\ninteractions across timescales enhances interpretability: short-term\nmeasurements primarily capture direct species couplings, whereas long-term\nobservations increasingly reflect indirect community feedback. Taken together,\nthese results establish short duration inferences, obtained either directly or\nextrapolated, as a principled way to disentangle direct from indirect\ninteractions. Building on this insight, we propose a model inference approach\nthat leverages multiple short time series rather than extended longitudinal\ndatasets."
                },
                "authors": [
                    {
                        "name": "Javier Aguilar"
                    },
                    {
                        "name": "Samir Suweis"
                    },
                    {
                        "name": "Amos Maritan"
                    },
                    {
                        "name": "Sandro Azaele"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Azaele"
                },
                "author": "Sandro Azaele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17437v2",
                "updated": "2025-08-26T16:57:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    57,
                    7,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-20T19:24:04Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    24,
                    4,
                    2,
                    232,
                    0
                ],
                "title": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pixie: Fast and Generalizable Supervised Learning of 3D Physics from\n  Pixels"
                },
                "summary": "Inferring the physical properties of 3D scenes from visual information is a\ncritical yet challenging task for creating interactive and realistic virtual\nworlds. While humans intuitively grasp material characteristics such as\nelasticity or stiffness, existing methods often rely on slow, per-scene\noptimization, limiting their generalizability and application. To address this\nproblem, we introduce PIXIE, a novel method that trains a generalizable neural\nnetwork to predict physical properties across multiple scenes from 3D visual\nfeatures purely using supervised losses. Once trained, our feed-forward network\ncan perform fast inference of plausible material fields, which coupled with a\nlearned static scene representation like Gaussian Splatting enables realistic\nphysics simulation under external forces. To facilitate this research, we also\ncollected PIXIEVERSE, one of the largest known datasets of paired 3D assets and\nphysic material annotations. Extensive evaluations demonstrate that PIXIE is\nabout 1.46-4.39x better and orders of magnitude faster than test-time\noptimization methods. By leveraging pretrained visual features like CLIP, our\nmethod can also zero-shot generalize to real-world scenes despite only ever\nbeen trained on synthetic data. https://pixie-3d.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring the physical properties of 3D scenes from visual information is a\ncritical yet challenging task for creating interactive and realistic virtual\nworlds. While humans intuitively grasp material characteristics such as\nelasticity or stiffness, existing methods often rely on slow, per-scene\noptimization, limiting their generalizability and application. To address this\nproblem, we introduce PIXIE, a novel method that trains a generalizable neural\nnetwork to predict physical properties across multiple scenes from 3D visual\nfeatures purely using supervised losses. Once trained, our feed-forward network\ncan perform fast inference of plausible material fields, which coupled with a\nlearned static scene representation like Gaussian Splatting enables realistic\nphysics simulation under external forces. To facilitate this research, we also\ncollected PIXIEVERSE, one of the largest known datasets of paired 3D assets and\nphysic material annotations. Extensive evaluations demonstrate that PIXIE is\nabout 1.46-4.39x better and orders of magnitude faster than test-time\noptimization methods. By leveraging pretrained visual features like CLIP, our\nmethod can also zero-shot generalize to real-world scenes despite only ever\nbeen trained on synthetic data. https://pixie-3d.github.io/"
                },
                "authors": [
                    {
                        "name": "Long Le"
                    },
                    {
                        "name": "Ryan Lucas"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Chuhao Chen"
                    },
                    {
                        "name": "Dinesh Jayaraman"
                    },
                    {
                        "name": "Eric Eaton"
                    },
                    {
                        "name": "Lingjie Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjie Liu"
                },
                "author": "Lingjie Liu",
                "arxiv_comment": "Website: https://pixie-3d.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19184v1",
                "updated": "2025-08-26T16:44:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    44,
                    30,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T16:44:30Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    44,
                    30,
                    1,
                    238,
                    0
                ],
                "title": "Separating Intent from Execution: A Probabilistic Approach to Pitch\n  Location Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Separating Intent from Execution: A Probabilistic Approach to Pitch\n  Location Accuracy"
                },
                "summary": "Control has long been recognized as a critical component of pitcher\nperformance, reflecting a pitcher's ability to execute pitches in alignment\nwith his intended targets. However, accurately inferring a pitcher's intentions\npresents a persistent challenge. Traditional metrics typically rely on\nuniformity assumptions, inferring intent based on the behavior of a ``typical''\npitcher across similar situations. In this study, we propose an alternative,\nindividualized approach to measuring control, one that eschews such assumptions\nin favor of personalized inference. We estimate a pitcher's intended location\non a pitch-by-pitch basis, conditioning on both individual tendencies and\nspecific game contexts. This allows us to assess control by comparing the\nactual pitch location to the inferred intended target, thereby aligning\nmeasurement more closely with the unique strategies of each pitcher. We\nintroduce xCTRL, a novel metric that quantifies control as the distance between\na pitch's actual location and its estimated intended location. We find that\nxCTRL exhibits strong stability and greater predictive power than existing\ncontrol metrics. By capturing pitcher-specific intent, xCTRL enhances our\nunderstanding of control and offers a more intuitive and accurate\nrepresentation of pitching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control has long been recognized as a critical component of pitcher\nperformance, reflecting a pitcher's ability to execute pitches in alignment\nwith his intended targets. However, accurately inferring a pitcher's intentions\npresents a persistent challenge. Traditional metrics typically rely on\nuniformity assumptions, inferring intent based on the behavior of a ``typical''\npitcher across similar situations. In this study, we propose an alternative,\nindividualized approach to measuring control, one that eschews such assumptions\nin favor of personalized inference. We estimate a pitcher's intended location\non a pitch-by-pitch basis, conditioning on both individual tendencies and\nspecific game contexts. This allows us to assess control by comparing the\nactual pitch location to the inferred intended target, thereby aligning\nmeasurement more closely with the unique strategies of each pitcher. We\nintroduce xCTRL, a novel metric that quantifies control as the distance between\na pitch's actual location and its estimated intended location. We find that\nxCTRL exhibits strong stability and greater predictive power than existing\ncontrol metrics. By capturing pitcher-specific intent, xCTRL enhances our\nunderstanding of control and offers a more intuitive and accurate\nrepresentation of pitching performance."
                },
                "authors": [
                    {
                        "name": "Matt Ludwig"
                    },
                    {
                        "name": "Ryan S. Brill"
                    },
                    {
                        "name": "Abraham J. Wyner"
                    }
                ],
                "author_detail": {
                    "name": "Abraham J. Wyner"
                },
                "author": "Abraham J. Wyner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15495v2",
                "updated": "2025-08-26T16:40:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    40,
                    49,
                    1,
                    238,
                    0
                ],
                "published": "2024-12-20T02:21:36Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    21,
                    36,
                    4,
                    355,
                    0
                ],
                "title": "TL-Training: A Task-Feature-Based Framework for Training Large Language\n  Models in Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TL-Training: A Task-Feature-Based Framework for Training Large Language\n  Models in Tool Use"
                },
                "summary": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zhongchao Shi"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Zhengyin Du"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyin Du"
                },
                "author": "Zhengyin Du",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04594v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04594v5",
                "updated": "2025-08-26T16:31:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    31,
                    32,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-07T17:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    37,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"
                },
                "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Girish Chandar Ganesan"
                    },
                    {
                        "name": "Xiaoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Liu"
                },
                "author": "Xiaoming Liu",
                "arxiv_comment": "I plan to re-format and re-write this paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04594v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04594v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19174v1",
                "updated": "2025-08-26T16:26:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    26,
                    1,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T16:26:01Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    26,
                    1,
                    1,
                    238,
                    0
                ],
                "title": "Applications of compact multipliers to algebrability of\n  $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$ and $(B(\\ell_2(\\mathbb{N}))\\setminus\n  K(\\ell_2(\\mathbb{N}))\\cup \\{ 0\\}.$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications of compact multipliers to algebrability of\n  $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$ and $(B(\\ell_2(\\mathbb{N}))\\setminus\n  K(\\ell_2(\\mathbb{N}))\\cup \\{ 0\\}.$"
                },
                "summary": "In present work we deal with the class $\\mathcal{C}=\\mathcal{C}_1\\cup\n\\mathcal{C}_2$ where $\\mathcal{C}_1$ (respectively, $\\mathcal{C}_2$) is formed\nby all separable Uniform algebras (respectively, separable commutative\nC$^*$-algebras) with no compact elements. For a given algebra $A$ in\n$\\mathcal{C}_1$ (respectively, $A$ in $\\mathcal{C}_2$) we show that $A$ is\nisometrically isomorphic as algebra (respectively, as C$^*$-algebra) to a\nsubalgebra $M$ of $\\ell_{\\infty}$ with $M\\subset (\\ell_{\\infty}\\setminus\nc_0)\\cup\\{0\\}.$ Under the additional assumption that $A$ is non-unital we\nverify that there exists a copy of $M(A)$ (the multipliers algebra of $A$ which\nis non-separable) inside $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$.\n  For an infinitely generated abelian C$^*$-algebra $B,$ we study the least\ncardinality possible of a system of generators ($gen_{C^*}(B)$). In fact we\ndeduce that $gen_{C^*}(B)$ coincides with the smallest cardinal number $n$ such\nthat an embedding of $\\Delta(B)$ (= the spectrum of $B$) in $\\mathbb{R}^n$\nexists - The finitely generated version of this result was proved by Nagisa. In\naddition, we introduce new concepts of algebrability in terms of $gen_{C^*}(B)$\n($(C^*)$-genalgebrability) and its natural variations.\n  From our methods we infer that there is $^*$-isomorphic copy of\n$\\ell_{\\infty}$ in $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$. In particular,\n$(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$ contains a copy of every separable\nBanach space. Moreover, all the positive answers of this work holds if we\nreplace the set $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$ with\n$(B(\\ell_2(\\mathbb{N}))\\setminus K(\\ell_2(\\mathbb{N}))\\cup \\{ 0\\}.$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In present work we deal with the class $\\mathcal{C}=\\mathcal{C}_1\\cup\n\\mathcal{C}_2$ where $\\mathcal{C}_1$ (respectively, $\\mathcal{C}_2$) is formed\nby all separable Uniform algebras (respectively, separable commutative\nC$^*$-algebras) with no compact elements. For a given algebra $A$ in\n$\\mathcal{C}_1$ (respectively, $A$ in $\\mathcal{C}_2$) we show that $A$ is\nisometrically isomorphic as algebra (respectively, as C$^*$-algebra) to a\nsubalgebra $M$ of $\\ell_{\\infty}$ with $M\\subset (\\ell_{\\infty}\\setminus\nc_0)\\cup\\{0\\}.$ Under the additional assumption that $A$ is non-unital we\nverify that there exists a copy of $M(A)$ (the multipliers algebra of $A$ which\nis non-separable) inside $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$.\n  For an infinitely generated abelian C$^*$-algebra $B,$ we study the least\ncardinality possible of a system of generators ($gen_{C^*}(B)$). In fact we\ndeduce that $gen_{C^*}(B)$ coincides with the smallest cardinal number $n$ such\nthat an embedding of $\\Delta(B)$ (= the spectrum of $B$) in $\\mathbb{R}^n$\nexists - The finitely generated version of this result was proved by Nagisa. In\naddition, we introduce new concepts of algebrability in terms of $gen_{C^*}(B)$\n($(C^*)$-genalgebrability) and its natural variations.\n  From our methods we infer that there is $^*$-isomorphic copy of\n$\\ell_{\\infty}$ in $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$. In particular,\n$(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$ contains a copy of every separable\nBanach space. Moreover, all the positive answers of this work holds if we\nreplace the set $(\\ell_{\\infty}\\setminus c_0)\\cup\\{0\\}$ with\n$(B(\\ell_2(\\mathbb{N}))\\setminus K(\\ell_2(\\mathbb{N}))\\cup \\{ 0\\}.$"
                },
                "authors": [
                    {
                        "name": "Willian Franca"
                    },
                    {
                        "name": "Jorge J. Garcés"
                    }
                ],
                "author_detail": {
                    "name": "Jorge J. Garcés"
                },
                "author": "Jorge J. Garcés",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.FA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary Primary: 46L05, 47L40, 46B87. Secondary: 46B45, 46B26",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19163v1",
                "updated": "2025-08-26T16:12:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    12,
                    12,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T16:12:12Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    12,
                    12,
                    1,
                    238,
                    0
                ],
                "title": "MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and\n  conteXtual clinical conversational evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and\n  conteXtual clinical conversational evaluation"
                },
                "summary": "Despite the growing use of large language models (LLMs) in clinical dialogue\nsystems, existing evaluations focus on task completion or fluency, offering\nlittle insight into the behavioral and risk management requirements essential\nfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion\nfRamework for safe Interactions and conteXtual clinical conversational\nevaluation), a structured, extensible framework for safety-oriented evaluation\nof clinical dialogue agents.\n  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical\nscenarios, expected system behaviors and failure modes derived through\nstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluator\nfor detecting safety-relevant dialogue failures, validated against expert\nclinician annotations; and (3) PatBot, a simulated patient agent capable of\nproducing diverse, scenario-conditioned responses, evaluated for realism and\nbehavioral fidelity with human factors expertise, and a patient-preference\nstudy.\n  Across three experiments, we show that MATRIX enables systematic, scalable\nsafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard\ndetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded\nassessment of 240 dialogues. We also conducted one of the first realism\nanalyses of LLM-based patient simulation, showing that PatBot reliably\nsimulates realistic patient behavior in quantitative and qualitative\nevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking\nfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios\nand 10 clinical domains.\n  MATRIX is the first framework to unify structured safety engineering with\nscalable, validated conversational AI evaluation, enabling regulator-aligned\nsafety auditing. We release all evaluation tools, prompts, structured\nscenarios, and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing use of large language models (LLMs) in clinical dialogue\nsystems, existing evaluations focus on task completion or fluency, offering\nlittle insight into the behavioral and risk management requirements essential\nfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion\nfRamework for safe Interactions and conteXtual clinical conversational\nevaluation), a structured, extensible framework for safety-oriented evaluation\nof clinical dialogue agents.\n  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical\nscenarios, expected system behaviors and failure modes derived through\nstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluator\nfor detecting safety-relevant dialogue failures, validated against expert\nclinician annotations; and (3) PatBot, a simulated patient agent capable of\nproducing diverse, scenario-conditioned responses, evaluated for realism and\nbehavioral fidelity with human factors expertise, and a patient-preference\nstudy.\n  Across three experiments, we show that MATRIX enables systematic, scalable\nsafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard\ndetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded\nassessment of 240 dialogues. We also conducted one of the first realism\nanalyses of LLM-based patient simulation, showing that PatBot reliably\nsimulates realistic patient behavior in quantitative and qualitative\nevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking\nfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios\nand 10 clinical domains.\n  MATRIX is the first framework to unify structured safety engineering with\nscalable, validated conversational AI evaluation, enabling regulator-aligned\nsafety auditing. We release all evaluation tools, prompts, structured\nscenarios, and datasets."
                },
                "authors": [
                    {
                        "name": "Ernest Lim"
                    },
                    {
                        "name": "Yajie Vera He"
                    },
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Kate Preston"
                    },
                    {
                        "name": "Mohita Chowdhury"
                    },
                    {
                        "name": "Louis Williams"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Katrina Mason"
                    },
                    {
                        "name": "Mariane Melo"
                    },
                    {
                        "name": "Tom Lawton"
                    },
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Ibrahim Habli"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Habli"
                },
                "author": "Ibrahim Habli",
                "arxiv_comment": "36 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T42, 92C50, 68Q60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19155v1",
                "updated": "2025-08-26T16:06:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    6,
                    22,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T16:06:22Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    6,
                    22,
                    1,
                    238,
                    0
                ],
                "title": "From Coverage to Consequences: BMI, Health Behaviors, and Self-rated\n  Health After Medicaid Contraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coverage to Consequences: BMI, Health Behaviors, and Self-rated\n  Health After Medicaid Contraction"
                },
                "summary": "Leveraging Tennessee's 2005 Medicaid contraction, I study the impact of\nlosing public health insurance on body weight and relevant health behaviors.\nUsing Behavioral Risk Factor Surveillance System (BRFSS) data from 1997 to\n2010, I estimate synthetic difference-in-differences models. The estimates\nsuggest that the reform increased Body Mass Index by 0.38 points and the\noverweight or obesity prevalence (BMI$\\geq$25) by $\\sim$4\\% among Tennessean\nchildless adults. My findings -- a 21\\% increase in the share of childless\nadults reporting ``poor'' health status (the lowest level on the five-point\nscale), a reduction in Medicaid-reimbursed utilization of pain and\nanti-inflammatory medications, and a reduction in participation in moderate\nphysical activities -- suggest that worsening unmanaged health conditions may\nbe a key pathway through which coverage loss affected weight gain.\nAdditionally, my analysis offers practical guidance for conducting robust\ninference in single treated cluster settings with limited pre-treatment data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Tennessee's 2005 Medicaid contraction, I study the impact of\nlosing public health insurance on body weight and relevant health behaviors.\nUsing Behavioral Risk Factor Surveillance System (BRFSS) data from 1997 to\n2010, I estimate synthetic difference-in-differences models. The estimates\nsuggest that the reform increased Body Mass Index by 0.38 points and the\noverweight or obesity prevalence (BMI$\\geq$25) by $\\sim$4\\% among Tennessean\nchildless adults. My findings -- a 21\\% increase in the share of childless\nadults reporting ``poor'' health status (the lowest level on the five-point\nscale), a reduction in Medicaid-reimbursed utilization of pain and\nanti-inflammatory medications, and a reduction in participation in moderate\nphysical activities -- suggest that worsening unmanaged health conditions may\nbe a key pathway through which coverage loss affected weight gain.\nAdditionally, my analysis offers practical guidance for conducting robust\ninference in single treated cluster settings with limited pre-treatment data."
                },
                "authors": [
                    {
                        "name": "Md Twfiqur Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Md Twfiqur Rahman"
                },
                "author": "Md Twfiqur Rahman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08249v2",
                "updated": "2025-08-26T15:53:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    53,
                    39,
                    1,
                    238,
                    0
                ],
                "published": "2024-07-11T07:51:57Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    7,
                    51,
                    57,
                    3,
                    193,
                    0
                ],
                "title": "GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and\n  Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and\n  Configuration"
                },
                "summary": "Communication network engineering in enterprise environments is traditionally\na complex, time-consuming, and error-prone manual process. Most research on\nnetwork engineering automation has concentrated on configuration synthesis,\noften overlooking changes in the physical network topology. This paper\nintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet\nis a novel framework that leverages a large language model (LLM) to streamline\nnetwork design workflows. It uses visual and textual modalities to interpret\nand update network topologies and device configurations based on user intents.\nGeNet was evaluated on enterprise network scenarios adapted from Cisco\ncertification exercises. Our results demonstrate GeNet's ability to interpret\nnetwork topology images accurately, potentially reducing network engineers'\nefforts and accelerating network design processes in enterprise environments.\nFurthermore, we show the importance of precise topology understanding when\nhandling intents that require modifications to the network's topology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication network engineering in enterprise environments is traditionally\na complex, time-consuming, and error-prone manual process. Most research on\nnetwork engineering automation has concentrated on configuration synthesis,\noften overlooking changes in the physical network topology. This paper\nintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet\nis a novel framework that leverages a large language model (LLM) to streamline\nnetwork design workflows. It uses visual and textual modalities to interpret\nand update network topologies and device configurations based on user intents.\nGeNet was evaluated on enterprise network scenarios adapted from Cisco\ncertification exercises. Our results demonstrate GeNet's ability to interpret\nnetwork topology images accurately, potentially reducing network engineers'\nefforts and accelerating network design processes in enterprise environments.\nFurthermore, we show the importance of precise topology understanding when\nhandling intents that require modifications to the network's topology."
                },
                "authors": [
                    {
                        "name": "Beni Ifland"
                    },
                    {
                        "name": "Elad Duani"
                    },
                    {
                        "name": "Rubin Krief"
                    },
                    {
                        "name": "Miro Ohana"
                    },
                    {
                        "name": "Aviram Zilberman"
                    },
                    {
                        "name": "Andres Murillo"
                    },
                    {
                        "name": "Ofir Manor"
                    },
                    {
                        "name": "Ortal Lavi"
                    },
                    {
                        "name": "Hikichi Kenji"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19140v1",
                "updated": "2025-08-26T15:49:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    49,
                    10,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:49:10Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    49,
                    10,
                    1,
                    238,
                    0
                ],
                "title": "A Bag of Tricks for Efficient Implicit Neural Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bag of Tricks for Efficient Implicit Neural Point Clouds"
                },
                "summary": "Implicit Neural Point Cloud (INPC) is a recent hybrid representation that\ncombines the expressiveness of neural fields with the efficiency of point-based\nrendering, achieving state-of-the-art image quality in novel view synthesis.\nHowever, as with other high-quality approaches that query neural networks\nduring rendering, the practical usability of INPC is limited by comparatively\nslow rendering. In this work, we present a collection of optimizations that\nsignificantly improve both the training and inference performance of INPC\nwithout sacrificing visual fidelity. The most significant modifications are an\nimproved rasterizer implementation, more effective sampling techniques, and the\nincorporation of pre-training for the convolutional neural network used for\nhole-filling. Furthermore, we demonstrate that points can be modeled as small\nGaussians during inference to further improve quality in extrapolated, e.g.,\nclose-up views of the scene. We design our implementations to be broadly\napplicable beyond INPC and systematically evaluate each modification in a\nseries of experiments. Our optimized INPC pipeline achieves up to 25% faster\ntraining, 2x faster rendering, and 20% reduced VRAM usage paired with slight\nimage quality improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Neural Point Cloud (INPC) is a recent hybrid representation that\ncombines the expressiveness of neural fields with the efficiency of point-based\nrendering, achieving state-of-the-art image quality in novel view synthesis.\nHowever, as with other high-quality approaches that query neural networks\nduring rendering, the practical usability of INPC is limited by comparatively\nslow rendering. In this work, we present a collection of optimizations that\nsignificantly improve both the training and inference performance of INPC\nwithout sacrificing visual fidelity. The most significant modifications are an\nimproved rasterizer implementation, more effective sampling techniques, and the\nincorporation of pre-training for the convolutional neural network used for\nhole-filling. Furthermore, we demonstrate that points can be modeled as small\nGaussians during inference to further improve quality in extrapolated, e.g.,\nclose-up views of the scene. We design our implementations to be broadly\napplicable beyond INPC and systematically evaluate each modification in a\nseries of experiments. Our optimized INPC pipeline achieves up to 25% faster\ntraining, 2x faster rendering, and 20% reduced VRAM usage paired with slight\nimage quality improvements."
                },
                "authors": [
                    {
                        "name": "Florian Hahlbohm"
                    },
                    {
                        "name": "Linus Franke"
                    },
                    {
                        "name": "Leon Overkämping"
                    },
                    {
                        "name": "Paula Wespe"
                    },
                    {
                        "name": "Susana Castillo"
                    },
                    {
                        "name": "Martin Eisemann"
                    },
                    {
                        "name": "Marcus Magnor"
                    }
                ],
                "author_detail": {
                    "name": "Marcus Magnor"
                },
                "author": "Marcus Magnor",
                "arxiv_comment": "Project page: https://fhahlbohm.github.io/inpc_v2/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06866v3",
                "updated": "2025-08-26T15:44:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    44,
                    42,
                    1,
                    238,
                    0
                ],
                "published": "2024-07-09T13:53:38Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    53,
                    38,
                    1,
                    191,
                    0
                ],
                "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context"
                },
                "summary": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly."
                },
                "authors": [
                    {
                        "name": "Victoria R. Li"
                    },
                    {
                        "name": "Yida Chen"
                    },
                    {
                        "name": "Naomi Saphra"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Saphra"
                },
                "author": "Naomi Saphra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19131v1",
                "updated": "2025-08-26T15:30:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    30,
                    19,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:30:19Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    30,
                    19,
                    1,
                    238,
                    0
                ],
                "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown\n  Environments"
                },
                "summary": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal."
                },
                "authors": [
                    {
                        "name": "Shreya Gummadi"
                    },
                    {
                        "name": "Mateus V. Gasparino"
                    },
                    {
                        "name": "Gianluca Capezzuto"
                    },
                    {
                        "name": "Marcelo Becker"
                    },
                    {
                        "name": "Girish Chowdhary"
                    }
                ],
                "author_detail": {
                    "name": "Girish Chowdhary"
                },
                "author": "Girish Chowdhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05123v3",
                "updated": "2025-08-26T15:28:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    28,
                    36,
                    1,
                    238,
                    0
                ],
                "published": "2024-02-04T13:32:01Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    13,
                    32,
                    1,
                    6,
                    35,
                    0
                ],
                "title": "A Survey on Data Selection for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Data Selection for LLM Instruction Tuning"
                },
                "summary": "Instruction tuning is a vital step of training large language models (LLMs),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLMs. Therefore,\nrecently a lot of studies focus on exploring the methods of selecting\nhigh-quality subset from instruction datasets, aiming to reduce training costs\nand enhance the instruction-following capabilities of LLMs. This paper presents\na comprehensive survey on data selection for LLM instruction tuning. Firstly,\nwe introduce the wildly used instruction datasets. Then, we propose a new\ntaxonomy of the data selection methods and provide a detailed introduction of\nrecent advances, and the evaluation strategies and results of data selection\nmethods are also elaborated in detail. Finally, we emphasize the open\nchallenges and present new frontiers of this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a vital step of training large language models (LLMs),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLMs. Therefore,\nrecently a lot of studies focus on exploring the methods of selecting\nhigh-quality subset from instruction datasets, aiming to reduce training costs\nand enhance the instruction-following capabilities of LLMs. This paper presents\na comprehensive survey on data selection for LLM instruction tuning. Firstly,\nwe introduce the wildly used instruction datasets. Then, we propose a new\ntaxonomy of the data selection methods and provide a detailed introduction of\nrecent advances, and the evaluation strategies and results of data selection\nmethods are also elaborated in detail. Finally, we emphasize the open\nchallenges and present new frontiers of this task."
                },
                "authors": [
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Dianhui Chu"
                },
                "author": "Dianhui Chu",
                "arxiv_doi": "10.1613/jair.1.17625",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1613/jair.1.17625",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in JAIR (Vol. 83, Article 32, 2025)",
                "arxiv_journal_ref": "Journal of Artificial Intelligence Research, 83:32, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00039v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00039v4",
                "updated": "2025-08-26T15:27:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    27,
                    25,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-29T18:36:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    36,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,\n  and Deterministic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,\n  and Deterministic Approach"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces an ontology-driven Graph RAG\nframework designed to overcome these limitations. We ground our knowledge graph\nin a formal, LRMoo-inspired model that distinguishes abstract legal Works from\ntheir versioned Expressions. We model temporal states as efficient aggregations\nthat reuse the versioned expressions (CTVs) of unchanged components, and we\nreify legislative events as first-class Action nodes to make causality explicit\nand queryable. This structured backbone enables a unified, planner-guided query\nstrategy that applies explicit policies to deterministically resolve complex\nrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,\nand (iii) auditable provenance reconstruction. Through a case study on the\nBrazilian Constitution, we demonstrate how this approach provides a verifiable,\ntemporally-correct substrate for LLMs, enabling higher-order analytical\ncapabilities while drastically reducing the risk of factual errors. The result\nis a practical framework for building more trustworthy and explainable legal AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces an ontology-driven Graph RAG\nframework designed to overcome these limitations. We ground our knowledge graph\nin a formal, LRMoo-inspired model that distinguishes abstract legal Works from\ntheir versioned Expressions. We model temporal states as efficient aggregations\nthat reuse the versioned expressions (CTVs) of unchanged components, and we\nreify legislative events as first-class Action nodes to make causality explicit\nand queryable. This structured backbone enables a unified, planner-guided query\nstrategy that applies explicit policies to deterministically resolve complex\nrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,\nand (iii) auditable provenance reconstruction. Through a case study on the\nBrazilian Constitution, we demonstrate how this approach provides a verifiable,\ntemporally-correct substrate for LLMs, enabling higher-order analytical\ncapabilities while drastically reducing the risk of factual errors. The result\nis a practical framework for building more trustworthy and explainable legal AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Hudson de Martim"
                    }
                ],
                "author_detail": {
                    "name": "Hudson de Martim"
                },
                "author": "Hudson de Martim",
                "arxiv_comment": "This is a major revision that significantly expands and deepens the\n  original manuscript. While the core ontological model remains the same, this\n  version provides a substantially more rigorous and detailed account of how\n  the framework is applied in practice, particularly within a\n  Retrieval-Augmented Generation (RAG) context",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00039v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00039v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12719v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12719v4",
                "updated": "2025-08-26T15:27:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    27,
                    18,
                    1,
                    238,
                    0
                ],
                "published": "2024-06-18T15:41:15Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    15,
                    41,
                    15,
                    1,
                    170,
                    0
                ],
                "title": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis"
                },
                "summary": "Large Language Models (LLMs), already shown to ace various unstructured text\ncomprehension tasks, have also remarkably been shown to tackle table\n(structured) comprehension tasks without specific training. Building on earlier\nstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),\nmodel scale, instruction tuning, and domain bias affect Tabular QA (TQA)\nrobustness by testing LLMs, under diverse augmentations and perturbations, on\ndiverse domains: Wikipedia-based $\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$,\nand scientific $\\textbf{SCITAB}$. Although instruction tuning and larger, newer\nLLMs deliver stronger, more robust TQA performance, data contamination and\nreliability issues, especially on $\\textbf{WTQ}$, remain unresolved. Through an\nin-depth attention analysis, we reveal a strong correlation between\nperturbation-induced shifts in attention dispersion and the drops in\nperformance, with sensitivity peaking in the model's middle layers. We\nhighlight the need for improved interpretable methodologies to develop more\nreliable LLMs for table comprehension. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and performance drops, with sensitivity peaking in the model's\nmiddle layers. Based on these findings, we argue for the development of\nstructure-aware self-attention mechanisms and domain-adaptive processing\ntechniques to improve the transparency, generalization, and real-world\nreliability of LLMs on tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), already shown to ace various unstructured text\ncomprehension tasks, have also remarkably been shown to tackle table\n(structured) comprehension tasks without specific training. Building on earlier\nstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),\nmodel scale, instruction tuning, and domain bias affect Tabular QA (TQA)\nrobustness by testing LLMs, under diverse augmentations and perturbations, on\ndiverse domains: Wikipedia-based $\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$,\nand scientific $\\textbf{SCITAB}$. Although instruction tuning and larger, newer\nLLMs deliver stronger, more robust TQA performance, data contamination and\nreliability issues, especially on $\\textbf{WTQ}$, remain unresolved. Through an\nin-depth attention analysis, we reveal a strong correlation between\nperturbation-induced shifts in attention dispersion and the drops in\nperformance, with sensitivity peaking in the model's middle layers. We\nhighlight the need for improved interpretable methodologies to develop more\nreliable LLMs for table comprehension. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and performance drops, with sensitivity peaking in the model's\nmiddle layers. Based on these findings, we argue for the development of\nstructure-aware self-attention mechanisms and domain-adaptive processing\ntechniques to improve the transparency, generalization, and real-world\nreliability of LLMs on tabular data."
                },
                "authors": [
                    {
                        "name": "Kushal Raj Bhandari"
                    },
                    {
                        "name": "Sixue Xing"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Jianxi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Gao"
                },
                "author": "Jianxi Gao",
                "arxiv_comment": "Accepted TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12719v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12719v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15665v2",
                "updated": "2025-08-26T15:21:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    21,
                    49,
                    1,
                    238,
                    0
                ],
                "published": "2025-01-26T20:09:11Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    20,
                    9,
                    11,
                    6,
                    26,
                    0
                ],
                "title": "StagFormer: Time Staggering Transformer Decoding for RunningLayers In\n  Parallel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StagFormer: Time Staggering Transformer Decoding for RunningLayers In\n  Parallel"
                },
                "summary": "Decoding in a Transformer based language model is inherently sequential as a\ntoken's embedding needs to pass through all the layers in the network before\nthe generation of the next token can begin. In this work, we propose a new\narchitecture StagFormer (Staggered Transformer), which staggers execution along\nthe sequence axis and thereby enables parallelizing the decoding process along\nthe depth of the model. We achieve this by breaking the dependency of the token\nrepresentation at time step $i$ in layer $l$ upon the representations of tokens\nuntil time step $i$ from layer $l-1$. Instead, we stagger the execution and\nonly allow a dependency on token representations until time step $i-1$. The\nlater sections of the Transformer still get access to the \"rich\"\nrepresentations from the prior section but only from those token positions\nwhich are one time step behind. StagFormer allows for different sections of the\nmodel to be executed in parallel yielding a potential speedup in decoding while\nbeing quality neutral in our simulations. We also explore many natural\nextensions of this idea. We present how weight-sharing across the different\nsections being staggered can be more practical in settings with limited memory.\nWe explore the efficacy of using a bounded window attention to pass information\nfrom one section to another which helps drive further latency gains for some\napplications. We also explore the scalability of the staggering idea over more\nthan 2 sections of the Transformer. Finally, we show how one can approximate a\nrecurrent model during inference using weight-sharing. This variant can lead to\nsubstantial gains in quality for short generations while being neutral in its\nlatency impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding in a Transformer based language model is inherently sequential as a\ntoken's embedding needs to pass through all the layers in the network before\nthe generation of the next token can begin. In this work, we propose a new\narchitecture StagFormer (Staggered Transformer), which staggers execution along\nthe sequence axis and thereby enables parallelizing the decoding process along\nthe depth of the model. We achieve this by breaking the dependency of the token\nrepresentation at time step $i$ in layer $l$ upon the representations of tokens\nuntil time step $i$ from layer $l-1$. Instead, we stagger the execution and\nonly allow a dependency on token representations until time step $i-1$. The\nlater sections of the Transformer still get access to the \"rich\"\nrepresentations from the prior section but only from those token positions\nwhich are one time step behind. StagFormer allows for different sections of the\nmodel to be executed in parallel yielding a potential speedup in decoding while\nbeing quality neutral in our simulations. We also explore many natural\nextensions of this idea. We present how weight-sharing across the different\nsections being staggered can be more practical in settings with limited memory.\nWe explore the efficacy of using a bounded window attention to pass information\nfrom one section to another which helps drive further latency gains for some\napplications. We also explore the scalability of the staggering idea over more\nthan 2 sections of the Transformer. Finally, we show how one can approximate a\nrecurrent model during inference using weight-sharing. This variant can lead to\nsubstantial gains in quality for short generations while being neutral in its\nlatency impact."
                },
                "authors": [
                    {
                        "name": "Dylan Cutler"
                    },
                    {
                        "name": "Arun Kandoor"
                    },
                    {
                        "name": "Nishanth Dikkala"
                    },
                    {
                        "name": "Nikunj Saunshi"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Rina Panigrahy"
                    }
                ],
                "author_detail": {
                    "name": "Rina Panigrahy"
                },
                "author": "Rina Panigrahy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20679v3",
                "updated": "2025-08-26T15:18:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    18,
                    44,
                    1,
                    238,
                    0
                ],
                "published": "2024-09-25T14:37:49Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    37,
                    49,
                    2,
                    269,
                    0
                ],
                "title": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU"
                },
                "summary": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Peng Zhu"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Sheng Xiang"
                    },
                    {
                        "name": "Qinyuan Liu"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Yuqi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Liang"
                },
                "author": "Yuqi Liang",
                "arxiv_doi": "10.1016/j.neucom.2025.130168",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neucom.2025.130168",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.20679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Neurocomputing 638 (2025) 130168",
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19115v1",
                "updated": "2025-08-26T15:17:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    17,
                    46,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:17:46Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    17,
                    46,
                    1,
                    238,
                    0
                ],
                "title": "SecureV2X: An Efficient and Privacy-Preserving System for\n  Vehicle-to-Everything (V2X) Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecureV2X: An Efficient and Privacy-Preserving System for\n  Vehicle-to-Everything (V2X) Applications"
                },
                "summary": "Autonomous driving and V2X technologies have developed rapidly in the past\ndecade, leading to improved safety and efficiency in modern transportation.\nThese systems interact with extensive networks of vehicles, roadside\ninfrastructure, and cloud resources to support their machine learning\ncapabilities. However, the widespread use of machine learning in V2X systems\nraises issues over the privacy of the data involved. This is particularly\nconcerning for smart-transit and driver safety applications which can\nimplicitly reveal user locations or explicitly disclose medical data such as\nEEG signals. To resolve these issues, we propose SecureV2X, a scalable,\nmulti-agent system for secure neural network inferences deployed between the\nserver and each vehicle. Under this setting, we study two multi-agent V2X\napplications: secure drowsiness detection, and secure red-light violation\ndetection. Our system achieves strong performance relative to baselines, and\nscales efficiently to support a large number of secure computation interactions\nsimultaneously. For instance, SecureV2X is $9.4 \\times$ faster, requires\n$143\\times$ fewer computational rounds, and involves $16.6\\times$ less\ncommunication on drowsiness detection compared to other secure systems.\nMoreover, it achieves a runtime nearly $100\\times$ faster than state-of-the-art\nbenchmarks in object detection tasks for red light violation detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving and V2X technologies have developed rapidly in the past\ndecade, leading to improved safety and efficiency in modern transportation.\nThese systems interact with extensive networks of vehicles, roadside\ninfrastructure, and cloud resources to support their machine learning\ncapabilities. However, the widespread use of machine learning in V2X systems\nraises issues over the privacy of the data involved. This is particularly\nconcerning for smart-transit and driver safety applications which can\nimplicitly reveal user locations or explicitly disclose medical data such as\nEEG signals. To resolve these issues, we propose SecureV2X, a scalable,\nmulti-agent system for secure neural network inferences deployed between the\nserver and each vehicle. Under this setting, we study two multi-agent V2X\napplications: secure drowsiness detection, and secure red-light violation\ndetection. Our system achieves strong performance relative to baselines, and\nscales efficiently to support a large number of secure computation interactions\nsimultaneously. For instance, SecureV2X is $9.4 \\times$ faster, requires\n$143\\times$ fewer computational rounds, and involves $16.6\\times$ less\ncommunication on drowsiness detection compared to other secure systems.\nMoreover, it achieves a runtime nearly $100\\times$ faster than state-of-the-art\nbenchmarks in object detection tasks for red light violation detection."
                },
                "authors": [
                    {
                        "name": "Joshua Lee"
                    },
                    {
                        "name": "Ali Arastehfard"
                    },
                    {
                        "name": "Weiran Liu"
                    },
                    {
                        "name": "Xuegang Ban"
                    },
                    {
                        "name": "Yuan Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Hong"
                },
                "author": "Yuan Hong",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3; I.2.6; I.5.1; F.1.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19114v1",
                "updated": "2025-08-26T15:17:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    17,
                    8,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:17:08Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    17,
                    8,
                    1,
                    238,
                    0
                ],
                "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and\n  Delivery using Voronoi-Based Relay Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and\n  Delivery using Voronoi-Based Relay Planning"
                },
                "summary": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration."
                },
                "authors": [
                    {
                        "name": "Alkesh K. Srivastava"
                    },
                    {
                        "name": "Jared Michael Levin"
                    },
                    {
                        "name": "Alexander Derrico"
                    },
                    {
                        "name": "Philip Dames"
                    }
                ],
                "author_detail": {
                    "name": "Philip Dames"
                },
                "author": "Philip Dames",
                "arxiv_comment": "Submission under review at the 2026 IEEE/SICE International Symposium\n  on System Integration (SII 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19113v1",
                "updated": "2025-08-26T15:15:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    15,
                    17,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:15:17Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    15,
                    17,
                    1,
                    238,
                    0
                ],
                "title": "Hybrid Deep Searcher: Integrating Parallel and Sequential Search\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Searcher: Integrating Parallel and Sequential Search\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) have demonstrated strong performance in\ncomplex, multi-step reasoning tasks. Existing methods enhance LRMs by\nsequentially integrating external knowledge retrieval; models iteratively\ngenerate queries, retrieve external information, and progressively reason over\nthis information. However, purely sequential querying increases inference\nlatency and context length, diminishing coherence and potentially reducing\naccuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search\nQA), a synthetic dataset automatically generated from Natural Questions,\nexplicitly designed to train LRMs to distinguish parallelizable from sequential\nqueries. HDS-QA comprises hybrid-hop questions that combine parallelizable\nindependent subqueries (executable simultaneously) and sequentially dependent\nsubqueries (requiring step-by-step resolution), along with synthetic\nreasoning-querying-retrieval paths involving parallel queries. We fine-tune an\nLRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms\nstate-of-the-art baselines across multiple benchmarks, notably achieving +15.9\nand +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both\nrequiring comprehensive and exhaustive search. Experimental results highlight\ntwo key advantages: HybridDeepSearcher reaches comparable accuracy with fewer\nsearch turns, significantly reducing inference latency, and it effectively\nscales as more turns are permitted. These results demonstrate the efficiency,\nscalability, and effectiveness of explicitly training LRMs to leverage hybrid\nparallel and sequential querying.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) have demonstrated strong performance in\ncomplex, multi-step reasoning tasks. Existing methods enhance LRMs by\nsequentially integrating external knowledge retrieval; models iteratively\ngenerate queries, retrieve external information, and progressively reason over\nthis information. However, purely sequential querying increases inference\nlatency and context length, diminishing coherence and potentially reducing\naccuracy. To address these limitations, we introduce HDS-QA (Hybrid Deep Search\nQA), a synthetic dataset automatically generated from Natural Questions,\nexplicitly designed to train LRMs to distinguish parallelizable from sequential\nqueries. HDS-QA comprises hybrid-hop questions that combine parallelizable\nindependent subqueries (executable simultaneously) and sequentially dependent\nsubqueries (requiring step-by-step resolution), along with synthetic\nreasoning-querying-retrieval paths involving parallel queries. We fine-tune an\nLRM using HDS-QA, naming the model HybridDeepSearcher, which outperforms\nstate-of-the-art baselines across multiple benchmarks, notably achieving +15.9\nand +11.5 F1 on FanOutQA and a subset of BrowseComp, respectively, both\nrequiring comprehensive and exhaustive search. Experimental results highlight\ntwo key advantages: HybridDeepSearcher reaches comparable accuracy with fewer\nsearch turns, significantly reducing inference latency, and it effectively\nscales as more turns are permitted. These results demonstrate the efficiency,\nscalability, and effectiveness of explicitly training LRMs to leverage hybrid\nparallel and sequential querying."
                },
                "authors": [
                    {
                        "name": "Dayoon Ko"
                    },
                    {
                        "name": "Jihyuk Kim"
                    },
                    {
                        "name": "Haeju Park"
                    },
                    {
                        "name": "Sohyeon Kim"
                    },
                    {
                        "name": "Dahyun Lee"
                    },
                    {
                        "name": "Yongrae Jo"
                    },
                    {
                        "name": "Gunhee Kim"
                    },
                    {
                        "name": "Moontae Lee"
                    },
                    {
                        "name": "Kyungjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyungjae Lee"
                },
                "author": "Kyungjae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19111v1",
                "updated": "2025-08-26T15:14:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    14,
                    19,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:14:19Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    14,
                    19,
                    1,
                    238,
                    0
                ],
                "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary\n  Perception in LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary\n  Perception in LVLMs"
                },
                "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs."
                },
                "authors": [
                    {
                        "name": "Zhikai Ding"
                    },
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Keping Bi"
                },
                "author": "Keping Bi",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19195v2",
                "updated": "2025-08-26T15:09:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    9,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2024-10-24T22:59:23Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    59,
                    23,
                    3,
                    298,
                    0
                ],
                "title": "Label Set Optimization via Activation Distribution Kurtosis for\n  Zero-shot Classification with Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label Set Optimization via Activation Distribution Kurtosis for\n  Zero-shot Classification with Generative Models"
                },
                "summary": "In-context learning (ICL) performance is highly sensitive to prompt design,\nyet the impact of class label options (e.g. lexicon or order) in zero-shot\nclassification remains underexplored. This study proposes LOADS (Label set\nOptimization via Activation Distribution kurtosiS), a post-hoc method for\nselecting optimal label sets in zero-shot ICL with large language models\n(LLMs). LOADS is built upon the observations in our empirical analysis, the\nfirst to systematically examine how label option design (i.e., lexical choice,\norder, and elaboration) impacts classification performance. This analysis shows\nthat the lexical choice of the labels in the prompt (such as agree vs. support\nin stance classification) plays an important role in both model performance and\nmodel's sensitivity to the label order. A further investigation demonstrates\nthat optimal label words tend to activate fewer outlier neurons in LLMs'\nfeed-forward networks. LOADS then leverages kurtosis to measure the neuron\nactivation distribution for label selection, requiring only a single forward\npass without gradient propagation or labelled data. The LOADS-selected label\nwords consistently demonstrate effectiveness for zero-shot ICL across\nclassification tasks, datasets, models and languages, achieving maximum\nperformance gain from 0.54 to 0.76 compared to the conventional approach of\nusing original dataset label words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) performance is highly sensitive to prompt design,\nyet the impact of class label options (e.g. lexicon or order) in zero-shot\nclassification remains underexplored. This study proposes LOADS (Label set\nOptimization via Activation Distribution kurtosiS), a post-hoc method for\nselecting optimal label sets in zero-shot ICL with large language models\n(LLMs). LOADS is built upon the observations in our empirical analysis, the\nfirst to systematically examine how label option design (i.e., lexical choice,\norder, and elaboration) impacts classification performance. This analysis shows\nthat the lexical choice of the labels in the prompt (such as agree vs. support\nin stance classification) plays an important role in both model performance and\nmodel's sensitivity to the label order. A further investigation demonstrates\nthat optimal label words tend to activate fewer outlier neurons in LLMs'\nfeed-forward networks. LOADS then leverages kurtosis to measure the neuron\nactivation distribution for label selection, requiring only a single forward\npass without gradient propagation or labelled data. The LOADS-selected label\nwords consistently demonstrate effectiveness for zero-shot ICL across\nclassification tasks, datasets, models and languages, achieving maximum\nperformance gain from 0.54 to 0.76 compared to the conventional approach of\nusing original dataset label words."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Carolina Scarton"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Scarton"
                },
                "author": "Carolina Scarton",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14705v2",
                "updated": "2025-08-26T15:07:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    7,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-20T13:29:24Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    13,
                    29,
                    24,
                    2,
                    232,
                    0
                ],
                "title": "Learning in Repeated Multi-Objective Stackelberg Games with Payoff\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Repeated Multi-Objective Stackelberg Games with Payoff\n  Manipulation"
                },
                "summary": "We study payoff manipulation in repeated multi-objective Stackelberg games,\nwhere a leader may strategically influence a follower's deterministic best\nresponse, e.g., by offering a share of their own payoff. We assume that the\nfollower's utility function, representing preferences over multiple objectives,\nis unknown but linear, and its weight parameter must be inferred through\ninteraction. This introduces a sequential decision-making challenge for the\nleader, who must balance preference elicitation with immediate utility\nmaximisation. We formalise this problem and propose manipulation policies based\non expected utility (EU) and long-term expected utility (longEU), which guide\nthe leader in selecting actions and offering incentives that trade off\nshort-term gains with long-term impact. We prove that under infinite repeated\ninteractions, longEU converges to the optimal manipulation. Empirical results\nacross benchmark environments demonstrate that our approach improves cumulative\nleader utility while promoting mutually beneficial outcomes, all without\nrequiring explicit negotiation or prior knowledge of the follower's utility\nfunction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study payoff manipulation in repeated multi-objective Stackelberg games,\nwhere a leader may strategically influence a follower's deterministic best\nresponse, e.g., by offering a share of their own payoff. We assume that the\nfollower's utility function, representing preferences over multiple objectives,\nis unknown but linear, and its weight parameter must be inferred through\ninteraction. This introduces a sequential decision-making challenge for the\nleader, who must balance preference elicitation with immediate utility\nmaximisation. We formalise this problem and propose manipulation policies based\non expected utility (EU) and long-term expected utility (longEU), which guide\nthe leader in selecting actions and offering incentives that trade off\nshort-term gains with long-term impact. We prove that under infinite repeated\ninteractions, longEU converges to the optimal manipulation. Empirical results\nacross benchmark environments demonstrate that our approach improves cumulative\nleader utility while promoting mutually beneficial outcomes, all without\nrequiring explicit negotiation or prior knowledge of the follower's utility\nfunction."
                },
                "authors": [
                    {
                        "name": "Phurinut Srisawad"
                    },
                    {
                        "name": "Juergen Branke"
                    },
                    {
                        "name": "Long Tran-Thanh"
                    }
                ],
                "author_detail": {
                    "name": "Long Tran-Thanh"
                },
                "author": "Long Tran-Thanh",
                "arxiv_comment": "Extended version of the paper accepted at the 28th European\n  Conference on Artificial Intelligence (ECAI 2025); Paper ID: M2635, Added\n  more experiments in the Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19098v1",
                "updated": "2025-08-26T14:59:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    30,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:59:30Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    30,
                    1,
                    238,
                    0
                ],
                "title": "CLEAR: Continuous Latent Autoregressive Modeling for High-quality and\n  Low-latency Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLEAR: Continuous Latent Autoregressive Modeling for High-quality and\n  Low-latency Speech Synthesis"
                },
                "summary": "Autoregressive (AR) language models have emerged as powerful solutions for\nzero-shot text-to-speech (TTS) synthesis, capable of generating natural speech\nfrom a few seconds of audio prompts. However, conventional AR-based TTS systems\nrelying on discrete audio tokens face the challenge of lossy compression during\ntokenization, requiring longer discrete token sequences to capture the same\ninformation as continuous ones, which adds inference latency and complicates AR\nmodeling. To address this challenge, this paper proposes the Continuous Latent\nAutoregressive model (CLEAR), a unified zero-shot TTS framework that directly\nmodels continuous audio representations. More specifically, CLEAR introduces an\nenhanced variational autoencoder with shortcut connections, which achieves a\nhigh compression ratio to map waveforms into compact continuous latents. A\nlightweight MLP-based rectified flow head that operates independently for each\nhidden state is presented to model the continuous latent probability\ndistribution, and trained jointly with the AR model within a single-stage\nframework. Experiments show that the proposed zero-shot CLEAR TTS can\nsynthesize high-quality speech with low latency. Compared to state-of-the-art\n(SOTA) TTS models, CLEAR delivers competitive performance in robustness,\nspeaker similarity and naturalness, while offering a lower real-time factor\n(RTF). In particular, CLEAR achieves SOTA results on the LibriSpeech test-clean\ndataset, with a word error rate of 1.88\\% and an RTF of 0.29. Moreover, CLEAR\nfacilitates streaming speech synthesis with a first-frame delay of 96ms, while\nmaintaining high-quality speech synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) language models have emerged as powerful solutions for\nzero-shot text-to-speech (TTS) synthesis, capable of generating natural speech\nfrom a few seconds of audio prompts. However, conventional AR-based TTS systems\nrelying on discrete audio tokens face the challenge of lossy compression during\ntokenization, requiring longer discrete token sequences to capture the same\ninformation as continuous ones, which adds inference latency and complicates AR\nmodeling. To address this challenge, this paper proposes the Continuous Latent\nAutoregressive model (CLEAR), a unified zero-shot TTS framework that directly\nmodels continuous audio representations. More specifically, CLEAR introduces an\nenhanced variational autoencoder with shortcut connections, which achieves a\nhigh compression ratio to map waveforms into compact continuous latents. A\nlightweight MLP-based rectified flow head that operates independently for each\nhidden state is presented to model the continuous latent probability\ndistribution, and trained jointly with the AR model within a single-stage\nframework. Experiments show that the proposed zero-shot CLEAR TTS can\nsynthesize high-quality speech with low latency. Compared to state-of-the-art\n(SOTA) TTS models, CLEAR delivers competitive performance in robustness,\nspeaker similarity and naturalness, while offering a lower real-time factor\n(RTF). In particular, CLEAR achieves SOTA results on the LibriSpeech test-clean\ndataset, with a word error rate of 1.88\\% and an RTF of 0.29. Moreover, CLEAR\nfacilitates streaming speech synthesis with a first-frame delay of 96ms, while\nmaintaining high-quality speech synthesis."
                },
                "authors": [
                    {
                        "name": "Chun Yat Wu"
                    },
                    {
                        "name": "Jiajun Deng"
                    },
                    {
                        "name": "Guinan Li"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Simon Lui"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lui"
                },
                "author": "Simon Lui",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19097v1",
                "updated": "2025-08-26T14:59:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    19,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:59:19Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    19,
                    1,
                    238,
                    0
                ],
                "title": "Reasoning LLMs in the Medical Domain: A Literature Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning LLMs in the Medical Domain: A Literature Survey"
                },
                "summary": "The emergence of advanced reasoning capabilities in Large Language Models\n(LLMs) marks a transformative development in healthcare applications. Beyond\nmerely expanding functional capabilities, these reasoning mechanisms enhance\ndecision transparency and explainability-critical requirements in medical\ncontexts. This survey examines the transformation of medical LLMs from basic\ninformation retrieval tools to sophisticated clinical reasoning systems capable\nof supporting complex healthcare decisions. We provide a thorough analysis of\nthe enabling technological foundations, with a particular focus on specialized\nprompting techniques like Chain-of-Thought and recent breakthroughs in\nReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates\npurpose-built medical frameworks while also examining emerging paradigms such\nas multi-agent collaborative systems and innovative prompting architectures.\nThe survey critically assesses current evaluation methodologies for medical\nvalidation and addresses persistent challenges in field interpretation\nlimitations, bias mitigation strategies, patient safety frameworks, and\nintegration of multimodal clinical data. Through this survey, we seek to\nestablish a roadmap for developing reliable LLMs that can serve as effective\npartners in clinical practice and medical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of advanced reasoning capabilities in Large Language Models\n(LLMs) marks a transformative development in healthcare applications. Beyond\nmerely expanding functional capabilities, these reasoning mechanisms enhance\ndecision transparency and explainability-critical requirements in medical\ncontexts. This survey examines the transformation of medical LLMs from basic\ninformation retrieval tools to sophisticated clinical reasoning systems capable\nof supporting complex healthcare decisions. We provide a thorough analysis of\nthe enabling technological foundations, with a particular focus on specialized\nprompting techniques like Chain-of-Thought and recent breakthroughs in\nReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates\npurpose-built medical frameworks while also examining emerging paradigms such\nas multi-agent collaborative systems and innovative prompting architectures.\nThe survey critically assesses current evaluation methodologies for medical\nvalidation and addresses persistent challenges in field interpretation\nlimitations, bias mitigation strategies, patient safety frameworks, and\nintegration of multimodal clinical data. Through this survey, we seek to\nestablish a roadmap for developing reliable LLMs that can serve as effective\npartners in clinical practice and medical research."
                },
                "authors": [
                    {
                        "name": "Armin Berger"
                    },
                    {
                        "name": "Sarthak Khanna"
                    },
                    {
                        "name": "David Berghaus"
                    },
                    {
                        "name": "Rafet Sifa"
                    }
                ],
                "author_detail": {
                    "name": "Rafet Sifa"
                },
                "author": "Rafet Sifa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19096v1",
                "updated": "2025-08-26T14:59:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    4,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:59:04Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    4,
                    1,
                    238,
                    0
                ],
                "title": "Trustworthy Agents for Electronic Health Records through Confidence\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Agents for Electronic Health Records through Confidence\n  Estimation"
                },
                "summary": "Large language models (LLMs) show promise for extracting information from\nElectronic Health Records (EHR) and supporting clinical decisions. However,\ndeployment in clinical settings faces challenges due to hallucination risks. We\npropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric\nquantifying the accuracy-reliability trade-off at varying confidence\nthresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating\nstepwise confidence estimation for clinical question answering. Experiments on\nMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under\nstrict reliability constraints, achieving improvements of 44.23%p and 25.34%p\nat HCAcc@70% while baseline methods fail at these thresholds. These results\nhighlight limitations of traditional accuracy metrics in evaluating healthcare\nAI agents. Our work contributes to developing trustworthy clinical agents that\ndeliver accurate information or transparently express uncertainty when\nconfidence is low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise for extracting information from\nElectronic Health Records (EHR) and supporting clinical decisions. However,\ndeployment in clinical settings faces challenges due to hallucination risks. We\npropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric\nquantifying the accuracy-reliability trade-off at varying confidence\nthresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating\nstepwise confidence estimation for clinical question answering. Experiments on\nMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under\nstrict reliability constraints, achieving improvements of 44.23%p and 25.34%p\nat HCAcc@70% while baseline methods fail at these thresholds. These results\nhighlight limitations of traditional accuracy metrics in evaluating healthcare\nAI agents. Our work contributes to developing trustworthy clinical agents that\ndeliver accurate information or transparently express uncertainty when\nconfidence is low."
                },
                "authors": [
                    {
                        "name": "Yongwoo Song"
                    },
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Mujeen Sung"
                    }
                ],
                "author_detail": {
                    "name": "Mujeen Sung"
                },
                "author": "Mujeen Sung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13241v2",
                "updated": "2025-08-26T14:51:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    49,
                    1,
                    238,
                    0
                ],
                "published": "2025-02-18T19:17:00Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    17,
                    0,
                    1,
                    49,
                    0
                ],
                "title": "Accretion onto supermassive and intermediate mass black holes in\n  cosmological simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accretion onto supermassive and intermediate mass black holes in\n  cosmological simulations"
                },
                "summary": "Accretion is the dominant contribution to the cosmic massive black hole\ndensity in the Universe today. Yet, modelling it in cosmological simulations is\nchallenging due to the dynamic range involved, as well as the theoretical\nuncertainties of the underlying mechanisms driving accretion from galactic to\nblack hole horizon scales. We present a simple, flexible parametrization for\ngas inflows onto massive black holes in order to manage this uncertainty in\nlarge-volume cosmological simulations. This is done as part of the \"Learning\nthe Universe'' collaboration, which aims to jointly infer the initial\nconditions and physical processes governing the evolution of the Universe using\na Bayesian forward-modelling approach. To allow such a forward-modelling, we\nupdate the prescription for accretion with a two-parameter free-fall based\ninflow estimate that allows for a radius-dependent inflow rate and add a simple\nmodel for unresolved accretion disks. We use uniform resolution cosmological\nhydrodynamical simulations and the IllustrisTNG framework to study the massive\nblack hole population and its dependence on the introduced model parameters.\nOnce the parameters of the accretion formula are chosen to result in a roughly\nsimilar redshift zero black hole mass density, the differences caused by the\ndetails in the accretion formula are moderate in the supermassive black hole\nregime, indicating that it is difficult to distinguish between accretion\nmechanisms based on luminous active galactic nuclei powered by supermassive\nblack holes. Applying the same models to intermediate mass black holes at high\nredshift, however, reveals significantly different accretion rates in high\nredshift, moderate luminosity active galactic nuclei and different frequencies\nand mass distributions of intermediate mass black hole mergers for the same\nblack hole formation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accretion is the dominant contribution to the cosmic massive black hole\ndensity in the Universe today. Yet, modelling it in cosmological simulations is\nchallenging due to the dynamic range involved, as well as the theoretical\nuncertainties of the underlying mechanisms driving accretion from galactic to\nblack hole horizon scales. We present a simple, flexible parametrization for\ngas inflows onto massive black holes in order to manage this uncertainty in\nlarge-volume cosmological simulations. This is done as part of the \"Learning\nthe Universe'' collaboration, which aims to jointly infer the initial\nconditions and physical processes governing the evolution of the Universe using\na Bayesian forward-modelling approach. To allow such a forward-modelling, we\nupdate the prescription for accretion with a two-parameter free-fall based\ninflow estimate that allows for a radius-dependent inflow rate and add a simple\nmodel for unresolved accretion disks. We use uniform resolution cosmological\nhydrodynamical simulations and the IllustrisTNG framework to study the massive\nblack hole population and its dependence on the introduced model parameters.\nOnce the parameters of the accretion formula are chosen to result in a roughly\nsimilar redshift zero black hole mass density, the differences caused by the\ndetails in the accretion formula are moderate in the supermassive black hole\nregime, indicating that it is difficult to distinguish between accretion\nmechanisms based on luminous active galactic nuclei powered by supermassive\nblack holes. Applying the same models to intermediate mass black holes at high\nredshift, however, reveals significantly different accretion rates in high\nredshift, moderate luminosity active galactic nuclei and different frequencies\nand mass distributions of intermediate mass black hole mergers for the same\nblack hole formation model."
                },
                "authors": [
                    {
                        "name": "Rainer Weinberger"
                    },
                    {
                        "name": "Aklant Bhowmick"
                    },
                    {
                        "name": "Laura Blecha"
                    },
                    {
                        "name": "Greg Bryan"
                    },
                    {
                        "name": "Johannes Buchner"
                    },
                    {
                        "name": "Lars Hernquist"
                    },
                    {
                        "name": "Julie Hlavacek-Larrondo"
                    },
                    {
                        "name": "Volker Springel"
                    }
                ],
                "author_detail": {
                    "name": "Volker Springel"
                },
                "author": "Volker Springel",
                "arxiv_doi": "10.1051/0004-6361/202554174",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554174",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 13 figures, comments welcome. This is a Learning the\n  Universe publication",
                "arxiv_journal_ref": "A&A 700, A52 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19089v1",
                "updated": "2025-08-26T14:51:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    10,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:51:10Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    10,
                    1,
                    238,
                    0
                ],
                "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource\n  Languages to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's All About In-Context Learning! Teaching Extremely Low-Resource\n  Languages to LLMs"
                },
                "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Carolina Scarton"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Scarton"
                },
                "author": "Carolina Scarton",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04831v2",
                "updated": "2025-08-26T14:49:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    49,
                    48,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-07T22:07:42Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    22,
                    7,
                    42,
                    2,
                    127,
                    0
                ],
                "title": "Steerable Scene Generation with Post Training and Inference-Time Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steerable Scene Generation with Post Training and Inference-Time Search"
                },
                "summary": "Training robots in simulation requires diverse 3D scenes that reflect the\nspecific challenges of downstream tasks. However, scenes that satisfy strict\ntask requirements, such as high-clutter environments with plausible spatial\narrangement, are rare and costly to curate manually. Instead, we generate\nlarge-scale scene data using procedural models that approximate realistic\nenvironments for robotic manipulation, and adapt it to task-specific goals. We\ndo this by training a unified diffusion-based generative model that predicts\nwhich objects to place from a fixed asset library, along with their SE(3)\nposes. This model serves as a flexible scene prior that can be adapted using\nreinforcement learning-based post training, conditional generation, or\ninference-time search, steering generation toward downstream objectives even\nwhen they differ from the original data distribution. Our method enables\ngoal-directed scene synthesis that respects physical feasibility and scales\nacross scene types. We introduce a novel MCTS-based inference-time search\nstrategy for diffusion models, enforce feasibility via projection and\nsimulation, and release a dataset of over 44 million SE(3) scenes spanning five\ndiverse environments. Website with videos, code, data, and model weights:\nhttps://steerable-scene-generation.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training robots in simulation requires diverse 3D scenes that reflect the\nspecific challenges of downstream tasks. However, scenes that satisfy strict\ntask requirements, such as high-clutter environments with plausible spatial\narrangement, are rare and costly to curate manually. Instead, we generate\nlarge-scale scene data using procedural models that approximate realistic\nenvironments for robotic manipulation, and adapt it to task-specific goals. We\ndo this by training a unified diffusion-based generative model that predicts\nwhich objects to place from a fixed asset library, along with their SE(3)\nposes. This model serves as a flexible scene prior that can be adapted using\nreinforcement learning-based post training, conditional generation, or\ninference-time search, steering generation toward downstream objectives even\nwhen they differ from the original data distribution. Our method enables\ngoal-directed scene synthesis that respects physical feasibility and scales\nacross scene types. We introduce a novel MCTS-based inference-time search\nstrategy for diffusion models, enforce feasibility via projection and\nsimulation, and release a dataset of over 44 million SE(3) scenes spanning five\ndiverse environments. Website with videos, code, data, and model weights:\nhttps://steerable-scene-generation.github.io/"
                },
                "authors": [
                    {
                        "name": "Nicholas Pfaff"
                    },
                    {
                        "name": "Hongkai Dai"
                    },
                    {
                        "name": "Sergey Zakharov"
                    },
                    {
                        "name": "Shun Iwase"
                    },
                    {
                        "name": "Russ Tedrake"
                    }
                ],
                "author_detail": {
                    "name": "Russ Tedrake"
                },
                "author": "Russ Tedrake",
                "arxiv_comment": "Project website: https://steerable-scene-generation.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19087v1",
                "updated": "2025-08-26T14:48:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    48,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:48:29Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    48,
                    29,
                    1,
                    238,
                    0
                ],
                "title": "APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nenormous computational demands severely limit deployment and real-time\nperformance. Quantization methods can help reduce computational costs, however,\nattaining the extreme efficiency associated with ultra-low-bit quantized LLMs\nat arbitrary precision presents challenges on GPUs. This is primarily due to\nthe limited support for GPU Tensor Cores, inefficient memory management, and\ninflexible kernel optimizations. To tackle these challenges, we propose a\ncomprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.\nFirstly, we introduce a novel data format, bipolar-INT, which allows for\nefficient and lossless conversion with signed INT, while also being more\nconducive to parallel computation. We also develop a matrix multiplication\n(MatMul) method allowing for arbitrary precision by dismantling and\nreassembling matrices at the bit level. This method provides flexible precision\nand optimizes the utilization of GPU Tensor Cores. In addition, we propose a\nmemory management system focused on data recovery, which strategically employs\nfast shared memory to substantially increase kernel execution speed and reduce\nmemory access latency. Finally, we develop a kernel mapping method that\ndynamically selects the optimal configurable hyperparameters of kernels for\nvarying matrix sizes, enabling optimal performance across different LLM\narchitectures and precision settings. In LLM inference, APT-LLM achieves up to\na 3.99$\\times$ speedup compared to FP16 baselines and a 2.16$\\times$ speedup\nover NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,\nAPT-LLM achieves up to 2.44$\\times$ speedup over FP16 and 1.65$\\times$ speedup\nover CUTLASS integer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nenormous computational demands severely limit deployment and real-time\nperformance. Quantization methods can help reduce computational costs, however,\nattaining the extreme efficiency associated with ultra-low-bit quantized LLMs\nat arbitrary precision presents challenges on GPUs. This is primarily due to\nthe limited support for GPU Tensor Cores, inefficient memory management, and\ninflexible kernel optimizations. To tackle these challenges, we propose a\ncomprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.\nFirstly, we introduce a novel data format, bipolar-INT, which allows for\nefficient and lossless conversion with signed INT, while also being more\nconducive to parallel computation. We also develop a matrix multiplication\n(MatMul) method allowing for arbitrary precision by dismantling and\nreassembling matrices at the bit level. This method provides flexible precision\nand optimizes the utilization of GPU Tensor Cores. In addition, we propose a\nmemory management system focused on data recovery, which strategically employs\nfast shared memory to substantially increase kernel execution speed and reduce\nmemory access latency. Finally, we develop a kernel mapping method that\ndynamically selects the optimal configurable hyperparameters of kernels for\nvarying matrix sizes, enabling optimal performance across different LLM\narchitectures and precision settings. In LLM inference, APT-LLM achieves up to\na 3.99$\\times$ speedup compared to FP16 baselines and a 2.16$\\times$ speedup\nover NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,\nAPT-LLM achieves up to 2.44$\\times$ speedup over FP16 and 1.65$\\times$ speedup\nover CUTLASS integer baselines."
                },
                "authors": [
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "arxiv_comment": "To appear in the IEEE Transactions on Computer-Aided Design of\n  Integrated Circuits and Systems (TCAD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19078v1",
                "updated": "2025-08-26T14:39:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    39,
                    0,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:39:00Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    39,
                    0,
                    1,
                    238,
                    0
                ],
                "title": "Federated Fine-Tuning of Sparsely-Activated Large Language Models on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Fine-Tuning of Sparsely-Activated Large Language Models on\n  Resource-Constrained Devices"
                },
                "summary": "Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models\n(LLMs) is challenging due to their massive computational requirements and the\nresource constraints of participants. Existing working attempts to fill this\ngap through model quantization, computation offloading, or expert pruning.\nHowever, they cannot achieve desired performance due to impractical system\nassumptions and a lack of consideration for MoE-specific characteristics. In\nthis paper, we propose FLUX, a system designed to enable federated fine-tuning\nof MoE-based LLMs across participants with constrained computing resources\n(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX\nintroduces three key innovations: (1) quantization-based local profiling to\nestimate expert activation with minimal overhead, (2) adaptive layer-aware\nexpert merging to reduce resource consumption while preserving accuracy, and\n(3) dynamic expert role assignment using an exploration-exploitation strategy\nto balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE\nand DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX\nsignificantly outperforms existing methods, achieving up to 4.75X speedup in\ntime-to-accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models\n(LLMs) is challenging due to their massive computational requirements and the\nresource constraints of participants. Existing working attempts to fill this\ngap through model quantization, computation offloading, or expert pruning.\nHowever, they cannot achieve desired performance due to impractical system\nassumptions and a lack of consideration for MoE-specific characteristics. In\nthis paper, we propose FLUX, a system designed to enable federated fine-tuning\nof MoE-based LLMs across participants with constrained computing resources\n(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX\nintroduces three key innovations: (1) quantization-based local profiling to\nestimate expert activation with minimal overhead, (2) adaptive layer-aware\nexpert merging to reduce resource consumption while preserving accuracy, and\n(3) dynamic expert role assignment using an exploration-exploitation strategy\nto balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE\nand DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX\nsignificantly outperforms existing methods, achieving up to 4.75X speedup in\ntime-to-accuracy."
                },
                "authors": [
                    {
                        "name": "Fahao Chen"
                    },
                    {
                        "name": "Jie Wan"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "arxiv_comment": "Accepted by EuroSys'26. The camera-ready version will be uploaded\n  later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19076v1",
                "updated": "2025-08-26T14:37:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    37,
                    48,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:37:48Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    37,
                    48,
                    1,
                    238,
                    0
                ],
                "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive\n  Global-Local Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive\n  Global-Local Guidance"
                },
                "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components."
                },
                "authors": [
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Yuan Chang"
                    },
                    {
                        "name": "Gaihong Yu"
                    },
                    {
                        "name": "Xiaoqiu Le"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqiu Le"
                },
                "author": "Xiaoqiu Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06029v2",
                "updated": "2025-08-26T14:34:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    34,
                    0,
                    1,
                    238,
                    0
                ],
                "published": "2025-03-08T03:02:21Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    3,
                    2,
                    21,
                    5,
                    67,
                    0
                ],
                "title": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?"
                },
                "summary": "Large Language Models (LLMs) have become integral to daily life, especially\nadvancing as intelligent assistants through on-device deployment on\nsmartphones. However, existing LLM evaluation benchmarks predominantly focus on\nobjective tasks like mathematics and coding in English, which do not\nnecessarily reflect the practical use cases of on-device LLMs in real-world\nmobile scenarios, especially for Chinese users. To address these gaps, we\nintroduce SmartBench, the first benchmark designed to evaluate the capabilities\nof on-device LLMs in Chinese mobile contexts. We analyze functionalities\nprovided by representative smartphone manufacturers and divide them into five\ncategories: text summarization, text Q&A, information extraction, content\ncreation, and notification management, further detailed into 20 specific tasks.\nFor each task, we construct high-quality datasets comprising 50 to 200\nquestion-answer pairs that reflect everyday mobile interactions, and we develop\nautomated evaluation criteria tailored for these tasks. We conduct\ncomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also\nassess their performance after quantized deployment on real smartphone NPUs.\nOur contributions provide a standardized framework for evaluating on-device\nLLMs in Chinese, promoting further development and optimization in this\ncritical area. Code and data will be available at\nhttps://github.com/vivo-ai-lab/SmartBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to daily life, especially\nadvancing as intelligent assistants through on-device deployment on\nsmartphones. However, existing LLM evaluation benchmarks predominantly focus on\nobjective tasks like mathematics and coding in English, which do not\nnecessarily reflect the practical use cases of on-device LLMs in real-world\nmobile scenarios, especially for Chinese users. To address these gaps, we\nintroduce SmartBench, the first benchmark designed to evaluate the capabilities\nof on-device LLMs in Chinese mobile contexts. We analyze functionalities\nprovided by representative smartphone manufacturers and divide them into five\ncategories: text summarization, text Q&A, information extraction, content\ncreation, and notification management, further detailed into 20 specific tasks.\nFor each task, we construct high-quality datasets comprising 50 to 200\nquestion-answer pairs that reflect everyday mobile interactions, and we develop\nautomated evaluation criteria tailored for these tasks. We conduct\ncomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also\nassess their performance after quantized deployment on real smartphone NPUs.\nOur contributions provide a standardized framework for evaluating on-device\nLLMs in Chinese, promoting further development and optimization in this\ncritical area. Code and data will be available at\nhttps://github.com/vivo-ai-lab/SmartBench."
                },
                "authors": [
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Haohao Gao"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Fangyuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fangyuan Li"
                },
                "author": "Fangyuan Li",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19074v1",
                "updated": "2025-08-26T14:32:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    32,
                    49,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:32:49Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    32,
                    49,
                    1,
                    238,
                    0
                ],
                "title": "An LLM-powered Natural-to-Robotic Language Translation Framework with\n  Correctness Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-powered Natural-to-Robotic Language Translation Framework with\n  Correctness Guarantees"
                },
                "summary": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs."
                },
                "authors": [
                    {
                        "name": "ZhenDong Chen"
                    },
                    {
                        "name": "ZhanShang Nie"
                    },
                    {
                        "name": "ShiXing Wan"
                    },
                    {
                        "name": "JunYi Li"
                    },
                    {
                        "name": "YongTian Cheng"
                    },
                    {
                        "name": "Shuai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhao"
                },
                "author": "Shuai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19069v1",
                "updated": "2025-08-26T14:26:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    26,
                    32,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:26:32Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    26,
                    32,
                    1,
                    238,
                    0
                ],
                "title": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An\n  Exploration of Scaling Laws by Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An\n  Exploration of Scaling Laws by Difficulty"
                },
                "summary": "Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems."
                },
                "authors": [
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuanze Hu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Ye Qiu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Wenjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wu"
                },
                "author": "Wenjun Wu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19062v1",
                "updated": "2025-08-26T14:21:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    21,
                    28,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:21:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    21,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "Is ozone a reliable proxy for molecular oxygen? III. The impact of\n  CH$_4$ on the O$_2$-O$_3$ relationship for Earth-like atmospheres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is ozone a reliable proxy for molecular oxygen? III. The impact of\n  CH$_4$ on the O$_2$-O$_3$ relationship for Earth-like atmospheres"
                },
                "summary": "In the search for life in the Universe, molecular oxygen (O$_2$) combined\nwith a reducing species, such as methane (CH$_4$), is considered a promising\ndisequilibrium biosignature. In cases where it would be difficult or impossible\nto detect O$_2$ (e.g., mid-IR or low O$_2$ levels), it has been suggested that\nozone (O$_3$), the photochemical product of O$_2$, could be used as a proxy for\ndetermining the abundance of O$_2$. As the O$_2$-O$_3$ relationship is\nnonlinear, the goal of this series of papers is to explore how it would change\nfor different host stars and atmospheric compositions and learning how to use\nO$_3$ to infer O$_2$. We used photochemistry and climate modeling to further\nexplore the O$_2$-O$_3$ relationship by modeling Earth-like planets with the\npresent atmospheric level (PAL) of O$_2$ between 0.01% to 150% along with high\nand low CH$_4$ abundances of 1000% and 10% PAL, respectively. Methane is of\ninterest not only because it is a biosignature, but also the source of hydrogen\natoms for hydrogen oxide (HO$_x$) which destroys O$_3$ through catalytic cycles\nand acts as a catalyst for the smog mechanism of O$_3$ formation in the lower\natmosphere. We find varying CH$_4$ causes changes to the O$_2$-O$_3$\nrelationship in ways that are highly dependent on both host star and O$_2$\nabundance. A striking result for high CH$_4$ models in high O$_2$ atmospheres\naround hotter hosts is that enough CH$_4$ is efficiently converted into H$_2$O\nto significantly impact stratospheric temperatures, and therefore the\nformation/destruction rates of O$_3$. Changes in HO$_x$ also influenced the\nHO$_x$ catalytic cycle and smog O$_3$, causing variations in harmful UV\nreaching the surface as well as changes in the 9.6~$\\mu$m O$_3$ feature in\nemission spectra. This demonstrates the need to explore the O$_2$-O$_3$\nrelationship in order to use O$_3$ as a reliable proxy for O$_2$ in future\nobservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the search for life in the Universe, molecular oxygen (O$_2$) combined\nwith a reducing species, such as methane (CH$_4$), is considered a promising\ndisequilibrium biosignature. In cases where it would be difficult or impossible\nto detect O$_2$ (e.g., mid-IR or low O$_2$ levels), it has been suggested that\nozone (O$_3$), the photochemical product of O$_2$, could be used as a proxy for\ndetermining the abundance of O$_2$. As the O$_2$-O$_3$ relationship is\nnonlinear, the goal of this series of papers is to explore how it would change\nfor different host stars and atmospheric compositions and learning how to use\nO$_3$ to infer O$_2$. We used photochemistry and climate modeling to further\nexplore the O$_2$-O$_3$ relationship by modeling Earth-like planets with the\npresent atmospheric level (PAL) of O$_2$ between 0.01% to 150% along with high\nand low CH$_4$ abundances of 1000% and 10% PAL, respectively. Methane is of\ninterest not only because it is a biosignature, but also the source of hydrogen\natoms for hydrogen oxide (HO$_x$) which destroys O$_3$ through catalytic cycles\nand acts as a catalyst for the smog mechanism of O$_3$ formation in the lower\natmosphere. We find varying CH$_4$ causes changes to the O$_2$-O$_3$\nrelationship in ways that are highly dependent on both host star and O$_2$\nabundance. A striking result for high CH$_4$ models in high O$_2$ atmospheres\naround hotter hosts is that enough CH$_4$ is efficiently converted into H$_2$O\nto significantly impact stratospheric temperatures, and therefore the\nformation/destruction rates of O$_3$. Changes in HO$_x$ also influenced the\nHO$_x$ catalytic cycle and smog O$_3$, causing variations in harmful UV\nreaching the surface as well as changes in the 9.6~$\\mu$m O$_3$ feature in\nemission spectra. This demonstrates the need to explore the O$_2$-O$_3$\nrelationship in order to use O$_3$ as a reliable proxy for O$_2$ in future\nobservations."
                },
                "authors": [
                    {
                        "name": "Thea Kozakis"
                    },
                    {
                        "name": "João M. Mendonça"
                    },
                    {
                        "name": "Lars A. Buchhave"
                    },
                    {
                        "name": "Luisa M. Lara"
                    }
                ],
                "author_detail": {
                    "name": "Luisa M. Lara"
                },
                "author": "Luisa M. Lara",
                "arxiv_doi": "10.1051/0004-6361/202556015",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202556015",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.19062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In production with A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19060v1",
                "updated": "2025-08-26T14:20:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    20,
                    21,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:20:21Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    20,
                    21,
                    1,
                    238,
                    0
                ],
                "title": "No Label Left Behind: A Unified Surface Defect Detection Model for all\n  Supervision Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Label Left Behind: A Unified Surface Defect Detection Model for all\n  Supervision Regimes"
                },
                "summary": "Surface defect detection is a critical task across numerous industries, aimed\nat efficiently identifying and localising imperfections or irregularities on\nmanufactured components. While numerous methods have been proposed, many fail\nto meet industrial demands for high performance, efficiency, and adaptability.\nExisting approaches are often constrained to specific supervision scenarios and\nstruggle to adapt to the diverse data annotations encountered in real-world\nmanufacturing processes, such as unsupervised, weakly supervised, mixed\nsupervision, and fully supervised settings. To address these challenges, we\npropose SuperSimpleNet, a highly efficient and adaptable discriminative model\nbuilt on the foundation of SimpleNet. SuperSimpleNet incorporates a novel\nsynthetic anomaly generation process, an enhanced classification head, and an\nimproved learning procedure, enabling efficient training in all four\nsupervision scenarios, making it the first model capable of fully leveraging\nall available data annotations. SuperSimpleNet sets a new standard for\nperformance across all scenarios, as demonstrated by its results on four\nchallenging benchmark datasets. Beyond accuracy, it is very fast, achieving an\ninference time below 10 ms. With its ability to unify diverse supervision\nparadigms while maintaining outstanding speed and reliability, SuperSimpleNet\nrepresents a promising step forward in addressing real-world manufacturing\nchallenges and bridging the gap between academic research and industrial\napplications. Code: https://github.com/blaz-r/SuperSimpleNet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface defect detection is a critical task across numerous industries, aimed\nat efficiently identifying and localising imperfections or irregularities on\nmanufactured components. While numerous methods have been proposed, many fail\nto meet industrial demands for high performance, efficiency, and adaptability.\nExisting approaches are often constrained to specific supervision scenarios and\nstruggle to adapt to the diverse data annotations encountered in real-world\nmanufacturing processes, such as unsupervised, weakly supervised, mixed\nsupervision, and fully supervised settings. To address these challenges, we\npropose SuperSimpleNet, a highly efficient and adaptable discriminative model\nbuilt on the foundation of SimpleNet. SuperSimpleNet incorporates a novel\nsynthetic anomaly generation process, an enhanced classification head, and an\nimproved learning procedure, enabling efficient training in all four\nsupervision scenarios, making it the first model capable of fully leveraging\nall available data annotations. SuperSimpleNet sets a new standard for\nperformance across all scenarios, as demonstrated by its results on four\nchallenging benchmark datasets. Beyond accuracy, it is very fast, achieving an\ninference time below 10 ms. With its ability to unify diverse supervision\nparadigms while maintaining outstanding speed and reliability, SuperSimpleNet\nrepresents a promising step forward in addressing real-world manufacturing\nchallenges and bridging the gap between academic research and industrial\napplications. Code: https://github.com/blaz-r/SuperSimpleNet"
                },
                "authors": [
                    {
                        "name": "Blaž Rolih"
                    },
                    {
                        "name": "Matic Fučka"
                    },
                    {
                        "name": "Danijel Skočaj"
                    }
                ],
                "author_detail": {
                    "name": "Danijel Skočaj"
                },
                "author": "Danijel Skočaj",
                "arxiv_comment": "Accepted by The Journal of Intelligent Manufacturing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20430v2",
                "updated": "2025-08-26T14:13:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    13,
                    25,
                    1,
                    238,
                    0
                ],
                "published": "2025-06-25T13:42:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    42,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning"
                },
                "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor."
                },
                "authors": [
                    {
                        "name": "Weike Zhao"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Yanjie Fan"
                    },
                    {
                        "name": "Xiaoman Zhang"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Yuze Sun"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17178v2",
                "updated": "2025-08-26T14:11:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    11,
                    22,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-23T03:52:24Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    3,
                    52,
                    24,
                    2,
                    204,
                    0
                ],
                "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs"
                },
                "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Enpei Niu"
                    },
                    {
                        "name": "Yin Hua"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05520v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05520v3",
                "updated": "2025-08-26T14:02:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    2,
                    57,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-07T22:31:56Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    22,
                    31,
                    56,
                    0,
                    188,
                    0
                ],
                "title": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for\n  Multimodal Medical VQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Clinical Collaboration: Multi-Agent Reasoning Systems for\n  Multimodal Medical VQA"
                },
                "summary": "Dermatological care via telemedicine often lacks the rich context of\nin-person visits. Clinicians must make diagnoses based on a handful of images\nand brief descriptions, without the benefit of physical exams, second opinions,\nor reference materials. While many medical AI systems attempt to bridge these\ngaps with domain-specific fine-tuning, this work hypothesized that mimicking\nclinical reasoning processes could offer a more effective path forward. This\nstudy tested seven vision-language models on medical visual question answering\nacross six configurations: baseline models, fine-tuned variants, and both\naugmented with either reasoning layers that combine multiple model\nperspectives, analogous to peer consultation, or retrieval-augmented generation\nthat incorporates medical literature at inference time, serving a role similar\nto reference-checking. While fine-tuning degraded performance in four of seven\nmodels with an average 30% decrease, baseline models collapsed on test data.\nClinical-inspired architectures, meanwhile, achieved up to 70% accuracy,\nmaintaining performance on unseen data while generating explainable,\nliterature-grounded outputs critical for clinical adoption. These findings\ndemonstrate that medical AI succeeds by reconstructing the collaborative and\nevidence-based practices fundamental to clinical diagnosis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dermatological care via telemedicine often lacks the rich context of\nin-person visits. Clinicians must make diagnoses based on a handful of images\nand brief descriptions, without the benefit of physical exams, second opinions,\nor reference materials. While many medical AI systems attempt to bridge these\ngaps with domain-specific fine-tuning, this work hypothesized that mimicking\nclinical reasoning processes could offer a more effective path forward. This\nstudy tested seven vision-language models on medical visual question answering\nacross six configurations: baseline models, fine-tuned variants, and both\naugmented with either reasoning layers that combine multiple model\nperspectives, analogous to peer consultation, or retrieval-augmented generation\nthat incorporates medical literature at inference time, serving a role similar\nto reference-checking. While fine-tuning degraded performance in four of seven\nmodels with an average 30% decrease, baseline models collapsed on test data.\nClinical-inspired architectures, meanwhile, achieved up to 70% accuracy,\nmaintaining performance on unseen data while generating explainable,\nliterature-grounded outputs critical for clinical adoption. These findings\ndemonstrate that medical AI succeeds by reconstructing the collaborative and\nevidence-based practices fundamental to clinical diagnosis."
                },
                "authors": [
                    {
                        "name": "Karishma Thakrar"
                    },
                    {
                        "name": "Shreyas Basavatia"
                    },
                    {
                        "name": "Akshay Daftardar"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Daftardar"
                },
                "author": "Akshay Daftardar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05520v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05520v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.10032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.10032v2",
                "updated": "2025-08-26T14:02:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    2,
                    48,
                    1,
                    238,
                    0
                ],
                "published": "2023-05-17T08:18:56Z",
                "published_parsed": [
                    2023,
                    5,
                    17,
                    8,
                    18,
                    56,
                    2,
                    137,
                    0
                ],
                "title": "A Survey on Causal Discovery: Theory and Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Causal Discovery: Theory and Practice"
                },
                "summary": "Understanding the laws that govern a phenomenon is the core of scientific\nprogress. This is especially true when the goal is to model the interplay\nbetween different aspects in a causal fashion. Indeed, causal inference itself\nis specifically designed to quantify the underlying relationships that connect\na cause to its effect. Causal discovery is a branch of the broader field of\ncausality in which causal graphs are recovered from data (whenever possible),\nenabling the identification and estimation of causal effects. In this paper, we\nexplore recent advancements in causal discovery in a unified manner, provide a\nconsistent overview of existing algorithms developed under different settings,\nreport useful tools and data, present real-world applications to understand why\nand how these methods can be fruitfully exploited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the laws that govern a phenomenon is the core of scientific\nprogress. This is especially true when the goal is to model the interplay\nbetween different aspects in a causal fashion. Indeed, causal inference itself\nis specifically designed to quantify the underlying relationships that connect\na cause to its effect. Causal discovery is a branch of the broader field of\ncausality in which causal graphs are recovered from data (whenever possible),\nenabling the identification and estimation of causal effects. In this paper, we\nexplore recent advancements in causal discovery in a unified manner, provide a\nconsistent overview of existing algorithms developed under different settings,\nreport useful tools and data, present real-world applications to understand why\nand how these methods can be fruitfully exploited."
                },
                "authors": [
                    {
                        "name": "Alessio Zanga"
                    },
                    {
                        "name": "Elif Ozkirimli"
                    },
                    {
                        "name": "Fabio Stella"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Stella"
                },
                "author": "Fabio Stella",
                "arxiv_doi": "10.1016/j.ijar.2022.09.004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.ijar.2022.09.004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.10032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.10032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19042v1",
                "updated": "2025-08-26T13:58:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    58,
                    31,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:58:31Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    58,
                    31,
                    1,
                    238,
                    0
                ],
                "title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents"
                },
                "summary": "We introduce the Concurrent Modular Agent (CMA), a framework that\norchestrates multiple Large-Language-Model (LLM)-based modules that operate\nfully asynchronously yet maintain a coherent and fault-tolerant behavioral\nloop. This framework addresses long-standing difficulties in agent\narchitectures by letting intention emerge from language-mediated interactions\namong autonomous processes. This approach enables flexible, adaptive, and\ncontext-dependent behavior through the combination of concurrently executed\nmodules that offload reasoning to an LLM, inter-module communication, and a\nsingle shared global state.We consider this approach to be a practical\nrealization of Minsky's Society of Mind theory. We demonstrate the viability of\nour system through two practical use-case studies. The emergent properties\nobserved in our system suggest that complex cognitive phenomena like\nself-awareness may indeed arise from the organized interaction of simpler\nprocesses, supporting Minsky-Society of Mind concept and opening new avenues\nfor artificial intelligence research. The source code for our work is available\nat: https://github.com/AlternativeMachine/concurrent-modular-agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Concurrent Modular Agent (CMA), a framework that\norchestrates multiple Large-Language-Model (LLM)-based modules that operate\nfully asynchronously yet maintain a coherent and fault-tolerant behavioral\nloop. This framework addresses long-standing difficulties in agent\narchitectures by letting intention emerge from language-mediated interactions\namong autonomous processes. This approach enables flexible, adaptive, and\ncontext-dependent behavior through the combination of concurrently executed\nmodules that offload reasoning to an LLM, inter-module communication, and a\nsingle shared global state.We consider this approach to be a practical\nrealization of Minsky's Society of Mind theory. We demonstrate the viability of\nour system through two practical use-case studies. The emergent properties\nobserved in our system suggest that complex cognitive phenomena like\nself-awareness may indeed arise from the organized interaction of simpler\nprocesses, supporting Minsky-Society of Mind concept and opening new avenues\nfor artificial intelligence research. The source code for our work is available\nat: https://github.com/AlternativeMachine/concurrent-modular-agent."
                },
                "authors": [
                    {
                        "name": "Norihiro Maruyama"
                    },
                    {
                        "name": "Takahide Yoshida"
                    },
                    {
                        "name": "Hiroki Sato"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Johnsmith"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02223v3",
                "updated": "2025-08-26T13:55:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    55,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2024-08-05T03:54:52Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    3,
                    54,
                    52,
                    0,
                    218,
                    0
                ],
                "title": "Large Language Model Aided QoS Prediction for Service Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Aided QoS Prediction for Service Recommendation"
                },
                "summary": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently."
                },
                "authors": [
                    {
                        "name": "Huiying Liu"
                    },
                    {
                        "name": "Zekun Zhang"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Qilin Wu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_doi": "10.1109/SSE67621.2025.00023",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SSE67621.2025.00023",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.02223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19035v1",
                "updated": "2025-08-26T13:54:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    54,
                    17,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:54:17Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    54,
                    17,
                    1,
                    238,
                    0
                ],
                "title": "Investigating Advanced Reasoning of Large Language Models via Black-Box\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Advanced Reasoning of Large Language Models via Black-Box\n  Interaction"
                },
                "summary": "Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement."
                },
                "authors": [
                    {
                        "name": "Congchi Yin"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Yankai Shu"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "Yunhan Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19026v1",
                "updated": "2025-08-26T13:43:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    43,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    43,
                    45,
                    1,
                    238,
                    0
                ],
                "title": "MovieCORE: COgnitive REasoning in Movies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MovieCORE: COgnitive REasoning in Movies"
                },
                "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html."
                },
                "authors": [
                    {
                        "name": "Gueter Josmy Faure"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Jia-Fong Yeh"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Hung-Ting Su"
                    },
                    {
                        "name": "Yung-Hao Tang"
                    },
                    {
                        "name": "Shang-Hong Lai"
                    },
                    {
                        "name": "Winston H. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Winston H. Hsu"
                },
                "author": "Winston H. Hsu",
                "arxiv_comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19402v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19402v3",
                "updated": "2025-08-26T13:31:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    31,
                    48,
                    1,
                    238,
                    0
                ],
                "published": "2025-02-26T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    51,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "General Intelligence Requires Reward-based Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Intelligence Requires Reward-based Pretraining"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a reasoning\nprior for RL that can then be transferred to natural language tasks, and (3)\nlearning more generalizable reasoning functions using a small context window to\nreduce exploiting spurious correlations between tokens. Such a reasoning system\ncoupled with a trained retrieval system and a large external memory bank as a\nknowledge store can overcome several limitations of existing architectures at\nlearning to reason in novel scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a reasoning\nprior for RL that can then be transferred to natural language tasks, and (3)\nlearning more generalizable reasoning functions using a small context window to\nreduce exploiting spurious correlations between tokens. Such a reasoning system\ncoupled with a trained retrieval system and a large external memory bank as a\nknowledge store can overcome several limitations of existing architectures at\nlearning to reason in novel scenarios."
                },
                "authors": [
                    {
                        "name": "Seungwook Han"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "arxiv_comment": "https://improbableai.notion.site/General-Intelligence-Requires-Reward-Based-Pretraining-2023b66e4cf580d3ab44c7860b75d25f?pvs=73",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19402v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19402v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.15787v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.15787v4",
                "updated": "2025-08-26T13:29:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    29,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2023-06-27T20:34:21Z",
                "published_parsed": [
                    2023,
                    6,
                    27,
                    20,
                    34,
                    21,
                    1,
                    178,
                    0
                ],
                "title": "Network inference via approximate Bayesian computation. Illustration on\n  a stochastic multi-population neural mass model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network inference via approximate Bayesian computation. Illustration on\n  a stochastic multi-population neural mass model"
                },
                "summary": "In this article, we propose an adapted sequential Monte Carlo approximate\nBayesian computation (SMC-ABC) algorithm for network inference in coupled\nstochastic differential equations (SDEs) used for multivariate time series\nmodeling. Our approach is motivated by neuroscience, specifically the challenge\nof estimating brain connectivity before and during epileptic seizures. To this\nend, we make four key contributions. First, we introduce a 6N-dimensional SDE\nto model the activity of N coupled neuronal populations, extending the\n(single-population) stochastic Jansen and Rit neural mass model used to\ndescribe human electroencephalography (EEG) rhythms, particularly epileptic\nactivity. Second, we construct a reliable and efficient numerical splitting\nscheme for the model simulation. Third, we apply the proposed adapted SMC-ABC\nalgorithm to the neural mass model and validate it on different types of\nsimulated data. Compared to standard SMC-ABC, our approach significantly\nreduces computational cost by requiring fewer model simulations to reach the\ndesired posterior region, thanks to the inclusion of binary parameters\ndescribing the presence or absence of coupling directions. Finally, we apply\nour method to real multi-channel EEG data, uncovering potential similarities in\npatients' brain activities across different epileptic seizures, as well as\ndifferences between pre-seizure and seizure periods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we propose an adapted sequential Monte Carlo approximate\nBayesian computation (SMC-ABC) algorithm for network inference in coupled\nstochastic differential equations (SDEs) used for multivariate time series\nmodeling. Our approach is motivated by neuroscience, specifically the challenge\nof estimating brain connectivity before and during epileptic seizures. To this\nend, we make four key contributions. First, we introduce a 6N-dimensional SDE\nto model the activity of N coupled neuronal populations, extending the\n(single-population) stochastic Jansen and Rit neural mass model used to\ndescribe human electroencephalography (EEG) rhythms, particularly epileptic\nactivity. Second, we construct a reliable and efficient numerical splitting\nscheme for the model simulation. Third, we apply the proposed adapted SMC-ABC\nalgorithm to the neural mass model and validate it on different types of\nsimulated data. Compared to standard SMC-ABC, our approach significantly\nreduces computational cost by requiring fewer model simulations to reach the\ndesired posterior region, thanks to the inclusion of binary parameters\ndescribing the presence or absence of coupling directions. Finally, we apply\nour method to real multi-channel EEG data, uncovering potential similarities in\npatients' brain activities across different epileptic seizures, as well as\ndifferences between pre-seizure and seizure periods."
                },
                "authors": [
                    {
                        "name": "Susanne Ditlevsen"
                    },
                    {
                        "name": "Massimiliano Tamborrino"
                    },
                    {
                        "name": "Irene Tubikanec"
                    }
                ],
                "author_detail": {
                    "name": "Irene Tubikanec"
                },
                "author": "Irene Tubikanec",
                "arxiv_comment": "37 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.15787v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.15787v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09487v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09487v3",
                "updated": "2025-08-26T13:19:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    19,
                    52,
                    1,
                    238,
                    0
                ],
                "published": "2025-03-12T15:46:12Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    15,
                    46,
                    12,
                    2,
                    71,
                    0
                ],
                "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness"
                },
                "summary": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While image-text foundation models have succeeded across diverse downstream\ntasks, they still face challenges in the presence of spurious correlations\nbetween the input and label. To address this issue, we propose a simple\nthree-step approach,Project-Probe-Aggregate (PPA), that enables\nparameter-efficient fine-tuning for foundation models without relying on group\nannotations. Building upon the failure-based debiasing scheme, our method, PPA,\nimproves its two key components: minority samples identification and the robust\ntraining algorithm. Specifically, we first train biased classifiers by\nprojecting image features onto the nullspace of class proxies from text\nencoders. Next, we infer group labels using the biased classifier and probe\ngroup targets with prior correction. Finally, we aggregate group weights of\neach class to produce the debiased classifier. Our theoretical analysis shows\nthat our PPA enhances minority group identification and is Bayes optimal for\nminimizing the balanced group error, mitigating spurious correlations.\nExtensive experimental results confirm the effectiveness of our PPA: it\noutperforms the state-of-the-art by an average worst-group accuracy while\nrequiring less than 0.01% tunable parameters without training group labels."
                },
                "authors": [
                    {
                        "name": "Beier Zhu"
                    },
                    {
                        "name": "Jiequan Cui"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chi Zhang"
                },
                "author": "Chi Zhang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09487v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09487v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19008v1",
                "updated": "2025-08-26T13:13:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    13,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:13:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    13,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "Sense of Self and Time in Borderline Personality. A Comparative\n  Robustness Study with Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sense of Self and Time in Borderline Personality. A Comparative\n  Robustness Study with Generative AI"
                },
                "summary": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias."
                },
                "authors": [
                    {
                        "name": "Marcin Moskalewicz"
                    },
                    {
                        "name": "Anna Sterna"
                    },
                    {
                        "name": "Marek Pokropski"
                    },
                    {
                        "name": "Paula Flores"
                    }
                ],
                "author_detail": {
                    "name": "Paula Flores"
                },
                "author": "Paula Flores",
                "arxiv_comment": "22 pages, 4 tables, submitted to \"Personality and Individual\n  Differences\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08829v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08829v3",
                "updated": "2025-08-26T13:09:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    9,
                    35,
                    1,
                    238,
                    0
                ],
                "published": "2025-03-11T19:08:31Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    19,
                    8,
                    31,
                    1,
                    70,
                    0
                ],
                "title": "Seal Your Backdoor with Variational Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seal Your Backdoor with Variational Defense"
                },
                "summary": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose VIBE, a model-agnostic framework that trains classifiers resilient\nto backdoor attacks. The key concept behind our approach is to treat malicious\ninputs and corrupted labels from the training dataset as observed random\nvariables, while the actual clean labels are latent. VIBE then recovers the\ncorresponding latent clean label posterior through variational inference. The\nresulting training procedure follows the expectation-maximization (EM)\nalgorithm. The E-step infers the clean pseudolabels by solving an\nentropy-regularized optimal transport problem, while the M-step updates the\nclassifier parameters via gradient descent. Being modular, VIBE can seamlessly\nintegrate with recent advancements in self-supervised representation learning,\nwhich enhance its ability to resist backdoor attacks. We experimentally\nvalidate the method effectiveness against contemporary backdoor attacks on\nstandard datasets, a large-scale setup with 1$k$ classes, and a dataset\npoisoned with multiple attacks. VIBE consistently outperforms previous defenses\nacross all tested scenarios."
                },
                "authors": [
                    {
                        "name": "Ivan Sabolić"
                    },
                    {
                        "name": "Matej Grcić"
                    },
                    {
                        "name": "Siniša Šegvić"
                    }
                ],
                "author_detail": {
                    "name": "Siniša Šegvić"
                },
                "author": "Siniša Šegvić",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08829v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08829v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19005v1",
                "updated": "2025-08-26T13:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    4,
                    28,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:04:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    4,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A\n  Framework and Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A\n  Framework and Benchmark"
                },
                "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI."
                },
                "authors": [
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Yipeng Hao"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Rui Zhen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Tianyu Huai"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18137v2",
                "updated": "2025-08-26T13:00:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    0,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T15:41:40Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    41,
                    40,
                    0,
                    237,
                    0
                ],
                "title": "Estimating the average treatment effect in cluster-randomized trials\n  with misclassified outcomes and non-random validation subsets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the average treatment effect in cluster-randomized trials\n  with misclassified outcomes and non-random validation subsets"
                },
                "summary": "Randomized trials are viewed as the benchmark for assessing causal effects of\ntreatments on outcomes of interest. Nonetheless, challenges such as measurement\nerror can undermine the standard causal assumptions for randomized trials. In\nASPIRE, a cluster-randomized trial, pediatric primary care clinics were\nassigned to one of two treatments aimed at promoting clinician delivery of a\nsecure firearm program to parents during well-child visits. A key outcome of\ninterest is thus parent receipt of the program at each visit. Clinicians\ndocumented program delivery in patients' electronic health records for all\nvisits, but their reporting is a proxy measure for the parent receipt outcome.\nParents were also surveyed to report directly on program receipt after their\nchild's visit; however, only a small subset of them completed the survey. Here,\nwe develop a causal inference framework for a binary outcome that is subject to\nmisclassification through silver-standard measures (clinician reports), but\ngold-standard measures (parent reports) are only available for a non-random\ninternal validation subset. We propose a method for identifying the average\ntreatment effect (ATE) that addresses the risk of bias due to misclassification\nand non-random validation selection, even when the outcome (parent receipt) may\ndirectly impact selection propensity (survey responsiveness). We show that ATE\nestimation relies on specifying the relationship between the gold- and\nsilver-standard outcome measures in the validation subset, which may depend on\ntreatment and covariates. Additionally, the clustered design is reflected in\nour causal assumptions and in our cluster-robust approach to estimation of the\nATE. Simulation studies demonstrate acceptable finite-sample operating\ncharacteristics of our ATE estimator, supporting its application to ASPIRE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized trials are viewed as the benchmark for assessing causal effects of\ntreatments on outcomes of interest. Nonetheless, challenges such as measurement\nerror can undermine the standard causal assumptions for randomized trials. In\nASPIRE, a cluster-randomized trial, pediatric primary care clinics were\nassigned to one of two treatments aimed at promoting clinician delivery of a\nsecure firearm program to parents during well-child visits. A key outcome of\ninterest is thus parent receipt of the program at each visit. Clinicians\ndocumented program delivery in patients' electronic health records for all\nvisits, but their reporting is a proxy measure for the parent receipt outcome.\nParents were also surveyed to report directly on program receipt after their\nchild's visit; however, only a small subset of them completed the survey. Here,\nwe develop a causal inference framework for a binary outcome that is subject to\nmisclassification through silver-standard measures (clinician reports), but\ngold-standard measures (parent reports) are only available for a non-random\ninternal validation subset. We propose a method for identifying the average\ntreatment effect (ATE) that addresses the risk of bias due to misclassification\nand non-random validation selection, even when the outcome (parent receipt) may\ndirectly impact selection propensity (survey responsiveness). We show that ATE\nestimation relies on specifying the relationship between the gold- and\nsilver-standard outcome measures in the validation subset, which may depend on\ntreatment and covariates. Additionally, the clustered design is reflected in\nour causal assumptions and in our cluster-robust approach to estimation of the\nATE. Simulation studies demonstrate acceptable finite-sample operating\ncharacteristics of our ATE estimator, supporting its application to ASPIRE."
                },
                "authors": [
                    {
                        "name": "Dane Isenberg"
                    },
                    {
                        "name": "Nandita Mitra"
                    },
                    {
                        "name": "Steven C. Marcus"
                    },
                    {
                        "name": "Rinad S. Beidas"
                    },
                    {
                        "name": "Kristin A. Linn"
                    }
                ],
                "author_detail": {
                    "name": "Kristin A. Linn"
                },
                "author": "Kristin A. Linn",
                "arxiv_comment": "corrected very minor typos in tables/figs and one small change to\n  abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19001v1",
                "updated": "2025-08-26T13:00:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    0,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:00:02Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    0,
                    2,
                    1,
                    238,
                    0
                ],
                "title": "Bayesian Joint Modeling of Zero-Inflated Longitudinal Data and Survival\n  with a Cure Fraction: Application to AIDS Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Joint Modeling of Zero-Inflated Longitudinal Data and Survival\n  with a Cure Fraction: Application to AIDS Data"
                },
                "summary": "We propose a comprehensive Bayesian joint modeling framework for\nzero-inflated longitudinal count data and time-to-event outcomes, explicitly\nincorporating a cure fraction to account for subjects who never experience the\nevent. The longitudinal sub-model employs a flexible mixed-effects Hurdle\nmodel, with distributional options including zero-inflated Poisson and\nzero-inflated negative binomial, accommodating excess zeros and overdispersion\ncommon in count data. The survival component is modeled using a Cox\nproportional hazards model combined with a mixture cure model to distinguish\ncured from susceptible individuals. To link the longitudinal and survival\nprocesses, we include a linear combination of current longitudinal values as\npredictors in the survival model. Inference is performed via Hamiltonian Monte\nCarlo, enabling efficient and robust parameter estimation. The joint model\nsupports dynamic predictions, facilitating real-time risk assessment and\npersonalized medicine. Model performance and estimation accuracy are validated\nthrough simulation studies. Finally, we illustrate the methodology using a\nreal-world HIV cohort dataset, demonstrating its practical utility in\npredicting patient survival outcomes and supporting personalized treatment\ndecisions. Our results highlight the benefits of integrating complex\nlongitudinal count data with survival information in clinical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a comprehensive Bayesian joint modeling framework for\nzero-inflated longitudinal count data and time-to-event outcomes, explicitly\nincorporating a cure fraction to account for subjects who never experience the\nevent. The longitudinal sub-model employs a flexible mixed-effects Hurdle\nmodel, with distributional options including zero-inflated Poisson and\nzero-inflated negative binomial, accommodating excess zeros and overdispersion\ncommon in count data. The survival component is modeled using a Cox\nproportional hazards model combined with a mixture cure model to distinguish\ncured from susceptible individuals. To link the longitudinal and survival\nprocesses, we include a linear combination of current longitudinal values as\npredictors in the survival model. Inference is performed via Hamiltonian Monte\nCarlo, enabling efficient and robust parameter estimation. The joint model\nsupports dynamic predictions, facilitating real-time risk assessment and\npersonalized medicine. Model performance and estimation accuracy are validated\nthrough simulation studies. Finally, we illustrate the methodology using a\nreal-world HIV cohort dataset, demonstrating its practical utility in\npredicting patient survival outcomes and supporting personalized treatment\ndecisions. Our results highlight the benefits of integrating complex\nlongitudinal count data with survival information in clinical research."
                },
                "authors": [
                    {
                        "name": "Taban Baghfalaki"
                    },
                    {
                        "name": "Mojtaba Ganjali"
                    }
                ],
                "author_detail": {
                    "name": "Mojtaba Ganjali"
                },
                "author": "Mojtaba Ganjali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13500v2",
                "updated": "2025-08-26T12:59:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    59,
                    55,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-19T04:20:14Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    4,
                    20,
                    14,
                    1,
                    231,
                    0
                ],
                "title": "LLM-Enhanced Linear Autoencoders for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Linear Autoencoders for Recommendation"
                },
                "summary": "Large language models (LLMs) have been widely adopted to enrich the semantic\nrepresentation of textual item information in recommender systems. However,\nexisting linear autoencoders (LAEs) that incorporate textual information rely\non sparse word co-occurrence patterns, limiting their ability to capture rich\ntextual semantics. To address this, we propose L3AE, the first integration of\nLLMs into the LAE framework. L3AE effectively integrates the heterogeneous\nknowledge of textual semantics and user-item interactions through a two-phase\noptimization strategy. (i) L3AE first constructs a semantic item-to-item\ncorrelation matrix from LLM-derived item representations. (ii) It then learns\nan item-to-item weight matrix from collaborative signals while distilling\nsemantic item correlations as regularization. Notably, each phase of L3AE is\noptimized through closed-form solutions, ensuring global optimality and\ncomputational efficiency. Extensive experiments demonstrate that L3AE\nconsistently outperforms state-of-the-art LLM-enhanced models on three\nbenchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.\nThe source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted to enrich the semantic\nrepresentation of textual item information in recommender systems. However,\nexisting linear autoencoders (LAEs) that incorporate textual information rely\non sparse word co-occurrence patterns, limiting their ability to capture rich\ntextual semantics. To address this, we propose L3AE, the first integration of\nLLMs into the LAE framework. L3AE effectively integrates the heterogeneous\nknowledge of textual semantics and user-item interactions through a two-phase\noptimization strategy. (i) L3AE first constructs a semantic item-to-item\ncorrelation matrix from LLM-derived item representations. (ii) It then learns\nan item-to-item weight matrix from collaborative signals while distilling\nsemantic item correlations as regularization. Notably, each phase of L3AE is\noptimized through closed-form solutions, ensuring global optimality and\ncomputational efficiency. Extensive experiments demonstrate that L3AE\nconsistently outperforms state-of-the-art LLM-enhanced models on three\nbenchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.\nThe source code is available at https://github.com/jaewan7599/L3AE_CIKM2025."
                },
                "authors": [
                    {
                        "name": "Jaewan Moon"
                    },
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_doi": "10.1145/3746252.3760952",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3760952",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15386v2",
                "updated": "2025-08-26T12:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    55,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-21T11:23:05Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    11,
                    23,
                    5,
                    2,
                    141,
                    0
                ],
                "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection"
                },
                "summary": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage."
                },
                "authors": [
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Biquan Bie"
                    },
                    {
                        "name": "Yunzhong Qiu"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18998v1",
                "updated": "2025-08-26T12:54:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    54,
                    23,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:54:23Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    54,
                    23,
                    1,
                    238,
                    0
                ],
                "title": "MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in\n  LLM-based Multilingual ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in\n  LLM-based Multilingual ASR"
                },
                "summary": "End-to-end multilingual ASR aims to transcribe speech from different\nlanguages into corresponding text, but is often limited by scarce multilingual\ndata. LLM-based ASR aligns speech encoder outputs with LLM input space via a\nprojector and has achieved notable success. However, prior work mainly improves\nperformance by increasing data, with little focus on cross-lingual knowledge\nsharing. Moreover, a single complex projector struggles to capture both shared\nand language-specific features effectively. In this work, we propose MOSA\n(Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to\ncombine lightweight adapters that learn shared and language-specific knowledge.\nThis enables better utilization of high-resource language data to support\nlow-resource languages, mitigating data scarcity issues. Experimental results\nshow that MOSA-Base achieves a 15.4\\% relative reduction in average WER\ncompared to the Baseline-Base and consistently outperforms it across all\nlanguages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained\nwith only 60\\% of its parameters. Similarly, MOSA-Large outperforms the\nBaseline-Large in average WER and demonstrates greater robustness to data\nimbalance. Ablation studies further indicate that MOSA is more effective at\nhandling individual languages and learning both language-specific and shared\nlinguistic knowledge. These findings support that, in LLM-based ASR, a mixture\nof simple adapters is more effective than a single, complex adapter design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end multilingual ASR aims to transcribe speech from different\nlanguages into corresponding text, but is often limited by scarce multilingual\ndata. LLM-based ASR aligns speech encoder outputs with LLM input space via a\nprojector and has achieved notable success. However, prior work mainly improves\nperformance by increasing data, with little focus on cross-lingual knowledge\nsharing. Moreover, a single complex projector struggles to capture both shared\nand language-specific features effectively. In this work, we propose MOSA\n(Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to\ncombine lightweight adapters that learn shared and language-specific knowledge.\nThis enables better utilization of high-resource language data to support\nlow-resource languages, mitigating data scarcity issues. Experimental results\nshow that MOSA-Base achieves a 15.4\\% relative reduction in average WER\ncompared to the Baseline-Base and consistently outperforms it across all\nlanguages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained\nwith only 60\\% of its parameters. Similarly, MOSA-Large outperforms the\nBaseline-Large in average WER and demonstrates greater robustness to data\nimbalance. Ablation studies further indicate that MOSA is more effective at\nhandling individual languages and learning both language-specific and shared\nlinguistic knowledge. These findings support that, in LLM-based ASR, a mixture\nof simple adapters is more effective than a single, complex adapter design."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yangui Fang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18993v1",
                "updated": "2025-08-26T12:48:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    48,
                    5,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:48:05Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    48,
                    5,
                    1,
                    238,
                    0
                ],
                "title": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks\n  Through Code Repository Leveraging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks\n  Through Code Repository Leveraging"
                },
                "summary": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench."
                },
                "authors": [
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Huacan Wang"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Shuo Lu"
                    },
                    {
                        "name": "Ziyang He"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Bill Sun"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Pin Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Pin Lyu"
                },
                "author": "Pin Lyu",
                "arxiv_comment": "Highly practical, Well-motivated, Actionable",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18992v1",
                "updated": "2025-08-26T12:46:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    46,
                    58,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:46:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    46,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "Automatic Prompt Optimization with Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Prompt Optimization with Prompt Distillation"
                },
                "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting."
                },
                "authors": [
                    {
                        "name": "Viktor N. Zhuravlev"
                    },
                    {
                        "name": "Artur R. Khairullin"
                    },
                    {
                        "name": "Ernest A. Dyagin"
                    },
                    {
                        "name": "Alena N. Sitkina"
                    },
                    {
                        "name": "Nikita I. Kulin"
                    }
                ],
                "author_detail": {
                    "name": "Nikita I. Kulin"
                },
                "author": "Nikita I. Kulin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13972v3",
                "updated": "2025-08-27T14:04:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    14,
                    4,
                    13,
                    2,
                    239,
                    0
                ],
                "published": "2025-05-20T06:12:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    6,
                    12,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping\n  Evaluation in LLM-based Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping\n  Evaluation in LLM-based Counterfactuals"
                },
                "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models: being the same model, belonging to the same model family, being\nindependent models, and having an distillation relationship. Through extensive\nexperiments involving two state-of-the-art LLM-based methods, three datasets,\nfour generator models, and 15 judge models, complemented by a user study (n =\n90), we demonstrate that judge models with an independent, non-fine-tuned\nrelationship to the generator model provide the most reliable label flipping\nevaluations. Relationships between the generator and judge models, which are\nclosely aligned with the user study for CDA, result in better model performance\nand robustness. Nevertheless, we find that the gap between the most effective\njudge models and the results obtained from the user study remains considerably\nlarge. This suggests that a fully automated pipeline for CDA may be inadequate\nand requires human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models: being the same model, belonging to the same model family, being\nindependent models, and having an distillation relationship. Through extensive\nexperiments involving two state-of-the-art LLM-based methods, three datasets,\nfour generator models, and 15 judge models, complemented by a user study (n =\n90), we demonstrate that judge models with an independent, non-fine-tuned\nrelationship to the generator model provide the most reliable label flipping\nevaluations. Relationships between the generator and judge models, which are\nclosely aligned with the user study for CDA, result in better model performance\nand robustness. Nevertheless, we find that the gap between the most effective\njudge models and the results obtained from the user study remains considerably\nlarge. This suggests that a fully automated pipeline for CDA may be inadequate\nand requires human intervention."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Van Bach Nguyen"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Luis Felipe Villa-Arenas"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "Accepted at INLG 2025, camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18976v1",
                "updated": "2025-08-26T12:22:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    22,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:22:45Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    22,
                    45,
                    1,
                    238,
                    0
                ],
                "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding\n  and Mitigating Contextual Vulnerability in Word-level Differential Privacy\n  Text Sanitization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding\n  and Mitigating Contextual Vulnerability in Word-level Differential Privacy\n  Text Sanitization"
                },
                "summary": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially."
                },
                "authors": [
                    {
                        "name": "Stephen Meisenbacher"
                    },
                    {
                        "name": "Alexandra Klymenko"
                    },
                    {
                        "name": "Andreea-Elena Bodea"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_doi": "10.1145/3733802.3764058",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733802.3764058",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06905v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06905v3",
                "updated": "2025-08-26T12:18:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    18,
                    14,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-09T09:36:21Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    9,
                    36,
                    21,
                    5,
                    221,
                    0
                ],
                "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiRef: Controllable Image Generation with Multiple Visual References"
                },
                "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/."
                },
                "authors": [
                    {
                        "name": "Ruoxi Chen"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Sinan Wang"
                    },
                    {
                        "name": "Shiyun Lang"
                    },
                    {
                        "name": "Petr Sushko"
                    },
                    {
                        "name": "Gaoyang Jiang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Ranjay Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Ranjay Krishna"
                },
                "author": "Ranjay Krishna",
                "arxiv_comment": "Accepted to ACM MM 2025 Datasets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06905v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06905v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18960v1",
                "updated": "2025-08-26T12:00:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    0,
                    38,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:00:38Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    0,
                    38,
                    1,
                    238,
                    0
                ],
                "title": "Enhancing compact convolutional transformers with super attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing compact convolutional transformers with super attention"
                },
                "summary": "In this paper, we propose a vision model that adopts token mixing,\nsequence-pooling, and convolutional tokenizers to achieve state-of-the-art\nperformance and efficient inference in fixed context-length tasks. In the\nCIFAR100 benchmark, our model significantly improves the baseline of the top 1%\nand top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%,\nwhile being more efficient than the Scaled Dot Product Attention (SDPA)\ntransformers when the context length is less than the embedding dimension and\nonly 60% the size. In addition, the architecture demonstrates high training\nstability and does not rely on techniques such as data augmentation like mixup,\npositional embeddings, or learning rate scheduling. We make our code available\non Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose a vision model that adopts token mixing,\nsequence-pooling, and convolutional tokenizers to achieve state-of-the-art\nperformance and efficient inference in fixed context-length tasks. In the\nCIFAR100 benchmark, our model significantly improves the baseline of the top 1%\nand top 5% validation accuracy from 36.50% to 46.29% and 66.33% to 76.31%,\nwhile being more efficient than the Scaled Dot Product Attention (SDPA)\ntransformers when the context length is less than the embedding dimension and\nonly 60% the size. In addition, the architecture demonstrates high training\nstability and does not rely on techniques such as data augmentation like mixup,\npositional embeddings, or learning rate scheduling. We make our code available\non Github."
                },
                "authors": [
                    {
                        "name": "Simpenzwe Honore Leandre"
                    },
                    {
                        "name": "Natenaile Asmamaw Shiferaw"
                    },
                    {
                        "name": "Dillip Rout"
                    }
                ],
                "author_detail": {
                    "name": "Dillip Rout"
                },
                "author": "Dillip Rout",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16267v2",
                "updated": "2025-08-26T11:54:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    54,
                    16,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-22T09:59:23Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    59,
                    23,
                    4,
                    234,
                    0
                ],
                "title": "From Confidence to Collapse in LLM Factual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Confidence to Collapse in LLM Factual Robustness"
                },
                "summary": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."
                },
                "authors": [
                    {
                        "name": "Alina Fastowski"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18955v1",
                "updated": "2025-08-26T11:49:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    49,
                    58,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:49:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    49,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "Interleaving Large Language Models for Compiler Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaving Large Language Models for Compiler Testing"
                },
                "summary": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers."
                },
                "authors": [
                    {
                        "name": "Yunbo Ni"
                    },
                    {
                        "name": "Shaohua Li"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Li"
                },
                "author": "Shaohua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18953v1",
                "updated": "2025-08-26T11:49:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    49,
                    42,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:49:42Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    49,
                    42,
                    1,
                    238,
                    0
                ],
                "title": "Novel Approaches to Artificial Intelligence Development Based on the\n  Nearest Neighbor Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Approaches to Artificial Intelligence Development Based on the\n  Nearest Neighbor Method"
                },
                "summary": "Modern neural network technologies, including large language models, have\nachieved remarkable success in various applied artificial intelligence\napplications, however, they face a range of fundamental limitations. Among them\nare hallucination effects, high computational complexity of training and\ninference, costly fine-tuning, and catastrophic forgetting issues. These\nlimitations significantly hinder the use of neural networks in critical areas\nsuch as medicine, industrial process management, and scientific research. This\narticle proposes an alternative approach based on the nearest neighbors method\nwith hierarchical clustering structures. Employing the k-nearest neighbors\nalgorithm significantly reduces or completely eliminates hallucination effects\nwhile simplifying model expansion and fine-tuning without the need for\nretraining the entire network. To overcome the high computational load of the\nk-nearest neighbors method, the paper proposes using tree-like data structures\nbased on Kohonen self-organizing maps, thereby greatly accelerating nearest\nneighbor searches. Tests conducted on handwritten digit recognition and simple\nsubtitle translation tasks confirmed the effectiveness of the proposed\napproach. With only a slight reduction in accuracy, the nearest neighbor search\ntime was reduced hundreds of times compared to exhaustive search methods. The\nproposed method features transparency and interpretability, closely aligns with\nhuman cognitive mechanisms, and demonstrates potential for extensive use in\ntasks requiring high reliability and explainable results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern neural network technologies, including large language models, have\nachieved remarkable success in various applied artificial intelligence\napplications, however, they face a range of fundamental limitations. Among them\nare hallucination effects, high computational complexity of training and\ninference, costly fine-tuning, and catastrophic forgetting issues. These\nlimitations significantly hinder the use of neural networks in critical areas\nsuch as medicine, industrial process management, and scientific research. This\narticle proposes an alternative approach based on the nearest neighbors method\nwith hierarchical clustering structures. Employing the k-nearest neighbors\nalgorithm significantly reduces or completely eliminates hallucination effects\nwhile simplifying model expansion and fine-tuning without the need for\nretraining the entire network. To overcome the high computational load of the\nk-nearest neighbors method, the paper proposes using tree-like data structures\nbased on Kohonen self-organizing maps, thereby greatly accelerating nearest\nneighbor searches. Tests conducted on handwritten digit recognition and simple\nsubtitle translation tasks confirmed the effectiveness of the proposed\napproach. With only a slight reduction in accuracy, the nearest neighbor search\ntime was reduced hundreds of times compared to exhaustive search methods. The\nproposed method features transparency and interpretability, closely aligns with\nhuman cognitive mechanisms, and demonstrates potential for extensive use in\ntasks requiring high reliability and explainable results."
                },
                "authors": [
                    {
                        "name": "I. I. Priezzhev"
                    },
                    {
                        "name": "D. A. Danko"
                    },
                    {
                        "name": "A. V. Shubin"
                    }
                ],
                "author_detail": {
                    "name": "A. V. Shubin"
                },
                "author": "A. V. Shubin",
                "arxiv_comment": "18 pages, 6 figures. Novel hierarchical neural networks based on\n  k-nearest neighbors method for addressing hallucination effects, training\n  complexity, and catastrophic forgetting in modern AI systems. Includes\n  mathematical formulations using Kohonen self-organizing maps and experimental\n  validation on MNIST handwritten digit recognition and machine translation\n  tasks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18949v1",
                "updated": "2025-08-26T11:42:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    42,
                    57,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:42:57Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    42,
                    57,
                    1,
                    238,
                    0
                ],
                "title": "Energy-Based Flow Matching for Generating 3D Molecular Structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Based Flow Matching for Generating 3D Molecular Structure"
                },
                "summary": "Molecular structure generation is a fundamental problem that involves\ndetermining the 3D positions of molecules' constituents. It has crucial\nbiological applications, such as molecular docking, protein folding, and\nmolecular design. Recent advances in generative modeling, such as diffusion\nmodels and flow matching, have made great progress on these tasks by modeling\nmolecular conformations as a distribution. In this work, we focus on flow\nmatching and adopt an energy-based perspective to improve training and\ninference of structure generation models. Our view results in a mapping\nfunction, represented by a deep network, that is directly learned to\n\\textit{iteratively} map random configurations, i.e. samples from the source\ndistribution, to target structures, i.e. points in the data manifold. This\nyields a conceptually simple and empirically effective flow matching setup that\nis theoretically justified and has interesting connections to fundamental\nproperties such as idempotency and stability, as well as the empirically useful\ntechniques such as structure refinement in AlphaFold. Experiments on protein\ndocking as well as protein backbone generation consistently demonstrate the\nmethod's effectiveness, where it outperforms recent baselines of\ntask-associated flow matching and diffusion models, using a similar\ncomputational budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular structure generation is a fundamental problem that involves\ndetermining the 3D positions of molecules' constituents. It has crucial\nbiological applications, such as molecular docking, protein folding, and\nmolecular design. Recent advances in generative modeling, such as diffusion\nmodels and flow matching, have made great progress on these tasks by modeling\nmolecular conformations as a distribution. In this work, we focus on flow\nmatching and adopt an energy-based perspective to improve training and\ninference of structure generation models. Our view results in a mapping\nfunction, represented by a deep network, that is directly learned to\n\\textit{iteratively} map random configurations, i.e. samples from the source\ndistribution, to target structures, i.e. points in the data manifold. This\nyields a conceptually simple and empirically effective flow matching setup that\nis theoretically justified and has interesting connections to fundamental\nproperties such as idempotency and stability, as well as the empirically useful\ntechniques such as structure refinement in AlphaFold. Experiments on protein\ndocking as well as protein backbone generation consistently demonstrate the\nmethod's effectiveness, where it outperforms recent baselines of\ntask-associated flow matching and diffusion models, using a similar\ncomputational budget."
                },
                "authors": [
                    {
                        "name": "Wenyin Zhou"
                    },
                    {
                        "name": "Christopher Iliffe Sprague"
                    },
                    {
                        "name": "Vsevolod Viliuga"
                    },
                    {
                        "name": "Matteo Tadiello"
                    },
                    {
                        "name": "Arne Elofsson"
                    },
                    {
                        "name": "Hossein Azizpour"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Azizpour"
                },
                "author": "Hossein Azizpour",
                "arxiv_comment": "Accepted to the International Conference on Machine Learning (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00348v2",
                "updated": "2025-08-26T11:42:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    42,
                    44,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-01T06:23:11Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    6,
                    23,
                    11,
                    4,
                    213,
                    0
                ],
                "title": "Unlocking New Paths for Science with Extreme-Mass-Ratio Inspirals:\n  Machine Learning-Enhanced MCMC for Accurate Parameter Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking New Paths for Science with Extreme-Mass-Ratio Inspirals:\n  Machine Learning-Enhanced MCMC for Accurate Parameter Inversion"
                },
                "summary": "The detection of gravitational waves from extreme-mass-ratio inspirals\n(EMRIs) in space-borne antennas like Taiji and LISA promises deep insights into\nstrong-field gravity and black hole physics. However, the complex, highly\ndegenerate, and non-convex likelihood landscapes characteristic of EMRI\nparameter spaces pose severe challenges for conventional Markov chain Monte\nCarlo (MCMC) methods. Under realistic instrumental noise and broad priors,\nthese methods demand impractical computational costs but are prone to becoming\ntrapped in local maxima, leading to biased and unreliable parameter estimates.\nTo address this, we introduce Flow-Matching Markov Chain Monte Carlo (FM-MCMC),\na novel Bayesian framework that integrates continuous normalizing flows (CNFs)\nwith parallel tempering MCMC (PTMCMC). By generating high-likelihood regions\nvia CNFs and refining them through PTMCMC, FM-MCMC enables robust exploration\nof the nontrivial parameter spaces, while achieving orders-of-magnitude\nimprovement in computational efficiency and, more importantly, ensuring\nstatistically reliable and unbiased inference. By enabling real-time, unbiased\nparameter inference, FM-MCMC could unlock the full scientific potential of EMRI\nobservations, and would serve as a scalable pipeline for precision\ngravitational-wave astronomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of gravitational waves from extreme-mass-ratio inspirals\n(EMRIs) in space-borne antennas like Taiji and LISA promises deep insights into\nstrong-field gravity and black hole physics. However, the complex, highly\ndegenerate, and non-convex likelihood landscapes characteristic of EMRI\nparameter spaces pose severe challenges for conventional Markov chain Monte\nCarlo (MCMC) methods. Under realistic instrumental noise and broad priors,\nthese methods demand impractical computational costs but are prone to becoming\ntrapped in local maxima, leading to biased and unreliable parameter estimates.\nTo address this, we introduce Flow-Matching Markov Chain Monte Carlo (FM-MCMC),\na novel Bayesian framework that integrates continuous normalizing flows (CNFs)\nwith parallel tempering MCMC (PTMCMC). By generating high-likelihood regions\nvia CNFs and refining them through PTMCMC, FM-MCMC enables robust exploration\nof the nontrivial parameter spaces, while achieving orders-of-magnitude\nimprovement in computational efficiency and, more importantly, ensuring\nstatistically reliable and unbiased inference. By enabling real-time, unbiased\nparameter inference, FM-MCMC could unlock the full scientific potential of EMRI\nobservations, and would serve as a scalable pipeline for precision\ngravitational-wave astronomy."
                },
                "authors": [
                    {
                        "name": "Bo Liang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Hanlin Song"
                    },
                    {
                        "name": "Zhenwei Lyu"
                    },
                    {
                        "name": "Minghui Du"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Ziren Luo"
                    },
                    {
                        "name": "Sensen He"
                    },
                    {
                        "name": "Haohao Gu"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Manjia Liang Yuxiang Xu"
                    },
                    {
                        "name": "Li-e Qiang"
                    },
                    {
                        "name": "Mingming Sun"
                    },
                    {
                        "name": "Wei-Liang Qian"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Liang Qian"
                },
                "author": "Wei-Liang Qian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.16902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.16902v2",
                "updated": "2025-08-26T11:41:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    41,
                    14,
                    1,
                    238,
                    0
                ],
                "published": "2023-06-29T12:48:00Z",
                "published_parsed": [
                    2023,
                    6,
                    29,
                    12,
                    48,
                    0,
                    3,
                    180,
                    0
                ],
                "title": "Integrating Large Language Model for Improved Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Model for Improved Causal Discovery"
                },
                "summary": "Recovering the structure of causal graphical models from observational data\nis an essential yet challenging task for causal discovery in scientific\nscenarios. Domain-specific causal discovery usually relies on expert validation\nor prior analysis to improve the reliability of recovered causality, which is\nyet limited by the scarcity of expert resources. Recently, Large Language\nModels (LLM) have been used for causal analysis across various domain-specific\nscenarios, suggesting its potential as autonomous expert roles in guiding\ndata-based structure learning. However, integrating LLMs into causal discovery\nfaces challenges due to inaccuracies in LLM-based reasoning on revealing the\nactual causal structure. To address this challenge, we propose an\nerror-tolerant LLM-driven causal discovery framework. The error-tolerant\nmechanism is designed three-fold with sufficient consideration on potential\ninaccuracies. In the LLM-based reasoning process, an accuracy-oriented\nprompting strategy restricts causal analysis to a reliable range. Next, a\nknowledge-to-structure transition aligns LLM-derived causal statements with\nstructural causal interactions. In the structure learning process, the\ngoodness-of-fit to data and adherence to LLM-derived priors are balanced to\nfurther address prior inaccuracies. Evaluation of eight real-world causal\nstructures demonstrates the efficacy of our LLM-driven approach in improving\ndata-based causal discovery, along with its robustness to inaccurate\nLLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering the structure of causal graphical models from observational data\nis an essential yet challenging task for causal discovery in scientific\nscenarios. Domain-specific causal discovery usually relies on expert validation\nor prior analysis to improve the reliability of recovered causality, which is\nyet limited by the scarcity of expert resources. Recently, Large Language\nModels (LLM) have been used for causal analysis across various domain-specific\nscenarios, suggesting its potential as autonomous expert roles in guiding\ndata-based structure learning. However, integrating LLMs into causal discovery\nfaces challenges due to inaccuracies in LLM-based reasoning on revealing the\nactual causal structure. To address this challenge, we propose an\nerror-tolerant LLM-driven causal discovery framework. The error-tolerant\nmechanism is designed three-fold with sufficient consideration on potential\ninaccuracies. In the LLM-based reasoning process, an accuracy-oriented\nprompting strategy restricts causal analysis to a reliable range. Next, a\nknowledge-to-structure transition aligns LLM-derived causal statements with\nstructural causal interactions. In the structure learning process, the\ngoodness-of-fit to data and adherence to LLM-derived priors are balanced to\nfurther address prior inaccuracies. Evaluation of eight real-world causal\nstructures demonstrates the efficacy of our LLM-driven approach in improving\ndata-based causal discovery, along with its robustness to inaccurate\nLLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD."
                },
                "authors": [
                    {
                        "name": "Taiyu Ban"
                    },
                    {
                        "name": "Lyuzhou Chen"
                    },
                    {
                        "name": "Derui Lyu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Qinrui Zhu"
                    },
                    {
                        "name": "Qiang Tu"
                    },
                    {
                        "name": "Huanhuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huanhuan Chen"
                },
                "author": "Huanhuan Chen",
                "arxiv_comment": "13 pages, 5 figures",
                "arxiv_journal_ref": "IEEE Transactions on Artificial Intelligence, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.16902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.16902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18947v1",
                "updated": "2025-08-26T11:40:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    40,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:40:02Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    40,
                    2,
                    1,
                    238,
                    0
                ],
                "title": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in\n  Security Operations Centres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in\n  Security Operations Centres"
                },
                "summary": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts."
                },
                "authors": [
                    {
                        "name": "Ronal Singh"
                    },
                    {
                        "name": "Shahroz Tariq"
                    },
                    {
                        "name": "Fatemeh Jalalvand"
                    },
                    {
                        "name": "Mohan Baruwal Chhetri"
                    },
                    {
                        "name": "Surya Nepal"
                    },
                    {
                        "name": "Cecile Paris"
                    },
                    {
                        "name": "Martin Lochner"
                    }
                ],
                "author_detail": {
                    "name": "Martin Lochner"
                },
                "author": "Martin Lochner",
                "arxiv_comment": "22 pages, 9 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18944v1",
                "updated": "2025-08-26T11:36:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    36,
                    14,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:36:14Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    36,
                    14,
                    1,
                    238,
                    0
                ],
                "title": "PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads"
                },
                "summary": "Achieving realistic hair strand synthesis is essential for creating lifelike\ndigital humans, but producing high-fidelity hair strand geometry remains a\nsignificant challenge. Existing methods require a complex setup for data\nacquisition, involving multi-view images captured in constrained studio\nenvironments. Additionally, these methods have longer hair volume estimation\nand strand synthesis times, which hinder efficiency. We introduce PanoHair, a\nmodel that estimates head geometry as signed distance fields using knowledge\ndistillation from a pre-trained generative teacher model for head synthesis.\nOur approach enables the prediction of semantic segmentation masks and 3D\norientations specifically for the hair region of the estimated geometry. Our\nmethod is generative and can generate diverse hairstyles with latent space\nmanipulations. For real images, our approach involves an inversion process to\ninfer latent codes and produces visually appealing hair strands, offering a\nstreamlined alternative to complex multi-view data acquisition setups. Given\nthe latent code, PanoHair generates a clean manifold mesh for the hair region\nin under 5 seconds, along with semantic and orientation maps, marking a\nsignificant improvement over existing methods, as demonstrated in our\nexperiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving realistic hair strand synthesis is essential for creating lifelike\ndigital humans, but producing high-fidelity hair strand geometry remains a\nsignificant challenge. Existing methods require a complex setup for data\nacquisition, involving multi-view images captured in constrained studio\nenvironments. Additionally, these methods have longer hair volume estimation\nand strand synthesis times, which hinder efficiency. We introduce PanoHair, a\nmodel that estimates head geometry as signed distance fields using knowledge\ndistillation from a pre-trained generative teacher model for head synthesis.\nOur approach enables the prediction of semantic segmentation masks and 3D\norientations specifically for the hair region of the estimated geometry. Our\nmethod is generative and can generate diverse hairstyles with latent space\nmanipulations. For real images, our approach involves an inversion process to\ninfer latent codes and produces visually appealing hair strands, offering a\nstreamlined alternative to complex multi-view data acquisition setups. Given\nthe latent code, PanoHair generates a clean manifold mesh for the hair region\nin under 5 seconds, along with semantic and orientation maps, marking a\nsignificant improvement over existing methods, as demonstrated in our\nexperiments."
                },
                "authors": [
                    {
                        "name": "Shashikant Verma"
                    },
                    {
                        "name": "Shanmuganathan Raman"
                    }
                ],
                "author_detail": {
                    "name": "Shanmuganathan Raman"
                },
                "author": "Shanmuganathan Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18942v1",
                "updated": "2025-08-26T11:31:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    31,
                    5,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:31:05Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    31,
                    5,
                    1,
                    238,
                    0
                ],
                "title": "EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G\n  Energy Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EnerSwap: Large-Scale, Privacy-First Automated Market Maker for V2G\n  Energy Trading"
                },
                "summary": "With the rapid growth of Electric Vehicle (EV) technology, EVs are destined\nto shape the future of transportation. The large number of EVs facilitates the\ndevelopment of the emerging vehicle-to-grid (V2G) technology, which realizes\nbidirectional energy exchanges between EVs and the power grid. This has led to\nthe setting up of electricity markets that are usually confined to a small\ngeographical location, often with a small number of participants. Usually,\nthese markets are manipulated by intermediaries responsible for collecting bids\nfrom prosumers, determining the market-clearing price, incorporating grid\nconstraints, and accounting for network losses. While centralized models can be\nhighly efficient, they grant excessive power to the intermediary by allowing\nthem to gain exclusive access to prosumers \\textquotesingle price preferences.\nThis opens the door to potential market manipulation and raises significant\nprivacy concerns for users, such as the location of energy providers. This lack\nof protection exposes users to potential risks, as untrustworthy servers and\nmalicious adversaries can exploit this information to infer trading activities\nand real identities. This work proposes a secure, decentralized exchange market\nbuilt on blockchain technology, utilizing a privacy-preserving Automated Market\nMaker (AMM) model to offer open and fair, and equal access to traders, and\nmitigates the most common trading-manipulation attacks. Additionally, it\nincorporates a scalable architecture based on geographical dynamic sharding,\nallowing for efficient resource allocation and improved performance as the\nmarket grows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of Electric Vehicle (EV) technology, EVs are destined\nto shape the future of transportation. The large number of EVs facilitates the\ndevelopment of the emerging vehicle-to-grid (V2G) technology, which realizes\nbidirectional energy exchanges between EVs and the power grid. This has led to\nthe setting up of electricity markets that are usually confined to a small\ngeographical location, often with a small number of participants. Usually,\nthese markets are manipulated by intermediaries responsible for collecting bids\nfrom prosumers, determining the market-clearing price, incorporating grid\nconstraints, and accounting for network losses. While centralized models can be\nhighly efficient, they grant excessive power to the intermediary by allowing\nthem to gain exclusive access to prosumers \\textquotesingle price preferences.\nThis opens the door to potential market manipulation and raises significant\nprivacy concerns for users, such as the location of energy providers. This lack\nof protection exposes users to potential risks, as untrustworthy servers and\nmalicious adversaries can exploit this information to infer trading activities\nand real identities. This work proposes a secure, decentralized exchange market\nbuilt on blockchain technology, utilizing a privacy-preserving Automated Market\nMaker (AMM) model to offer open and fair, and equal access to traders, and\nmitigates the most common trading-manipulation attacks. Additionally, it\nincorporates a scalable architecture based on geographical dynamic sharding,\nallowing for efficient resource allocation and improved performance as the\nmarket grows."
                },
                "authors": [
                    {
                        "name": "Ahmed Mounsf Rafik Bendada"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "arxiv_comment": "11 pages, 7 figures, 1 table, 1 algorithm, Paper accepted in 27th\n  MSWiM Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18933v1",
                "updated": "2025-08-26T11:20:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    20,
                    39,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:20:39Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    20,
                    39,
                    1,
                    238,
                    0
                ],
                "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging\n  Counterfactual Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging\n  Counterfactual Augmentation"
                },
                "summary": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis."
                },
                "authors": [
                    {
                        "name": "David Egea"
                    },
                    {
                        "name": "Barproda Halder"
                    },
                    {
                        "name": "Sanghamitra Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Sanghamitra Dutta"
                },
                "author": "Sanghamitra Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15761v2",
                "updated": "2025-08-26T10:56:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    56,
                    4,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-21T17:56:10Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    17,
                    56,
                    10,
                    3,
                    233,
                    0
                ],
                "title": "Waver: Wave Your Way to Lifelike Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Waver: Wave Your Way to Lifelike Video Generation"
                },
                "summary": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Waver, a high-performance foundation model for unified image and\nvideo generation. Waver can directly generate videos with durations ranging\nfrom 5 to 10 seconds at a native resolution of 720p, which are subsequently\nupscaled to 1080p. The model simultaneously supports text-to-video (T2V),\nimage-to-video (I2V), and text-to-image (T2I) generation within a single,\nintegrated framework. We introduce a Hybrid Stream DiT architecture to enhance\nmodality alignment and accelerate training convergence. To ensure training data\nquality, we establish a comprehensive data curation pipeline and manually\nannotate and train an MLLM-based video quality model to filter for the\nhighest-quality samples. Furthermore, we provide detailed training and\ninference recipes to facilitate the generation of high-quality videos. Building\non these contributions, Waver excels at capturing complex motion, achieving\nsuperior motion amplitude and temporal consistency in video synthesis. Notably,\nit ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial\nAnalysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming\nexisting open-source models and matching or surpassing state-of-the-art\ncommercial solutions. We hope this technical report will help the community\nmore efficiently train high-quality video generation models and accelerate\nprogress in video generation technologies. Official page:\nhttps://github.com/FoundationVision/Waver."
                },
                "authors": [
                    {
                        "name": "Yifu Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Yuqi Zhang"
                    },
                    {
                        "name": "Yifei Hu"
                    },
                    {
                        "name": "Fengda Zhu"
                    },
                    {
                        "name": "Chuang Lin"
                    },
                    {
                        "name": "Xiaofeng Mei"
                    },
                    {
                        "name": "Yi Jiang"
                    },
                    {
                        "name": "Bingyue Peng"
                    },
                    {
                        "name": "Zehuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zehuan Yuan"
                },
                "author": "Zehuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16949v2",
                "updated": "2025-08-26T10:52:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    52,
                    15,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-23T08:47:31Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    47,
                    31,
                    5,
                    235,
                    0
                ],
                "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. This work is still in progress, and we will release\nthe code, the models, and the datasets soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. This work is still in progress, and we will release\nthe code, the models, and the datasets soon."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sunzhu Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Jiale Zhao"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Yihe Zhou"
                    },
                    {
                        "name": "Hengtong Lu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "arxiv_comment": "This work is still in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18918v1",
                "updated": "2025-08-26T10:44:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    44,
                    33,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:44:33Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    44,
                    33,
                    1,
                    238,
                    0
                ],
                "title": "DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM\n  with Audio Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM\n  with Audio Modality"
                },
                "summary": "We present DESAMO, an on-device smart home system for elder-friendly use\npowered by Audio LLM, that supports natural and private interactions. While\nconventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,\noften struggling with the unclear speech common among elderly users and unable\nto handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio\ninput directly, enabling a robust understanding of user intent and critical\nevents, such as falls or calls for help.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DESAMO, an on-device smart home system for elder-friendly use\npowered by Audio LLM, that supports natural and private interactions. While\nconventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,\noften struggling with the unclear speech common among elderly users and unable\nto handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio\ninput directly, enabling a robust understanding of user intent and critical\nevents, such as falls or calls for help."
                },
                "authors": [
                    {
                        "name": "Youngwon Choi"
                    },
                    {
                        "name": "Donghyuk Jung"
                    },
                    {
                        "name": "Hwayeon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hwayeon Kim"
                },
                "author": "Hwayeon Kim",
                "arxiv_comment": "2 pages, 2 figures. Accepted for presentation as a UIST 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24011v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24011v2",
                "updated": "2025-08-26T10:24:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    24,
                    11,
                    1,
                    238,
                    0
                ],
                "published": "2025-03-31T12:38:21Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    38,
                    21,
                    0,
                    90,
                    0
                ],
                "title": "Simulations in Statistical Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations in Statistical Workflows"
                },
                "summary": "Simulations play important and diverse roles in statistical workflows, for\nexample, in model specification, checking, validation, and even directly in\nmodel inference. Over the past decades, the application areas and overall\npotential of simulations in statistical workflows have expanded significantly,\ndriven by the development of new simulation-based algorithms and exponentially\nincreasing computational resources. In this paper, we examine past and current\ntrends in the field and offer perspectives on how simulations may shape the\nfuture of statistical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations play important and diverse roles in statistical workflows, for\nexample, in model specification, checking, validation, and even directly in\nmodel inference. Over the past decades, the application areas and overall\npotential of simulations in statistical workflows have expanded significantly,\ndriven by the development of new simulation-based algorithms and exponentially\nincreasing computational resources. In this paper, we examine past and current\ntrends in the field and offer perspectives on how simulations may shape the\nfuture of statistical practice."
                },
                "authors": [
                    {
                        "name": "Paul-Christian Bürkner"
                    },
                    {
                        "name": "Marvin Schmitt"
                    },
                    {
                        "name": "Stefan T. Radev"
                    }
                ],
                "author_detail": {
                    "name": "Stefan T. Radev"
                },
                "author": "Stefan T. Radev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24011v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24011v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16701v2",
                "updated": "2025-08-26T10:23:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    23,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-22T06:00:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    0,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "Generative Artificial Intelligence and Agents in Research and Teaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence and Agents in Research and Teaching"
                },
                "summary": "This study provides a comprehensive analysis of the development, functioning,\nand application of generative artificial intelligence (GenAI) and large\nlanguage models (LLMs), with an emphasis on their implications for research and\neducation. It traces the conceptual evolution from artificial intelligence (AI)\nthrough machine learning (ML) and deep learning (DL) to transformer\narchitectures, which constitute the foundation of contemporary generative\nsystems. Technical aspects, including prompting strategies, word embeddings,\nand probabilistic sampling methods (temperature, top-k, and top-p), are\nexamined alongside the emergence of autonomous agents. These elements are\nconsidered in relation to both the opportunities they create and the\nlimitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research\nprocess, from ideation and literature review to research design, data\ncollection, analysis, interpretation, and dissemination. While particular\nattention is given to geographical research, the discussion extends to wider\nacademic contexts. A parallel strand addresses the pedagogical applications of\nGenAI, encompassing course and lesson design, teaching delivery, assessment,\nand feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges\nposed by GenAI. Issues of bias, intellectual property, governance, and\naccountability are assessed, alongside the ecological footprint of LLMs and\nemerging technological strategies for mitigation. The concluding section\nconsiders near- and long-term futures of GenAI, including scenarios of\nsustained adoption, regulation, and potential decline. By situating GenAI\nwithin both scholarly practice and educational contexts, the study contributes\nto critical debates on its transformative potential and societal\nresponsibilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides a comprehensive analysis of the development, functioning,\nand application of generative artificial intelligence (GenAI) and large\nlanguage models (LLMs), with an emphasis on their implications for research and\neducation. It traces the conceptual evolution from artificial intelligence (AI)\nthrough machine learning (ML) and deep learning (DL) to transformer\narchitectures, which constitute the foundation of contemporary generative\nsystems. Technical aspects, including prompting strategies, word embeddings,\nand probabilistic sampling methods (temperature, top-k, and top-p), are\nexamined alongside the emergence of autonomous agents. These elements are\nconsidered in relation to both the opportunities they create and the\nlimitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research\nprocess, from ideation and literature review to research design, data\ncollection, analysis, interpretation, and dissemination. While particular\nattention is given to geographical research, the discussion extends to wider\nacademic contexts. A parallel strand addresses the pedagogical applications of\nGenAI, encompassing course and lesson design, teaching delivery, assessment,\nand feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges\nposed by GenAI. Issues of bias, intellectual property, governance, and\naccountability are assessed, alongside the ecological footprint of LLMs and\nemerging technological strategies for mitigation. The concluding section\nconsiders near- and long-term futures of GenAI, including scenarios of\nsustained adoption, regulation, and potential decline. By situating GenAI\nwithin both scholarly practice and educational contexts, the study contributes\nto critical debates on its transformative potential and societal\nresponsibilities."
                },
                "authors": [
                    {
                        "name": "Jussi S. Jauhiainen"
                    },
                    {
                        "name": "Aurora Toppari"
                    }
                ],
                "author_detail": {
                    "name": "Aurora Toppari"
                },
                "author": "Aurora Toppari",
                "arxiv_comment": "113 pages, 6 figures, 13 tables, 2 appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18905v1",
                "updated": "2025-08-26T10:22:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    22,
                    37,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:22:37Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    22,
                    37,
                    1,
                    238,
                    0
                ],
                "title": "Interactive Evaluation of Large Language Models for Multi-Requirement\n  Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Evaluation of Large Language Models for Multi-Requirement\n  Software Engineering Tasks"
                },
                "summary": "Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents."
                },
                "authors": [
                    {
                        "name": "Dimitrios Rontogiannis"
                    },
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Nicolas Baldwin"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Dimitrios Gunopulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Gunopulos"
                },
                "author": "Dimitrios Gunopulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09396v2",
                "updated": "2025-08-26T10:19:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    19,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-14T13:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    51,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners"
                },
                "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation."
                },
                "authors": [
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18596v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18596v4",
                "updated": "2025-08-26T10:08:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    8,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-24T08:44:33Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    8,
                    44,
                    33,
                    5,
                    144,
                    0
                ],
                "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models"
                },
                "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication."
                },
                "authors": [
                    {
                        "name": "Chen Han"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xijin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xijin Tang"
                },
                "author": "Xijin Tang",
                "arxiv_comment": "This paper has been accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18596v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18891v1",
                "updated": "2025-08-26T10:05:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    5,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:05:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    5,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "pyFAST: A Modular PyTorch Framework for Time Series Modeling with\n  Multi-source and Sparse Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pyFAST: A Modular PyTorch Framework for Time Series Modeling with\n  Multi-source and Sparse Data"
                },
                "summary": "Modern time series analysis demands frameworks that are flexible, efficient,\nand extensible. However, many existing Python libraries exhibit limitations in\nmodularity and in their native support for irregular, multi-source, or sparse\ndata. We introduce pyFAST, a research-oriented PyTorch framework that\nexplicitly decouples data processing from model computation, fostering a\ncleaner separation of concerns and facilitating rapid experimentation. Its data\nengine is engineered for complex scenarios, supporting multi-source loading,\nprotein sequence handling, efficient sequence- and patch-level padding, dynamic\nnormalization, and mask-based modeling for both imputation and forecasting.\npyFAST integrates LLM-inspired architectures for the alignment-free fusion of\nsparse data sources and offers native sparse metrics, specialized loss\nfunctions, and flexible exogenous data fusion. Training utilities include\nbatch-based streaming aggregation for evaluation and device synergy to maximize\ncomputational efficiency. A comprehensive suite of classical and deep learning\nmodels (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a\nmodular architecture that encourages extension. Released under the MIT license\nat GitHub, pyFAST provides a compact yet powerful platform for advancing time\nseries research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern time series analysis demands frameworks that are flexible, efficient,\nand extensible. However, many existing Python libraries exhibit limitations in\nmodularity and in their native support for irregular, multi-source, or sparse\ndata. We introduce pyFAST, a research-oriented PyTorch framework that\nexplicitly decouples data processing from model computation, fostering a\ncleaner separation of concerns and facilitating rapid experimentation. Its data\nengine is engineered for complex scenarios, supporting multi-source loading,\nprotein sequence handling, efficient sequence- and patch-level padding, dynamic\nnormalization, and mask-based modeling for both imputation and forecasting.\npyFAST integrates LLM-inspired architectures for the alignment-free fusion of\nsparse data sources and offers native sparse metrics, specialized loss\nfunctions, and flexible exogenous data fusion. Training utilities include\nbatch-based streaming aggregation for evaluation and device synergy to maximize\ncomputational efficiency. A comprehensive suite of classical and deep learning\nmodels (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a\nmodular architecture that encourages extension. Released under the MIT license\nat GitHub, pyFAST provides a compact yet powerful platform for advancing time\nseries research and applications."
                },
                "authors": [
                    {
                        "name": "Zhijin Wang"
                    },
                    {
                        "name": "Senzhen Wu"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Xiufeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiufeng Liu"
                },
                "author": "Xiufeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18884v1",
                "updated": "2025-08-26T09:59:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    59,
                    44,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:59:44Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    59,
                    44,
                    1,
                    238,
                    0
                ],
                "title": "HAEPO: History-Aggregated Exploratory Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAEPO: History-Aggregated Exploratory Policy Optimization"
                },
                "summary": "Exploration is essential in modern learning, from reinforcement learning\nenvironments with small neural policies to large language models (LLMs).\nExisting work, such as DPO, leverages full sequence log-likelihoods to capture\nan entire trajectory of the model's decisions, while methods like GRPO\naggregate per-token ratios into a trajectory-level update. However, both often\nlimit exploration on long-horizon tasks. We introduce History-Aggregated\nExploratory Policy Optimization (HAEPO), a history-aware exploratory loss to\ncombat these shortcomings. HAEPO compresses each trajectory into the sum of its\nlogarithmic probabilities (a cumulative logarithmic likelihood), and applies a\nPlackett-Luce softmax across trajectories to obtain normalized weights\nproportional to their returns, thus encouraging broader exploration. We add\nentropy regularization to stabilize the aggressive updates to prevent premature\ncollapse and a soft KL penalty relative to a frozen copy of the previous\n(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,\naligns closely with true rewards, and demonstrates robust learning behavior\nbetter or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO\nprovides a stable and interpretable framework by explicitly leveraging\nfull-trajectory history while balancing exploration and stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration is essential in modern learning, from reinforcement learning\nenvironments with small neural policies to large language models (LLMs).\nExisting work, such as DPO, leverages full sequence log-likelihoods to capture\nan entire trajectory of the model's decisions, while methods like GRPO\naggregate per-token ratios into a trajectory-level update. However, both often\nlimit exploration on long-horizon tasks. We introduce History-Aggregated\nExploratory Policy Optimization (HAEPO), a history-aware exploratory loss to\ncombat these shortcomings. HAEPO compresses each trajectory into the sum of its\nlogarithmic probabilities (a cumulative logarithmic likelihood), and applies a\nPlackett-Luce softmax across trajectories to obtain normalized weights\nproportional to their returns, thus encouraging broader exploration. We add\nentropy regularization to stabilize the aggressive updates to prevent premature\ncollapse and a soft KL penalty relative to a frozen copy of the previous\n(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,\naligns closely with true rewards, and demonstrates robust learning behavior\nbetter or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO\nprovides a stable and interpretable framework by explicitly leveraging\nfull-trajectory history while balancing exploration and stability."
                },
                "authors": [
                    {
                        "name": "Gaurish Trivedi"
                    },
                    {
                        "name": "Alakh Sharma"
                    },
                    {
                        "name": "Kartikey Singh Bhandari"
                    },
                    {
                        "name": "Dhruv Kumar"
                    },
                    {
                        "name": "Pratik Narang"
                    },
                    {
                        "name": "Jagat Sesh Challa"
                    }
                ],
                "author_detail": {
                    "name": "Jagat Sesh Challa"
                },
                "author": "Jagat Sesh Challa",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18880v1",
                "updated": "2025-08-26T09:56:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    56,
                    26,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:56:26Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    56,
                    26,
                    1,
                    238,
                    0
                ],
                "title": "Judicial Requirements for Generative AI in Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judicial Requirements for Generative AI in Legal Reasoning"
                },
                "summary": "Large Language Models (LLMs) are being integrated into professional domains,\nyet their limitations in high-stakes fields like law remain poorly understood.\nThis paper defines the core capabilities that an AI system must possess to\nfunction as a reliable reasoning tool in judicial decision-making. Using the\nIRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the\nstudy focuses on the most challenging phases of legal adjudication: determining\nthe applicable Rule (R) and performing the Application (A) of that rule to the\nfacts of a case. From a judicial perspective, the analysis deconstructs legal\nreasoning into a series of core requirements, including the ability to select\nthe correct legal framework across jurisdictions, generate sound arguments\nbased on the doctrine of legal sources, distinguish ratio decidendi from obiter\ndictum in case law, resolve ambiguity arising from general clauses like\n\"reasonableness\", manage conflicting legal provisions, and correctly apply the\nburden of proof. The paper then maps various AI enhancement mechanisms, such as\nRetrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic\nAI, to these requirements, assessing their potential to bridge the gap between\nthe probabilistic nature of LLMs and the rigorous, choice-driven demands of\nlegal interpretation. The findings indicate that while these techniques can\naddress specific challenges, significant challenges remain, particularly in\ntasks requiring discretion and transparent, justifiable reasoning. Our paper\nconcludes that the most effective current role for AI in law is a dual one: as\na high-volume assistant for simple, repetitive cases and as a sophisticated\n\"sparring partner\" for human experts in complex matters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being integrated into professional domains,\nyet their limitations in high-stakes fields like law remain poorly understood.\nThis paper defines the core capabilities that an AI system must possess to\nfunction as a reliable reasoning tool in judicial decision-making. Using the\nIRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the\nstudy focuses on the most challenging phases of legal adjudication: determining\nthe applicable Rule (R) and performing the Application (A) of that rule to the\nfacts of a case. From a judicial perspective, the analysis deconstructs legal\nreasoning into a series of core requirements, including the ability to select\nthe correct legal framework across jurisdictions, generate sound arguments\nbased on the doctrine of legal sources, distinguish ratio decidendi from obiter\ndictum in case law, resolve ambiguity arising from general clauses like\n\"reasonableness\", manage conflicting legal provisions, and correctly apply the\nburden of proof. The paper then maps various AI enhancement mechanisms, such as\nRetrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic\nAI, to these requirements, assessing their potential to bridge the gap between\nthe probabilistic nature of LLMs and the rigorous, choice-driven demands of\nlegal interpretation. The findings indicate that while these techniques can\naddress specific challenges, significant challenges remain, particularly in\ntasks requiring discretion and transparent, justifiable reasoning. Our paper\nconcludes that the most effective current role for AI in law is a dual one: as\na high-volume assistant for simple, repetitive cases and as a sophisticated\n\"sparring partner\" for human experts in complex matters."
                },
                "authors": [
                    {
                        "name": "Eljas Linna"
                    },
                    {
                        "name": "Tuula Linna"
                    }
                ],
                "author_detail": {
                    "name": "Tuula Linna"
                },
                "author": "Tuula Linna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2207.13340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2207.13340v2",
                "updated": "2025-08-26T09:55:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    55,
                    56,
                    1,
                    238,
                    0
                ],
                "published": "2022-07-27T07:48:29Z",
                "published_parsed": [
                    2022,
                    7,
                    27,
                    7,
                    48,
                    29,
                    2,
                    208,
                    0
                ],
                "title": "PointFix: Learning to Fix Domain Bias for Robust Online Stereo\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PointFix: Learning to Fix Domain Bias for Robust Online Stereo\n  Adaptation"
                },
                "summary": "Online stereo adaptation tackles the domain shift problem, caused by\ndifferent environments between synthetic (training) and real (test) datasets,\nto promptly adapt stereo models in dynamic real-world applications such as\nautonomous driving. However, previous methods often fail to counteract\nparticular regions related to dynamic objects with more severe environmental\nchanges. To mitigate this issue, we propose to incorporate an auxiliary\npoint-selective network into a meta-learning framework, called PointFix, to\nprovide a robust initialization of stereo models for online stereo adaptation.\nIn a nutshell, our auxiliary network learns to fix local variants intensively\nby effectively back-propagating local information through the meta-gradient for\nthe robust initialization of the baseline model. This network is\nmodel-agnostic, so can be used in any kind of architectures in a plug-and-play\nmanner. We conduct extensive experiments to verify the effectiveness of our\nmethod under three adaptation settings such as short-, mid-, and long-term\nsequences. Experimental results show that the proper initialization of the base\nstereo model by the auxiliary network enables our learning paradigm to achieve\nstate-of-the-art performance at inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online stereo adaptation tackles the domain shift problem, caused by\ndifferent environments between synthetic (training) and real (test) datasets,\nto promptly adapt stereo models in dynamic real-world applications such as\nautonomous driving. However, previous methods often fail to counteract\nparticular regions related to dynamic objects with more severe environmental\nchanges. To mitigate this issue, we propose to incorporate an auxiliary\npoint-selective network into a meta-learning framework, called PointFix, to\nprovide a robust initialization of stereo models for online stereo adaptation.\nIn a nutshell, our auxiliary network learns to fix local variants intensively\nby effectively back-propagating local information through the meta-gradient for\nthe robust initialization of the baseline model. This network is\nmodel-agnostic, so can be used in any kind of architectures in a plug-and-play\nmanner. We conduct extensive experiments to verify the effectiveness of our\nmethod under three adaptation settings such as short-, mid-, and long-term\nsequences. Experimental results show that the proper initialization of the base\nstereo model by the auxiliary network enables our learning paradigm to achieve\nstate-of-the-art performance at inference."
                },
                "authors": [
                    {
                        "name": "Kwonyoung Kim"
                    },
                    {
                        "name": "Jungin Park"
                    },
                    {
                        "name": "Jiyoung Lee"
                    },
                    {
                        "name": "Dongbo Min"
                    },
                    {
                        "name": "Kwanghoon Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Kwanghoon Sohn"
                },
                "author": "Kwanghoon Sohn",
                "arxiv_comment": "Accepted to ECCV 2022",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2207.13340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2207.13340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18877v1",
                "updated": "2025-08-26T09:51:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    51,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:51:02Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    51,
                    2,
                    1,
                    238,
                    0
                ],
                "title": "Optimization of Latent-Space Compression using Game-Theoretic Techniques\n  for Transformer-Based Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization of Latent-Space Compression using Game-Theoretic Techniques\n  for Transformer-Based Vector Search"
                },
                "summary": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval."
                },
                "authors": [
                    {
                        "name": "Kushagra Agrawal"
                    },
                    {
                        "name": "Nisharg Nargund"
                    },
                    {
                        "name": "Oishani Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Oishani Banerjee"
                },
                "author": "Oishani Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18872v1",
                "updated": "2025-08-26T09:46:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    59,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:46:59Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    59,
                    1,
                    238,
                    0
                ],
                "title": "Empowering Computing Education Researchers Through LLM-Assisted Content\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Computing Education Researchers Through LLM-Assisted Content\n  Analysis"
                },
                "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline."
                },
                "authors": [
                    {
                        "name": "Laurie Gale"
                    },
                    {
                        "name": "Sebastian Mateos Nicolajsen"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Mateos Nicolajsen"
                },
                "author": "Sebastian Mateos Nicolajsen",
                "arxiv_comment": "7 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18870v1",
                "updated": "2025-08-26T09:46:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    20,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:46:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflectivePrompt: Reflective evolution in autoprompting algorithms"
                },
                "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting."
                },
                "authors": [
                    {
                        "name": "Viktor N. Zhuravlev"
                    },
                    {
                        "name": "Artur R. Khairullin"
                    },
                    {
                        "name": "Ernest A. Dyagin"
                    },
                    {
                        "name": "Alena N. Sitkina"
                    },
                    {
                        "name": "Nikita I. Kulin"
                    }
                ],
                "author_detail": {
                    "name": "Nikita I. Kulin"
                },
                "author": "Nikita I. Kulin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18859v1",
                "updated": "2025-08-26T09:38:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    38,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:38:29Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    38,
                    29,
                    1,
                    238,
                    0
                ],
                "title": "Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Meta-Learning for Controllable Full-Frame Video Stabilization"
                },
                "summary": "Video stabilization remains a fundamental problem in computer vision,\nparticularly pixel-level synthesis solutions for video stabilization, which\nsynthesize full-frame outputs, add to the complexity of this task. These\nmethods aim to enhance stability while synthesizing full-frame videos, but the\ninherent diversity in motion profiles and visual content present in each video\nsequence makes robust generalization with fixed parameters difficult. To\naddress this, we present a novel method that improves pixel-level synthesis\nvideo stabilization methods by rapidly adapting models to each input video at\ntest time. The proposed approach takes advantage of low-level visual cues\navailable during inference to improve both the stability and visual quality of\nthe output. Notably, the proposed rapid adaptation achieves significant\nperformance gains even with a single adaptation pass. We further propose a jerk\nlocalization module and a targeted adaptation strategy, which focuses the\nadaptation on high-jerk segments for maximizing stability with fewer adaptation\nsteps. The proposed methodology enables modern stabilizers to overcome the\nlongstanding SOTA approaches while maintaining the full frame nature of the\nmodern methods, while offering users with control mechanisms akin to classical\napproaches. Extensive experiments on diverse real-world datasets demonstrate\nthe versatility of the proposed method. Our approach consistently improves the\nperformance of various full-frame synthesis models in both qualitative and\nquantitative terms, including results on downstream applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video stabilization remains a fundamental problem in computer vision,\nparticularly pixel-level synthesis solutions for video stabilization, which\nsynthesize full-frame outputs, add to the complexity of this task. These\nmethods aim to enhance stability while synthesizing full-frame videos, but the\ninherent diversity in motion profiles and visual content present in each video\nsequence makes robust generalization with fixed parameters difficult. To\naddress this, we present a novel method that improves pixel-level synthesis\nvideo stabilization methods by rapidly adapting models to each input video at\ntest time. The proposed approach takes advantage of low-level visual cues\navailable during inference to improve both the stability and visual quality of\nthe output. Notably, the proposed rapid adaptation achieves significant\nperformance gains even with a single adaptation pass. We further propose a jerk\nlocalization module and a targeted adaptation strategy, which focuses the\nadaptation on high-jerk segments for maximizing stability with fewer adaptation\nsteps. The proposed methodology enables modern stabilizers to overcome the\nlongstanding SOTA approaches while maintaining the full frame nature of the\nmodern methods, while offering users with control mechanisms akin to classical\napproaches. Extensive experiments on diverse real-world datasets demonstrate\nthe versatility of the proposed method. Our approach consistently improves the\nperformance of various full-frame synthesis models in both qualitative and\nquantitative terms, including results on downstream applications."
                },
                "authors": [
                    {
                        "name": "Muhammad Kashif Ali"
                    },
                    {
                        "name": "Eun Woo Im"
                    },
                    {
                        "name": "Dongjin Kim"
                    },
                    {
                        "name": "Tae Hyun Kim"
                    },
                    {
                        "name": "Vivek Gupta"
                    },
                    {
                        "name": "Haonan Luo"
                    },
                    {
                        "name": "Tianrui Li"
                    }
                ],
                "author_detail": {
                    "name": "Tianrui Li"
                },
                "author": "Tianrui Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18853v1",
                "updated": "2025-08-26T09:34:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    34,
                    5,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:34:05Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    34,
                    5,
                    1,
                    238,
                    0
                ],
                "title": "Think before you fit: parameter identifiability, sensitivity and\n  uncertainty in systems biology models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think before you fit: parameter identifiability, sensitivity and\n  uncertainty in systems biology models"
                },
                "summary": "Reliable predictions from systems biology models require knowing whether\nparameters can be estimated from available data, and with what certainty.\nIdentifiability analysis reveals whether parameters are learnable in principle\n(structural identifiability) and in practice (practical identifiability). We\nintroduce the core ideas using linear models, highlighting how experimental\ndesign and output sensitivity shape identifiability. In nonlinear models,\nidentifiability can vary with parameter values, motivating global and\nsimulation-based approaches. We summarise computational methods for assessing\nidentifiability noting that weakly identifiable parameters can undermine\npredictions beyond the calibration dataset. Strategies to improve\nidentifiability include measuring different outputs, refining model structure,\nand adding prior knowledge. Far from a technical afterthought, identifiability\ndetermines the limits of inference and prediction. Recognising and addressing\nidentifiability is essential for building models that are not only well-fitted\nto data, but also capable of delivering predictions with robust, quantifiable\nuncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable predictions from systems biology models require knowing whether\nparameters can be estimated from available data, and with what certainty.\nIdentifiability analysis reveals whether parameters are learnable in principle\n(structural identifiability) and in practice (practical identifiability). We\nintroduce the core ideas using linear models, highlighting how experimental\ndesign and output sensitivity shape identifiability. In nonlinear models,\nidentifiability can vary with parameter values, motivating global and\nsimulation-based approaches. We summarise computational methods for assessing\nidentifiability noting that weakly identifiable parameters can undermine\npredictions beyond the calibration dataset. Strategies to improve\nidentifiability include measuring different outputs, refining model structure,\nand adding prior knowledge. Far from a technical afterthought, identifiability\ndetermines the limits of inference and prediction. Recognising and addressing\nidentifiability is essential for building models that are not only well-fitted\nto data, but also capable of delivering predictions with robust, quantifiable\nuncertainty."
                },
                "authors": [
                    {
                        "name": "Simon P. Preston"
                    },
                    {
                        "name": "Richard D. Wilkinson"
                    },
                    {
                        "name": "Richard H. Clayton"
                    },
                    {
                        "name": "Mike J. Chappell"
                    },
                    {
                        "name": "Gary R. Mirams"
                    }
                ],
                "author_detail": {
                    "name": "Gary R. Mirams"
                },
                "author": "Gary R. Mirams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P10, 92BXX",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18850v1",
                "updated": "2025-08-26T09:29:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    29,
                    23,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:29:23Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    29,
                    23,
                    1,
                    238,
                    0
                ],
                "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via\n  Cluster-Level Collective Primitive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via\n  Cluster-Level Collective Primitive"
                },
                "summary": "Large language model (LLM) decoding suffers from high latency due to\nfragmented execution across operators and heavy reliance on off-chip memory for\ndata exchange and reduction. This execution model limits opportunities for\nfusion and incurs significant memory traffic and kernel launch overhead. While\nmodern architectures such as NVIDIA Hopper provide distributed shared memory\nand low-latency intra-cluster interconnects, they expose only low-level data\nmovement instructions, lacking structured abstractions for collective on-chip\ncommunication. To bridge this software-hardware gap, we introduce two\ncluster-level communication primitives, ClusterReduce and ClusterGather, which\nabstract common communication patterns and enable structured, high-speed data\nexchange and reduction between thread blocks within a cluster, allowing\nintermediate results to be on-chip without involving off-chip memory. Building\non these abstractions, we design ClusterFusion, an execution framework that\nschedules communication and computation jointly to expand operator fusion scope\nby composing decoding stages such as QKV Projection, Attention, and Output\nProjection into a single fused kernels. Evaluations on H100 GPUs show that\nClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on\naverage in end-to-end latency across different models and configurations. The\nsource code is available at https://github.com/xinhao-luo/ClusterFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) decoding suffers from high latency due to\nfragmented execution across operators and heavy reliance on off-chip memory for\ndata exchange and reduction. This execution model limits opportunities for\nfusion and incurs significant memory traffic and kernel launch overhead. While\nmodern architectures such as NVIDIA Hopper provide distributed shared memory\nand low-latency intra-cluster interconnects, they expose only low-level data\nmovement instructions, lacking structured abstractions for collective on-chip\ncommunication. To bridge this software-hardware gap, we introduce two\ncluster-level communication primitives, ClusterReduce and ClusterGather, which\nabstract common communication patterns and enable structured, high-speed data\nexchange and reduction between thread blocks within a cluster, allowing\nintermediate results to be on-chip without involving off-chip memory. Building\non these abstractions, we design ClusterFusion, an execution framework that\nschedules communication and computation jointly to expand operator fusion scope\nby composing decoding stages such as QKV Projection, Attention, and Output\nProjection into a single fused kernels. Evaluations on H100 GPUs show that\nClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on\naverage in end-to-end latency across different models and configurations. The\nsource code is available at https://github.com/xinhao-luo/ClusterFusion."
                },
                "authors": [
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shihan Fang"
                    },
                    {
                        "name": "Ziyu Huang"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.14940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14940v2",
                "updated": "2025-08-26T17:59:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-20T02:59:39Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    2,
                    59,
                    39,
                    2,
                    232,
                    0
                ],
                "title": "Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using\n  a Retrieval-Augmented Model Selection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cohort-Aware Agents for Individualized Lung Cancer Risk Prediction Using\n  a Retrieval-Augmented Model Selection Framework"
                },
                "summary": "Accurate lung cancer risk prediction remains challenging due to substantial\nvariability across patient populations and clinical settings -- no single model\nperforms best for all cohorts. To address this, we propose a personalized lung\ncancer risk prediction agent that dynamically selects the most appropriate\nmodel for each patient by combining cohort-specific knowledge with modern\nretrieval and reasoning techniques. Given a patient's CT scan and structured\nmetadata -- including demographic, clinical, and nodule-level features -- the\nagent first performs cohort retrieval using FAISS-based similarity search\nacross nine diverse real-world cohorts to identify the most relevant patient\npopulation from a multi-institutional database. Second, a Large Language Model\n(LLM) is prompted with the retrieved cohort and its associated performance\nmetrics to recommend the optimal prediction algorithm from a pool of eight\nrepresentative models, including classical linear risk models (e.g., Mayo,\nBrock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computer\nvision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent\npipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,\ncohort-aware risk prediction personalized to each patient's profile. Building\non this architecture, the agent supports flexible and cohort-driven model\nselection across diverse clinical populations, offering a practical path toward\nindividualized risk assessment in real-world lung cancer screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate lung cancer risk prediction remains challenging due to substantial\nvariability across patient populations and clinical settings -- no single model\nperforms best for all cohorts. To address this, we propose a personalized lung\ncancer risk prediction agent that dynamically selects the most appropriate\nmodel for each patient by combining cohort-specific knowledge with modern\nretrieval and reasoning techniques. Given a patient's CT scan and structured\nmetadata -- including demographic, clinical, and nodule-level features -- the\nagent first performs cohort retrieval using FAISS-based similarity search\nacross nine diverse real-world cohorts to identify the most relevant patient\npopulation from a multi-institutional database. Second, a Large Language Model\n(LLM) is prompted with the retrieved cohort and its associated performance\nmetrics to recommend the optimal prediction algorithm from a pool of eight\nrepresentative models, including classical linear risk models (e.g., Mayo,\nBrock), temporally-aware models (e.g., TD-VIT, DLSTM), and multi-modal computer\nvision-based approaches (e.g., Liao, Sybil, DLS, DLI). This two-stage agent\npipeline -- retrieval via FAISS and reasoning via LLM -- enables dynamic,\ncohort-aware risk prediction personalized to each patient's profile. Building\non this architecture, the agent supports flexible and cohort-driven model\nselection across diverse clinical populations, offering a practical path toward\nindividualized risk assessment in real-world lung cancer screening."
                },
                "authors": [
                    {
                        "name": "Chongyu Qu"
                    },
                    {
                        "name": "Allen J. Luna"
                    },
                    {
                        "name": "Thomas Z. Li"
                    },
                    {
                        "name": "Junchao Zhu"
                    },
                    {
                        "name": "Junlin Guo"
                    },
                    {
                        "name": "Juming Xiong"
                    },
                    {
                        "name": "Kim L. Sandler"
                    },
                    {
                        "name": "Bennett A. Landman"
                    },
                    {
                        "name": "Yuankai Huo"
                    }
                ],
                "author_detail": {
                    "name": "Yuankai Huo"
                },
                "author": "Yuankai Huo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19227v1",
                "updated": "2025-08-26T17:43:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    43,
                    20,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:43:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    43,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "Generative Interfaces for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Interfaces for Language Models"
                },
                "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction."
                },
                "authors": [
                    {
                        "name": "Jiaqi Chen"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Yutong Zhang"
                    },
                    {
                        "name": "Yijia Shao"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14252v2",
                "updated": "2025-08-26T17:22:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    22,
                    52,
                    1,
                    238,
                    0
                ],
                "published": "2024-11-21T15:59:29Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    15,
                    59,
                    29,
                    3,
                    326,
                    0
                ],
                "title": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Intents to Conversations: Generating Intent-Driven Dialogues with\n  Contrastive Learning for Multi-Turn Classification"
                },
                "summary": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In conversational AI systems, a critical challenge in training effective\nmulti-turn intent classification models lies in the generation of large-scale,\ndomain-specific, multilingual dialogue datasets. In this paper, we introduce\nChain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs)\nwith Large Language Models (LLMs) to generate intent-driven, context-aware\ndialogues through self-play. Our method first extracts domain-specific intent\ntransition patterns from real-world e-commerce chat logs, which guide the\nmodeling of turn-level dynamics and intent sequences. LLMs are then employed to\nparameterize the emission probabilities of HMMs, enabling the generation of\nnatural, coherent utterances aligned with predicted intents and dialogue\ncontext. We further propose MINT-CL, a multi-task contrastive learning\nframework for multi-turn intent classification, which improves performance\nwhile reducing dependence on large-scale annotated datasets. Empirical results\ndemonstrate that our approach outperforms competitive baselines in both\ndialogue generation quality and classification accuracy, particularly in\nmultilingual settings. To facilitate future research, we release MINT-E, a\ncomprehensive, multilingual, intent-aware multi-turn dialogue corpus derived\nfrom the e-commerce domain. The reproduced source code and dataset are\navailable at https://github.com/junhua/chain-of-intent."
                },
                "authors": [
                    {
                        "name": "Junhua Liu"
                    },
                    {
                        "name": "Yong Keat Tan"
                    },
                    {
                        "name": "Bin Fu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "arxiv_comment": "Accepted to Proceedings of CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16839v2",
                "updated": "2025-08-26T17:13:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    13,
                    21,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-22T23:34:37Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    23,
                    34,
                    37,
                    4,
                    234,
                    0
                ],
                "title": "Route-and-Execute: Auditable Model-Card Matching and Specialty-Level\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Route-and-Execute: Auditable Model-Card Matching and Specialty-Level\n  Deployment"
                },
                "summary": "Clinical workflows are fragmented as a patchwork of scripts and task-specific\nnetworks that often handle triage, task selection, and model deployment. These\npipelines are rarely streamlined for data science pipeline, reducing efficiency\nand raising operational costs. Workflows also lack data-driven model\nidentification (from imaging/tabular inputs) and standardized delivery of model\noutputs. In response, we present a practical, healthcare-first framework that\nuses a single vision-language model (VLM) in two complementary roles. First\n(Solution 1), the VLM acts as an aware model-card matcher that routes an\nincoming image to the appropriate specialist model via a three-stage workflow\n(modality -> primary abnormality -> model-card id). Checks are provided by (i)\nstagewise prompts that allow early exit via None/Normal/Other and (ii) a\nstagewise answer selector that arbitrates between the top-2 candidates at each\nstage, reducing the chance of an incorrect selection and aligning the workflow\nwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on\nspecialty-specific datasets ensuring a single model covers multiple downstream\ntasks within each specialty, maintaining performance while simplifying\ndeployment. Across gastroenterology, hematology, ophthalmology, and pathology,\nour single-model deployment matches or approaches specialized baselines.\n  Compared with pipelines composed of many task-specific agents, this approach\nshows that one VLM can both decide and do. It may reduce effort by data\nscientists, shorten monitoring, increase the transparency of model selection\n(with per-stage justifications), and lower integration overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical workflows are fragmented as a patchwork of scripts and task-specific\nnetworks that often handle triage, task selection, and model deployment. These\npipelines are rarely streamlined for data science pipeline, reducing efficiency\nand raising operational costs. Workflows also lack data-driven model\nidentification (from imaging/tabular inputs) and standardized delivery of model\noutputs. In response, we present a practical, healthcare-first framework that\nuses a single vision-language model (VLM) in two complementary roles. First\n(Solution 1), the VLM acts as an aware model-card matcher that routes an\nincoming image to the appropriate specialist model via a three-stage workflow\n(modality -> primary abnormality -> model-card id). Checks are provided by (i)\nstagewise prompts that allow early exit via None/Normal/Other and (ii) a\nstagewise answer selector that arbitrates between the top-2 candidates at each\nstage, reducing the chance of an incorrect selection and aligning the workflow\nwith clinical risk tolerance. Second (Solution 2), we fine-tune the VLM on\nspecialty-specific datasets ensuring a single model covers multiple downstream\ntasks within each specialty, maintaining performance while simplifying\ndeployment. Across gastroenterology, hematology, ophthalmology, and pathology,\nour single-model deployment matches or approaches specialized baselines.\n  Compared with pipelines composed of many task-specific agents, this approach\nshows that one VLM can both decide and do. It may reduce effort by data\nscientists, shorten monitoring, increase the transparency of model selection\n(with per-stage justifications), and lower integration overhead."
                },
                "authors": [
                    {
                        "name": "Shayan Vassef"
                    },
                    {
                        "name": "Soorya Ram Shimegekar"
                    },
                    {
                        "name": "Abhay Goyal"
                    },
                    {
                        "name": "Koustuv Saha"
                    },
                    {
                        "name": "Pi Zonooz"
                    },
                    {
                        "name": "Navin Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Navin Kumar"
                },
                "author": "Navin Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13358v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13358v3",
                "updated": "2025-08-26T17:11:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    11,
                    42,
                    1,
                    238,
                    0
                ],
                "published": "2025-02-19T01:41:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    1,
                    41,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text\n  Modifications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text\n  Modifications"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating strong capabilities in tasks such as text generation,\nsummarization, and reasoning. Recently, their potential for automating precise\ntext editing tasks across specialized domains, such as programming code, LaTeX,\nand structured database languages, has gained attention. However, current\nstate-of-the-art LLMs still struggle with executing precise, instruction-driven\nedits, particularly when structural accuracy and strict adherence to domain\nconventions are required. To address these challenges, we introduce\nInstrEditBench, an automated benchmark dataset comprising over 30,000\nstructured editing tasks spanning diverse domains, including Wikipedia\narticles, LaTeX documents, source code, and database languages. Using this\nbenchmark, we develop FineEdit, a specialized editing model explicitly trained\nfor accurate, context-aware text modifications. Experimental evaluations\ndemonstrate that FineEdit outperforms state-of-the-art models, achieving\nimprovements of approximately 10\\% over Gemini models on single-turn edits, up\nto 30\\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by\nover 40\\% on direct editing tasks. FineEdit also effectively generalizes to\nrealistic multi-turn editing scenarios, highlighting its practical\napplicability. To facilitate further research and reproducibility, we release\nFineEdit at https://github.com/StuRinDQB/FineEdit} and\nhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing, demonstrating strong capabilities in tasks such as text generation,\nsummarization, and reasoning. Recently, their potential for automating precise\ntext editing tasks across specialized domains, such as programming code, LaTeX,\nand structured database languages, has gained attention. However, current\nstate-of-the-art LLMs still struggle with executing precise, instruction-driven\nedits, particularly when structural accuracy and strict adherence to domain\nconventions are required. To address these challenges, we introduce\nInstrEditBench, an automated benchmark dataset comprising over 30,000\nstructured editing tasks spanning diverse domains, including Wikipedia\narticles, LaTeX documents, source code, and database languages. Using this\nbenchmark, we develop FineEdit, a specialized editing model explicitly trained\nfor accurate, context-aware text modifications. Experimental evaluations\ndemonstrate that FineEdit outperforms state-of-the-art models, achieving\nimprovements of approximately 10\\% over Gemini models on single-turn edits, up\nto 30\\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by\nover 40\\% on direct editing tasks. FineEdit also effectively generalizes to\nrealistic multi-turn editing scenarios, highlighting its practical\napplicability. To facilitate further research and reproducibility, we release\nFineEdit at https://github.com/StuRinDQB/FineEdit} and\nhttps://huggingface.co/datasets/YimingZeng/FineEdit_bench."
                },
                "authors": [
                    {
                        "name": "Yiming Zeng"
                    },
                    {
                        "name": "Wanhao Yu"
                    },
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Tao Ren"
                    },
                    {
                        "name": "Yu Ma"
                    },
                    {
                        "name": "Jinghan Cao"
                    },
                    {
                        "name": "Xiyan Chen"
                    },
                    {
                        "name": "Tingting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yu"
                },
                "author": "Tingting Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13358v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13358v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19203v1",
                "updated": "2025-08-26T17:04:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    4,
                    25,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:04:25Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    4,
                    25,
                    1,
                    238,
                    0
                ],
                "title": "Optimizing Highway Traffic Flow in Mixed Autonomy: A Multiagent\n  Truncated Rollout Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Highway Traffic Flow in Mixed Autonomy: A Multiagent\n  Truncated Rollout Approach"
                },
                "summary": "The development of connected and autonomous vehicles (CAVs) offers\nsubstantial opportunities to enhance traffic efficiency. However, in mixed\nautonomy environments where CAVs coexist with human-driven vehicles (HDVs),\nachieving efficient coordination among CAVs remains challenging due to\nheterogeneous driving behaviors. To address this, this paper proposes a\nmultiagent truncated rollout approach that enhances CAV speed coordination to\nimprove highway throughput while reducing computational overhead. In this\napproach, a traffic density evolution equation is formulated that\ncomprehensively accounts for the presence or absence of CAVs, and a distributed\ncoordination control framework is established accordingly. By incorporating\nkinematic information from neighbor agents and employing an agent-by-agent\nsequential solution mechanism, our method enables explicit cooperation among\nCAVs. Furthermore, we introduce a truncated rollout scheme that adaptively\nshortens the optimization horizon based on the evaluation of control sequences.\nThis significantly reduces the time complexity, thereby improving real-time\nperformance and scalability. Theoretical analysis provides rigorous guarantees\non the stability and performance improvement of the system. Simulations\nconducted on real-world bottleneck scenarios demonstrate that, in large-scale\nmixed traffic flows, the proposed method outperforms conventional model\npredictive control methods by reducing both the average travel time in the\nbottleneck area and overall computational time, highlighting its strong\npotential for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of connected and autonomous vehicles (CAVs) offers\nsubstantial opportunities to enhance traffic efficiency. However, in mixed\nautonomy environments where CAVs coexist with human-driven vehicles (HDVs),\nachieving efficient coordination among CAVs remains challenging due to\nheterogeneous driving behaviors. To address this, this paper proposes a\nmultiagent truncated rollout approach that enhances CAV speed coordination to\nimprove highway throughput while reducing computational overhead. In this\napproach, a traffic density evolution equation is formulated that\ncomprehensively accounts for the presence or absence of CAVs, and a distributed\ncoordination control framework is established accordingly. By incorporating\nkinematic information from neighbor agents and employing an agent-by-agent\nsequential solution mechanism, our method enables explicit cooperation among\nCAVs. Furthermore, we introduce a truncated rollout scheme that adaptively\nshortens the optimization horizon based on the evaluation of control sequences.\nThis significantly reduces the time complexity, thereby improving real-time\nperformance and scalability. Theoretical analysis provides rigorous guarantees\non the stability and performance improvement of the system. Simulations\nconducted on real-world bottleneck scenarios demonstrate that, in large-scale\nmixed traffic flows, the proposed method outperforms conventional model\npredictive control methods by reducing both the average travel time in the\nbottleneck area and overall computational time, highlighting its strong\npotential for practical deployment."
                },
                "authors": [
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Xi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Xi Xiong"
                },
                "author": "Xi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19202v1",
                "updated": "2025-08-26T17:04:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    4,
                    23,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:04:23Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    4,
                    23,
                    1,
                    238,
                    0
                ],
                "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and\n  Reasoning"
                },
                "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning."
                },
                "authors": [
                    {
                        "name": "Alan Li"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Arpan Sarkar"
                    },
                    {
                        "name": "Doug Downey"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "28 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19201v1",
                "updated": "2025-08-26T17:03:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    46,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:03:46Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    46,
                    1,
                    238,
                    0
                ],
                "title": "Understanding Tool-Integrated Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Tool-Integrated Reasoning"
                },
                "summary": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning."
                },
                "authors": [
                    {
                        "name": "Heng Lin"
                    },
                    {
                        "name": "Zhongwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongwen Xu"
                },
                "author": "Zhongwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19200v1",
                "updated": "2025-08-26T17:03:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:03:43Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    3,
                    43,
                    1,
                    238,
                    0
                ],
                "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ramon Llull's Thinking Machine for Automated Ideation"
                },
                "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI."
                },
                "authors": [
                    {
                        "name": "Xinran Zhao"
                    },
                    {
                        "name": "Boyuan Zheng"
                    },
                    {
                        "name": "Chenglei Si"
                    },
                    {
                        "name": "Haofei Yu"
                    },
                    {
                        "name": "Ken Liu"
                    },
                    {
                        "name": "Runlong Zhou"
                    },
                    {
                        "name": "Ruochen Li"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "21 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19183v1",
                "updated": "2025-08-26T16:41:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    41,
                    4,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T16:41:04Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    41,
                    4,
                    1,
                    238,
                    0
                ],
                "title": "Get Global Guarantees: On the Probabilistic Nature of Perturbation\n  Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Get Global Guarantees: On the Probabilistic Nature of Perturbation\n  Robustness"
                },
                "summary": "In safety-critical deep learning applications, robustness measures the\nability of neural models that handle imperceptible perturbations in input data,\nwhich may lead to potential safety hazards. Existing pre-deployment robustness\nassessment methods typically suffer from significant trade-offs between\ncomputational cost and measurement precision, limiting their practical utility.\nTo address these limitations, this paper conducts a comprehensive comparative\nanalysis of existing robustness definitions and associated assessment\nmethodologies. We propose tower robustness to evaluate robustness, which is a\nnovel, practical metric based on hypothesis testing to quantitatively evaluate\nprobabilistic robustness, enabling more rigorous and efficient pre-deployment\nassessments. Our extensive comparative evaluation illustrates the advantages\nand applicability of our proposed approach, thereby advancing the systematic\nunderstanding and enhancement of model robustness in safety-critical deep\nlearning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In safety-critical deep learning applications, robustness measures the\nability of neural models that handle imperceptible perturbations in input data,\nwhich may lead to potential safety hazards. Existing pre-deployment robustness\nassessment methods typically suffer from significant trade-offs between\ncomputational cost and measurement precision, limiting their practical utility.\nTo address these limitations, this paper conducts a comprehensive comparative\nanalysis of existing robustness definitions and associated assessment\nmethodologies. We propose tower robustness to evaluate robustness, which is a\nnovel, practical metric based on hypothesis testing to quantitatively evaluate\nprobabilistic robustness, enabling more rigorous and efficient pre-deployment\nassessments. Our extensive comparative evaluation illustrates the advantages\nand applicability of our proposed approach, thereby advancing the systematic\nunderstanding and enhancement of model robustness in safety-critical deep\nlearning applications."
                },
                "authors": [
                    {
                        "name": "Wenchuan Mu"
                    },
                    {
                        "name": "Kwan Hui Lim"
                    }
                ],
                "author_detail": {
                    "name": "Kwan Hui Lim"
                },
                "author": "Kwan Hui Lim",
                "arxiv_doi": "10.1145/3746252.3761039",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761039",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15495v2",
                "updated": "2025-08-26T16:40:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    40,
                    49,
                    1,
                    238,
                    0
                ],
                "published": "2024-12-20T02:21:36Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    2,
                    21,
                    36,
                    4,
                    355,
                    0
                ],
                "title": "TL-Training: A Task-Feature-Based Framework for Training Large Language\n  Models in Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TL-Training: A Task-Feature-Based Framework for Training Large Language\n  Models in Tool Use"
                },
                "summary": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) achieve remarkable advancements by leveraging\ntools to interact with environments, a critical step toward generalized AI.\nHowever, the standard supervised fine-tuning (SFT) approach, which relies on\nlarge-scale datasets, often overlooks task-specific characteristics in tool\nuse, leading to performance bottlenecks. To address this issue, we analyze\nthree existing LLMs and uncover key insights: training data can inadvertently\nimpede tool-use behavior, token importance is distributed unevenly, and errors\nin tool calls fall into a small set of categories. Building on these findings,\nwe propose~\\emph{TL-Training}, a task-feature-based framework that mitigates\nthe effects of suboptimal training data, dynamically adjusts token weights to\nprioritize key tokens during SFT, and incorporates a robust reward mechanism\ntailored to error categories, optimized through proximal policy optimization.\nWe validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four\nopen-source test sets. Our results demonstrate that the LLM trained by our\nmethod matches or surpasses both open- and closed-source LLMs in tool-use\nperformance using only 1,217 training data points. Additionally, our method\nenhances robustness in noisy environments and improves general task\nperformance, offering a scalable and efficient paradigm for tool-use training\nin LLMs. Code and data are available at\nhttps://github.com/Junjie-Ye/TL-Training."
                },
                "authors": [
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Sixian Li"
                    },
                    {
                        "name": "Yuming Yang"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Zhongchao Shi"
                    },
                    {
                        "name": "Jianping Fan"
                    },
                    {
                        "name": "Zhengyin Du"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyin Du"
                },
                "author": "Zhengyin Du",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04594v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04594v5",
                "updated": "2025-08-26T16:31:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    31,
                    32,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-07T17:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    37,
                    23,
                    2,
                    127,
                    0
                ],
                "title": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection"
                },
                "summary": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting 3D attributes is crucial for monocular 3D object\ndetection (Mono3D), with depth estimation posing the greatest challenge due to\nthe inherent ambiguity in mapping 2D images to 3D space. While existing methods\nleverage multiple depth cues (e.g., estimating depth uncertainty, modeling\ndepth error) to improve depth accuracy, they overlook that accurate depth\nprediction requires conditioning on other 3D attributes, as these attributes\nare intrinsically inter-correlated through the 3D to 2D projection, which\nultimately limits overall accuracy and stability. Inspired by Chain-of-Thought\n(CoT) in large language models (LLMs), this paper proposes MonoCoP, which\nleverages a Chain-of-Prediction (CoP) to predict attributes sequentially and\nconditionally via three key designs. First, it employs a lightweight\nAttributeNet (AN) for each 3D attribute to learn attribute-specific features.\nNext, MonoCoP constructs an explicit chain to propagate these learned features\nfrom one attribute to the next. Finally, MonoCoP uses a residual connection to\naggregate features for each attribute along the chain, ensuring that later\nattribute predictions are conditioned on all previously processed attributes\nwithout forgetting the features of earlier ones. Experimental results show that\nour MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI\nleaderboard without requiring additional data and further surpasses existing\nmethods on the Waymo and nuScenes frontal datasets."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Abhinav Kumar"
                    },
                    {
                        "name": "Girish Chandar Ganesan"
                    },
                    {
                        "name": "Xiaoming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Liu"
                },
                "author": "Xiaoming Liu",
                "arxiv_comment": "I plan to re-format and re-write this paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04594v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04594v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19163v1",
                "updated": "2025-08-26T16:12:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    12,
                    12,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T16:12:12Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    12,
                    12,
                    1,
                    238,
                    0
                ],
                "title": "MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and\n  conteXtual clinical conversational evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MATRIX: Multi-Agent simulaTion fRamework for safe Interactions and\n  conteXtual clinical conversational evaluation"
                },
                "summary": "Despite the growing use of large language models (LLMs) in clinical dialogue\nsystems, existing evaluations focus on task completion or fluency, offering\nlittle insight into the behavioral and risk management requirements essential\nfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion\nfRamework for safe Interactions and conteXtual clinical conversational\nevaluation), a structured, extensible framework for safety-oriented evaluation\nof clinical dialogue agents.\n  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical\nscenarios, expected system behaviors and failure modes derived through\nstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluator\nfor detecting safety-relevant dialogue failures, validated against expert\nclinician annotations; and (3) PatBot, a simulated patient agent capable of\nproducing diverse, scenario-conditioned responses, evaluated for realism and\nbehavioral fidelity with human factors expertise, and a patient-preference\nstudy.\n  Across three experiments, we show that MATRIX enables systematic, scalable\nsafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard\ndetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded\nassessment of 240 dialogues. We also conducted one of the first realism\nanalyses of LLM-based patient simulation, showing that PatBot reliably\nsimulates realistic patient behavior in quantitative and qualitative\nevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking\nfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios\nand 10 clinical domains.\n  MATRIX is the first framework to unify structured safety engineering with\nscalable, validated conversational AI evaluation, enabling regulator-aligned\nsafety auditing. We release all evaluation tools, prompts, structured\nscenarios, and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing use of large language models (LLMs) in clinical dialogue\nsystems, existing evaluations focus on task completion or fluency, offering\nlittle insight into the behavioral and risk management requirements essential\nfor safety-critical systems. This paper presents MATRIX (Multi-Agent simulaTion\nfRamework for safe Interactions and conteXtual clinical conversational\nevaluation), a structured, extensible framework for safety-oriented evaluation\nof clinical dialogue agents.\n  MATRIX integrates three components: (1) a safety-aligned taxonomy of clinical\nscenarios, expected system behaviors and failure modes derived through\nstructured safety engineering methods; (2) BehvJudge, an LLM-based evaluator\nfor detecting safety-relevant dialogue failures, validated against expert\nclinician annotations; and (3) PatBot, a simulated patient agent capable of\nproducing diverse, scenario-conditioned responses, evaluated for realism and\nbehavioral fidelity with human factors expertise, and a patient-preference\nstudy.\n  Across three experiments, we show that MATRIX enables systematic, scalable\nsafety evaluation. BehvJudge with Gemini 2.5-Pro achieves expert-level hazard\ndetection (F1 0.96, sensitivity 0.999), outperforming clinicians in a blinded\nassessment of 240 dialogues. We also conducted one of the first realism\nanalyses of LLM-based patient simulation, showing that PatBot reliably\nsimulates realistic patient behavior in quantitative and qualitative\nevaluations. Using MATRIX, we demonstrate its effectiveness in benchmarking\nfive LLM agents across 2,100 simulated dialogues spanning 14 hazard scenarios\nand 10 clinical domains.\n  MATRIX is the first framework to unify structured safety engineering with\nscalable, validated conversational AI evaluation, enabling regulator-aligned\nsafety auditing. We release all evaluation tools, prompts, structured\nscenarios, and datasets."
                },
                "authors": [
                    {
                        "name": "Ernest Lim"
                    },
                    {
                        "name": "Yajie Vera He"
                    },
                    {
                        "name": "Jared Joselowitz"
                    },
                    {
                        "name": "Kate Preston"
                    },
                    {
                        "name": "Mohita Chowdhury"
                    },
                    {
                        "name": "Louis Williams"
                    },
                    {
                        "name": "Aisling Higham"
                    },
                    {
                        "name": "Katrina Mason"
                    },
                    {
                        "name": "Mariane Melo"
                    },
                    {
                        "name": "Tom Lawton"
                    },
                    {
                        "name": "Yan Jia"
                    },
                    {
                        "name": "Ibrahim Habli"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Habli"
                },
                "author": "Ibrahim Habli",
                "arxiv_comment": "36 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T42, 92C50, 68Q60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08249v2",
                "updated": "2025-08-26T15:53:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    53,
                    39,
                    1,
                    238,
                    0
                ],
                "published": "2024-07-11T07:51:57Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    7,
                    51,
                    57,
                    3,
                    193,
                    0
                ],
                "title": "GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and\n  Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeNet: A Multimodal LLM-Based Co-Pilot for Network Topology and\n  Configuration"
                },
                "summary": "Communication network engineering in enterprise environments is traditionally\na complex, time-consuming, and error-prone manual process. Most research on\nnetwork engineering automation has concentrated on configuration synthesis,\noften overlooking changes in the physical network topology. This paper\nintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet\nis a novel framework that leverages a large language model (LLM) to streamline\nnetwork design workflows. It uses visual and textual modalities to interpret\nand update network topologies and device configurations based on user intents.\nGeNet was evaluated on enterprise network scenarios adapted from Cisco\ncertification exercises. Our results demonstrate GeNet's ability to interpret\nnetwork topology images accurately, potentially reducing network engineers'\nefforts and accelerating network design processes in enterprise environments.\nFurthermore, we show the importance of precise topology understanding when\nhandling intents that require modifications to the network's topology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication network engineering in enterprise environments is traditionally\na complex, time-consuming, and error-prone manual process. Most research on\nnetwork engineering automation has concentrated on configuration synthesis,\noften overlooking changes in the physical network topology. This paper\nintroduces GeNet, a multimodal co-pilot for enterprise network engineers. GeNet\nis a novel framework that leverages a large language model (LLM) to streamline\nnetwork design workflows. It uses visual and textual modalities to interpret\nand update network topologies and device configurations based on user intents.\nGeNet was evaluated on enterprise network scenarios adapted from Cisco\ncertification exercises. Our results demonstrate GeNet's ability to interpret\nnetwork topology images accurately, potentially reducing network engineers'\nefforts and accelerating network design processes in enterprise environments.\nFurthermore, we show the importance of precise topology understanding when\nhandling intents that require modifications to the network's topology."
                },
                "authors": [
                    {
                        "name": "Beni Ifland"
                    },
                    {
                        "name": "Elad Duani"
                    },
                    {
                        "name": "Rubin Krief"
                    },
                    {
                        "name": "Miro Ohana"
                    },
                    {
                        "name": "Aviram Zilberman"
                    },
                    {
                        "name": "Andres Murillo"
                    },
                    {
                        "name": "Ofir Manor"
                    },
                    {
                        "name": "Ortal Lavi"
                    },
                    {
                        "name": "Hikichi Kenji"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06866v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06866v3",
                "updated": "2025-08-26T15:44:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    44,
                    42,
                    1,
                    238,
                    0
                ],
                "published": "2024-07-09T13:53:38Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    13,
                    53,
                    38,
                    1,
                    191,
                    0
                ],
                "title": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context"
                },
                "summary": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the biases of language models in production are extensively documented,\nthe biases of their guardrails have been neglected. This paper studies how\ncontextual information about the user influences the likelihood of an LLM to\nrefuse to execute a request. By generating user biographies that offer\nideological and demographic information, we find a number of biases in\nguardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas\nare more likely to trigger a refusal guardrail when requesting censored or\nillegal information. Guardrails are also sycophantic, refusing to comply with\nrequests for a political position the user is likely to disagree with. We find\nthat certain identity groups and seemingly innocuous information, e.g., sports\nfandom, can elicit changes in guardrail sensitivity similar to direct\nstatements of political ideology. For each demographic category and even for\nAmerican football team fandom, we find that ChatGPT appears to infer a likely\npolitical ideology and modify guardrail behavior accordingly."
                },
                "authors": [
                    {
                        "name": "Victoria R. Li"
                    },
                    {
                        "name": "Yida Chen"
                    },
                    {
                        "name": "Naomi Saphra"
                    }
                ],
                "author_detail": {
                    "name": "Naomi Saphra"
                },
                "author": "Naomi Saphra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06866v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06866v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19131v1",
                "updated": "2025-08-26T15:30:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    30,
                    19,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:30:19Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    30,
                    19,
                    1,
                    238,
                    0
                ],
                "title": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZeST: an LLM-based Zero-Shot Traversability Navigation for Unknown\n  Environments"
                },
                "summary": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of robotics and autonomous navigation systems hinges on the\nability to accurately predict terrain traversability. Traditional methods for\ngenerating datasets to train these prediction models often involve putting\nrobots into potentially hazardous environments, posing risks to equipment and\nsafety. To solve this problem, we present ZeST, a novel approach leveraging\nvisual reasoning capabilities of Large Language Models (LLMs) to create a\ntraversability map in real-time without exposing robots to danger. Our approach\nnot only performs zero-shot traversability and mitigates the risks associated\nwith real-world data collection but also accelerates the development of\nadvanced navigation systems, offering a cost-effective and scalable solution.\nTo support our findings, we present navigation results, in both controlled\nindoor and unstructured outdoor environments. As shown in the experiments, our\nmethod provides safer navigation when compared to other state-of-the-art\nmethods, constantly reaching the final goal."
                },
                "authors": [
                    {
                        "name": "Shreya Gummadi"
                    },
                    {
                        "name": "Mateus V. Gasparino"
                    },
                    {
                        "name": "Gianluca Capezzuto"
                    },
                    {
                        "name": "Marcelo Becker"
                    },
                    {
                        "name": "Girish Chowdhary"
                    }
                ],
                "author_detail": {
                    "name": "Girish Chowdhary"
                },
                "author": "Girish Chowdhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05123v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05123v3",
                "updated": "2025-08-26T15:28:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    28,
                    36,
                    1,
                    238,
                    0
                ],
                "published": "2024-02-04T13:32:01Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    13,
                    32,
                    1,
                    6,
                    35,
                    0
                ],
                "title": "A Survey on Data Selection for LLM Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Data Selection for LLM Instruction Tuning"
                },
                "summary": "Instruction tuning is a vital step of training large language models (LLMs),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLMs. Therefore,\nrecently a lot of studies focus on exploring the methods of selecting\nhigh-quality subset from instruction datasets, aiming to reduce training costs\nand enhance the instruction-following capabilities of LLMs. This paper presents\na comprehensive survey on data selection for LLM instruction tuning. Firstly,\nwe introduce the wildly used instruction datasets. Then, we propose a new\ntaxonomy of the data selection methods and provide a detailed introduction of\nrecent advances, and the evaluation strategies and results of data selection\nmethods are also elaborated in detail. Finally, we emphasize the open\nchallenges and present new frontiers of this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning is a vital step of training large language models (LLMs),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLMs. Therefore,\nrecently a lot of studies focus on exploring the methods of selecting\nhigh-quality subset from instruction datasets, aiming to reduce training costs\nand enhance the instruction-following capabilities of LLMs. This paper presents\na comprehensive survey on data selection for LLM instruction tuning. Firstly,\nwe introduce the wildly used instruction datasets. Then, we propose a new\ntaxonomy of the data selection methods and provide a detailed introduction of\nrecent advances, and the evaluation strategies and results of data selection\nmethods are also elaborated in detail. Finally, we emphasize the open\nchallenges and present new frontiers of this task."
                },
                "authors": [
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Qianlong Du"
                    },
                    {
                        "name": "Jiajun Zhang"
                    },
                    {
                        "name": "Zhiying Tu"
                    },
                    {
                        "name": "Dianhui Chu"
                    }
                ],
                "author_detail": {
                    "name": "Dianhui Chu"
                },
                "author": "Dianhui Chu",
                "arxiv_doi": "10.1613/jair.1.17625",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1613/jair.1.17625",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.05123v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05123v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in JAIR (Vol. 83, Article 32, 2025)",
                "arxiv_journal_ref": "Journal of Artificial Intelligence Research, 83:32, 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19130v1",
                "updated": "2025-08-26T15:28:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    28,
                    7,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:28:07Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    28,
                    7,
                    1,
                    238,
                    0
                ],
                "title": "Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for\n  Energy Efficient Multi-Operator Cellular Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing is Caring: Analysis of Hybrid Network Sharing Strategies for\n  Energy Efficient Multi-Operator Cellular Systems"
                },
                "summary": "This paper introduces a novel analytical framework for evaluating\nenergy-efficient, QoS-aware network-sharing strategies in cellular networks.\nLeveraging stochastic geometry, our framework enables the systematic assessment\nof network performance across a range of sharing paradigms, including both\nconventional single-operator scenarios and advanced hybrid strategies that\nenable full integration and cooperation among multiple mobile network\noperators. Our framework incorporates diverse user densities, rate\nrequirements, and energy consumption models to ensure comprehensive analysis.\nApplying our results to real-world datasets from French mobile network\noperators, we demonstrate that hybrid network sharing can yield substantial\nenergy savings, up to $35\\%$, while maintaining quality of service.\nFurthermore, our results allow us to characterizing how the benefits of network\nsharing vary as a function of the geographical and functional characteristics\nof the deployment area. These findings highlight the potential of collaborative\nsharing strategies to enhance operational efficiency and sustainability in\nnext-generation cellular networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel analytical framework for evaluating\nenergy-efficient, QoS-aware network-sharing strategies in cellular networks.\nLeveraging stochastic geometry, our framework enables the systematic assessment\nof network performance across a range of sharing paradigms, including both\nconventional single-operator scenarios and advanced hybrid strategies that\nenable full integration and cooperation among multiple mobile network\noperators. Our framework incorporates diverse user densities, rate\nrequirements, and energy consumption models to ensure comprehensive analysis.\nApplying our results to real-world datasets from French mobile network\noperators, we demonstrate that hybrid network sharing can yield substantial\nenergy savings, up to $35\\%$, while maintaining quality of service.\nFurthermore, our results allow us to characterizing how the benefits of network\nsharing vary as a function of the geographical and functional characteristics\nof the deployment area. These findings highlight the potential of collaborative\nsharing strategies to enhance operational efficiency and sustainability in\nnext-generation cellular networks."
                },
                "authors": [
                    {
                        "name": "Laura Finarelli"
                    },
                    {
                        "name": "Maoquan Ni"
                    },
                    {
                        "name": "Michela Meo"
                    },
                    {
                        "name": "Falko Dressler"
                    },
                    {
                        "name": "Gianluca Rizzo"
                    }
                ],
                "author_detail": {
                    "name": "Gianluca Rizzo"
                },
                "author": "Gianluca Rizzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19129v1",
                "updated": "2025-08-26T15:27:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    27,
                    57,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:27:57Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    27,
                    57,
                    1,
                    238,
                    0
                ],
                "title": "Space-Time Coded RIS-Assisted Wireless Systems with Practical Reflection\n  Models: Error Rate Analysis and Negative Moment-Based Optimization with\n  Saddle Point Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space-Time Coded RIS-Assisted Wireless Systems with Practical Reflection\n  Models: Error Rate Analysis and Negative Moment-Based Optimization with\n  Saddle Point Approximation"
                },
                "summary": "RIS-assisted communication has recently attracted significant attention for\nenhancing wireless performance in challenging environments, making accurate\nerror analysis under practical hardware constraints crucial for future\nmulti-antenna systems. This paper presents a theoretical framework for SER\nanalysis of RIS-assisted multiple antenna systems employing OSTBC under\npractical reflection models with amplitude-dependent and quantized phase\nresponses. By exploiting the Gramian structure of the cascaded channel f, we\nderive exact MGF expressions of the nonzero eigenvalue of f'f for small RIS\nsizes. For large-scale RIS deployments, where closed-form analysis becomes\nintractable, we employ Saddle Point Approximation to approximate the eigenvalue\ndistribution. Using these results, we derive unified SER expressions using\nexact and SPA-based MGF formulations, applicable to arbitrary RIS sizes, phase\nconfiguration, and both identical and non-identical amplitude responses.\nExtensive Monte Carlo simulations confirm the accuracy of the proposed SER\nexpressions, demonstrating very close agreement for all configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-assisted communication has recently attracted significant attention for\nenhancing wireless performance in challenging environments, making accurate\nerror analysis under practical hardware constraints crucial for future\nmulti-antenna systems. This paper presents a theoretical framework for SER\nanalysis of RIS-assisted multiple antenna systems employing OSTBC under\npractical reflection models with amplitude-dependent and quantized phase\nresponses. By exploiting the Gramian structure of the cascaded channel f, we\nderive exact MGF expressions of the nonzero eigenvalue of f'f for small RIS\nsizes. For large-scale RIS deployments, where closed-form analysis becomes\nintractable, we employ Saddle Point Approximation to approximate the eigenvalue\ndistribution. Using these results, we derive unified SER expressions using\nexact and SPA-based MGF formulations, applicable to arbitrary RIS sizes, phase\nconfiguration, and both identical and non-identical amplitude responses.\nExtensive Monte Carlo simulations confirm the accuracy of the proposed SER\nexpressions, demonstrating very close agreement for all configurations."
                },
                "authors": [
                    {
                        "name": "Tayfun Yilmaz"
                    },
                    {
                        "name": "Haci Ilhan"
                    },
                    {
                        "name": "Ibrahim Hokelek"
                    }
                ],
                "author_detail": {
                    "name": "Ibrahim Hokelek"
                },
                "author": "Ibrahim Hokelek",
                "arxiv_comment": "This work has been submitted for consideration in an IEEE journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00039v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00039v4",
                "updated": "2025-08-26T15:27:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    27,
                    25,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-29T18:36:57Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    36,
                    57,
                    1,
                    119,
                    0
                ],
                "title": "An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,\n  and Deterministic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ontology-Driven Graph RAG for Legal Norms: A Hierarchical, Temporal,\n  and Deterministic Approach"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces an ontology-driven Graph RAG\nframework designed to overcome these limitations. We ground our knowledge graph\nin a formal, LRMoo-inspired model that distinguishes abstract legal Works from\ntheir versioned Expressions. We model temporal states as efficient aggregations\nthat reuse the versioned expressions (CTVs) of unchanged components, and we\nreify legislative events as first-class Action nodes to make causality explicit\nand queryable. This structured backbone enables a unified, planner-guided query\nstrategy that applies explicit policies to deterministically resolve complex\nrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,\nand (iii) auditable provenance reconstruction. Through a case study on the\nBrazilian Constitution, we demonstrate how this approach provides a verifiable,\ntemporally-correct substrate for LLMs, enabling higher-order analytical\ncapabilities while drastically reducing the risk of factual errors. The result\nis a practical framework for building more trustworthy and explainable legal AI\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems in the legal domain face a\ncritical challenge: standard, flat-text retrieval is blind to the hierarchical,\ndiachronic, and causal structure of law, leading to anachronistic and\nunreliable answers. This paper introduces an ontology-driven Graph RAG\nframework designed to overcome these limitations. We ground our knowledge graph\nin a formal, LRMoo-inspired model that distinguishes abstract legal Works from\ntheir versioned Expressions. We model temporal states as efficient aggregations\nthat reuse the versioned expressions (CTVs) of unchanged components, and we\nreify legislative events as first-class Action nodes to make causality explicit\nand queryable. This structured backbone enables a unified, planner-guided query\nstrategy that applies explicit policies to deterministically resolve complex\nrequests for (i) point-in-time retrieval, (ii) hierarchical impact analysis,\nand (iii) auditable provenance reconstruction. Through a case study on the\nBrazilian Constitution, we demonstrate how this approach provides a verifiable,\ntemporally-correct substrate for LLMs, enabling higher-order analytical\ncapabilities while drastically reducing the risk of factual errors. The result\nis a practical framework for building more trustworthy and explainable legal AI\nsystems."
                },
                "authors": [
                    {
                        "name": "Hudson de Martim"
                    }
                ],
                "author_detail": {
                    "name": "Hudson de Martim"
                },
                "author": "Hudson de Martim",
                "arxiv_comment": "This is a major revision that significantly expands and deepens the\n  original manuscript. While the core ontological model remains the same, this\n  version provides a substantially more rigorous and detailed account of how\n  the framework is applied in practice, particularly within a\n  Retrieval-Augmented Generation (RAG) context",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00039v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00039v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12719v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12719v4",
                "updated": "2025-08-26T15:27:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    27,
                    18,
                    1,
                    238,
                    0
                ],
                "published": "2024-06-18T15:41:15Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    15,
                    41,
                    15,
                    1,
                    170,
                    0
                ],
                "title": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Robustness of Language Models for Tabular Question\n  Answering via Attention Analysis"
                },
                "summary": "Large Language Models (LLMs), already shown to ace various unstructured text\ncomprehension tasks, have also remarkably been shown to tackle table\n(structured) comprehension tasks without specific training. Building on earlier\nstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),\nmodel scale, instruction tuning, and domain bias affect Tabular QA (TQA)\nrobustness by testing LLMs, under diverse augmentations and perturbations, on\ndiverse domains: Wikipedia-based $\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$,\nand scientific $\\textbf{SCITAB}$. Although instruction tuning and larger, newer\nLLMs deliver stronger, more robust TQA performance, data contamination and\nreliability issues, especially on $\\textbf{WTQ}$, remain unresolved. Through an\nin-depth attention analysis, we reveal a strong correlation between\nperturbation-induced shifts in attention dispersion and the drops in\nperformance, with sensitivity peaking in the model's middle layers. We\nhighlight the need for improved interpretable methodologies to develop more\nreliable LLMs for table comprehension. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and performance drops, with sensitivity peaking in the model's\nmiddle layers. Based on these findings, we argue for the development of\nstructure-aware self-attention mechanisms and domain-adaptive processing\ntechniques to improve the transparency, generalization, and real-world\nreliability of LLMs on tabular data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), already shown to ace various unstructured text\ncomprehension tasks, have also remarkably been shown to tackle table\n(structured) comprehension tasks without specific training. Building on earlier\nstudies of LLMs for tabular tasks, we probe how in-context learning (ICL),\nmodel scale, instruction tuning, and domain bias affect Tabular QA (TQA)\nrobustness by testing LLMs, under diverse augmentations and perturbations, on\ndiverse domains: Wikipedia-based $\\textbf{WTQ}$, financial $\\textbf{TAT-QA}$,\nand scientific $\\textbf{SCITAB}$. Although instruction tuning and larger, newer\nLLMs deliver stronger, more robust TQA performance, data contamination and\nreliability issues, especially on $\\textbf{WTQ}$, remain unresolved. Through an\nin-depth attention analysis, we reveal a strong correlation between\nperturbation-induced shifts in attention dispersion and the drops in\nperformance, with sensitivity peaking in the model's middle layers. We\nhighlight the need for improved interpretable methodologies to develop more\nreliable LLMs for table comprehension. Through an in-depth attention analysis,\nwe reveal a strong correlation between perturbation-induced shifts in attention\ndispersion and performance drops, with sensitivity peaking in the model's\nmiddle layers. Based on these findings, we argue for the development of\nstructure-aware self-attention mechanisms and domain-adaptive processing\ntechniques to improve the transparency, generalization, and real-world\nreliability of LLMs on tabular data."
                },
                "authors": [
                    {
                        "name": "Kushal Raj Bhandari"
                    },
                    {
                        "name": "Sixue Xing"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Jianxi Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxi Gao"
                },
                "author": "Jianxi Gao",
                "arxiv_comment": "Accepted TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12719v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12719v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20679v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20679v3",
                "updated": "2025-08-26T15:18:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    18,
                    44,
                    1,
                    238,
                    0
                ],
                "published": "2024-09-25T14:37:49Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    37,
                    49,
                    2,
                    269,
                    0
                ],
                "title": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCI-GRU: Stock Prediction Model Based on Multi-Head Cross-Attention and\n  Improved GRU"
                },
                "summary": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As financial markets grow increasingly complex in the big data era, accurate\nstock prediction has become more critical. Traditional time series models, such\nas GRUs, have been widely used but often struggle to capture the intricate\nnonlinear dynamics of markets, particularly in the flexible selection and\neffective utilization of key historical information. Recently, methods like\nGraph Neural Networks and Reinforcement Learning have shown promise in stock\nprediction but require high data quality and quantity, and they tend to exhibit\ninstability when dealing with data sparsity and noise. Moreover, the training\nand inference processes for these models are typically complex and\ncomputationally expensive, limiting their broad deployment in practical\napplications. Existing approaches also generally struggle to capture\nunobservable latent market states effectively, such as market sentiment and\nexpectations, microstructural factors, and participant behavior patterns,\nleading to an inadequate understanding of market dynamics and subsequently\nimpact prediction accuracy. To address these challenges, this paper proposes a\nstock prediction model, MCI-GRU, based on a multi-head cross-attention\nmechanism and an improved GRU. First, we enhance the GRU model by replacing the\nreset gate with an attention mechanism, thereby increasing the model's\nflexibility in selecting and utilizing historical information. Second, we\ndesign a multi-head cross-attention mechanism for learning unobservable latent\nmarket state representations, which are further enriched through interactions\nwith both temporal features and cross-sectional features. Finally, extensive\nexperiments on four main stock markets show that the proposed method\noutperforms SOTA techniques across multiple metrics. Additionally, its\nsuccessful application in real-world fund management operations confirms its\neffectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Peng Zhu"
                    },
                    {
                        "name": "Yuante Li"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Sheng Xiang"
                    },
                    {
                        "name": "Qinyuan Liu"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Yuqi Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Liang"
                },
                "author": "Yuqi Liang",
                "arxiv_doi": "10.1016/j.neucom.2025.130168",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.neucom.2025.130168",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.20679v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20679v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Neurocomputing 638 (2025) 130168",
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19114v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19114v1",
                "updated": "2025-08-26T15:17:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    17,
                    8,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:17:08Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    17,
                    8,
                    1,
                    238,
                    0
                ],
                "title": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and\n  Delivery using Voronoi-Based Relay Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELIVER: A System for LLM-Guided Coordinated Multi-Robot Pickup and\n  Delivery using Voronoi-Based Relay Planning"
                },
                "summary": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DELIVER (Directed Execution of Language-instructed Item Via\nEngineered Relay), a fully integrated framework for cooperative multi-robot\npickup and delivery driven by natural language commands. DELIVER unifies\nnatural language understanding, spatial decomposition, relay planning, and\nmotion execution to enable scalable, collision-free coordination in real-world\nsettings. Given a spoken or written instruction, a lightweight instance of\nLLaMA3 interprets the command to extract pickup and delivery locations. The\nenvironment is partitioned using a Voronoi tessellation to define\nrobot-specific operating regions. Robots then compute optimal relay points\nalong shared boundaries and coordinate handoffs. A finite-state machine governs\neach robot's behavior, enabling robust execution. We implement DELIVER on the\nMultiTRAIL simulation platform and validate it in both ROS2-based Gazebo\nsimulations and real-world hardware using TurtleBot3 robots. Empirical results\nshow that DELIVER maintains consistent mission cost across varying team sizes\nwhile reducing per-agent workload by up to 55% compared to a single-agent\nsystem. Moreover, the number of active relay agents remains low even as team\nsize increases, demonstrating the system's scalability and efficient agent\nutilization. These findings underscore DELIVER's modular and extensible\narchitecture for language-guided multi-robot coordination, advancing the\nfrontiers of cyber-physical system integration."
                },
                "authors": [
                    {
                        "name": "Alkesh K. Srivastava"
                    },
                    {
                        "name": "Jared Michael Levin"
                    },
                    {
                        "name": "Alexander Derrico"
                    },
                    {
                        "name": "Philip Dames"
                    }
                ],
                "author_detail": {
                    "name": "Philip Dames"
                },
                "author": "Philip Dames",
                "arxiv_comment": "Submission under review at the 2026 IEEE/SICE International Symposium\n  on System Integration (SII 2026)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19114v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19114v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19111v1",
                "updated": "2025-08-26T15:14:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    14,
                    19,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T15:14:19Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    14,
                    19,
                    1,
                    238,
                    0
                ],
                "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary\n  Perception in LVLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary\n  Perception in LVLMs"
                },
                "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs."
                },
                "authors": [
                    {
                        "name": "Zhikai Ding"
                    },
                    {
                        "name": "Shiyu Ni"
                    },
                    {
                        "name": "Keping Bi"
                    }
                ],
                "author_detail": {
                    "name": "Keping Bi"
                },
                "author": "Keping Bi",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19195v2",
                "updated": "2025-08-26T15:09:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    9,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2024-10-24T22:59:23Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    22,
                    59,
                    23,
                    3,
                    298,
                    0
                ],
                "title": "Label Set Optimization via Activation Distribution Kurtosis for\n  Zero-shot Classification with Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Label Set Optimization via Activation Distribution Kurtosis for\n  Zero-shot Classification with Generative Models"
                },
                "summary": "In-context learning (ICL) performance is highly sensitive to prompt design,\nyet the impact of class label options (e.g. lexicon or order) in zero-shot\nclassification remains underexplored. This study proposes LOADS (Label set\nOptimization via Activation Distribution kurtosiS), a post-hoc method for\nselecting optimal label sets in zero-shot ICL with large language models\n(LLMs). LOADS is built upon the observations in our empirical analysis, the\nfirst to systematically examine how label option design (i.e., lexical choice,\norder, and elaboration) impacts classification performance. This analysis shows\nthat the lexical choice of the labels in the prompt (such as agree vs. support\nin stance classification) plays an important role in both model performance and\nmodel's sensitivity to the label order. A further investigation demonstrates\nthat optimal label words tend to activate fewer outlier neurons in LLMs'\nfeed-forward networks. LOADS then leverages kurtosis to measure the neuron\nactivation distribution for label selection, requiring only a single forward\npass without gradient propagation or labelled data. The LOADS-selected label\nwords consistently demonstrate effectiveness for zero-shot ICL across\nclassification tasks, datasets, models and languages, achieving maximum\nperformance gain from 0.54 to 0.76 compared to the conventional approach of\nusing original dataset label words.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) performance is highly sensitive to prompt design,\nyet the impact of class label options (e.g. lexicon or order) in zero-shot\nclassification remains underexplored. This study proposes LOADS (Label set\nOptimization via Activation Distribution kurtosiS), a post-hoc method for\nselecting optimal label sets in zero-shot ICL with large language models\n(LLMs). LOADS is built upon the observations in our empirical analysis, the\nfirst to systematically examine how label option design (i.e., lexical choice,\norder, and elaboration) impacts classification performance. This analysis shows\nthat the lexical choice of the labels in the prompt (such as agree vs. support\nin stance classification) plays an important role in both model performance and\nmodel's sensitivity to the label order. A further investigation demonstrates\nthat optimal label words tend to activate fewer outlier neurons in LLMs'\nfeed-forward networks. LOADS then leverages kurtosis to measure the neuron\nactivation distribution for label selection, requiring only a single forward\npass without gradient propagation or labelled data. The LOADS-selected label\nwords consistently demonstrate effectiveness for zero-shot ICL across\nclassification tasks, datasets, models and languages, achieving maximum\nperformance gain from 0.54 to 0.76 compared to the conventional approach of\nusing original dataset label words."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Carolina Scarton"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Scarton"
                },
                "author": "Carolina Scarton",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18180v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18180v3",
                "updated": "2025-08-26T15:01:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    15,
                    1,
                    38,
                    1,
                    238,
                    0
                ],
                "published": "2024-05-28T13:47:21Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    13,
                    47,
                    21,
                    1,
                    149,
                    0
                ],
                "title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive\n  Shielding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Reinforcement Learning in Black-Box Environments via Adaptive\n  Shielding"
                },
                "summary": "Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical challenge towards their deployment in many real-world\nscenarios. When prior knowledge of the domain or task is unavailable, training\nRL agents in unknown, black-box environments presents an even greater safety\nrisk. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder),\na novel post-shielding technique that distinguishes safe and unsafe features of\nstate-action pairs during training, and uses this knowledge to protect the RL\nagent from executing actions that yield likely hazardous outcomes. Our\ncomprehensive experimental evaluation against state-of-the-art safe RL\nexploration techniques shows that ADVICE significantly reduces safety\nviolations (approx 50%) during training, with a competitive outcome reward\ncompared to other techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering safe exploration of reinforcement learning (RL) agents during\ntraining is a critical challenge towards their deployment in many real-world\nscenarios. When prior knowledge of the domain or task is unavailable, training\nRL agents in unknown, black-box environments presents an even greater safety\nrisk. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder),\na novel post-shielding technique that distinguishes safe and unsafe features of\nstate-action pairs during training, and uses this knowledge to protect the RL\nagent from executing actions that yield likely hazardous outcomes. Our\ncomprehensive experimental evaluation against state-of-the-art safe RL\nexploration techniques shows that ADVICE significantly reduces safety\nviolations (approx 50%) during training, with a competitive outcome reward\ncompared to other techniques."
                },
                "authors": [
                    {
                        "name": "Daniel Bethell"
                    },
                    {
                        "name": "Simos Gerasimou"
                    },
                    {
                        "name": "Radu Calinescu"
                    },
                    {
                        "name": "Calum Imrie"
                    }
                ],
                "author_detail": {
                    "name": "Calum Imrie"
                },
                "author": "Calum Imrie",
                "arxiv_comment": "To be published in ECAI 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18180v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18180v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19097v1",
                "updated": "2025-08-26T14:59:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    19,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:59:19Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    19,
                    1,
                    238,
                    0
                ],
                "title": "Reasoning LLMs in the Medical Domain: A Literature Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning LLMs in the Medical Domain: A Literature Survey"
                },
                "summary": "The emergence of advanced reasoning capabilities in Large Language Models\n(LLMs) marks a transformative development in healthcare applications. Beyond\nmerely expanding functional capabilities, these reasoning mechanisms enhance\ndecision transparency and explainability-critical requirements in medical\ncontexts. This survey examines the transformation of medical LLMs from basic\ninformation retrieval tools to sophisticated clinical reasoning systems capable\nof supporting complex healthcare decisions. We provide a thorough analysis of\nthe enabling technological foundations, with a particular focus on specialized\nprompting techniques like Chain-of-Thought and recent breakthroughs in\nReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates\npurpose-built medical frameworks while also examining emerging paradigms such\nas multi-agent collaborative systems and innovative prompting architectures.\nThe survey critically assesses current evaluation methodologies for medical\nvalidation and addresses persistent challenges in field interpretation\nlimitations, bias mitigation strategies, patient safety frameworks, and\nintegration of multimodal clinical data. Through this survey, we seek to\nestablish a roadmap for developing reliable LLMs that can serve as effective\npartners in clinical practice and medical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of advanced reasoning capabilities in Large Language Models\n(LLMs) marks a transformative development in healthcare applications. Beyond\nmerely expanding functional capabilities, these reasoning mechanisms enhance\ndecision transparency and explainability-critical requirements in medical\ncontexts. This survey examines the transformation of medical LLMs from basic\ninformation retrieval tools to sophisticated clinical reasoning systems capable\nof supporting complex healthcare decisions. We provide a thorough analysis of\nthe enabling technological foundations, with a particular focus on specialized\nprompting techniques like Chain-of-Thought and recent breakthroughs in\nReinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates\npurpose-built medical frameworks while also examining emerging paradigms such\nas multi-agent collaborative systems and innovative prompting architectures.\nThe survey critically assesses current evaluation methodologies for medical\nvalidation and addresses persistent challenges in field interpretation\nlimitations, bias mitigation strategies, patient safety frameworks, and\nintegration of multimodal clinical data. Through this survey, we seek to\nestablish a roadmap for developing reliable LLMs that can serve as effective\npartners in clinical practice and medical research."
                },
                "authors": [
                    {
                        "name": "Armin Berger"
                    },
                    {
                        "name": "Sarthak Khanna"
                    },
                    {
                        "name": "David Berghaus"
                    },
                    {
                        "name": "Rafet Sifa"
                    }
                ],
                "author_detail": {
                    "name": "Rafet Sifa"
                },
                "author": "Rafet Sifa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19096v1",
                "updated": "2025-08-26T14:59:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    4,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:59:04Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    59,
                    4,
                    1,
                    238,
                    0
                ],
                "title": "Trustworthy Agents for Electronic Health Records through Confidence\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Agents for Electronic Health Records through Confidence\n  Estimation"
                },
                "summary": "Large language models (LLMs) show promise for extracting information from\nElectronic Health Records (EHR) and supporting clinical decisions. However,\ndeployment in clinical settings faces challenges due to hallucination risks. We\npropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric\nquantifying the accuracy-reliability trade-off at varying confidence\nthresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating\nstepwise confidence estimation for clinical question answering. Experiments on\nMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under\nstrict reliability constraints, achieving improvements of 44.23%p and 25.34%p\nat HCAcc@70% while baseline methods fail at these thresholds. These results\nhighlight limitations of traditional accuracy metrics in evaluating healthcare\nAI agents. Our work contributes to developing trustworthy clinical agents that\ndeliver accurate information or transparently express uncertainty when\nconfidence is low.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise for extracting information from\nElectronic Health Records (EHR) and supporting clinical decisions. However,\ndeployment in clinical settings faces challenges due to hallucination risks. We\npropose Hallucination Controlled Accuracy at k% (HCAcc@k%), a novel metric\nquantifying the accuracy-reliability trade-off at varying confidence\nthresholds. We introduce TrustEHRAgent, a confidence-aware agent incorporating\nstepwise confidence estimation for clinical question answering. Experiments on\nMIMIC-III and eICU datasets show TrustEHRAgent outperforms baselines under\nstrict reliability constraints, achieving improvements of 44.23%p and 25.34%p\nat HCAcc@70% while baseline methods fail at these thresholds. These results\nhighlight limitations of traditional accuracy metrics in evaluating healthcare\nAI agents. Our work contributes to developing trustworthy clinical agents that\ndeliver accurate information or transparently express uncertainty when\nconfidence is low."
                },
                "authors": [
                    {
                        "name": "Yongwoo Song"
                    },
                    {
                        "name": "Minbyul Jeong"
                    },
                    {
                        "name": "Mujeen Sung"
                    }
                ],
                "author_detail": {
                    "name": "Mujeen Sung"
                },
                "author": "Mujeen Sung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15234v2",
                "updated": "2025-08-26T14:58:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    58,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-06-18T08:18:05Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    8,
                    18,
                    5,
                    2,
                    169,
                    0
                ],
                "title": "First Steps Toward the Development of a Straight-Line Reference\n  Alignment System for Future Accelerators at CERN Using Pseudo-Nondiffracting\n  Layer Beams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Steps Toward the Development of a Straight-Line Reference\n  Alignment System for Future Accelerators at CERN Using Pseudo-Nondiffracting\n  Layer Beams"
                },
                "summary": "This paper presents experimental results that allow for the performance\nevaluation of a straight-line reference alignment system based on\npseudo-nondiffracting Layer beams. Sensors, developed specifically for this\nsystem, feature four linear CMOS chips and a square aperture. This allows for\nsimultaneous measurements along the beam path without disrupting the laser\nreference. Measurements, conducted over a distance of 2 m from the first to the\nlast sensor, were compared with a laser tracker measurement to assess the\nsensor performance. The alignment reference generated by the Layer Beams\nexhibited a repeatability and reproducibility root-mean-square error (RMSE) of\nless than 30 ${\\mu}$m. The relative alignment precision for a known\ndisplacement was validated with a standard deviation of 4.3 ${\\mu}$m. The\nresults highlight the underlying sources of noise, which are induced mainly by\nthe cover glass, the protective film of the pixels, and the dark noise of the\nCMOS chips. Solutions to address these challenges are proposed. Additionally, a\nproof-of-concept for future development of a radiation-hard sensor utilizing\noptical fiber matrices is demonstrated. The RMSE of the reference position\ndetection introduced by the fiber matrix remained below 1.3 ${\\mu}$m. This\nwould allow the sensor to be used reliably in high-radiation environments\ntypical for accelerator facilities. This study serves as a foundational step\ntoward developing a robust straight-line reference alignment system based on\npseudo-nondiffracting Layer beams intended for deployment in the accelerator\nfacilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents experimental results that allow for the performance\nevaluation of a straight-line reference alignment system based on\npseudo-nondiffracting Layer beams. Sensors, developed specifically for this\nsystem, feature four linear CMOS chips and a square aperture. This allows for\nsimultaneous measurements along the beam path without disrupting the laser\nreference. Measurements, conducted over a distance of 2 m from the first to the\nlast sensor, were compared with a laser tracker measurement to assess the\nsensor performance. The alignment reference generated by the Layer Beams\nexhibited a repeatability and reproducibility root-mean-square error (RMSE) of\nless than 30 ${\\mu}$m. The relative alignment precision for a known\ndisplacement was validated with a standard deviation of 4.3 ${\\mu}$m. The\nresults highlight the underlying sources of noise, which are induced mainly by\nthe cover glass, the protective film of the pixels, and the dark noise of the\nCMOS chips. Solutions to address these challenges are proposed. Additionally, a\nproof-of-concept for future development of a radiation-hard sensor utilizing\noptical fiber matrices is demonstrated. The RMSE of the reference position\ndetection introduced by the fiber matrix remained below 1.3 ${\\mu}$m. This\nwould allow the sensor to be used reliably in high-radiation environments\ntypical for accelerator facilities. This study serves as a foundational step\ntoward developing a robust straight-line reference alignment system based on\npseudo-nondiffracting Layer beams intended for deployment in the accelerator\nfacilities."
                },
                "authors": [
                    {
                        "name": "Martin Dušek"
                    },
                    {
                        "name": "Sebastian Figura"
                    },
                    {
                        "name": "Jakub Michal Polak"
                    },
                    {
                        "name": "Solomon William Kamugasa"
                    },
                    {
                        "name": "Dirk Mergelkuhl"
                    },
                    {
                        "name": "Witold Niewiem"
                    },
                    {
                        "name": "Štěpán Kunc"
                    },
                    {
                        "name": "Jean-Christophe Gayde"
                    },
                    {
                        "name": "Miroslav Šulc"
                    }
                ],
                "author_detail": {
                    "name": "Miroslav Šulc"
                },
                "author": "Miroslav Šulc",
                "arxiv_comment": "19 pages, 10 figures, Submitted to Measurement Science and Technology\n  (This is a preprint and it has not yet been peer reviewed), Supplementary\n  material available as ancillary files",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19090v1",
                "updated": "2025-08-26T14:51:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    25,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:51:25Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    25,
                    1,
                    238,
                    0
                ],
                "title": "Building an Open CGRA Ecosystem for Agile Innovation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building an Open CGRA Ecosystem for Agile Innovation"
                },
                "summary": "Modern computing workloads, particularly in AI and edge applications, demand\nhardware-software co-design to meet aggressive performance and energy targets.\nSuch co-design benefits from open and agile platforms that replace closed,\nvertically integrated development with modular, community-driven ecosystems.\nCoarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance\nof flexibility and efficiency are particularly well-suited for this paradigm.\nWhen built on open-source hardware generators and software toolchains, CGRAs\nprovide a compelling foundation for architectural exploration, cross-layer\noptimization, and real-world deployment. In this paper, we will present an open\nCGRA ecosystem that we have developed to support agile innovation across the\nstack. Our contributions include HyCUBE, a CGRA with a reconfigurable\nsingle-cycle multi-hop interconnect for efficient data movement; PACE, which\nembeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing;\nand Morpher, a fully open-source, architecture-adaptive CGRA design framework\nthat supports design space exploration, compilation, simulation, and\nvalidation. By embracing openness at every layer, we aim to lower barriers to\ninnovation, enable reproducible research, and demonstrate how CGRAs can anchor\nthe next wave of agile hardware development. We will conclude with a call for a\nunified abstraction layer for CGRAs and spatial accelerators, one that\ndecouples hardware specialization from software development. Such a\nrepresentation would unlock architectural portability, compiler innovation, and\na scalable, open foundation for spatial computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computing workloads, particularly in AI and edge applications, demand\nhardware-software co-design to meet aggressive performance and energy targets.\nSuch co-design benefits from open and agile platforms that replace closed,\nvertically integrated development with modular, community-driven ecosystems.\nCoarse-Grained Reconfigurable Architectures (CGRAs), with their unique balance\nof flexibility and efficiency are particularly well-suited for this paradigm.\nWhen built on open-source hardware generators and software toolchains, CGRAs\nprovide a compelling foundation for architectural exploration, cross-layer\noptimization, and real-world deployment. In this paper, we will present an open\nCGRA ecosystem that we have developed to support agile innovation across the\nstack. Our contributions include HyCUBE, a CGRA with a reconfigurable\nsingle-cycle multi-hop interconnect for efficient data movement; PACE, which\nembeds a power-efficient HyCUBE within a RISC-V SoC targeting edge computing;\nand Morpher, a fully open-source, architecture-adaptive CGRA design framework\nthat supports design space exploration, compilation, simulation, and\nvalidation. By embracing openness at every layer, we aim to lower barriers to\ninnovation, enable reproducible research, and demonstrate how CGRAs can anchor\nthe next wave of agile hardware development. We will conclude with a call for a\nunified abstraction layer for CGRAs and spatial accelerators, one that\ndecouples hardware specialization from software development. Such a\nrepresentation would unlock architectural portability, compiler innovation, and\na scalable, open foundation for spatial computing."
                },
                "authors": [
                    {
                        "name": "Rohan Juneja"
                    },
                    {
                        "name": "Pranav Dangi"
                    },
                    {
                        "name": "Thilini Kaushalya Bandara"
                    },
                    {
                        "name": "Zhaoying Li"
                    },
                    {
                        "name": "Dhananjaya Wijerathne"
                    },
                    {
                        "name": "Li-Shiuan Peh"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19089v1",
                "updated": "2025-08-26T14:51:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    10,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:51:10Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    51,
                    10,
                    1,
                    238,
                    0
                ],
                "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource\n  Languages to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's All About In-Context Learning! Teaching Extremely Low-Resource\n  Languages to LLMs"
                },
                "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts."
                },
                "authors": [
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Zhixue Zhao"
                    },
                    {
                        "name": "Carolina Scarton"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Scarton"
                },
                "author": "Carolina Scarton",
                "arxiv_comment": "Accepted by EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19087v1",
                "updated": "2025-08-26T14:48:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    48,
                    29,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:48:29Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    48,
                    29,
                    1,
                    238,
                    0
                ],
                "title": "APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APT-LLM: Exploiting Arbitrary-Precision Tensor Core Computing for LLM\n  Acceleration"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nenormous computational demands severely limit deployment and real-time\nperformance. Quantization methods can help reduce computational costs, however,\nattaining the extreme efficiency associated with ultra-low-bit quantized LLMs\nat arbitrary precision presents challenges on GPUs. This is primarily due to\nthe limited support for GPU Tensor Cores, inefficient memory management, and\ninflexible kernel optimizations. To tackle these challenges, we propose a\ncomprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.\nFirstly, we introduce a novel data format, bipolar-INT, which allows for\nefficient and lossless conversion with signed INT, while also being more\nconducive to parallel computation. We also develop a matrix multiplication\n(MatMul) method allowing for arbitrary precision by dismantling and\nreassembling matrices at the bit level. This method provides flexible precision\nand optimizes the utilization of GPU Tensor Cores. In addition, we propose a\nmemory management system focused on data recovery, which strategically employs\nfast shared memory to substantially increase kernel execution speed and reduce\nmemory access latency. Finally, we develop a kernel mapping method that\ndynamically selects the optimal configurable hyperparameters of kernels for\nvarying matrix sizes, enabling optimal performance across different LLM\narchitectures and precision settings. In LLM inference, APT-LLM achieves up to\na 3.99$\\times$ speedup compared to FP16 baselines and a 2.16$\\times$ speedup\nover NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,\nAPT-LLM achieves up to 2.44$\\times$ speedup over FP16 and 1.65$\\times$ speedup\nover CUTLASS integer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nenormous computational demands severely limit deployment and real-time\nperformance. Quantization methods can help reduce computational costs, however,\nattaining the extreme efficiency associated with ultra-low-bit quantized LLMs\nat arbitrary precision presents challenges on GPUs. This is primarily due to\nthe limited support for GPU Tensor Cores, inefficient memory management, and\ninflexible kernel optimizations. To tackle these challenges, we propose a\ncomprehensive acceleration scheme for arbitrary precision LLMs, namely APT-LLM.\nFirstly, we introduce a novel data format, bipolar-INT, which allows for\nefficient and lossless conversion with signed INT, while also being more\nconducive to parallel computation. We also develop a matrix multiplication\n(MatMul) method allowing for arbitrary precision by dismantling and\nreassembling matrices at the bit level. This method provides flexible precision\nand optimizes the utilization of GPU Tensor Cores. In addition, we propose a\nmemory management system focused on data recovery, which strategically employs\nfast shared memory to substantially increase kernel execution speed and reduce\nmemory access latency. Finally, we develop a kernel mapping method that\ndynamically selects the optimal configurable hyperparameters of kernels for\nvarying matrix sizes, enabling optimal performance across different LLM\narchitectures and precision settings. In LLM inference, APT-LLM achieves up to\na 3.99$\\times$ speedup compared to FP16 baselines and a 2.16$\\times$ speedup\nover NVIDIA CUTLASS INT4 acceleration on RTX 3090. On RTX 4090 and H800,\nAPT-LLM achieves up to 2.44$\\times$ speedup over FP16 and 1.65$\\times$ speedup\nover CUTLASS integer baselines."
                },
                "authors": [
                    {
                        "name": "Shaobo Ma"
                    },
                    {
                        "name": "Chao Fang"
                    },
                    {
                        "name": "Haikuo Shao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "arxiv_comment": "To appear in the IEEE Transactions on Computer-Aided Design of\n  Integrated Circuits and Systems (TCAD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19078v1",
                "updated": "2025-08-26T14:39:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    39,
                    0,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:39:00Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    39,
                    0,
                    1,
                    238,
                    0
                ],
                "title": "Federated Fine-Tuning of Sparsely-Activated Large Language Models on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Fine-Tuning of Sparsely-Activated Large Language Models on\n  Resource-Constrained Devices"
                },
                "summary": "Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models\n(LLMs) is challenging due to their massive computational requirements and the\nresource constraints of participants. Existing working attempts to fill this\ngap through model quantization, computation offloading, or expert pruning.\nHowever, they cannot achieve desired performance due to impractical system\nassumptions and a lack of consideration for MoE-specific characteristics. In\nthis paper, we propose FLUX, a system designed to enable federated fine-tuning\nof MoE-based LLMs across participants with constrained computing resources\n(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX\nintroduces three key innovations: (1) quantization-based local profiling to\nestimate expert activation with minimal overhead, (2) adaptive layer-aware\nexpert merging to reduce resource consumption while preserving accuracy, and\n(3) dynamic expert role assignment using an exploration-exploitation strategy\nto balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE\nand DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX\nsignificantly outperforms existing methods, achieving up to 4.75X speedup in\ntime-to-accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models\n(LLMs) is challenging due to their massive computational requirements and the\nresource constraints of participants. Existing working attempts to fill this\ngap through model quantization, computation offloading, or expert pruning.\nHowever, they cannot achieve desired performance due to impractical system\nassumptions and a lack of consideration for MoE-specific characteristics. In\nthis paper, we propose FLUX, a system designed to enable federated fine-tuning\nof MoE-based LLMs across participants with constrained computing resources\n(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX\nintroduces three key innovations: (1) quantization-based local profiling to\nestimate expert activation with minimal overhead, (2) adaptive layer-aware\nexpert merging to reduce resource consumption while preserving accuracy, and\n(3) dynamic expert role assignment using an exploration-exploitation strategy\nto balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE\nand DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX\nsignificantly outperforms existing methods, achieving up to 4.75X speedup in\ntime-to-accuracy."
                },
                "authors": [
                    {
                        "name": "Fahao Chen"
                    },
                    {
                        "name": "Jie Wan"
                    },
                    {
                        "name": "Peng Li"
                    },
                    {
                        "name": "Zhou Su"
                    },
                    {
                        "name": "Dongxiao Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dongxiao Yu"
                },
                "author": "Dongxiao Yu",
                "arxiv_comment": "Accepted by EuroSys'26. The camera-ready version will be uploaded\n  later",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19076v1",
                "updated": "2025-08-26T14:37:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    37,
                    48,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:37:48Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    37,
                    48,
                    1,
                    238,
                    0
                ],
                "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive\n  Global-Local Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive\n  Global-Local Guidance"
                },
                "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components."
                },
                "authors": [
                    {
                        "name": "Ziyue Li"
                    },
                    {
                        "name": "Yuan Chang"
                    },
                    {
                        "name": "Gaihong Yu"
                    },
                    {
                        "name": "Xiaoqiu Le"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqiu Le"
                },
                "author": "Xiaoqiu Le",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06029v2",
                "updated": "2025-08-26T14:34:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    34,
                    0,
                    1,
                    238,
                    0
                ],
                "published": "2025-03-08T03:02:21Z",
                "published_parsed": [
                    2025,
                    3,
                    8,
                    3,
                    2,
                    21,
                    5,
                    67,
                    0
                ],
                "title": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?"
                },
                "summary": "Large Language Models (LLMs) have become integral to daily life, especially\nadvancing as intelligent assistants through on-device deployment on\nsmartphones. However, existing LLM evaluation benchmarks predominantly focus on\nobjective tasks like mathematics and coding in English, which do not\nnecessarily reflect the practical use cases of on-device LLMs in real-world\nmobile scenarios, especially for Chinese users. To address these gaps, we\nintroduce SmartBench, the first benchmark designed to evaluate the capabilities\nof on-device LLMs in Chinese mobile contexts. We analyze functionalities\nprovided by representative smartphone manufacturers and divide them into five\ncategories: text summarization, text Q&A, information extraction, content\ncreation, and notification management, further detailed into 20 specific tasks.\nFor each task, we construct high-quality datasets comprising 50 to 200\nquestion-answer pairs that reflect everyday mobile interactions, and we develop\nautomated evaluation criteria tailored for these tasks. We conduct\ncomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also\nassess their performance after quantized deployment on real smartphone NPUs.\nOur contributions provide a standardized framework for evaluating on-device\nLLMs in Chinese, promoting further development and optimization in this\ncritical area. Code and data will be available at\nhttps://github.com/vivo-ai-lab/SmartBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to daily life, especially\nadvancing as intelligent assistants through on-device deployment on\nsmartphones. However, existing LLM evaluation benchmarks predominantly focus on\nobjective tasks like mathematics and coding in English, which do not\nnecessarily reflect the practical use cases of on-device LLMs in real-world\nmobile scenarios, especially for Chinese users. To address these gaps, we\nintroduce SmartBench, the first benchmark designed to evaluate the capabilities\nof on-device LLMs in Chinese mobile contexts. We analyze functionalities\nprovided by representative smartphone manufacturers and divide them into five\ncategories: text summarization, text Q&A, information extraction, content\ncreation, and notification management, further detailed into 20 specific tasks.\nFor each task, we construct high-quality datasets comprising 50 to 200\nquestion-answer pairs that reflect everyday mobile interactions, and we develop\nautomated evaluation criteria tailored for these tasks. We conduct\ncomprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also\nassess their performance after quantized deployment on real smartphone NPUs.\nOur contributions provide a standardized framework for evaluating on-device\nLLMs in Chinese, promoting further development and optimization in this\ncritical area. Code and data will be available at\nhttps://github.com/vivo-ai-lab/SmartBench."
                },
                "authors": [
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Haohao Gao"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Shuai Ren"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Fangyuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Fangyuan Li"
                },
                "author": "Fangyuan Li",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19074v1",
                "updated": "2025-08-26T14:32:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    32,
                    49,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:32:49Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    32,
                    49,
                    1,
                    238,
                    0
                ],
                "title": "An LLM-powered Natural-to-Robotic Language Translation Framework with\n  Correctness Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-powered Natural-to-Robotic Language Translation Framework with\n  Correctness Guarantees"
                },
                "summary": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Language Models (LLM) are increasingly being deployed in robotics\nto generate robot control programs for specific user tasks, enabling embodied\nintelligence. Existing methods primarily focus on LLM training and prompt\ndesign that utilize LLMs to generate executable programs directly from user\ntasks in natural language. However, due to the inconsistency of the LLMs and\nthe high complexity of the tasks, such best-effort approaches often lead to\ntremendous programming errors in the generated code, which significantly\nundermines the effectiveness especially when the light-weight LLMs are applied.\nThis paper introduces a natural-robotic language translation framework that (i)\nprovides correctness verification for generated control programs and (ii)\nenhances the performance of LLMs in program generation via feedback-based\nfine-tuning for the programs. To achieve this, a Robot Skill Language (RSL) is\nproposed to abstract away from the intricate details of the control programs,\nbridging the natural language tasks with the underlying robot skills. Then, the\nRSL compiler and debugger are constructed to verify RSL programs generated by\nthe LLM and provide error feedback to the LLM for refining the outputs until\nbeing verified by the compiler. This provides correctness guarantees for the\nLLM-generated programs before being offloaded to the robots for execution,\nsignificantly enhancing the effectiveness of LLM-powered robotic applications.\nExperiments demonstrate NRTrans outperforms the existing method under a range\nof LLMs and tasks, and achieves a high success rate for light-weight LLMs."
                },
                "authors": [
                    {
                        "name": "ZhenDong Chen"
                    },
                    {
                        "name": "ZhanShang Nie"
                    },
                    {
                        "name": "ShiXing Wan"
                    },
                    {
                        "name": "JunYi Li"
                    },
                    {
                        "name": "YongTian Cheng"
                    },
                    {
                        "name": "Shuai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Zhao"
                },
                "author": "Shuai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19069v1",
                "updated": "2025-08-26T14:26:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    26,
                    32,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:26:32Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    26,
                    32,
                    1,
                    238,
                    0
                ],
                "title": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An\n  Exploration of Scaling Laws by Difficulty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Structured Templates Facilitate LLMs in Tackling Harder Tasks? : An\n  Exploration of Scaling Laws by Difficulty"
                },
                "summary": "Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured, procedural reasoning is essential for Large Language Models\n(LLMs), especially in mathematics. While post-training methods have improved\nLLM performance, they still fall short in capturing deep procedural logic on\ncomplex tasks. To tackle the issue, in this paper, we first investigate this\nlimitation and uncover a novel finding: a Scaling Law by Difficulty, which\nreveals that model performance follows a U-shaped curve with respect to\ntraining data complexity -- excessive low-difficulty data impedes abstraction,\nwhile high-difficulty data significantly enhances reasoning ability. Motivated\nby this, we propose the Structured Solution Template (SST) framework, which\nuses solution templates and a curriculum of varied difficulty to explicitly\nteach procedural reasoning. Specifically, SST comprises (1) fine-tuning with\nstructured solution-template chains and dynamically weighted loss to prioritize\nprocedural logic, (2) prompt-time injection of solution templates as cognitive\nscaffolds to guide inference, and (3) integrated curriculum fine-tuning that\nexplicitly teaches the model to self-plan - execute - self-correct. Experiments\non GSM8K, AIME24, and new Dynamic En benchmark show that SST significantly\nimproves both accuracy and efficiency, especially on harder problems."
                },
                "authors": [
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Yuanze Hu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Ye Qiu"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Wenjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Wu"
                },
                "author": "Wenjun Wu",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20430v2",
                "updated": "2025-08-26T14:13:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    13,
                    25,
                    1,
                    238,
                    0
                ],
                "published": "2025-06-25T13:42:26Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    42,
                    26,
                    2,
                    176,
                    0
                ],
                "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning"
                },
                "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor."
                },
                "authors": [
                    {
                        "name": "Weike Zhao"
                    },
                    {
                        "name": "Chaoyi Wu"
                    },
                    {
                        "name": "Yanjie Fan"
                    },
                    {
                        "name": "Xiaoman Zhang"
                    },
                    {
                        "name": "Pengcheng Qiu"
                    },
                    {
                        "name": "Yuze Sun"
                    },
                    {
                        "name": "Xiao Zhou"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Ya Zhang"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Kun Sun"
                    },
                    {
                        "name": "Weidi Xie"
                    }
                ],
                "author_detail": {
                    "name": "Weidi Xie"
                },
                "author": "Weidi Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17178v2",
                "updated": "2025-08-26T14:11:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    11,
                    22,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-23T03:52:24Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    3,
                    52,
                    24,
                    2,
                    204,
                    0
                ],
                "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs"
                },
                "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Enpei Niu"
                    },
                    {
                        "name": "Yin Hua"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19055v1",
                "updated": "2025-08-26T14:11:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    11,
                    22,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T14:11:22Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    14,
                    11,
                    22,
                    1,
                    238,
                    0
                ],
                "title": "Private Quantum Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Quantum Database"
                },
                "summary": "Quantum databases open an exciting new frontier in data management by\noffering privacy guarantees that classical systems cannot match. Traditional\nengines tackle user privacy, which hides the records being queried, or data\nprivacy, which prevents a user from learning more than she has queried. We\npropose a quantum database that protects both by leveraging quantum mechanics:\nwhen the user measures her chosen basis, the superposition collapses and the\nunqueried rows become physically inaccessible. We encode relational tables as a\nsequence of Quantum Random Access Codes (QRACs) over mutually unbiased bases\n(MUBs), transmit a bounded number of quantum states, and let a single,\ndestructive measurement reconstruct only the selected tuple. This allows us to\npreserve data privacy and user privacy at once without trusted hardware or\nheavyweight cryptography. Moreover, we envision a novel hybrid\nquantum-classical architecture ready for early deployment, which ensures\ncompatibility with the limitations of today's Noisy Intermediate-Scale Quantum\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum databases open an exciting new frontier in data management by\noffering privacy guarantees that classical systems cannot match. Traditional\nengines tackle user privacy, which hides the records being queried, or data\nprivacy, which prevents a user from learning more than she has queried. We\npropose a quantum database that protects both by leveraging quantum mechanics:\nwhen the user measures her chosen basis, the superposition collapses and the\nunqueried rows become physically inaccessible. We encode relational tables as a\nsequence of Quantum Random Access Codes (QRACs) over mutually unbiased bases\n(MUBs), transmit a bounded number of quantum states, and let a single,\ndestructive measurement reconstruct only the selected tuple. This allows us to\npreserve data privacy and user privacy at once without trusted hardware or\nheavyweight cryptography. Moreover, we envision a novel hybrid\nquantum-classical architecture ready for early deployment, which ensures\ncompatibility with the limitations of today's Noisy Intermediate-Scale Quantum\ndevices."
                },
                "authors": [
                    {
                        "name": "Giancarlo Gatti"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19042v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19042v1",
                "updated": "2025-08-26T13:58:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    58,
                    31,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:58:31Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    58,
                    31,
                    1,
                    238,
                    0
                ],
                "title": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Concurrent Modular Agent: Framework for Autonomous LLM Agents"
                },
                "summary": "We introduce the Concurrent Modular Agent (CMA), a framework that\norchestrates multiple Large-Language-Model (LLM)-based modules that operate\nfully asynchronously yet maintain a coherent and fault-tolerant behavioral\nloop. This framework addresses long-standing difficulties in agent\narchitectures by letting intention emerge from language-mediated interactions\namong autonomous processes. This approach enables flexible, adaptive, and\ncontext-dependent behavior through the combination of concurrently executed\nmodules that offload reasoning to an LLM, inter-module communication, and a\nsingle shared global state.We consider this approach to be a practical\nrealization of Minsky's Society of Mind theory. We demonstrate the viability of\nour system through two practical use-case studies. The emergent properties\nobserved in our system suggest that complex cognitive phenomena like\nself-awareness may indeed arise from the organized interaction of simpler\nprocesses, supporting Minsky-Society of Mind concept and opening new avenues\nfor artificial intelligence research. The source code for our work is available\nat: https://github.com/AlternativeMachine/concurrent-modular-agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Concurrent Modular Agent (CMA), a framework that\norchestrates multiple Large-Language-Model (LLM)-based modules that operate\nfully asynchronously yet maintain a coherent and fault-tolerant behavioral\nloop. This framework addresses long-standing difficulties in agent\narchitectures by letting intention emerge from language-mediated interactions\namong autonomous processes. This approach enables flexible, adaptive, and\ncontext-dependent behavior through the combination of concurrently executed\nmodules that offload reasoning to an LLM, inter-module communication, and a\nsingle shared global state.We consider this approach to be a practical\nrealization of Minsky's Society of Mind theory. We demonstrate the viability of\nour system through two practical use-case studies. The emergent properties\nobserved in our system suggest that complex cognitive phenomena like\nself-awareness may indeed arise from the organized interaction of simpler\nprocesses, supporting Minsky-Society of Mind concept and opening new avenues\nfor artificial intelligence research. The source code for our work is available\nat: https://github.com/AlternativeMachine/concurrent-modular-agent."
                },
                "authors": [
                    {
                        "name": "Norihiro Maruyama"
                    },
                    {
                        "name": "Takahide Yoshida"
                    },
                    {
                        "name": "Hiroki Sato"
                    },
                    {
                        "name": "Atsushi Masumori"
                    },
                    {
                        "name": "Johnsmith"
                    },
                    {
                        "name": "Takashi Ikegami"
                    }
                ],
                "author_detail": {
                    "name": "Takashi Ikegami"
                },
                "author": "Takashi Ikegami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19042v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19042v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02223v3",
                "updated": "2025-08-26T13:55:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    55,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2024-08-05T03:54:52Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    3,
                    54,
                    52,
                    0,
                    218,
                    0
                ],
                "title": "Large Language Model Aided QoS Prediction for Service Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Aided QoS Prediction for Service Recommendation"
                },
                "summary": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen rapid improvement in the recent years,\nand have been used in a wider range of applications. After being trained on\nlarge text corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. On the WSDream dataset, llmQoS is shown to overcome\nthe data sparsity issue inherent to the QoS prediction problem, and outperforms\ncomparable baseline models consistently."
                },
                "authors": [
                    {
                        "name": "Huiying Liu"
                    },
                    {
                        "name": "Zekun Zhang"
                    },
                    {
                        "name": "Honghao Li"
                    },
                    {
                        "name": "Qilin Wu"
                    },
                    {
                        "name": "Yiwen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiwen Zhang"
                },
                "author": "Yiwen Zhang",
                "arxiv_doi": "10.1109/SSE67621.2025.00023",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SSE67621.2025.00023",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.02223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19035v1",
                "updated": "2025-08-26T13:54:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    54,
                    17,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:54:17Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    54,
                    17,
                    1,
                    238,
                    0
                ],
                "title": "Investigating Advanced Reasoning of Large Language Models via Black-Box\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Advanced Reasoning of Large Language Models via Black-Box\n  Interaction"
                },
                "summary": "Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing tasks fall short in evaluating reasoning ability of Large Language\nModels (LLMs) in an interactive, unknown environment. This deficiency leads to\nthe isolated assessment of deductive, inductive, and abductive reasoning,\nneglecting the integrated reasoning process that is indispensable for humans\ndiscovery of real world. We introduce a novel evaluation paradigm,\n\\textit{black-box interaction}, to tackle this challenge. A black-box is\ndefined by a hidden function that maps a specific set of inputs to outputs.\nLLMs are required to unravel the hidden function behind the black-box by\ninteracting with it in given exploration turns, and reasoning over observed\ninput-output pairs. Leveraging this idea, we build the \\textsc{Oracle}\nbenchmark which comprises 6 types of black-box task and 96 black-boxes. 19\nmodern LLMs are benchmarked. o3 ranks first in 5 of the 6 tasks, achieving over\n70\\% accuracy on most easy black-boxes. But it still struggles with some hard\nblack-box tasks, where its average performance drops below 40\\%. Further\nanalysis indicates a universal difficulty among LLMs: They lack the high-level\nplanning capability to develop efficient and adaptive exploration strategies\nfor hypothesis refinement."
                },
                "authors": [
                    {
                        "name": "Congchi Yin"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Yankai Shu"
                    },
                    {
                        "name": "Alex Gu"
                    },
                    {
                        "name": "Yunhan Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19026v1",
                "updated": "2025-08-26T13:43:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    43,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:43:45Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    43,
                    45,
                    1,
                    238,
                    0
                ],
                "title": "MovieCORE: COgnitive REasoning in Movies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MovieCORE: COgnitive REasoning in Movies"
                },
                "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html."
                },
                "authors": [
                    {
                        "name": "Gueter Josmy Faure"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Jia-Fong Yeh"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Hung-Ting Su"
                    },
                    {
                        "name": "Yung-Hao Tang"
                    },
                    {
                        "name": "Shang-Hong Lai"
                    },
                    {
                        "name": "Winston H. Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Winston H. Hsu"
                },
                "author": "Winston H. Hsu",
                "arxiv_comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19402v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19402v3",
                "updated": "2025-08-26T13:31:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    31,
                    48,
                    1,
                    238,
                    0
                ],
                "published": "2025-02-26T18:51:12Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    18,
                    51,
                    12,
                    2,
                    57,
                    0
                ],
                "title": "General Intelligence Requires Reward-based Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General Intelligence Requires Reward-based Pretraining"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a reasoning\nprior for RL that can then be transferred to natural language tasks, and (3)\nlearning more generalizable reasoning functions using a small context window to\nreduce exploiting spurious correlations between tokens. Such a reasoning system\ncoupled with a trained retrieval system and a large external memory bank as a\nknowledge store can overcome several limitations of existing architectures at\nlearning to reason in novel scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive real-world utility,\nexemplifying artificial useful intelligence (AUI). However, their ability to\nreason adaptively and robustly -- the hallmarks of artificial general\nintelligence (AGI) -- remains fragile. While LLMs seemingly succeed in\ncommonsense reasoning, programming, and mathematics, they struggle to\ngeneralize algorithmic understanding across novel contexts. Our experiments\nwith algorithmic tasks in esoteric programming languages reveal that LLM's\nreasoning overfits to the training data and is limited in its transferability.\nWe hypothesize that the core issue underlying such limited transferability is\nthe coupling of reasoning and knowledge in LLMs.\n  To transition from AUI to AGI, we propose disentangling knowledge and\nreasoning through three key directions: (1) pretaining to reason using RL from\nscratch as an alternative to the widely used next-token prediction pretraining,\n(2) using a curriculum of synthetic tasks to ease the learning of a reasoning\nprior for RL that can then be transferred to natural language tasks, and (3)\nlearning more generalizable reasoning functions using a small context window to\nreduce exploiting spurious correlations between tokens. Such a reasoning system\ncoupled with a trained retrieval system and a large external memory bank as a\nknowledge store can overcome several limitations of existing architectures at\nlearning to reason in novel scenarios."
                },
                "authors": [
                    {
                        "name": "Seungwook Han"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Pulkit Agrawal"
                },
                "author": "Pulkit Agrawal",
                "arxiv_comment": "https://improbableai.notion.site/General-Intelligence-Requires-Reward-Based-Pretraining-2023b66e4cf580d3ab44c7860b75d25f?pvs=73",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19402v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19402v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19008v1",
                "updated": "2025-08-26T13:13:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    13,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:13:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    13,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "Sense of Self and Time in Borderline Personality. A Comparative\n  Robustness Study with Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sense of Self and Time in Borderline Personality. A Comparative\n  Robustness Study with Generative AI"
                },
                "summary": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study examines the capacity of large language models (LLMs) to support\nphenomenological qualitative analysis of first-person experience in Borderline\nPersonality Disorder (BPD), understood as a disorder of temporality and\nselfhood. Building on a prior human-led thematic analysis of 24 inpatients'\nlife-story interviews, we compared three LLMs (OpenAI GPT-4o, Google Gemini 2.5\nPro, Anthropic Claude Opus 4) prompted to mimic the interpretative style of the\noriginal investigators. The models were evaluated with blinded and non-blinded\nexpert judges in phenomenology and clinical psychology. Assessments included\nsemantic congruence, Jaccard coefficients, and multidimensional validity\nratings (credibility, coherence, substantiveness, and groundness in data).\nResults showed variable overlap with the human analysis, from 0 percent in GPT\nto 42 percent in Claude and 58 percent in Gemini, and a low Jaccard coefficient\n(0.21-0.28). However, the models recovered themes omitted by humans. Gemini's\noutput most closely resembled the human analysis, with validity scores\nsignificantly higher than GPT and Claude (p < 0.0001), and was judged as human\nby blinded experts. All scores strongly correlated (R > 0.78) with the quantity\nof text and words per theme, highlighting both the variability and potential of\nAI-augmented thematic analysis to mitigate human interpretative bias."
                },
                "authors": [
                    {
                        "name": "Marcin Moskalewicz"
                    },
                    {
                        "name": "Anna Sterna"
                    },
                    {
                        "name": "Marek Pokropski"
                    },
                    {
                        "name": "Paula Flores"
                    }
                ],
                "author_detail": {
                    "name": "Paula Flores"
                },
                "author": "Paula Flores",
                "arxiv_comment": "22 pages, 4 tables, submitted to \"Personality and Individual\n  Differences\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19005v1",
                "updated": "2025-08-26T13:04:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    4,
                    28,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T13:04:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    13,
                    4,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A\n  Framework and Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A\n  Framework and Benchmark"
                },
                "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI."
                },
                "authors": [
                    {
                        "name": "Yuxuan Cai"
                    },
                    {
                        "name": "Yipeng Hao"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Rui Zhen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Yutao Yang"
                    },
                    {
                        "name": "Junsong Li"
                    },
                    {
                        "name": "Qianjun Pan"
                    },
                    {
                        "name": "Tianyu Huai"
                    },
                    {
                        "name": "Qin Chen"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Liang He"
                    }
                ],
                "author_detail": {
                    "name": "Liang He"
                },
                "author": "Liang He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13500v2",
                "updated": "2025-08-26T12:59:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    59,
                    55,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-19T04:20:14Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    4,
                    20,
                    14,
                    1,
                    231,
                    0
                ],
                "title": "LLM-Enhanced Linear Autoencoders for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enhanced Linear Autoencoders for Recommendation"
                },
                "summary": "Large language models (LLMs) have been widely adopted to enrich the semantic\nrepresentation of textual item information in recommender systems. However,\nexisting linear autoencoders (LAEs) that incorporate textual information rely\non sparse word co-occurrence patterns, limiting their ability to capture rich\ntextual semantics. To address this, we propose L3AE, the first integration of\nLLMs into the LAE framework. L3AE effectively integrates the heterogeneous\nknowledge of textual semantics and user-item interactions through a two-phase\noptimization strategy. (i) L3AE first constructs a semantic item-to-item\ncorrelation matrix from LLM-derived item representations. (ii) It then learns\nan item-to-item weight matrix from collaborative signals while distilling\nsemantic item correlations as regularization. Notably, each phase of L3AE is\noptimized through closed-form solutions, ensuring global optimality and\ncomputational efficiency. Extensive experiments demonstrate that L3AE\nconsistently outperforms state-of-the-art LLM-enhanced models on three\nbenchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.\nThe source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted to enrich the semantic\nrepresentation of textual item information in recommender systems. However,\nexisting linear autoencoders (LAEs) that incorporate textual information rely\non sparse word co-occurrence patterns, limiting their ability to capture rich\ntextual semantics. To address this, we propose L3AE, the first integration of\nLLMs into the LAE framework. L3AE effectively integrates the heterogeneous\nknowledge of textual semantics and user-item interactions through a two-phase\noptimization strategy. (i) L3AE first constructs a semantic item-to-item\ncorrelation matrix from LLM-derived item representations. (ii) It then learns\nan item-to-item weight matrix from collaborative signals while distilling\nsemantic item correlations as regularization. Notably, each phase of L3AE is\noptimized through closed-form solutions, ensuring global optimality and\ncomputational efficiency. Extensive experiments demonstrate that L3AE\nconsistently outperforms state-of-the-art LLM-enhanced models on three\nbenchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.\nThe source code is available at https://github.com/jaewan7599/L3AE_CIKM2025."
                },
                "authors": [
                    {
                        "name": "Jaewan Moon"
                    },
                    {
                        "name": "Seongmin Park"
                    },
                    {
                        "name": "Jongwuk Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jongwuk Lee"
                },
                "author": "Jongwuk Lee",
                "arxiv_doi": "10.1145/3746252.3760952",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3760952",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15386v2",
                "updated": "2025-08-26T12:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    55,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-21T11:23:05Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    11,
                    23,
                    5,
                    2,
                    141,
                    0
                ],
                "title": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation\n  and Language Generation for Explainable QA Hallucination Detection"
                },
                "summary": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful, but hallucinations remain\na vital obstacle to their trustworthy use. While previous works improved the\ncapability of hallucination detection by measuring uncertainty, they all lack\nthe ability to explain the provenance behind why hallucinations occur, i.e.,\nwhich part of the inputs tends to trigger hallucinations. Recent works on the\nprompt attack indicate that uncertainty exists in semantic propagation, where\nattention mechanisms gradually fuse local token information into high-level\nsemantics across layers. Meanwhile, uncertainty also emerges in language\ngeneration, due to its probability-based selection of high-level semantics for\nsampled generations. Based on that, we propose RePPL to recalibrate uncertainty\nmeasurement by these two aspects, which dispatches explainable uncertainty\nscores to each token and aggregates in Perplexity-style Log-Average form as\ntotal score. Experiments show that our method achieves the best comprehensive\ndetection performance across various QA datasets on advanced models (average\nAUC of 0.833), and our method is capable of producing token-level uncertainty\nscores as explanations for the hallucination. Leveraging these scores, we\npreliminarily find the chaotic pattern of hallucination and showcase its\npromising usage."
                },
                "authors": [
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Biquan Bie"
                    },
                    {
                        "name": "Yunzhong Qiu"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18998v1",
                "updated": "2025-08-26T12:54:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    54,
                    23,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:54:23Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    54,
                    23,
                    1,
                    238,
                    0
                ],
                "title": "MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in\n  LLM-based Multilingual ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MOSA: Mixtures of Simple Adapters Outperform Monolithic Approaches in\n  LLM-based Multilingual ASR"
                },
                "summary": "End-to-end multilingual ASR aims to transcribe speech from different\nlanguages into corresponding text, but is often limited by scarce multilingual\ndata. LLM-based ASR aligns speech encoder outputs with LLM input space via a\nprojector and has achieved notable success. However, prior work mainly improves\nperformance by increasing data, with little focus on cross-lingual knowledge\nsharing. Moreover, a single complex projector struggles to capture both shared\nand language-specific features effectively. In this work, we propose MOSA\n(Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to\ncombine lightweight adapters that learn shared and language-specific knowledge.\nThis enables better utilization of high-resource language data to support\nlow-resource languages, mitigating data scarcity issues. Experimental results\nshow that MOSA-Base achieves a 15.4\\% relative reduction in average WER\ncompared to the Baseline-Base and consistently outperforms it across all\nlanguages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained\nwith only 60\\% of its parameters. Similarly, MOSA-Large outperforms the\nBaseline-Large in average WER and demonstrates greater robustness to data\nimbalance. Ablation studies further indicate that MOSA is more effective at\nhandling individual languages and learning both language-specific and shared\nlinguistic knowledge. These findings support that, in LLM-based ASR, a mixture\nof simple adapters is more effective than a single, complex adapter design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end multilingual ASR aims to transcribe speech from different\nlanguages into corresponding text, but is often limited by scarce multilingual\ndata. LLM-based ASR aligns speech encoder outputs with LLM input space via a\nprojector and has achieved notable success. However, prior work mainly improves\nperformance by increasing data, with little focus on cross-lingual knowledge\nsharing. Moreover, a single complex projector struggles to capture both shared\nand language-specific features effectively. In this work, we propose MOSA\n(Mixture of Simple Adapters), leveraging a Mixture-of-Experts mechanism to\ncombine lightweight adapters that learn shared and language-specific knowledge.\nThis enables better utilization of high-resource language data to support\nlow-resource languages, mitigating data scarcity issues. Experimental results\nshow that MOSA-Base achieves a 15.4\\% relative reduction in average WER\ncompared to the Baseline-Base and consistently outperforms it across all\nlanguages. Remarkably, MOSA-Base surpasses the Baseline-Base even when trained\nwith only 60\\% of its parameters. Similarly, MOSA-Large outperforms the\nBaseline-Large in average WER and demonstrates greater robustness to data\nimbalance. Ablation studies further indicate that MOSA is more effective at\nhandling individual languages and learning both language-specific and shared\nlinguistic knowledge. These findings support that, in LLM-based ASR, a mixture\nof simple adapters is more effective than a single, complex adapter design."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Jing Peng"
                    },
                    {
                        "name": "Yangui Fang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18993v1",
                "updated": "2025-08-26T12:48:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    48,
                    5,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:48:05Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    48,
                    5,
                    1,
                    238,
                    0
                ],
                "title": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks\n  Through Code Repository Leveraging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks\n  Through Code Repository Leveraging"
                },
                "summary": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond scratch coding, exploiting large-scale code repositories (e.g.,\nGitHub) for practical tasks is vital in real-world software development, yet\ncurrent benchmarks rarely evaluate code agents in such authentic,\nworkflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a\nbenchmark designed to systematically assess this capability via 54 realistic\ntasks across 7 modalities and 7 domains. Each task pairs a relevant repository\nwith an automated, human-curated evaluation harness specifying practical\nsuccess criteria. Beyond measuring execution and task success, we also propose\nthe alpha-value metric to quantify the economic benefit of agent performance,\nwhich integrates task success rates, token cost, and average developer\nsalaries. Experiments across three state-of-the-art agent frameworks with\nmultiple advanced LLMs show that leveraging code repositories for complex task\nsolving remains challenging: even the best-performing system, OpenHands+Claude\n3.7, solves only 48.15% of tasks. Error analysis attributes over half of\nfailures to seemingly mundane yet critical steps like environment setup and\ndependency resolution, highlighting the need for more robust workflow\nmanagement and increased timeout preparedness. By releasing GitTaskBench, we\naim to drive progress and attention toward repository-aware code reasoning,\nexecution, and deployment -- moving agents closer to solving complex,\nend-to-end real-world tasks. The benchmark and code are open-sourced at\nhttps://github.com/QuantaAlpha/GitTaskBench."
                },
                "authors": [
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Huacan Wang"
                    },
                    {
                        "name": "Shuo Zhang"
                    },
                    {
                        "name": "Shuo Lu"
                    },
                    {
                        "name": "Ziyang He"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Bill Sun"
                    },
                    {
                        "name": "Hongzhang Liu"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ronghao Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Pin Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Pin Lyu"
                },
                "author": "Pin Lyu",
                "arxiv_comment": "Highly practical, Well-motivated, Actionable",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18992v1",
                "updated": "2025-08-26T12:46:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    46,
                    58,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:46:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    46,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "Automatic Prompt Optimization with Prompt Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Prompt Optimization with Prompt Distillation"
                },
                "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting."
                },
                "authors": [
                    {
                        "name": "Viktor N. Zhuravlev"
                    },
                    {
                        "name": "Artur R. Khairullin"
                    },
                    {
                        "name": "Ernest A. Dyagin"
                    },
                    {
                        "name": "Alena N. Sitkina"
                    },
                    {
                        "name": "Nikita I. Kulin"
                    }
                ],
                "author_detail": {
                    "name": "Nikita I. Kulin"
                },
                "author": "Nikita I. Kulin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00624v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00624v2",
                "updated": "2025-08-26T12:41:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    41,
                    1,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-31T16:28:51Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    16,
                    28,
                    51,
                    5,
                    151,
                    0
                ],
                "title": "Real-Time Sounding in ISAC networks: Design and Implementation of a\n  Multi-Node Testbed with Synchronized Airborne and Ground-Based Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Sounding in ISAC networks: Design and Implementation of a\n  Multi-Node Testbed with Synchronized Airborne and Ground-Based Sensors"
                },
                "summary": "As integrated sensing and communication (ISAC) capabilities become more\nprevalent in the mobile 6G radio landscape, there is a substantial opportunity\nto enhance situational awareness across diverse applications through\nmulti-static radar sensing within meshed ISAC networks. To facilitate the\ndevelopment and testing of detection and localization algorithms across diverse\nscenarios, this paper introduces a synchronized distributed channel sounding\ntestbed with airborne and ground-based multi-channel transceiver nodes with\ncentimeter-level positioning accuracy enabled by real-time kinematic (RTK) and\ninertial navigation system (INS) data. Our modular experimental measurement\nsystem is designed to include stationary sensor nodes and light-weight to\nmedium-weight mobile nodes deployable on unmanned aerial vehicles (UAVs), cars,\npedestrians, and cyclists. Utilizing commercial off-the-shelf (COTS) hardware,\nspecifically software defined radios (SDRs), the testbed encourages\nreproducibility in academic research laboratories. We detail the individual\nmodules and integration steps required to achieve the specified performance.\nThe testbed's capabilities are validated through a real-world measurement\ncampaign, including stationary and flying sensor nodes, aimed at detecting\nradar targets such as vertical take-off and landing (VTOL) aircrafts, small\nhexacopters, cars and vulnerable road users (VRUs) in air-to-air (A2A) and\nair-to-ground (A2G) scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As integrated sensing and communication (ISAC) capabilities become more\nprevalent in the mobile 6G radio landscape, there is a substantial opportunity\nto enhance situational awareness across diverse applications through\nmulti-static radar sensing within meshed ISAC networks. To facilitate the\ndevelopment and testing of detection and localization algorithms across diverse\nscenarios, this paper introduces a synchronized distributed channel sounding\ntestbed with airborne and ground-based multi-channel transceiver nodes with\ncentimeter-level positioning accuracy enabled by real-time kinematic (RTK) and\ninertial navigation system (INS) data. Our modular experimental measurement\nsystem is designed to include stationary sensor nodes and light-weight to\nmedium-weight mobile nodes deployable on unmanned aerial vehicles (UAVs), cars,\npedestrians, and cyclists. Utilizing commercial off-the-shelf (COTS) hardware,\nspecifically software defined radios (SDRs), the testbed encourages\nreproducibility in academic research laboratories. We detail the individual\nmodules and integration steps required to achieve the specified performance.\nThe testbed's capabilities are validated through a real-world measurement\ncampaign, including stationary and flying sensor nodes, aimed at detecting\nradar targets such as vertical take-off and landing (VTOL) aircrafts, small\nhexacopters, cars and vulnerable road users (VRUs) in air-to-air (A2A) and\nair-to-ground (A2G) scenarios."
                },
                "authors": [
                    {
                        "name": "Julia Beuster"
                    },
                    {
                        "name": "Carsten Andrich"
                    },
                    {
                        "name": "Sebastian Giehl"
                    },
                    {
                        "name": "Marc Miranda"
                    },
                    {
                        "name": "Lorenz Mohr"
                    },
                    {
                        "name": "Dieter Novotny"
                    },
                    {
                        "name": "Tom Kaufmann"
                    },
                    {
                        "name": "Christian Schneider"
                    },
                    {
                        "name": "Reiner S. Thomä"
                    }
                ],
                "author_detail": {
                    "name": "Reiner S. Thomä"
                },
                "author": "Reiner S. Thomä",
                "arxiv_comment": "submission at WSA2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00624v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00624v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13972v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13972v3",
                "updated": "2025-08-27T14:04:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    14,
                    4,
                    13,
                    2,
                    239,
                    0
                ],
                "published": "2025-05-20T06:12:17Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    6,
                    12,
                    17,
                    1,
                    140,
                    0
                ],
                "title": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping\n  Evaluation in LLM-based Counterfactuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truth or Twist? Optimal Model Selection for Reliable Label Flipping\n  Evaluation in LLM-based Counterfactuals"
                },
                "summary": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models: being the same model, belonging to the same model family, being\nindependent models, and having an distillation relationship. Through extensive\nexperiments involving two state-of-the-art LLM-based methods, three datasets,\nfour generator models, and 15 judge models, complemented by a user study (n =\n90), we demonstrate that judge models with an independent, non-fine-tuned\nrelationship to the generator model provide the most reliable label flipping\nevaluations. Relationships between the generator and judge models, which are\nclosely aligned with the user study for CDA, result in better model performance\nand robustness. Nevertheless, we find that the gap between the most effective\njudge models and the results obtained from the user study remains considerably\nlarge. This suggests that a fully automated pipeline for CDA may be inadequate\nand requires human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual examples are widely employed to enhance the performance and\nrobustness of large language models (LLMs) through counterfactual data\naugmentation (CDA). However, the selection of the judge model used to evaluate\nlabel flipping, the primary metric for assessing the validity of generated\ncounterfactuals for CDA, yields inconsistent results. To decipher this, we\ndefine four types of relationships between the counterfactual generator and\njudge models: being the same model, belonging to the same model family, being\nindependent models, and having an distillation relationship. Through extensive\nexperiments involving two state-of-the-art LLM-based methods, three datasets,\nfour generator models, and 15 judge models, complemented by a user study (n =\n90), we demonstrate that judge models with an independent, non-fine-tuned\nrelationship to the generator model provide the most reliable label flipping\nevaluations. Relationships between the generator and judge models, which are\nclosely aligned with the user study for CDA, result in better model performance\nand robustness. Nevertheless, we find that the gap between the most effective\njudge models and the results obtained from the user study remains considerably\nlarge. This suggests that a fully automated pipeline for CDA may be inadequate\nand requires human intervention."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Van Bach Nguyen"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Luis Felipe Villa-Arenas"
                    },
                    {
                        "name": "Christin Seifert"
                    },
                    {
                        "name": "Sebastian Möller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "Accepted at INLG 2025, camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13972v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13972v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18976v1",
                "updated": "2025-08-26T12:22:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    22,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:22:45Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    22,
                    45,
                    1,
                    238,
                    0
                ],
                "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding\n  and Mitigating Contextual Vulnerability in Word-level Differential Privacy\n  Text Sanitization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding\n  and Mitigating Contextual Vulnerability in Word-level Differential Privacy\n  Text Sanitization"
                },
                "summary": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially."
                },
                "authors": [
                    {
                        "name": "Stephen Meisenbacher"
                    },
                    {
                        "name": "Alexandra Klymenko"
                    },
                    {
                        "name": "Andreea-Elena Bodea"
                    },
                    {
                        "name": "Florian Matthes"
                    }
                ],
                "author_detail": {
                    "name": "Florian Matthes"
                },
                "author": "Florian Matthes",
                "arxiv_doi": "10.1145/3733802.3764058",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3733802.3764058",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06905v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06905v3",
                "updated": "2025-08-26T12:18:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    18,
                    14,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-09T09:36:21Z",
                "published_parsed": [
                    2025,
                    8,
                    9,
                    9,
                    36,
                    21,
                    5,
                    221,
                    0
                ],
                "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiRef: Controllable Image Generation with Multiple Visual References"
                },
                "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/."
                },
                "authors": [
                    {
                        "name": "Ruoxi Chen"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Sinan Wang"
                    },
                    {
                        "name": "Shiyun Lang"
                    },
                    {
                        "name": "Petr Sushko"
                    },
                    {
                        "name": "Gaoyang Jiang"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Ranjay Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Ranjay Krishna"
                },
                "author": "Ranjay Krishna",
                "arxiv_comment": "Accepted to ACM MM 2025 Datasets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06905v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06905v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16267v2",
                "updated": "2025-08-26T11:54:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    54,
                    16,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-22T09:59:23Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    9,
                    59,
                    23,
                    4,
                    234,
                    0
                ],
                "title": "From Confidence to Collapse in LLM Factual Robustness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Confidence to Collapse in LLM Factual Robustness"
                },
                "summary": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."
                },
                "authors": [
                    {
                        "name": "Alina Fastowski"
                    },
                    {
                        "name": "Bardh Prenkaj"
                    },
                    {
                        "name": "Gjergji Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Gjergji Kasneci"
                },
                "author": "Gjergji Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18955v1",
                "updated": "2025-08-26T11:49:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    49,
                    58,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:49:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    49,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "Interleaving Large Language Models for Compiler Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interleaving Large Language Models for Compiler Testing"
                },
                "summary": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing compilers with AI models, especially large language models (LLMs),\nhas shown great promise. However, current approaches struggle with two key\nproblems: The generated programs for testing compilers are often too simple,\nand extensive testing with the LLMs is computationally expensive. In this\npaper, we propose a novel compiler testing framework that decouples the testing\nprocess into two distinct phases: an offline phase and an online phase. In the\noffline phase, we use LLMs to generate a collection of small but feature-rich\ncode pieces. In the online phase, we reuse these code pieces by strategically\ncombining them to build high-quality and valid test programs, which are then\nused to test compilers.\n  We implement this idea in a tool, LegoFuzz, for testing C compilers. The\nresults are striking: we found 66 bugs in GCC and LLVM, the most widely used C\ncompilers. Almost half of the bugs are miscompilation bugs, which are serious\nand hard-to-find bugs that none of the existing LLM-based tools could find. We\nbelieve this efficient design opens up new possibilities for using AI models in\nsoftware testing beyond just C compilers."
                },
                "authors": [
                    {
                        "name": "Yunbo Ni"
                    },
                    {
                        "name": "Shaohua Li"
                    }
                ],
                "author_detail": {
                    "name": "Shaohua Li"
                },
                "author": "Shaohua Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18950v1",
                "updated": "2025-08-26T11:44:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    44,
                    30,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:44:30Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    44,
                    30,
                    1,
                    238,
                    0
                ],
                "title": "SIREN: Software Identification and Recognition in HPC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIREN: Software Identification and Recognition in HPC Systems"
                },
                "summary": "HPC systems use monitoring and operational data analytics to ensure\nefficiency, performance, and orderly operations. Application-specific insights\nare crucial for analyzing the increasing complexity and diversity of HPC\nworkloads, particularly through the identification of unknown software and\nrecognition of repeated executions, which facilitate system optimization and\nsecurity improvements. However, traditional identification methods using job or\nfile names are unreliable for arbitrary user-provided names (a.out). Fuzzy\nhashing of executables detects similarities despite changes in executable\nversion or compilation approach while preserving privacy and file integrity,\novercoming these limitations. We introduce SIREN, a process-level data\ncollection framework for software identification and recognition. SIREN\nimproves observability in HPC by enabling analysis of process metadata,\nenvironment information, and executable fuzzy hashes. Findings from a first\nopt-in deployment campaign on LUMI show SIREN's ability to provide insights\ninto software usage, recognition of repeated executions of known applications,\nand similarity-based identification of unknown applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPC systems use monitoring and operational data analytics to ensure\nefficiency, performance, and orderly operations. Application-specific insights\nare crucial for analyzing the increasing complexity and diversity of HPC\nworkloads, particularly through the identification of unknown software and\nrecognition of repeated executions, which facilitate system optimization and\nsecurity improvements. However, traditional identification methods using job or\nfile names are unreliable for arbitrary user-provided names (a.out). Fuzzy\nhashing of executables detects similarities despite changes in executable\nversion or compilation approach while preserving privacy and file integrity,\novercoming these limitations. We introduce SIREN, a process-level data\ncollection framework for software identification and recognition. SIREN\nimproves observability in HPC by enabling analysis of process metadata,\nenvironment information, and executable fuzzy hashes. Findings from a first\nopt-in deployment campaign on LUMI show SIREN's ability to provide insights\ninto software usage, recognition of repeated executions of known applications,\nand similarity-based identification of unknown applications."
                },
                "authors": [
                    {
                        "name": "Thomas Jakobsche"
                    },
                    {
                        "name": "Fredrik Robertsén"
                    },
                    {
                        "name": "Jessica R. Jones"
                    },
                    {
                        "name": "Utz-Uwe Haus"
                    },
                    {
                        "name": "Florina M. Ciorba"
                    }
                ],
                "author_detail": {
                    "name": "Florina M. Ciorba"
                },
                "author": "Florina M. Ciorba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.16902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.16902v2",
                "updated": "2025-08-26T11:41:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    41,
                    14,
                    1,
                    238,
                    0
                ],
                "published": "2023-06-29T12:48:00Z",
                "published_parsed": [
                    2023,
                    6,
                    29,
                    12,
                    48,
                    0,
                    3,
                    180,
                    0
                ],
                "title": "Integrating Large Language Model for Improved Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Model for Improved Causal Discovery"
                },
                "summary": "Recovering the structure of causal graphical models from observational data\nis an essential yet challenging task for causal discovery in scientific\nscenarios. Domain-specific causal discovery usually relies on expert validation\nor prior analysis to improve the reliability of recovered causality, which is\nyet limited by the scarcity of expert resources. Recently, Large Language\nModels (LLM) have been used for causal analysis across various domain-specific\nscenarios, suggesting its potential as autonomous expert roles in guiding\ndata-based structure learning. However, integrating LLMs into causal discovery\nfaces challenges due to inaccuracies in LLM-based reasoning on revealing the\nactual causal structure. To address this challenge, we propose an\nerror-tolerant LLM-driven causal discovery framework. The error-tolerant\nmechanism is designed three-fold with sufficient consideration on potential\ninaccuracies. In the LLM-based reasoning process, an accuracy-oriented\nprompting strategy restricts causal analysis to a reliable range. Next, a\nknowledge-to-structure transition aligns LLM-derived causal statements with\nstructural causal interactions. In the structure learning process, the\ngoodness-of-fit to data and adherence to LLM-derived priors are balanced to\nfurther address prior inaccuracies. Evaluation of eight real-world causal\nstructures demonstrates the efficacy of our LLM-driven approach in improving\ndata-based causal discovery, along with its robustness to inaccurate\nLLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering the structure of causal graphical models from observational data\nis an essential yet challenging task for causal discovery in scientific\nscenarios. Domain-specific causal discovery usually relies on expert validation\nor prior analysis to improve the reliability of recovered causality, which is\nyet limited by the scarcity of expert resources. Recently, Large Language\nModels (LLM) have been used for causal analysis across various domain-specific\nscenarios, suggesting its potential as autonomous expert roles in guiding\ndata-based structure learning. However, integrating LLMs into causal discovery\nfaces challenges due to inaccuracies in LLM-based reasoning on revealing the\nactual causal structure. To address this challenge, we propose an\nerror-tolerant LLM-driven causal discovery framework. The error-tolerant\nmechanism is designed three-fold with sufficient consideration on potential\ninaccuracies. In the LLM-based reasoning process, an accuracy-oriented\nprompting strategy restricts causal analysis to a reliable range. Next, a\nknowledge-to-structure transition aligns LLM-derived causal statements with\nstructural causal interactions. In the structure learning process, the\ngoodness-of-fit to data and adherence to LLM-derived priors are balanced to\nfurther address prior inaccuracies. Evaluation of eight real-world causal\nstructures demonstrates the efficacy of our LLM-driven approach in improving\ndata-based causal discovery, along with its robustness to inaccurate\nLLM-derived priors. Codes are available at https://github.com/tyMadara/LLM-CD."
                },
                "authors": [
                    {
                        "name": "Taiyu Ban"
                    },
                    {
                        "name": "Lyuzhou Chen"
                    },
                    {
                        "name": "Derui Lyu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Qinrui Zhu"
                    },
                    {
                        "name": "Qiang Tu"
                    },
                    {
                        "name": "Huanhuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huanhuan Chen"
                },
                "author": "Huanhuan Chen",
                "arxiv_comment": "13 pages, 5 figures",
                "arxiv_journal_ref": "IEEE Transactions on Artificial Intelligence, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.16902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.16902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18947v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18947v1",
                "updated": "2025-08-26T11:40:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    40,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:40:02Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    40,
                    2,
                    1,
                    238,
                    0
                ],
                "title": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in\n  Security Operations Centres",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs in the SOC: An Empirical Study of Human-AI Collaboration in\n  Security Operations Centres"
                },
                "summary": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into Security Operations\nCentres (SOCs) presents a transformative, yet still evolving, opportunity to\nreduce analyst workload through human-AI collaboration. However, their\nreal-world application in SOCs remains underexplored. To address this gap, we\npresent a longitudinal study of 3,090 analyst queries from 45 SOC analysts over\n10 months. Our analysis reveals that analysts use LLMs as on-demand aids for\nsensemaking and context-building, rather than for making high-stakes\ndeterminations, preserving analyst decision authority. The majority of queries\nare related to interpreting low-level telemetry (e.g., commands) and refining\ntechnical communication through short (1-3 turn) interactions. Notably, 93% of\nqueries align with established cybersecurity competencies (NICE Framework),\nunderscoring the relevance of LLM use for SOC-related tasks. Despite variations\nin tasks and engagement, usage trends indicate a shift from occasional\nexploration to routine integration, with growing adoption and sustained use\namong a subset of analysts. We find that LLMs function as flexible, on-demand\ncognitive aids that augment, rather than replace, SOC expertise. Our study\nprovides actionable guidance for designing context-aware, human-centred AI\nassistance in security operations, highlighting the need for further\nin-the-wild research on real-world analyst-LLM collaboration, challenges, and\nimpacts."
                },
                "authors": [
                    {
                        "name": "Ronal Singh"
                    },
                    {
                        "name": "Shahroz Tariq"
                    },
                    {
                        "name": "Fatemeh Jalalvand"
                    },
                    {
                        "name": "Mohan Baruwal Chhetri"
                    },
                    {
                        "name": "Surya Nepal"
                    },
                    {
                        "name": "Cecile Paris"
                    },
                    {
                        "name": "Martin Lochner"
                    }
                ],
                "author_detail": {
                    "name": "Martin Lochner"
                },
                "author": "Martin Lochner",
                "arxiv_comment": "22 pages, 9 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18947v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18947v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18933v1",
                "updated": "2025-08-26T11:20:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    20,
                    39,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T11:20:39Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    11,
                    20,
                    39,
                    1,
                    238,
                    0
                ],
                "title": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging\n  Counterfactual Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VISION: Robust and Interpretable Code Vulnerability Detection Leveraging\n  Counterfactual Augmentation"
                },
                "summary": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated detection of vulnerabilities in source code is an essential\ncybersecurity challenge, underpinning trust in digital systems and services.\nGraph Neural Networks (GNNs) have emerged as a promising approach as they can\nlearn structural and logical code relationships in a data-driven manner.\nHowever, their performance is severely constrained by training data imbalances\nand label noise. GNNs often learn 'spurious' correlations from superficial code\nsimilarities, producing detectors that fail to generalize well to unseen\nreal-world data. In this work, we propose a unified framework for robust and\ninterpretable vulnerability detection, called VISION, to mitigate spurious\ncorrelations by systematically augmenting a counterfactual training dataset.\nCounterfactuals are samples with minimal semantic modifications but opposite\nlabels. Our framework includes: (i) generating counterfactuals by prompting a\nLarge Language Model (LLM); (ii) targeted GNN training on paired code examples\nwith opposite labels; and (iii) graph-based interpretability to identify the\ncrucial code statements relevant for vulnerability predictions while ignoring\nspurious ones. We find that VISION reduces spurious learning and enables more\nrobust, generalizable detection, improving overall accuracy (from 51.8% to\n97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-group\naccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20\nvulnerability. We further demonstrate gains using proposed metrics: intra-class\nattribution variance, inter-class attribution distance, and node score\ndependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (real\nand counterfactual) from the high-impact CWE-20 category. Finally, VISION\nadvances transparent and trustworthy AI-based cybersecurity systems through\ninteractive visualization for human-in-the-loop analysis."
                },
                "authors": [
                    {
                        "name": "David Egea"
                    },
                    {
                        "name": "Barproda Halder"
                    },
                    {
                        "name": "Sanghamitra Dutta"
                    }
                ],
                "author_detail": {
                    "name": "Sanghamitra Dutta"
                },
                "author": "Sanghamitra Dutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16949v2",
                "updated": "2025-08-26T10:52:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    52,
                    15,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-23T08:47:31Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    47,
                    31,
                    5,
                    235,
                    0
                ],
                "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. This work is still in progress, and we will release\nthe code, the models, and the datasets soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. This work is still in progress, and we will release\nthe code, the models, and the datasets soon."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sunzhu Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Jiale Zhao"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Yihe Zhou"
                    },
                    {
                        "name": "Hengtong Lu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "arxiv_comment": "This work is still in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18918v1",
                "updated": "2025-08-26T10:44:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    44,
                    33,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:44:33Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    44,
                    33,
                    1,
                    238,
                    0
                ],
                "title": "DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM\n  with Audio Modality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM\n  with Audio Modality"
                },
                "summary": "We present DESAMO, an on-device smart home system for elder-friendly use\npowered by Audio LLM, that supports natural and private interactions. While\nconventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,\noften struggling with the unclear speech common among elderly users and unable\nto handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio\ninput directly, enabling a robust understanding of user intent and critical\nevents, such as falls or calls for help.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DESAMO, an on-device smart home system for elder-friendly use\npowered by Audio LLM, that supports natural and private interactions. While\nconventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades,\noften struggling with the unclear speech common among elderly users and unable\nto handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio\ninput directly, enabling a robust understanding of user intent and critical\nevents, such as falls or calls for help."
                },
                "authors": [
                    {
                        "name": "Youngwon Choi"
                    },
                    {
                        "name": "Donghyuk Jung"
                    },
                    {
                        "name": "Hwayeon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hwayeon Kim"
                },
                "author": "Hwayeon Kim",
                "arxiv_comment": "2 pages, 2 figures. Accepted for presentation as a UIST 2025 Poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16701v2",
                "updated": "2025-08-26T10:23:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    23,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-22T06:00:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    0,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "Generative Artificial Intelligence and Agents in Research and Teaching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence and Agents in Research and Teaching"
                },
                "summary": "This study provides a comprehensive analysis of the development, functioning,\nand application of generative artificial intelligence (GenAI) and large\nlanguage models (LLMs), with an emphasis on their implications for research and\neducation. It traces the conceptual evolution from artificial intelligence (AI)\nthrough machine learning (ML) and deep learning (DL) to transformer\narchitectures, which constitute the foundation of contemporary generative\nsystems. Technical aspects, including prompting strategies, word embeddings,\nand probabilistic sampling methods (temperature, top-k, and top-p), are\nexamined alongside the emergence of autonomous agents. These elements are\nconsidered in relation to both the opportunities they create and the\nlimitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research\nprocess, from ideation and literature review to research design, data\ncollection, analysis, interpretation, and dissemination. While particular\nattention is given to geographical research, the discussion extends to wider\nacademic contexts. A parallel strand addresses the pedagogical applications of\nGenAI, encompassing course and lesson design, teaching delivery, assessment,\nand feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges\nposed by GenAI. Issues of bias, intellectual property, governance, and\naccountability are assessed, alongside the ecological footprint of LLMs and\nemerging technological strategies for mitigation. The concluding section\nconsiders near- and long-term futures of GenAI, including scenarios of\nsustained adoption, regulation, and potential decline. By situating GenAI\nwithin both scholarly practice and educational contexts, the study contributes\nto critical debates on its transformative potential and societal\nresponsibilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study provides a comprehensive analysis of the development, functioning,\nand application of generative artificial intelligence (GenAI) and large\nlanguage models (LLMs), with an emphasis on their implications for research and\neducation. It traces the conceptual evolution from artificial intelligence (AI)\nthrough machine learning (ML) and deep learning (DL) to transformer\narchitectures, which constitute the foundation of contemporary generative\nsystems. Technical aspects, including prompting strategies, word embeddings,\nand probabilistic sampling methods (temperature, top-k, and top-p), are\nexamined alongside the emergence of autonomous agents. These elements are\nconsidered in relation to both the opportunities they create and the\nlimitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research\nprocess, from ideation and literature review to research design, data\ncollection, analysis, interpretation, and dissemination. While particular\nattention is given to geographical research, the discussion extends to wider\nacademic contexts. A parallel strand addresses the pedagogical applications of\nGenAI, encompassing course and lesson design, teaching delivery, assessment,\nand feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges\nposed by GenAI. Issues of bias, intellectual property, governance, and\naccountability are assessed, alongside the ecological footprint of LLMs and\nemerging technological strategies for mitigation. The concluding section\nconsiders near- and long-term futures of GenAI, including scenarios of\nsustained adoption, regulation, and potential decline. By situating GenAI\nwithin both scholarly practice and educational contexts, the study contributes\nto critical debates on its transformative potential and societal\nresponsibilities."
                },
                "authors": [
                    {
                        "name": "Jussi S. Jauhiainen"
                    },
                    {
                        "name": "Aurora Toppari"
                    }
                ],
                "author_detail": {
                    "name": "Aurora Toppari"
                },
                "author": "Aurora Toppari",
                "arxiv_comment": "113 pages, 6 figures, 13 tables, 2 appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18905v1",
                "updated": "2025-08-26T10:22:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    22,
                    37,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:22:37Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    22,
                    37,
                    1,
                    238,
                    0
                ],
                "title": "Interactive Evaluation of Large Language Models for Multi-Requirement\n  Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Evaluation of Large Language Models for Multi-Requirement\n  Software Engineering Tasks"
                },
                "summary": "Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standard single-turn, static benchmarks fall short in evaluating the nuanced\ncapabilities of Large Language Models (LLMs) on complex tasks such as software\nengineering. In this work, we propose a novel interactive evaluation framework\nthat assesses LLMs on multi-requirement programming tasks through structured,\nfeedback-driven dialogue. Each task is modeled as a requirement dependency\ngraph, and an ``interviewer'' LLM, aware of the ground-truth solution, provides\nminimal, targeted hints to an ``interviewee'' model to help correct errors and\nfulfill target constraints. This dynamic protocol enables fine-grained\ndiagnostic insights into model behavior, uncovering strengths and systematic\nweaknesses that static benchmarks fail to measure. We build on DevAI, a\nbenchmark of 55 curated programming tasks, by adding ground-truth solutions and\nevaluating the relevance and utility of interviewer hints through expert\nannotation. Our results highlight the importance of dynamic evaluation in\nadvancing the development of collaborative code-generating agents."
                },
                "authors": [
                    {
                        "name": "Dimitrios Rontogiannis"
                    },
                    {
                        "name": "Maxime Peyrard"
                    },
                    {
                        "name": "Nicolas Baldwin"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Dimitrios Gunopulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Gunopulos"
                },
                "author": "Dimitrios Gunopulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09396v2",
                "updated": "2025-08-26T10:19:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    19,
                    45,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-14T13:51:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    13,
                    51,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Influence of Human-inspired Agentic Sophistication in LLM-driven\n  Strategic Reasoners"
                },
                "summary": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid rise of large language models (LLMs) has shifted artificial\nintelligence (AI) research toward agentic systems, motivating the use of weaker\nand more flexible notions of agency. However, this shift raises key questions\nabout the extent to which LLM-based agents replicate human strategic reasoning,\nparticularly in game-theoretic settings. In this context, we examine the role\nof agentic sophistication in shaping artificial reasoners' performance by\nevaluating three agent designs: a simple game-theoretic model, an unstructured\nLLM-as-agent model, and an LLM integrated into a traditional agentic framework.\nUsing guessing games as a testbed, we benchmarked these agents against human\nparticipants across general reasoning patterns and individual role-based\nobjectives. Furthermore, we introduced obfuscated game scenarios to assess\nagents' ability to generalise beyond training distributions. Our analysis,\ncovering over 2000 reasoning samples across 25 agent configurations, shows that\nhuman-inspired cognitive structures can enhance LLM agents' alignment with\nhuman strategic behaviour. Still, the relationship between agentic design\ncomplexity and human-likeness is non-linear, highlighting a critical dependence\non underlying LLM capabilities and suggesting limits to simple architectural\naugmentation."
                },
                "authors": [
                    {
                        "name": "Vince Trencsenyi"
                    },
                    {
                        "name": "Agnieszka Mensfelt"
                    },
                    {
                        "name": "Kostas Stathis"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Stathis"
                },
                "author": "Kostas Stathis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18902v1",
                "updated": "2025-08-26T10:19:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    19,
                    16,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:19:16Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    19,
                    16,
                    1,
                    238,
                    0
                ],
                "title": "Adaptive 6G Networks-in-Network Management for Industrial Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive 6G Networks-in-Network Management for Industrial Applications"
                },
                "summary": "This paper presents the application of Dynamic Spectrum Management (DSM) for\nfuture 6G industrial networks, establishing an efficient controller for the\nNetworks-in-Network (NiN) concept. The proposed architecture integrates nomadic\nas well as static sub-networks (SNs with diverse Quality of Service (QoS)\nrequirements within the coverage area of an overlayer network, managed by a\ncentralized spectrum manager (SM). Control plane connectivity between the SNs\nand the DSM is ensured by the self-organizing KIRA routing protocol. The\ndemonstrated system enables scalable, zero-touch connectivity and supports\nnomadic SNs through seamless discovery and reconfiguration. SNs are implemented\nfor modular Industrial Internet of Things (IIoT) scenarios, as well as for\nmission-critical control loops and for logistics or nomadic behavior. The DSM\nframework dynamically adapts spectrum allocation to meet real-time demands\nwhile ensuring reliable operation. The demonstration highlights the potential\nof DSM and NiNs to support flexible, dense, and heterogeneous wireless\ndeployments in reconfigurable manufacturing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the application of Dynamic Spectrum Management (DSM) for\nfuture 6G industrial networks, establishing an efficient controller for the\nNetworks-in-Network (NiN) concept. The proposed architecture integrates nomadic\nas well as static sub-networks (SNs with diverse Quality of Service (QoS)\nrequirements within the coverage area of an overlayer network, managed by a\ncentralized spectrum manager (SM). Control plane connectivity between the SNs\nand the DSM is ensured by the self-organizing KIRA routing protocol. The\ndemonstrated system enables scalable, zero-touch connectivity and supports\nnomadic SNs through seamless discovery and reconfiguration. SNs are implemented\nfor modular Industrial Internet of Things (IIoT) scenarios, as well as for\nmission-critical control loops and for logistics or nomadic behavior. The DSM\nframework dynamically adapts spectrum allocation to meet real-time demands\nwhile ensuring reliable operation. The demonstration highlights the potential\nof DSM and NiNs to support flexible, dense, and heterogeneous wireless\ndeployments in reconfigurable manufacturing environments."
                },
                "authors": [
                    {
                        "name": "Daniel Lindenschmitt"
                    },
                    {
                        "name": "Paul Seehofer"
                    },
                    {
                        "name": "Marius Schmitz"
                    },
                    {
                        "name": "Jan Mertes"
                    },
                    {
                        "name": "Roland Bless"
                    },
                    {
                        "name": "Martina Zitterbart"
                    },
                    {
                        "name": "Jan C. Aurich"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18898v1",
                "updated": "2025-08-26T10:14:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    14,
                    16,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:14:16Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    14,
                    16,
                    1,
                    238,
                    0
                ],
                "title": "Interpretable Decision-Making for End-to-End Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Decision-Making for End-to-End Autonomous Driving"
                },
                "summary": "Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy AI is mandatory for the broad deployment of autonomous vehicles.\nAlthough end-to-end approaches derive control commands directly from raw data,\ninterpreting these decisions remains challenging, especially in complex urban\nscenarios. This is mainly attributed to very deep neural networks with\nnon-linear decision boundaries, making it challenging to grasp the logic behind\nAI-driven decisions. This paper presents a method to enhance interpretability\nwhile optimizing control commands in autonomous driving. To address this, we\npropose loss functions that promote the interpretability of our model by\ngenerating sparse and localized feature maps. The feature activations allow us\nto explain which image regions contribute to the predicted control command. We\nconduct comprehensive ablation studies on the feature extraction step and\nvalidate our method on the CARLA benchmarks. We also demonstrate that our\napproach improves interpretability, which correlates with reducing infractions,\nyielding a safer, high-performance driving model. Notably, our monocular,\nnon-ensemble model surpasses the top-performing approaches from the CARLA\nLeaderboard by achieving lower infraction scores and the highest route\ncompletion rate, all while ensuring interpretability."
                },
                "authors": [
                    {
                        "name": "Mona Mirzaie"
                    },
                    {
                        "name": "Bodo Rosenhahn"
                    }
                ],
                "author_detail": {
                    "name": "Bodo Rosenhahn"
                },
                "author": "Bodo Rosenhahn",
                "arxiv_comment": "Accepted to the ICCV 2025 2nd Workshop on the Challenge Of\n  Out-of-Label Hazards in Autonomous Driving (2COOOL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01459v2",
                "updated": "2025-08-26T10:08:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    8,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2024-12-02T12:51:45Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    12,
                    51,
                    45,
                    0,
                    337,
                    0
                ],
                "title": "Perception Gaps in Risk, Benefit, and Value Between Experts and Public\n  Challenge Socially Accepted AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception Gaps in Risk, Benefit, and Value Between Experts and Public\n  Challenge Socially Accepted AI"
                },
                "summary": "Artificial Intelligence (AI) is reshaping many societal domains, raising\ncritical questions about its risks, benefits, and the potential misalignment\nbetween public and academic perspectives. This study examines how the general\npublic (N=1110) -- individuals who interact with or are impacted by AI\ntechnologies -- and academic AI experts (N=119) -- those elites shaping AI\ndevelopment -- perceive AI's capabilities and impact across 71 scenarios. These\nscenarios span domains such as sustainability, healthcare, job performance,\nsocietal inequality, art, and warfare. Participants evaluated these scenarios\nacross four dimensions using the psychometric model: likelihood, perceived risk\nand benefit, and overall value (or sentiment). The results suggest significant\ndifferences: experts consistently anticipate higher probabilities, perceive\nlower risks, report greater benefits, and express more positive sentiment\ntoward AI compared to the non-experts. Moreover, both groups apply different\nweighting schemes: experts discount risk more heavily relative to benefit than\nnon-experts. Visual mappings of these evaluations uncover areas convergent\nevaluations (e.g., AI performing medical diagnoses or criminal use) as well as\ntension points (e.g., decision of legal cases, political decision making),\nhighlighting areas where communication and policy interventions may be needed.\nThese findings underscore a critical translational challenge: if AI research\nand deployment are to align with societal priorities, the perception gap\nbetween developers and the public must be better understood and addressed. Our\nresults provide an empirical foundation for value-sensitive AI governance and\ntrust-building strategies across stakeholder groups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) is reshaping many societal domains, raising\ncritical questions about its risks, benefits, and the potential misalignment\nbetween public and academic perspectives. This study examines how the general\npublic (N=1110) -- individuals who interact with or are impacted by AI\ntechnologies -- and academic AI experts (N=119) -- those elites shaping AI\ndevelopment -- perceive AI's capabilities and impact across 71 scenarios. These\nscenarios span domains such as sustainability, healthcare, job performance,\nsocietal inequality, art, and warfare. Participants evaluated these scenarios\nacross four dimensions using the psychometric model: likelihood, perceived risk\nand benefit, and overall value (or sentiment). The results suggest significant\ndifferences: experts consistently anticipate higher probabilities, perceive\nlower risks, report greater benefits, and express more positive sentiment\ntoward AI compared to the non-experts. Moreover, both groups apply different\nweighting schemes: experts discount risk more heavily relative to benefit than\nnon-experts. Visual mappings of these evaluations uncover areas convergent\nevaluations (e.g., AI performing medical diagnoses or criminal use) as well as\ntension points (e.g., decision of legal cases, political decision making),\nhighlighting areas where communication and policy interventions may be needed.\nThese findings underscore a critical translational challenge: if AI research\nand deployment are to align with societal priorities, the perception gap\nbetween developers and the public must be better understood and addressed. Our\nresults provide an empirical foundation for value-sensitive AI governance and\ntrust-building strategies across stakeholder groups."
                },
                "authors": [
                    {
                        "name": "Philipp Brauner"
                    },
                    {
                        "name": "Felix Glawe"
                    },
                    {
                        "name": "Gian Luca Liehner"
                    },
                    {
                        "name": "Luisa Vervier"
                    },
                    {
                        "name": "Martina Ziefle"
                    }
                ],
                "author_detail": {
                    "name": "Martina Ziefle"
                },
                "author": "Martina Ziefle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18596v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18596v4",
                "updated": "2025-08-26T10:08:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    8,
                    51,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-24T08:44:33Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    8,
                    44,
                    33,
                    5,
                    144,
                    0
                ],
                "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World\n  Debate with Large Language Models"
                },
                "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards\ninterpretable misinformation detection. The code will be released publicly\nafter the official publication."
                },
                "authors": [
                    {
                        "name": "Chen Han"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xijin Tang"
                    }
                ],
                "author_detail": {
                    "name": "Xijin Tang"
                },
                "author": "Xijin Tang",
                "arxiv_comment": "This paper has been accepted to EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18596v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18596v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18891v1",
                "updated": "2025-08-26T10:05:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    5,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T10:05:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    10,
                    5,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "pyFAST: A Modular PyTorch Framework for Time Series Modeling with\n  Multi-source and Sparse Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "pyFAST: A Modular PyTorch Framework for Time Series Modeling with\n  Multi-source and Sparse Data"
                },
                "summary": "Modern time series analysis demands frameworks that are flexible, efficient,\nand extensible. However, many existing Python libraries exhibit limitations in\nmodularity and in their native support for irregular, multi-source, or sparse\ndata. We introduce pyFAST, a research-oriented PyTorch framework that\nexplicitly decouples data processing from model computation, fostering a\ncleaner separation of concerns and facilitating rapid experimentation. Its data\nengine is engineered for complex scenarios, supporting multi-source loading,\nprotein sequence handling, efficient sequence- and patch-level padding, dynamic\nnormalization, and mask-based modeling for both imputation and forecasting.\npyFAST integrates LLM-inspired architectures for the alignment-free fusion of\nsparse data sources and offers native sparse metrics, specialized loss\nfunctions, and flexible exogenous data fusion. Training utilities include\nbatch-based streaming aggregation for evaluation and device synergy to maximize\ncomputational efficiency. A comprehensive suite of classical and deep learning\nmodels (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a\nmodular architecture that encourages extension. Released under the MIT license\nat GitHub, pyFAST provides a compact yet powerful platform for advancing time\nseries research and applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern time series analysis demands frameworks that are flexible, efficient,\nand extensible. However, many existing Python libraries exhibit limitations in\nmodularity and in their native support for irregular, multi-source, or sparse\ndata. We introduce pyFAST, a research-oriented PyTorch framework that\nexplicitly decouples data processing from model computation, fostering a\ncleaner separation of concerns and facilitating rapid experimentation. Its data\nengine is engineered for complex scenarios, supporting multi-source loading,\nprotein sequence handling, efficient sequence- and patch-level padding, dynamic\nnormalization, and mask-based modeling for both imputation and forecasting.\npyFAST integrates LLM-inspired architectures for the alignment-free fusion of\nsparse data sources and offers native sparse metrics, specialized loss\nfunctions, and flexible exogenous data fusion. Training utilities include\nbatch-based streaming aggregation for evaluation and device synergy to maximize\ncomputational efficiency. A comprehensive suite of classical and deep learning\nmodels (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within a\nmodular architecture that encourages extension. Released under the MIT license\nat GitHub, pyFAST provides a compact yet powerful platform for advancing time\nseries research and applications."
                },
                "authors": [
                    {
                        "name": "Zhijin Wang"
                    },
                    {
                        "name": "Senzhen Wu"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Xiufeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiufeng Liu"
                },
                "author": "Xiufeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18884v1",
                "updated": "2025-08-26T09:59:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    59,
                    44,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:59:44Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    59,
                    44,
                    1,
                    238,
                    0
                ],
                "title": "HAEPO: History-Aggregated Exploratory Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAEPO: History-Aggregated Exploratory Policy Optimization"
                },
                "summary": "Exploration is essential in modern learning, from reinforcement learning\nenvironments with small neural policies to large language models (LLMs).\nExisting work, such as DPO, leverages full sequence log-likelihoods to capture\nan entire trajectory of the model's decisions, while methods like GRPO\naggregate per-token ratios into a trajectory-level update. However, both often\nlimit exploration on long-horizon tasks. We introduce History-Aggregated\nExploratory Policy Optimization (HAEPO), a history-aware exploratory loss to\ncombat these shortcomings. HAEPO compresses each trajectory into the sum of its\nlogarithmic probabilities (a cumulative logarithmic likelihood), and applies a\nPlackett-Luce softmax across trajectories to obtain normalized weights\nproportional to their returns, thus encouraging broader exploration. We add\nentropy regularization to stabilize the aggressive updates to prevent premature\ncollapse and a soft KL penalty relative to a frozen copy of the previous\n(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,\naligns closely with true rewards, and demonstrates robust learning behavior\nbetter or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO\nprovides a stable and interpretable framework by explicitly leveraging\nfull-trajectory history while balancing exploration and stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploration is essential in modern learning, from reinforcement learning\nenvironments with small neural policies to large language models (LLMs).\nExisting work, such as DPO, leverages full sequence log-likelihoods to capture\nan entire trajectory of the model's decisions, while methods like GRPO\naggregate per-token ratios into a trajectory-level update. However, both often\nlimit exploration on long-horizon tasks. We introduce History-Aggregated\nExploratory Policy Optimization (HAEPO), a history-aware exploratory loss to\ncombat these shortcomings. HAEPO compresses each trajectory into the sum of its\nlogarithmic probabilities (a cumulative logarithmic likelihood), and applies a\nPlackett-Luce softmax across trajectories to obtain normalized weights\nproportional to their returns, thus encouraging broader exploration. We add\nentropy regularization to stabilize the aggressive updates to prevent premature\ncollapse and a soft KL penalty relative to a frozen copy of the previous\n(reference) policy. Empirically, HAEPO converges fast, explores thoroughly,\naligns closely with true rewards, and demonstrates robust learning behavior\nbetter or at par with PPO, GRPO, and DPO across diverse tasks. Thus, HAEPO\nprovides a stable and interpretable framework by explicitly leveraging\nfull-trajectory history while balancing exploration and stability."
                },
                "authors": [
                    {
                        "name": "Gaurish Trivedi"
                    },
                    {
                        "name": "Alakh Sharma"
                    },
                    {
                        "name": "Kartikey Singh Bhandari"
                    },
                    {
                        "name": "Dhruv Kumar"
                    },
                    {
                        "name": "Pratik Narang"
                    },
                    {
                        "name": "Jagat Sesh Challa"
                    }
                ],
                "author_detail": {
                    "name": "Jagat Sesh Challa"
                },
                "author": "Jagat Sesh Challa",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18880v1",
                "updated": "2025-08-26T09:56:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    56,
                    26,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:56:26Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    56,
                    26,
                    1,
                    238,
                    0
                ],
                "title": "Judicial Requirements for Generative AI in Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judicial Requirements for Generative AI in Legal Reasoning"
                },
                "summary": "Large Language Models (LLMs) are being integrated into professional domains,\nyet their limitations in high-stakes fields like law remain poorly understood.\nThis paper defines the core capabilities that an AI system must possess to\nfunction as a reliable reasoning tool in judicial decision-making. Using the\nIRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the\nstudy focuses on the most challenging phases of legal adjudication: determining\nthe applicable Rule (R) and performing the Application (A) of that rule to the\nfacts of a case. From a judicial perspective, the analysis deconstructs legal\nreasoning into a series of core requirements, including the ability to select\nthe correct legal framework across jurisdictions, generate sound arguments\nbased on the doctrine of legal sources, distinguish ratio decidendi from obiter\ndictum in case law, resolve ambiguity arising from general clauses like\n\"reasonableness\", manage conflicting legal provisions, and correctly apply the\nburden of proof. The paper then maps various AI enhancement mechanisms, such as\nRetrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic\nAI, to these requirements, assessing their potential to bridge the gap between\nthe probabilistic nature of LLMs and the rigorous, choice-driven demands of\nlegal interpretation. The findings indicate that while these techniques can\naddress specific challenges, significant challenges remain, particularly in\ntasks requiring discretion and transparent, justifiable reasoning. Our paper\nconcludes that the most effective current role for AI in law is a dual one: as\na high-volume assistant for simple, repetitive cases and as a sophisticated\n\"sparring partner\" for human experts in complex matters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being integrated into professional domains,\nyet their limitations in high-stakes fields like law remain poorly understood.\nThis paper defines the core capabilities that an AI system must possess to\nfunction as a reliable reasoning tool in judicial decision-making. Using the\nIRAC (Issue-Rule-Application-Conclusion) model as an analytical framework, the\nstudy focuses on the most challenging phases of legal adjudication: determining\nthe applicable Rule (R) and performing the Application (A) of that rule to the\nfacts of a case. From a judicial perspective, the analysis deconstructs legal\nreasoning into a series of core requirements, including the ability to select\nthe correct legal framework across jurisdictions, generate sound arguments\nbased on the doctrine of legal sources, distinguish ratio decidendi from obiter\ndictum in case law, resolve ambiguity arising from general clauses like\n\"reasonableness\", manage conflicting legal provisions, and correctly apply the\nburden of proof. The paper then maps various AI enhancement mechanisms, such as\nRetrieval-Augmented Generation (RAG), multi-agent systems, and neuro-symbolic\nAI, to these requirements, assessing their potential to bridge the gap between\nthe probabilistic nature of LLMs and the rigorous, choice-driven demands of\nlegal interpretation. The findings indicate that while these techniques can\naddress specific challenges, significant challenges remain, particularly in\ntasks requiring discretion and transparent, justifiable reasoning. Our paper\nconcludes that the most effective current role for AI in law is a dual one: as\na high-volume assistant for simple, repetitive cases and as a sophisticated\n\"sparring partner\" for human experts in complex matters."
                },
                "authors": [
                    {
                        "name": "Eljas Linna"
                    },
                    {
                        "name": "Tuula Linna"
                    }
                ],
                "author_detail": {
                    "name": "Tuula Linna"
                },
                "author": "Tuula Linna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18877v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18877v1",
                "updated": "2025-08-26T09:51:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    51,
                    2,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:51:02Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    51,
                    2,
                    1,
                    238,
                    0
                ],
                "title": "Optimization of Latent-Space Compression using Game-Theoretic Techniques\n  for Transformer-Based Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization of Latent-Space Compression using Game-Theoretic Techniques\n  for Transformer-Based Vector Search"
                },
                "summary": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval."
                },
                "authors": [
                    {
                        "name": "Kushagra Agrawal"
                    },
                    {
                        "name": "Nisharg Nargund"
                    },
                    {
                        "name": "Oishani Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Oishani Banerjee"
                },
                "author": "Oishani Banerjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18877v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18877v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18872v1",
                "updated": "2025-08-26T09:46:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    59,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:46:59Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    59,
                    1,
                    238,
                    0
                ],
                "title": "Empowering Computing Education Researchers Through LLM-Assisted Content\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Computing Education Researchers Through LLM-Assisted Content\n  Analysis"
                },
                "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline."
                },
                "authors": [
                    {
                        "name": "Laurie Gale"
                    },
                    {
                        "name": "Sebastian Mateos Nicolajsen"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Mateos Nicolajsen"
                },
                "author": "Sebastian Mateos Nicolajsen",
                "arxiv_comment": "7 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18871v1",
                "updated": "2025-08-26T09:46:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    42,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:46:42Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    42,
                    1,
                    238,
                    0
                ],
                "title": "Energy-Efficient Precoding for Dense VCSEL-Based OWC Systems Under a\n  Cooperative Broadcast Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Efficient Precoding for Dense VCSEL-Based OWC Systems Under a\n  Cooperative Broadcast Model"
                },
                "summary": "As 6G and beyond aim for sustainable, high-capacity wireless connectivity,\noptical wireless communication (OWC) has emerged as a compelling\nsolution.Recent advances in vertical-cavity surface-emitting laser (VCSEL)\narrays have significantly enhanced OWC performance, enabling high-speed,\nlow-power data transmission. However, dense VCSEL deployments introduce\nchallenges related to interference and energy efficiency (EE). This paper\nproposes a scalable precoding framework for EE maximization in fully\ncooperative VCSEL-based OWC broadcast systems. We formulate a non-convex\noptimization problem to design the precoding matrix under practical optical\nconstraints while guaranteeing minimum user rates. To solve this, we apply\nDinkelbach's method to handle the fractional objective and the inner\napproximation technique to iteratively convexify and solve the problem.\nSimulation results show that our approach consistently outperforms regularized\nzero-forcing in terms of EE, particularly in large-scale deployments,\ndemonstrating its potential for next-generation sustainable dense OWC networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As 6G and beyond aim for sustainable, high-capacity wireless connectivity,\noptical wireless communication (OWC) has emerged as a compelling\nsolution.Recent advances in vertical-cavity surface-emitting laser (VCSEL)\narrays have significantly enhanced OWC performance, enabling high-speed,\nlow-power data transmission. However, dense VCSEL deployments introduce\nchallenges related to interference and energy efficiency (EE). This paper\nproposes a scalable precoding framework for EE maximization in fully\ncooperative VCSEL-based OWC broadcast systems. We formulate a non-convex\noptimization problem to design the precoding matrix under practical optical\nconstraints while guaranteeing minimum user rates. To solve this, we apply\nDinkelbach's method to handle the fractional objective and the inner\napproximation technique to iteratively convexify and solve the problem.\nSimulation results show that our approach consistently outperforms regularized\nzero-forcing in terms of EE, particularly in large-scale deployments,\ndemonstrating its potential for next-generation sustainable dense OWC networks."
                },
                "authors": [
                    {
                        "name": "Hossein Safi"
                    },
                    {
                        "name": "Asim Ihsan"
                    },
                    {
                        "name": "Hossien B. Eldeeb"
                    },
                    {
                        "name": "Bastien Bechadergue"
                    },
                    {
                        "name": "Iman Tavakkolnia"
                    },
                    {
                        "name": "Harald Haas"
                    }
                ],
                "author_detail": {
                    "name": "Harald Haas"
                },
                "author": "Harald Haas",
                "arxiv_comment": "Accepted in 2025 IEEE Global Communications Conference (GLOBECOM)\n  Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18870v1",
                "updated": "2025-08-26T09:46:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    20,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:46:20Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    46,
                    20,
                    1,
                    238,
                    0
                ],
                "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReflectivePrompt: Reflective evolution in autoprompting algorithms"
                },
                "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting."
                },
                "authors": [
                    {
                        "name": "Viktor N. Zhuravlev"
                    },
                    {
                        "name": "Artur R. Khairullin"
                    },
                    {
                        "name": "Ernest A. Dyagin"
                    },
                    {
                        "name": "Alena N. Sitkina"
                    },
                    {
                        "name": "Nikita I. Kulin"
                    }
                ],
                "author_detail": {
                    "name": "Nikita I. Kulin"
                },
                "author": "Nikita I. Kulin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18850v1",
                "updated": "2025-08-26T09:29:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    29,
                    23,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:29:23Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    29,
                    23,
                    1,
                    238,
                    0
                ],
                "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via\n  Cluster-Level Collective Primitive",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via\n  Cluster-Level Collective Primitive"
                },
                "summary": "Large language model (LLM) decoding suffers from high latency due to\nfragmented execution across operators and heavy reliance on off-chip memory for\ndata exchange and reduction. This execution model limits opportunities for\nfusion and incurs significant memory traffic and kernel launch overhead. While\nmodern architectures such as NVIDIA Hopper provide distributed shared memory\nand low-latency intra-cluster interconnects, they expose only low-level data\nmovement instructions, lacking structured abstractions for collective on-chip\ncommunication. To bridge this software-hardware gap, we introduce two\ncluster-level communication primitives, ClusterReduce and ClusterGather, which\nabstract common communication patterns and enable structured, high-speed data\nexchange and reduction between thread blocks within a cluster, allowing\nintermediate results to be on-chip without involving off-chip memory. Building\non these abstractions, we design ClusterFusion, an execution framework that\nschedules communication and computation jointly to expand operator fusion scope\nby composing decoding stages such as QKV Projection, Attention, and Output\nProjection into a single fused kernels. Evaluations on H100 GPUs show that\nClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on\naverage in end-to-end latency across different models and configurations. The\nsource code is available at https://github.com/xinhao-luo/ClusterFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) decoding suffers from high latency due to\nfragmented execution across operators and heavy reliance on off-chip memory for\ndata exchange and reduction. This execution model limits opportunities for\nfusion and incurs significant memory traffic and kernel launch overhead. While\nmodern architectures such as NVIDIA Hopper provide distributed shared memory\nand low-latency intra-cluster interconnects, they expose only low-level data\nmovement instructions, lacking structured abstractions for collective on-chip\ncommunication. To bridge this software-hardware gap, we introduce two\ncluster-level communication primitives, ClusterReduce and ClusterGather, which\nabstract common communication patterns and enable structured, high-speed data\nexchange and reduction between thread blocks within a cluster, allowing\nintermediate results to be on-chip without involving off-chip memory. Building\non these abstractions, we design ClusterFusion, an execution framework that\nschedules communication and computation jointly to expand operator fusion scope\nby composing decoding stages such as QKV Projection, Attention, and Output\nProjection into a single fused kernels. Evaluations on H100 GPUs show that\nClusterFusion outperforms state-of-the-art inference frameworks by 1.61x on\naverage in end-to-end latency across different models and configurations. The\nsource code is available at https://github.com/xinhao-luo/ClusterFusion."
                },
                "authors": [
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Shihan Fang"
                    },
                    {
                        "name": "Ziyu Huang"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18847v1",
                "updated": "2025-08-26T09:25:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    25,
                    32,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:25:32Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    25,
                    32,
                    1,
                    238,
                    0
                ],
                "title": "ConfTuner: Training Large Language Models to Express Their Confidence\n  Verbally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfTuner: Training Large Language Models to Express Their Confidence\n  Verbally"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner."
                },
                "authors": [
                    {
                        "name": "Yibo Li"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18824v1",
                "updated": "2025-08-26T09:01:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    1,
                    50,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T09:01:50Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    9,
                    1,
                    50,
                    1,
                    238,
                    0
                ],
                "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models:\n  Diversity, Complexity and Correctness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arrows of Math Reasoning Data Synthesis for Large Language Models:\n  Diversity, Complexity and Correctness"
                },
                "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach."
                },
                "authors": [
                    {
                        "name": "Sirui Chen"
                    },
                    {
                        "name": "Changxin Tian"
                    },
                    {
                        "name": "Binbin Hu"
                    },
                    {
                        "name": "Kunlong Chen"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Zhiqiang Zhang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18166v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18166v3",
                "updated": "2025-08-27T07:58:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    7,
                    58,
                    8,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-25T16:16:06Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    16,
                    6,
                    0,
                    237,
                    0
                ],
                "title": "PCR-CA: Parallel Codebook Representations with Contrastive Alignment for\n  Multiple-Category App Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PCR-CA: Parallel Codebook Representations with Contrastive Alignment for\n  Multiple-Category App Recommendation"
                },
                "summary": "Modern app store recommender systems struggle with multiple-category apps, as\ntraditional taxonomies fail to capture overlapping semantics, leading to\nsuboptimal personalization. We propose PCR-CA (Parallel Codebook\nRepresentations with Contrastive Alignment), an end-to-end framework for\nimproved CTR prediction. PCR-CA first extracts compact multimodal embeddings\nfrom app text, then introduces a Parallel Codebook VQ-AE module that learns\ndiscrete semantic representations across multiple codebooks in parallel --\nunlike hierarchical residual quantization (RQ-VAE). This design enables\nindependent encoding of diverse aspects (e.g., gameplay, art style), better\nmodeling multiple-category semantics. To bridge semantic and collaborative\nsignals, we employ a contrastive alignment loss at both the user and item\nlevels, enhancing representation learning for long-tail items. Additionally, a\ndual-attention fusion mechanism combines ID-based and semantic features to\ncapture user interests, especially for long-tail apps. Experiments on a\nlarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong\nbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further\nvalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvement\nin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new\nframework has now been fully deployed on the Microsoft Store.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern app store recommender systems struggle with multiple-category apps, as\ntraditional taxonomies fail to capture overlapping semantics, leading to\nsuboptimal personalization. We propose PCR-CA (Parallel Codebook\nRepresentations with Contrastive Alignment), an end-to-end framework for\nimproved CTR prediction. PCR-CA first extracts compact multimodal embeddings\nfrom app text, then introduces a Parallel Codebook VQ-AE module that learns\ndiscrete semantic representations across multiple codebooks in parallel --\nunlike hierarchical residual quantization (RQ-VAE). This design enables\nindependent encoding of diverse aspects (e.g., gameplay, art style), better\nmodeling multiple-category semantics. To bridge semantic and collaborative\nsignals, we employ a contrastive alignment loss at both the user and item\nlevels, enhancing representation learning for long-tail items. Additionally, a\ndual-attention fusion mechanism combines ID-based and semantic features to\ncapture user interests, especially for long-tail apps. Experiments on a\nlarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strong\nbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing further\nvalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvement\nin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The new\nframework has now been fully deployed on the Microsoft Store."
                },
                "authors": [
                    {
                        "name": "Bin Tan"
                    },
                    {
                        "name": "Wangyao Ge"
                    },
                    {
                        "name": "Yidi Wang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Jeff Burtoft"
                    },
                    {
                        "name": "Hao Fan"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "arxiv_comment": "9 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18166v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18166v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18819v1",
                "updated": "2025-08-26T08:58:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    58,
                    35,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:58:35Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    58,
                    35,
                    1,
                    238,
                    0
                ],
                "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph\n  Autoencoders for Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph\n  Autoencoders for Fake News Detection"
                },
                "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability."
                },
                "authors": [
                    {
                        "name": "Shubham Gupta"
                    },
                    {
                        "name": "Shraban Kumar Chatterjee"
                    },
                    {
                        "name": "Suman Kundu"
                    }
                ],
                "author_detail": {
                    "name": "Suman Kundu"
                },
                "author": "Suman Kundu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17489v2",
                "updated": "2025-08-26T08:53:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    53,
                    33,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-24T18:47:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    18,
                    47,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "A Dynamic Approach to Collaborative Document Writing (Full Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Dynamic Approach to Collaborative Document Writing (Full Version)"
                },
                "summary": "We introduce a model for collaborative text aggregation in which an agent\ncommunity coauthors a document, modeled as an unordered collection of\nparagraphs, using a dynamic mechanism: agents propose paragraphs and vote on\nthose suggested by others. We formalize the setting and explore its\nrealizations, concentrating on voting mechanisms that aggregate votes into a\nsingle, dynamic document. We focus on two desiderata: the eventual stability of\nthe process and its expected social welfare. Following an impossibility result,\nwe describe several aggregation methods and report on agent-based simulations\nthat utilize natural language processing (NLP) and large-language models (LLMs)\nto model agents and their contexts. Using these simulations, we demonstrate\npromising results regarding the possibility of rapid convergence to a high\nsocial welfare collaborative text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a model for collaborative text aggregation in which an agent\ncommunity coauthors a document, modeled as an unordered collection of\nparagraphs, using a dynamic mechanism: agents propose paragraphs and vote on\nthose suggested by others. We formalize the setting and explore its\nrealizations, concentrating on voting mechanisms that aggregate votes into a\nsingle, dynamic document. We focus on two desiderata: the eventual stability of\nthe process and its expected social welfare. Following an impossibility result,\nwe describe several aggregation methods and report on agent-based simulations\nthat utilize natural language processing (NLP) and large-language models (LLMs)\nto model agents and their contexts. Using these simulations, we demonstrate\npromising results regarding the possibility of rapid convergence to a high\nsocial welfare collaborative text."
                },
                "authors": [
                    {
                        "name": "Avital Finanser"
                    },
                    {
                        "name": "Nimrod Talmon"
                    }
                ],
                "author_detail": {
                    "name": "Nimrod Talmon"
                },
                "author": "Nimrod Talmon",
                "arxiv_comment": "Accepted at the 27th European Conference on Artificial Intelligence\n  (ECAI 2025). This arXiv version presents the complete work with supplementary\n  materials and additional experimental details not included in the conference\n  proceedings. Git repository: https://github.com/AvitalFinanaser/Dynamic-CDW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13152v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13152v3",
                "updated": "2025-08-26T08:52:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    52,
                    15,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-17T14:13:50Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    14,
                    13,
                    50,
                    3,
                    198,
                    0
                ],
                "title": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on\n  Multimodal Large Language Models"
                },
                "summary": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language navigation (VLN) were mainly attributed to\nemerging large language models (LLMs). These methods exhibited excellent\ngeneralization capabilities in instruction understanding and task reasoning.\nHowever, they were constrained by the fixed knowledge bases and reasoning\nabilities of LLMs, preventing fully incorporating experiential knowledge and\nthus resulting in a lack of efficient evolutionary capacity. To address this,\nwe drew inspiration from the evolution capabilities of natural agents, and\nproposed a self-evolving VLN framework (SE-VLN) to endow VLN agents with the\nability to continuously evolve during testing. To the best of our knowledge, it\nwas the first time that an multimodal LLM-powered self-evolving VLN framework\nwas proposed. Specifically, SE-VLN comprised three core modules, i.e., a\nhierarchical memory module to transfer successful and failure cases into\nreusable knowledge, a retrieval-augmented thought-based reasoning module to\nretrieve experience and enable multi-step decision-making, and a reflection\nmodule to realize continual evolution. Comprehensive tests illustrated that the\nSE-VLN achieved navigation success rates of 57% and 35.2% in unseen\nenvironments, representing absolute performance improvements of 23.9% and 15.0%\nover current state-of-the-art methods on R2R and REVERSE datasets,\nrespectively. Moreover, the SE-VLN showed performance improvement with\nincreasing experience repository, elucidating its great potential as a\nself-evolving agent framework for VLN."
                },
                "authors": [
                    {
                        "name": "Xiangyu Dong"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Jiang Gao"
                    },
                    {
                        "name": "Haozhou Li"
                    },
                    {
                        "name": "Xiaoguang Ma"
                    },
                    {
                        "name": "Yaoming Zhou"
                    },
                    {
                        "name": "Fuhai Chen"
                    },
                    {
                        "name": "Juan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Juan Liu"
                },
                "author": "Juan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13152v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13152v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14582v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14582v2",
                "updated": "2025-08-26T08:50:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    50,
                    21,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-20T16:38:32Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    16,
                    38,
                    32,
                    1,
                    140,
                    0
                ],
                "title": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with\n  Capability in Mind for Better Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with\n  Capability in Mind for Better Reasoning"
                },
                "summary": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies - targeting entire chains, core reasoning, and verification - we\nfind that verification pruning consistently improves accuracy while reducing\ntoken usage, whereas reasoning or indiscriminate pruning degrades performance.\nOur study reveals that effective pruning aligns supervision with model capacity\nrather than merely shortening inputs. Gains hold across tasks, model scales,\nand CoT capability, with larger models benefiting more from pruning due to\nricher but more redundant reasoning. Our empirical findings highlight pruning\nas a structural optimization strategy for aligning CoT reasoning with SLM\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its\nverbose, self-reflective style often hinders effective distillation into small\nlanguage models (SLMs). We revisit Long-CoT compression through the lens of\ncapability alignment and ask: Can pruning improve reasoning? We propose\nPrune-on-Logic, a structure-aware framework that transforms Long-CoT into logic\ngraphs and selectively prunes low-utility reasoning steps under\nself-verification constraints. Through systematic analysis across three pruning\nstrategies - targeting entire chains, core reasoning, and verification - we\nfind that verification pruning consistently improves accuracy while reducing\ntoken usage, whereas reasoning or indiscriminate pruning degrades performance.\nOur study reveals that effective pruning aligns supervision with model capacity\nrather than merely shortening inputs. Gains hold across tasks, model scales,\nand CoT capability, with larger models benefiting more from pruning due to\nricher but more redundant reasoning. Our empirical findings highlight pruning\nas a structural optimization strategy for aligning CoT reasoning with SLM\ncapacity."
                },
                "authors": [
                    {
                        "name": "Shangziqi Zhao"
                    },
                    {
                        "name": "Jiahao Yuan"
                    },
                    {
                        "name": "Guisong Yang"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "19 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14582v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14582v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18812v1",
                "updated": "2025-08-26T08:47:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    47,
                    58,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:47:58Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    47,
                    58,
                    1,
                    238,
                    0
                ],
                "title": "STARec: An Efficient Agent Framework for Recommender Systems via\n  Autonomous Deliberate Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STARec: An Efficient Agent Framework for Recommender Systems via\n  Autonomous Deliberate Reasoning"
                },
                "summary": "While modern recommender systems are instrumental in navigating information\nabundance, they remain fundamentally limited by static user modeling and\nreactive decision-making paradigms. Current large language model (LLM)-based\nagents inherit these shortcomings through their overreliance on heuristic\npattern matching, yielding recommendations prone to shallow correlation bias,\nlimited causal inference, and brittleness in sparse-data scenarios. We\nintroduce STARec, a slow-thinking augmented agent framework that endows\nrecommender systems with autonomous deliberative reasoning capabilities. Each\nuser is modeled as an agent with parallel cognitions: fast response for\nimmediate interactions and slow reasoning that performs chain-of-thought\nrationales. To cultivate intrinsic slow thinking, we develop anchored\nreinforcement training - a two-stage paradigm combining structured knowledge\ndistillation from advanced reasoning models with preference-aligned reward\nshaping. This hybrid approach scaffolds agents in acquiring foundational\ncapabilities (preference summarization, rationale generation) while enabling\ndynamic policy adaptation through simulated feedback loops. Experiments on\nMovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves\nsubstantial performance gains compared with state-of-the-art baselines, despite\nusing only 0.4% of the full training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern recommender systems are instrumental in navigating information\nabundance, they remain fundamentally limited by static user modeling and\nreactive decision-making paradigms. Current large language model (LLM)-based\nagents inherit these shortcomings through their overreliance on heuristic\npattern matching, yielding recommendations prone to shallow correlation bias,\nlimited causal inference, and brittleness in sparse-data scenarios. We\nintroduce STARec, a slow-thinking augmented agent framework that endows\nrecommender systems with autonomous deliberative reasoning capabilities. Each\nuser is modeled as an agent with parallel cognitions: fast response for\nimmediate interactions and slow reasoning that performs chain-of-thought\nrationales. To cultivate intrinsic slow thinking, we develop anchored\nreinforcement training - a two-stage paradigm combining structured knowledge\ndistillation from advanced reasoning models with preference-aligned reward\nshaping. This hybrid approach scaffolds agents in acquiring foundational\ncapabilities (preference summarization, rationale generation) while enabling\ndynamic policy adaptation through simulated feedback loops. Experiments on\nMovieLens 1M and Amazon CDs benchmarks demonstrate that STARec achieves\nsubstantial performance gains compared with state-of-the-art baselines, despite\nusing only 0.4% of the full training data."
                },
                "authors": [
                    {
                        "name": "Chenghao Wu"
                    },
                    {
                        "name": "Ruiyang Ren"
                    },
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Ruirui Wang"
                    },
                    {
                        "name": "Zhongrui Ma"
                    },
                    {
                        "name": "Qi Ye"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_doi": "10.1145/3746252.3760995",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3760995",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 34th ACM International Conference on\n  Information and Knowledge Management (CIKM 2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18810v1",
                "updated": "2025-08-26T08:41:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    41,
                    30,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:41:30Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    41,
                    30,
                    1,
                    238,
                    0
                ],
                "title": "Near-Field Challenges in Ultra-Wideband ISAC: Beamforming Strategies and\n  System Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Field Challenges in Ultra-Wideband ISAC: Beamforming Strategies and\n  System Insights"
                },
                "summary": "The shift toward sixth-generation (6G) wireless networks places integrated\nsensing and communications (ISAC) at the core of future applications such as\nautonomous driving, extended reality, and smart manufacturing. However, the\ncombination of large antenna arrays and ultra-wide bandwidths brings near-field\npropagation effects and beam squint to the forefront, fundamentally challenging\ntraditional far-field designs. True time delay units (TTDs) offer a potential\nsolution, but their cost and hardware complexity limit scalability. In this\narticle, we present practical beamforming strategies for near-field\nultra-wideband ISAC systems. We explore codebook designs across analog and\ndigital domains that mitigate beam squint, ensure reliable user coverage, and\nenhance sensing accuracy. We further validate these approaches through\nlarge-scale system-level simulations, including 3D map-based evaluations that\nreflect real-world urban environments. Our results demonstrate how carefully\ndesigned beamforming can balance communication throughput with sensing\nperformance, achieving reliable coverage and efficient resource use even under\nsevere near-field conditions. We conclude by highlighting open challenges in\nhardware, algorithms, and system integration, pointing toward research\ndirections that will shape the deployment of 6G-ready ISAC networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shift toward sixth-generation (6G) wireless networks places integrated\nsensing and communications (ISAC) at the core of future applications such as\nautonomous driving, extended reality, and smart manufacturing. However, the\ncombination of large antenna arrays and ultra-wide bandwidths brings near-field\npropagation effects and beam squint to the forefront, fundamentally challenging\ntraditional far-field designs. True time delay units (TTDs) offer a potential\nsolution, but their cost and hardware complexity limit scalability. In this\narticle, we present practical beamforming strategies for near-field\nultra-wideband ISAC systems. We explore codebook designs across analog and\ndigital domains that mitigate beam squint, ensure reliable user coverage, and\nenhance sensing accuracy. We further validate these approaches through\nlarge-scale system-level simulations, including 3D map-based evaluations that\nreflect real-world urban environments. Our results demonstrate how carefully\ndesigned beamforming can balance communication throughput with sensing\nperformance, achieving reliable coverage and efficient resource use even under\nsevere near-field conditions. We conclude by highlighting open challenges in\nhardware, algorithms, and system integration, pointing toward research\ndirections that will shape the deployment of 6G-ready ISAC networks."
                },
                "authors": [
                    {
                        "name": "Yonghwi Kim"
                    },
                    {
                        "name": "Sang-Hyun Park"
                    },
                    {
                        "name": "Siyun Yang"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    },
                    {
                        "name": "Linglong Dai"
                    },
                    {
                        "name": "Chan-Byoung Chae"
                    }
                ],
                "author_detail": {
                    "name": "Chan-Byoung Chae"
                },
                "author": "Chan-Byoung Chae",
                "arxiv_comment": "7 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18803v1",
                "updated": "2025-08-26T08:38:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    38,
                    1,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:38:01Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    38,
                    1,
                    1,
                    238,
                    0
                ],
                "title": "A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT\n  Networks"
                },
                "summary": "The proliferation of Internet of things (IoT) devices in smart cities,\ntransportation, healthcare, and industrial applications, coupled with the\nexplosive growth of AI-driven services, has increased demands for efficient\ndistributed computing architectures and networks, driving cloud-edge-terminal\ncollaborative intelligence (CETCI) as a fundamental paradigm within the\nartificial intelligence of things (AIoT) community. With advancements in deep\nlearning, large language models (LLMs), and edge computing, CETCI has made\nsignificant progress with emerging AIoT applications, moving beyond isolated\nlayer optimization to deployable collaborative intelligence systems for AIoT\n(CISAIOT), a practical research focus in AI, distributed computing, and\ncommunications. This survey describes foundational architectures, enabling\ntechnologies, and scenarios of CETCI paradigms, offering a tutorial-style\nreview for CISAIOT beginners. We systematically analyze architectural\ncomponents spanning cloud, edge, and terminal layers, examining core\ntechnologies including network virtualization, container orchestration, and\nsoftware-defined networking, while presenting categorizations of collaboration\nparadigms that cover task offloading, resource allocation, and optimization\nacross heterogeneous infrastructures. Furthermore, we explain intelligent\ncollaboration learning frameworks by reviewing advances in federated learning,\ndistributed deep learning, edge-cloud model evolution, and reinforcement\nlearning-based methods. Finally, we discuss challenges (e.g., scalability,\nheterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum\ncomputing, digital twin), highlighting how integration of distributed computing\nand communication can address open issues and guide development of robust,\nefficient, and secure collaborative AIoT systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of Internet of things (IoT) devices in smart cities,\ntransportation, healthcare, and industrial applications, coupled with the\nexplosive growth of AI-driven services, has increased demands for efficient\ndistributed computing architectures and networks, driving cloud-edge-terminal\ncollaborative intelligence (CETCI) as a fundamental paradigm within the\nartificial intelligence of things (AIoT) community. With advancements in deep\nlearning, large language models (LLMs), and edge computing, CETCI has made\nsignificant progress with emerging AIoT applications, moving beyond isolated\nlayer optimization to deployable collaborative intelligence systems for AIoT\n(CISAIOT), a practical research focus in AI, distributed computing, and\ncommunications. This survey describes foundational architectures, enabling\ntechnologies, and scenarios of CETCI paradigms, offering a tutorial-style\nreview for CISAIOT beginners. We systematically analyze architectural\ncomponents spanning cloud, edge, and terminal layers, examining core\ntechnologies including network virtualization, container orchestration, and\nsoftware-defined networking, while presenting categorizations of collaboration\nparadigms that cover task offloading, resource allocation, and optimization\nacross heterogeneous infrastructures. Furthermore, we explain intelligent\ncollaboration learning frameworks by reviewing advances in federated learning,\ndistributed deep learning, edge-cloud model evolution, and reinforcement\nlearning-based methods. Finally, we discuss challenges (e.g., scalability,\nheterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum\ncomputing, digital twin), highlighting how integration of distributed computing\nand communication can address open issues and guide development of robust,\nefficient, and secure collaborative AIoT systems."
                },
                "authors": [
                    {
                        "name": "Jiaqi Wu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Lixu Wang"
                    },
                    {
                        "name": "Zehua Wang"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Zijian Tian"
                    },
                    {
                        "name": "Richard Yu"
                    },
                    {
                        "name": "Victor C. M. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Victor C. M. Leung"
                },
                "author": "Victor C. M. Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18799v1",
                "updated": "2025-08-26T08:34:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    34,
                    4,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:34:04Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    34,
                    4,
                    1,
                    238,
                    0
                ],
                "title": "Robust and Label-Efficient Deep Waste Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Label-Efficient Deep Waste Detection"
                },
                "summary": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective waste sorting is critical for sustainable recycling, yet AI\nresearch in this domain continues to lag behind commercial systems due to\nlimited datasets and reliance on legacy object detectors. In this work, we\nadvance AI-driven waste detection by establishing strong baselines and\nintroducing an ensemble-based semi-supervised learning framework. We first\nbenchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on\nthe real-world ZeroWaste dataset, demonstrating that while class-only prompts\nperform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy.\nNext, to address domain-specific limitations, we fine-tune modern\ntransformer-based detectors, achieving a new baseline of 51.6 mAP. We then\npropose a soft pseudo-labeling strategy that fuses ensemble predictions using\nspatial and consensus-aware weighting, enabling robust semi-supervised\ntraining. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations\nachieve performance gains that surpass fully supervised training, underscoring\nthe effectiveness of scalable annotation pipelines. Our work contributes to the\nresearch community by establishing rigorous baselines, introducing a robust\nensemble-based pseudo-labeling pipeline, generating high-quality annotations\nfor the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models\nunder real-world waste sorting conditions. Our code is available at:\nhttps://github.com/h-abid97/robust-waste-detection."
                },
                "authors": [
                    {
                        "name": "Hassan Abid"
                    },
                    {
                        "name": "Khan Muhammad"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted to BMVC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18798v1",
                "updated": "2025-08-26T08:30:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    30,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:30:34Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    30,
                    34,
                    1,
                    238,
                    0
                ],
                "title": "CASP: An evaluation dataset for formal verification of C code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CASP: An evaluation dataset for formal verification of C code"
                },
                "summary": "Recent developments in Large Language Models (LLMs) have shown promise in\nautomating code generation, yet the generated programs lack rigorous\ncorrectness guarantees. Formal verification can address this shortcoming, but\nrequires expertise and is time-consuming to apply. Currently, there is no\ndataset of verified C code paired with formal specifications that enables\nsystematic benchmarking in this space. To fill this gap, we present a curated\nevaluation dataset of C code paired with formal specifications written in\nANSI/ISO C Specification Language (ACSL). We develop a multi-stage filtering\nprocess to carefully extract 506 pairs of C code and formal specifications from\nThe Stack 1 and The Stack 2. We first identify C files annotated with formal\nlanguages. Then, we ensure that the annotated C files formally verify, and\nemploy LLMs to improve non-verifying files. Furthermore, we post-process the\nremaining files into pairs of C code and ACSL specifications, where each\nspecification-implementation pair is formally verified using Frama-C. To ensure\nthe quality of the pairs, a manual inspection is conducted to confirm the\ncorrectness of every pair. The resulting dataset of C-ACSL specification pairs\n(CASP) provides a foundation for benchmarking and further research on\nintegrating automated code generation with verified correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent developments in Large Language Models (LLMs) have shown promise in\nautomating code generation, yet the generated programs lack rigorous\ncorrectness guarantees. Formal verification can address this shortcoming, but\nrequires expertise and is time-consuming to apply. Currently, there is no\ndataset of verified C code paired with formal specifications that enables\nsystematic benchmarking in this space. To fill this gap, we present a curated\nevaluation dataset of C code paired with formal specifications written in\nANSI/ISO C Specification Language (ACSL). We develop a multi-stage filtering\nprocess to carefully extract 506 pairs of C code and formal specifications from\nThe Stack 1 and The Stack 2. We first identify C files annotated with formal\nlanguages. Then, we ensure that the annotated C files formally verify, and\nemploy LLMs to improve non-verifying files. Furthermore, we post-process the\nremaining files into pairs of C code and ACSL specifications, where each\nspecification-implementation pair is formally verified using Frama-C. To ensure\nthe quality of the pairs, a manual inspection is conducted to confirm the\ncorrectness of every pair. The resulting dataset of C-ACSL specification pairs\n(CASP) provides a foundation for benchmarking and further research on\nintegrating automated code generation with verified correctness."
                },
                "authors": [
                    {
                        "name": "Niclas Hertzberg"
                    },
                    {
                        "name": "Merlijn Sevenhuijsen"
                    },
                    {
                        "name": "Liv Kåreborn"
                    },
                    {
                        "name": "Anna Lokrantz"
                    }
                ],
                "author_detail": {
                    "name": "Anna Lokrantz"
                },
                "author": "Anna Lokrantz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18797v1",
                "updated": "2025-08-26T08:29:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    29,
                    5,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:29:05Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    29,
                    5,
                    1,
                    238,
                    0
                ],
                "title": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMACE: Causality Empowered Multi-Agents in Minecraft Cooperative\n  Tasks"
                },
                "summary": "Minecraft, as an open-world virtual interactive environment, has become a\nprominent platform for research on agent decision-making and execution.\nExisting works primarily adopt a single Large Language Model (LLM) agent to\ncomplete various in-game tasks. However, for complex tasks requiring lengthy\nsequences of actions, single-agent approaches often face challenges related to\ninefficiency and limited fault tolerance. Despite these issues, research on\nmulti-agent collaboration remains scarce. In this paper, we propose CausalMACE,\na holistic causality planning framework designed to enhance multi-agent\nsystems, in which we incorporate causality to manage dependencies among\nsubtasks. Technically, our proposed framework introduces two modules: an\noverarching task graph for global task planning and a causality-based module\nfor dependency management, where inherent rules are adopted to perform causal\nintervention. Experimental results demonstrate our approach achieves\nstate-of-the-art performance in multi-agent cooperative tasks of Minecraft.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minecraft, as an open-world virtual interactive environment, has become a\nprominent platform for research on agent decision-making and execution.\nExisting works primarily adopt a single Large Language Model (LLM) agent to\ncomplete various in-game tasks. However, for complex tasks requiring lengthy\nsequences of actions, single-agent approaches often face challenges related to\ninefficiency and limited fault tolerance. Despite these issues, research on\nmulti-agent collaboration remains scarce. In this paper, we propose CausalMACE,\na holistic causality planning framework designed to enhance multi-agent\nsystems, in which we incorporate causality to manage dependencies among\nsubtasks. Technically, our proposed framework introduces two modules: an\noverarching task graph for global task planning and a causality-based module\nfor dependency management, where inherent rules are adopted to perform causal\nintervention. Experimental results demonstrate our approach achieves\nstate-of-the-art performance in multi-agent cooperative tasks of Minecraft."
                },
                "authors": [
                    {
                        "name": "Qi Chai"
                    },
                    {
                        "name": "Zhang Zheng"
                    },
                    {
                        "name": "Junlong Ren"
                    },
                    {
                        "name": "Deheng Ye"
                    },
                    {
                        "name": "Zichuan Lin"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18784v1",
                "updated": "2025-08-26T08:11:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    11,
                    50,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:11:50Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    11,
                    50,
                    1,
                    238,
                    0
                ],
                "title": "Insights into User Interface Innovations from a Design Thinking Workshop\n  at deRSE25",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights into User Interface Innovations from a Design Thinking Workshop\n  at deRSE25"
                },
                "summary": "Large Language Models have become widely adopted tools due to their versatile\ncapabilities, yet their user interfaces remain limited, often following rigid,\nlinear interaction paradigms. In this paper, we present insights from a design\nthinking workshop held at the deRSE25 conference aiming at collaboratively\ndeveloping innovative user interface concepts for LLMs. During the workshop,\nparticipants identified common use cases, evaluated the strengths and\nshortcomings of current LLM interfaces, and created visualizations of new\ninteraction concepts emphasizing flexible context management, dynamic\nconversation branching, and enhanced mechanisms for user control. We describe\nhow these participant-generated ideas advanced our own whiteboard-based UI\napproach. The ongoing development of this interface is guided by the\nhuman-centered design process - an iterative, user-focused methodology that\nemphasizes continuous refinement through user feedback. Broader implications\nfor future LLM interface development are discussed, advocating for increased\nattention to UI innovation grounded in user-centered design principles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have become widely adopted tools due to their versatile\ncapabilities, yet their user interfaces remain limited, often following rigid,\nlinear interaction paradigms. In this paper, we present insights from a design\nthinking workshop held at the deRSE25 conference aiming at collaboratively\ndeveloping innovative user interface concepts for LLMs. During the workshop,\nparticipants identified common use cases, evaluated the strengths and\nshortcomings of current LLM interfaces, and created visualizations of new\ninteraction concepts emphasizing flexible context management, dynamic\nconversation branching, and enhanced mechanisms for user control. We describe\nhow these participant-generated ideas advanced our own whiteboard-based UI\napproach. The ongoing development of this interface is guided by the\nhuman-centered design process - an iterative, user-focused methodology that\nemphasizes continuous refinement through user feedback. Broader implications\nfor future LLM interface development are discussed, advocating for increased\nattention to UI innovation grounded in user-centered design principles."
                },
                "authors": [
                    {
                        "name": "Maximilian Frank"
                    },
                    {
                        "name": "Simon Lund"
                    }
                ],
                "author_detail": {
                    "name": "Simon Lund"
                },
                "author": "Simon Lund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18190v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18190v2",
                "updated": "2025-08-26T08:10:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    10,
                    55,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-25T16:48:51Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    16,
                    48,
                    51,
                    0,
                    237,
                    0
                ],
                "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering"
                },
                "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor."
                },
                "authors": [
                    {
                        "name": "Zirui Tang"
                    },
                    {
                        "name": "Boyu Niu"
                    },
                    {
                        "name": "Xuanhe Zhou"
                    },
                    {
                        "name": "Boxiu Li"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xinyi Zhang"
                    },
                    {
                        "name": "Fan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fan Wu"
                },
                "author": "Fan Wu",
                "arxiv_comment": "Extension of our SIGMOD 2026 paper. Please refer to source code\n  available at: https://github.com/weAIDB/ST-Raptor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18190v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18190v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11841v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11841v4",
                "updated": "2025-08-26T08:10:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    10,
                    41,
                    1,
                    238,
                    0
                ],
                "published": "2025-01-21T02:51:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    51,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "Survey on Monocular Metric Depth Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Survey on Monocular Metric Depth Estimation"
                },
                "summary": "Monocular Depth Estimation (MDE) enables spatial understanding, 3D\nreconstruction, and autonomous navigation, yet deep learning approaches often\npredict only relative depth without a consistent metric scale. This limitation\nreduces reliability in applications such as visual SLAM, precise 3D modeling,\nand view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this\nchallenge by producing depth maps with absolute scale, ensuring geometric\nconsistency and enabling deployment without additional calibration. This survey\nreviews the evolution of MMDE, from geometry-based methods to state-of-the-art\ndeep models, with emphasis on the datasets that drive progress. Key benchmarks,\nincluding KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of\nmodality, scene type, and application domain. Methodological advances are\nanalyzed, covering domain generalization, boundary preservation, and the\nintegration of synthetic and real data. Techniques such as unsupervised and\nsemi-supervised learning, patch-based inference, architectural innovations, and\ngenerative modeling are evaluated for their strengths and limitations. By\nsynthesizing current progress, highlighting the importance of high-quality\ndatasets, and identifying open challenges, this survey provides a structured\nreference for advancing MMDE and supporting its adoption in real-world computer\nvision systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Depth Estimation (MDE) enables spatial understanding, 3D\nreconstruction, and autonomous navigation, yet deep learning approaches often\npredict only relative depth without a consistent metric scale. This limitation\nreduces reliability in applications such as visual SLAM, precise 3D modeling,\nand view synthesis. Monocular Metric Depth Estimation (MMDE) overcomes this\nchallenge by producing depth maps with absolute scale, ensuring geometric\nconsistency and enabling deployment without additional calibration. This survey\nreviews the evolution of MMDE, from geometry-based methods to state-of-the-art\ndeep models, with emphasis on the datasets that drive progress. Key benchmarks,\nincluding KITTI, NYU-D, ApolloScape, and TartanAir, are examined in terms of\nmodality, scene type, and application domain. Methodological advances are\nanalyzed, covering domain generalization, boundary preservation, and the\nintegration of synthetic and real data. Techniques such as unsupervised and\nsemi-supervised learning, patch-based inference, architectural innovations, and\ngenerative modeling are evaluated for their strengths and limitations. By\nsynthesizing current progress, highlighting the importance of high-quality\ndatasets, and identifying open challenges, this survey provides a structured\nreference for advancing MMDE and supporting its adoption in real-world computer\nvision systems."
                },
                "authors": [
                    {
                        "name": "Jiuling Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiuling Zhang"
                },
                "author": "Jiuling Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11841v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11841v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18783v1",
                "updated": "2025-08-26T08:10:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    10,
                    1,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:10:01Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    10,
                    1,
                    1,
                    238,
                    0
                ],
                "title": "Controllable Conversational Theme Detection Track at DSTC 12",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllable Conversational Theme Detection Track at DSTC 12"
                },
                "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository."
                },
                "authors": [
                    {
                        "name": "Igor Shalyminov"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jake Vincent"
                    },
                    {
                        "name": "Siffi Singh"
                    },
                    {
                        "name": "Jason Cai"
                    },
                    {
                        "name": "James Gung"
                    },
                    {
                        "name": "Raphael Shu"
                    },
                    {
                        "name": "Saab Mansour"
                    }
                ],
                "author_detail": {
                    "name": "Saab Mansour"
                },
                "author": "Saab Mansour",
                "arxiv_comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18780v1",
                "updated": "2025-08-26T08:04:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    4,
                    4,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T08:04:04Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    4,
                    4,
                    1,
                    238,
                    0
                ],
                "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical\n  Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical\n  Error Correction"
                },
                "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC."
                },
                "authors": [
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xunjian Yin"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Xiaojun Wan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Wan"
                },
                "author": "Xiaojun Wan",
                "arxiv_comment": "Code will be released upon publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07581v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07581v3",
                "updated": "2025-08-26T08:03:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    8,
                    3,
                    56,
                    1,
                    238,
                    0
                ],
                "published": "2025-05-12T14:05:17Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    14,
                    5,
                    17,
                    0,
                    132,
                    0
                ],
                "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large\n  Language Models"
                },
                "summary": "Leveraging large language model (LLM) based agents to simulate human social\nbehaviors has recently gained significant attention. In this paper, we\nintroduce a novel social simulator called YuLan-OneSim. Compared to previous\nworks, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free\nscenario construction: Users can simply describe and refine their simulation\nscenarios through natural language interactions with our simulator. All\nsimulation code is automatically generated, significantly reducing the need for\nprogramming expertise. (2) Comprehensive default scenarios: We implement 50\ndefault simulation scenarios spanning 8 domains, including economics,\nsociology, politics, psychology, organization, demographics, law, and\ncommunication, broadening access for a diverse range of social researchers. (3)\nEvolvable simulation: Our simulator is capable of receiving external feedback\nand automatically fine-tuning the backbone LLMs, significantly enhancing the\nsimulation quality. (4) Large-scale simulation: By developing a fully\nresponsive agent framework and a distributed simulation architecture, our\nsimulator can handle up to 100,000 agents, ensuring more stable and reliable\nsimulation results. (5) AI social researcher: Leveraging the above features, we\ndevelop an AI social researcher. Users only need to propose a research topic,\nand the AI researcher will automatically analyze the input, construct\nsimulation environments, summarize results, generate technical reports, review\nand refine the reports--completing the social science research loop. To\ndemonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate\nthe quality of the automatically generated scenarios, the reliability,\nefficiency, and scalability of the simulation process, as well as the\nperformance of the AI social researcher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging large language model (LLM) based agents to simulate human social\nbehaviors has recently gained significant attention. In this paper, we\nintroduce a novel social simulator called YuLan-OneSim. Compared to previous\nworks, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free\nscenario construction: Users can simply describe and refine their simulation\nscenarios through natural language interactions with our simulator. All\nsimulation code is automatically generated, significantly reducing the need for\nprogramming expertise. (2) Comprehensive default scenarios: We implement 50\ndefault simulation scenarios spanning 8 domains, including economics,\nsociology, politics, psychology, organization, demographics, law, and\ncommunication, broadening access for a diverse range of social researchers. (3)\nEvolvable simulation: Our simulator is capable of receiving external feedback\nand automatically fine-tuning the backbone LLMs, significantly enhancing the\nsimulation quality. (4) Large-scale simulation: By developing a fully\nresponsive agent framework and a distributed simulation architecture, our\nsimulator can handle up to 100,000 agents, ensuring more stable and reliable\nsimulation results. (5) AI social researcher: Leveraging the above features, we\ndevelop an AI social researcher. Users only need to propose a research topic,\nand the AI researcher will automatically analyze the input, construct\nsimulation environments, summarize results, generate technical reports, review\nand refine the reports--completing the social science research loop. To\ndemonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate\nthe quality of the automatically generated scenarios, the reliability,\nefficiency, and scalability of the simulation process, as well as the\nperformance of the AI social researcher."
                },
                "authors": [
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Heyang Gao"
                    },
                    {
                        "name": "Xiaohe Bo"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07581v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07581v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18773v1",
                "updated": "2025-08-26T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    57,
                    28,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    57,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks."
                },
                "authors": [
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Jiangjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiangjie Chen"
                },
                "author": "Jiangjie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08846v3",
                "updated": "2025-08-26T07:56:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    56,
                    10,
                    1,
                    238,
                    0
                ],
                "published": "2024-09-13T14:04:39Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    14,
                    4,
                    39,
                    4,
                    257,
                    0
                ],
                "title": "Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint\n  Transfer via Vector Addition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprint Vector: Enabling Scalable and Efficient Model Fingerprint\n  Transfer via Vector Addition"
                },
                "summary": "Backdoor-based fingerprinting has emerged as an effective technique for\ntracing the ownership of large language models. However, in real-world\ndeployment scenarios, developers often instantiate multiple downstream models\nfrom a shared base model, and applying fingerprinting to each variant\nindividually incurs prohibitive computational overhead. While inheritance-based\napproaches -- where fingerprints are embedded into the base model and expected\nto persist through fine-tuning -- appear attractive, they suffer from three key\nlimitations: late-stage fingerprinting, fingerprint instability, and\ninterference with downstream adaptation. To address these challenges, we\npropose a novel mechanism called the Fingerprint Vector. Our method first\nembeds a fingerprint into the base model via backdoor-based fine-tuning, then\nextracts a task-specific parameter delta as a fingerprint vector by computing\nthe difference between the fingerprinted and clean models. This vector can be\ndirectly added to any structurally compatible downstream model, allowing the\nfingerprint to be transferred post hoc without additional fine-tuning.\nExtensive experiments show that Fingerprint Vector achieves comparable or\nsuperior performance to direct injection across key desiderata. It maintains\nstrong effectiveness across diverse model architectures as well as mainstream\ndownstream variants within the same family. It also preserves harmlessness and\nrobustness in most cases. Even when slight robustness degradation is observed,\nthe impact remains within acceptable bounds and is outweighed by the\nscalability benefits of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor-based fingerprinting has emerged as an effective technique for\ntracing the ownership of large language models. However, in real-world\ndeployment scenarios, developers often instantiate multiple downstream models\nfrom a shared base model, and applying fingerprinting to each variant\nindividually incurs prohibitive computational overhead. While inheritance-based\napproaches -- where fingerprints are embedded into the base model and expected\nto persist through fine-tuning -- appear attractive, they suffer from three key\nlimitations: late-stage fingerprinting, fingerprint instability, and\ninterference with downstream adaptation. To address these challenges, we\npropose a novel mechanism called the Fingerprint Vector. Our method first\nembeds a fingerprint into the base model via backdoor-based fine-tuning, then\nextracts a task-specific parameter delta as a fingerprint vector by computing\nthe difference between the fingerprinted and clean models. This vector can be\ndirectly added to any structurally compatible downstream model, allowing the\nfingerprint to be transferred post hoc without additional fine-tuning.\nExtensive experiments show that Fingerprint Vector achieves comparable or\nsuperior performance to direct injection across key desiderata. It maintains\nstrong effectiveness across diverse model architectures as well as mainstream\ndownstream variants within the same family. It also preserves harmlessness and\nrobustness in most cases. Even when slight robustness degradation is observed,\nthe impact remains within acceptable bounds and is outweighed by the\nscalability benefits of our approach."
                },
                "authors": [
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Qichen Liu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Wenpeng Xing"
                    },
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Mohan Li"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]