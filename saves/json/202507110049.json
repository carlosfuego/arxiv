[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v3",
                "updated": "2025-07-09T04:43:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    4,
                    43,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06517v1",
                "updated": "2025-07-09T03:33:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T03:33:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    3,
                    33,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and\n  Deep Layers"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance."
                },
                "authors": [
                    {
                        "name": "Zicong Tang"
                    },
                    {
                        "name": "Shi Luohe"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "arxiv_comment": "Accepted by ACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18890v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18890v2",
                "updated": "2025-07-09T02:35:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    2,
                    35,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-26T07:10:08Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    7,
                    10,
                    8,
                    2,
                    57,
                    0
                ],
                "title": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation"
                },
                "summary": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating ultra-long sequences with large language models (LLMs) has become\nincreasingly crucial but remains a highly time-intensive task, particularly for\nsequences up to 100K tokens. While traditional speculative decoding methods\nexist, simply extending their generation limits fails to accelerate the process\nand can be detrimental. Through an in-depth analysis, we identify three major\nchallenges hindering efficient generation: frequent model reloading, dynamic\nkey-value (KV) management and repetitive generation. To address these issues,\nwe introduce TOKENSWIFT, a novel framework designed to substantially accelerate\nthe generation process of ultra-long sequences while maintaining the target\nmodel's inherent quality. Experimental results demonstrate that TOKENSWIFT\nachieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,\n14B) and architectures (MHA, GQA). This acceleration translates to hours of\ntime savings for ultra-long sequence generation, establishing TOKENSWIFT as a\nscalable and effective solution at unprecedented lengths. Code can be found at\nhttps://github.com/bigai-nlco/TokenSwift."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Junzhe Shen"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "arxiv_comment": "Accepted By ICML25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18890v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18890v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v2",
                "updated": "2025-07-08T21:23:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    21,
                    23,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "21 pages, 10 figures. Supplement 31 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06349v1",
                "updated": "2025-07-08T19:20:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T19:20:30Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    19,
                    20,
                    30,
                    1,
                    189,
                    0
                ],
                "title": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Queue SSD I/O Modeling & Its Implications for Data Structure\n  Design"
                },
                "summary": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the performance profiles of storage devices and how best to\nutilize them has always been non-trivial due to factors such as seek times,\ncaching, scheduling, concurrent access, flash wear-out, and garbage collection.\nHowever, analytical frameworks that provide simplified abstractions of storage\nperformance can still be accurate enough to evaluate external memory algorithms\nand data structures at the design stage. For example, the Disk Access Machine\n(DAM) model assumes that a storage device transfers data in fixed-size blocks\nof size B and that all transfers have unit latency. This abstraction is already\nsufficient to explain some of the benefits of data structures such as B-trees\nand Log-Structured Merge trees (LSM trees); however, storage technology\nadvances have significantly reduced current models' accuracy and utility.\n  This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new\nstorage abstraction. This model builds upon previous models and aims to more\naccurately represent the performance characteristics of modern storage\nhardware. We identify key performance-critical aspects of modern multi-queue\nsolid-state drives on which we base our model and demonstrate these\ncharacteristics on actual hardware. We then show how our model can be applied\nto LSM-tree-based storage engines to optimize them for modern storage hardware.\nWe highlight that leveraging concurrent access is crucial for fully utilizing\nthe high throughput of multi-queue SSDs, enabling designs that may appear\ncounterintuitive under traditional paradigms We then validate these insights\nthrough experiments using Facebook's LSM-tree-based key-value store, RocksDB.\nWe conclude that the MQSSD model offers a more accurate abstraction of modern\nhardware than previous models, allowing for greater insight and optimization."
                },
                "authors": [
                    {
                        "name": "Erin Ransom"
                    },
                    {
                        "name": "Andrew Lim"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v3",
                "updated": "2025-07-08T12:34:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    12,
                    34,
                    10,
                    1,
                    189,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07061v1",
                "updated": "2025-07-08T09:20:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "published": "2025-07-08T09:20:12Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    9,
                    20,
                    12,
                    1,
                    189,
                    0
                ],
                "title": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Ensemble Embedding Approach for Improving Semantic Caching\n  Performance in LLM-based Systems"
                },
                "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems."
                },
                "authors": [
                    {
                        "name": "Shervin Ghaffari"
                    },
                    {
                        "name": "Zohre Bahranifard"
                    },
                    {
                        "name": "Mohammad Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Akbari"
                },
                "author": "Mohammad Akbari",
                "arxiv_comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v4",
                "updated": "2025-07-08T07:10:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    7,
                    10,
                    6,
                    1,
                    189,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, an image and video generative DiT variant\nenhanced with Long-Skip-Connections (LSCs) - the key efficiency component in\nU-Nets. Theoretical spectral norm and visualization analysis demonstrate how\nLSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized\ndynamic feature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across the image and video generation tasks demonstrate that\nSkip-DiT achieves: (1) 4.4 times training acceleration and faster convergence,\n(2) 1.5-2 times inference acceleration with negligible quality loss and high\nfidelity to the original output, outperforming existing DiT caching methods\nacross various quantitative metrics. Our findings establish\nLong-Skip-Connections as critical architectural components for stable and\nefficient diffusion transformers. Codes are provided in the\nhttps://github.com/OpenSparseLLMs/Skip-DiT."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03622v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03622v3",
                "updated": "2025-07-08T02:15:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    2,
                    15,
                    7,
                    1,
                    189,
                    0
                ],
                "published": "2023-06-06T12:19:05Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    12,
                    19,
                    5,
                    1,
                    157,
                    0
                ],
                "title": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Torpor: GPU-Enabled Serverless Computing for Low-Latency,\n  Resource-Efficient Inference"
                },
                "summary": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing offers a compelling cloud model for online inference\nservices. However, existing serverless platforms lack efficient support for\nGPUs, hindering their ability to deliver high-performance inference. In this\npaper, we present Torpor, a serverless platform for GPU-efficient, low-latency\ninference. To enable efficient sharing of a node's GPUs among numerous\ninference functions, Torpor maintains models in main memory and dynamically\nswaps them onto GPUs upon request arrivals (i.e., late binding with model\nswapping). Torpor uses various techniques, including asynchronous API\nredirection, GPU runtime sharing, pipelined model execution, and efficient GPU\nmemory management, to minimize latency overhead caused by model swapping.\nAdditionally, we design an interference-aware request scheduling algorithm that\nutilizes high-speed GPU interconnects to meet latency service-level objectives\n(SLOs) for individual inference functions. We have implemented Torpor and\nevaluated its performance in a production environment. Utilizing late binding\nand model swapping, Torpor can concurrently serve hundreds of inference\nfunctions on a worker node with 4 GPUs, while achieving latency performance\ncomparable to native execution, where each model is cached exclusively on a\nGPU. Pilot deployment in a leading commercial serverless cloud shows that\nTorpor reduces the GPU provisioning cost by 70% and 65% for users and the\nplatform, respectively."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Dong Chen"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Xiaonan Luo"
                    },
                    {
                        "name": "Zhuohao Li"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    },
                    {
                        "name": "Dapeng Nie"
                    },
                    {
                        "name": "Haoran Yang"
                    },
                    {
                        "name": "Yu Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yu Ding"
                },
                "author": "Yu Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03622v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03622v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v2",
                "updated": "2025-07-08T00:51:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    51,
                    16,
                    1,
                    189,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05240v1",
                "updated": "2025-07-07T17:49:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T17:49:41Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    17,
                    49,
                    41,
                    0,
                    188,
                    0
                ],
                "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling"
                },
                "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}."
                },
                "authors": [
                    {
                        "name": "Meng Wei"
                    },
                    {
                        "name": "Chenyang Wan"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Yuqiang Yang"
                    },
                    {
                        "name": "Xiaohan Mao"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Wenzhe Cai"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04967v1",
                "updated": "2025-07-07T13:10:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T13:10:01Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    10,
                    1,
                    0,
                    188,
                    0
                ],
                "title": "The Case for Instance-Optimized LLMs in OLAP Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Case for Instance-Optimized LLMs in OLAP Databases"
                },
                "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."
                },
                "authors": [
                    {
                        "name": "Bardia Mohammadi"
                    },
                    {
                        "name": "Laurent Bindschaedler"
                    }
                ],
                "author_detail": {
                    "name": "Laurent Bindschaedler"
                },
                "author": "Laurent Bindschaedler",
                "arxiv_journal_ref": "27th International Workshop on Design, Optimization, Languages and\n  Analytical Processing of Big Data 2025. CEUR-WS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v2",
                "updated": "2025-07-07T09:25:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    9,
                    25,
                    21,
                    0,
                    188,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04697v1",
                "updated": "2025-07-07T06:33:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "published": "2025-07-07T06:33:59Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    6,
                    33,
                    59,
                    0,
                    188,
                    0
                ],
                "title": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Evaluation of General Purpose Large Language Models for\n  Basic Linear Algebra Subprograms Code Generation"
                },
                "summary": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI technology based on Large Language Models (LLM) has been\ndeveloped and applied to assist or automatically generate program codes. In\nthis paper, we evaluate the capability of existing general LLMs for Basic\nLinear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs\nprovided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model,\nand o4-mini, one of the o-series of Reasoning models. Both have been released\nin April 2025. For the routines from level-1 to 3 BLAS, we tried to generate\n(1) C code without optimization from routine name only, (2) C code with basic\nperformance optimizations (thread parallelization, SIMD vectorization, and\ncache blocking) from routine name only, and (3) C code with basic performance\noptimizations based on Fortran reference code. As a result, we found that\ncorrect code can be generated in many cases even when only routine name are\ngiven. We also confirmed that thread parallelization with OpenMP, SIMD\nvectorization, and cache blocking can be implemented to some extent, and that\nthe code is faster than the reference code."
                },
                "authors": [
                    {
                        "name": "Daichi Mukunoki"
                    },
                    {
                        "name": "Shun-ichiro Hayashi"
                    },
                    {
                        "name": "Tetsuya Hoshino"
                    },
                    {
                        "name": "Takahiro Katagiri"
                    }
                ],
                "author_detail": {
                    "name": "Takahiro Katagiri"
                },
                "author": "Takahiro Katagiri",
                "arxiv_comment": "8 pages, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v1",
                "updated": "2025-07-06T15:08:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels; however, their dependence on softmax attention poses a major\ncomputational bottleneck, particularly in long-context settings. In this work,\nrather than following prevalent approaches such as linear attention (or SSMs)\nand local attention, we introduce an intermediate design called \\rat between\nrecurrence and attention mechanisms. It partitions the input into chunks,\napplies a simple linear recurrence within each chunk to capture local\ndependencies, and then performs softmax attention across chunks to model\nlong-range interactions. By adjusting the size of the chunk, \\rat enables\nflexible trade-offs, combining the strengths of RNN and attention. Empirically,\nwith a chunk size of 16, the \\rat layer achieves a \\(7\\times\\) improvement in\ntraining speed with 100K token sequences and \\(9\\times\\) in generation at 4K\nsequence length, while maintaining similar or sometimes even better accuracy\ncompared to standard attention. We demonstrate this by training 1.3B parameter\nmodels from scratch and performing large-scale evaluations, including short-\nand long-context benchmarks, as well as supervised fine-tuning~(SFT). We\nfurther propose a hybrid architecture that interleaves \\rat with local\nattention. By combining efficient long-range modeling with strong local\ninteractions, this hybrid design not only improves inference speed and reduces\ncache memory usage compared to attention, but also consistently enhances\nperformance, for example, achieving an average 1 point gain in commonsense\nreasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase\nin a summarization SFT task. Code is available at\nhttps://github.com/CLAIRE-Labo/RAT"
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01110v2",
                "updated": "2025-07-05T15:51:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    51,
                    57,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-01T18:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    18,
                    12,
                    43,
                    1,
                    182,
                    0
                ],
                "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale\n  Reconstruction with External Memory"
                },
                "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Lukas Radl"
                    },
                    {
                        "name": "Thomas Khler"
                    },
                    {
                        "name": "Michael Steiner"
                    },
                    {
                        "name": "Dieter Schmalstieg"
                    },
                    {
                        "name": "Markus Steinberger"
                    }
                ],
                "author_detail": {
                    "name": "Markus Steinberger"
                },
                "author": "Markus Steinberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v2",
                "updated": "2025-07-05T15:40:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    15,
                    40,
                    51,
                    5,
                    186,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v2",
                "updated": "2025-07-05T13:37:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    13,
                    37,
                    48,
                    5,
                    186,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems. MemScope enables\nprecise characterization of the temporal behavior of available memory modules\nunder configurable contention stress scenarios. MemScope leverages kernel-level\ncontrol over physical memory allocation, cache maintenance, CPU state,\ninterrupts, and I/O device activity to accurately benchmark heterogeneous\nmemory subsystems. This gives us the privilege to directly map pieces of\ncontiguous physical memory and instantiate allocators, allowing us to finely\ncontrol cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to\nprecisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Gabriel Franco"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03980v1",
                "updated": "2025-07-05T10:11:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T10:11:37Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    10,
                    11,
                    37,
                    5,
                    186,
                    0
                ],
                "title": "Combination generators with optimal cache utilization and communication\n  free parallel execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combination generators with optimal cache utilization and communication\n  free parallel execution"
                },
                "summary": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient and elegant combination generator for producing all\ncombinations of size less than or equal to K, designed for exhaustive\ngeneration and combinatorial optimization tasks. This generator can be\nimplemented to achieve what we define as optimal efficiency: constant amortized\ntime, optimal cache utilization, embarrassingly parallel execution, and a\nrecursive structure compatible with pruning-based search. These properties are\ndifficult to satisfy simultaneously in existing generators. For example,\nclassical Gray code or lexicographic generators are typically list-based and\nsequentially defined, making them difficult to vectorized, inefficient in cache\nusage, and inherently hard to parallelize. Generators based on unranking\nmethods, while easy to parallelize, are non-recursive. These limitations reduce\ntheir applicability in our target applications, where both computational\nefficiency and recursion are crucial. We adapt Bird's algebra of\nprogramming-style calculation to derive our algorithms, a formalism for\ndeveloping correct-by-construction programs from specifications. As a result,\nall generators in this paper are first formulated in their clearest\nspecification, and efficient definitions are derived constructively through\nequational reasoning, resulting in concise and elegant divide-and-conquer\ndefinitions. Beyond presenting a combination generator, we extend our approach\nto construct generators for K-permutations, nested combinations of\ncombinations, and nested permutation-combination structures. To the best of our\nknowledge, the literature has not previously reported generators for these\nnested structures. We also develop sequential variants that produce\nconfigurations in Gray code-compatible orders -- such as the revolving door\nordering -- which are particularly useful for constructing nested generators."
                },
                "authors": [
                    {
                        "name": "Xi He"
                    },
                    {
                        "name": "Max. A. Little"
                    }
                ],
                "author_detail": {
                    "name": "Max. A. Little"
                },
                "author": "Max. A. Little",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03919v1",
                "updated": "2025-07-05T06:55:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "published": "2025-07-05T06:55:45Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    6,
                    55,
                    45,
                    5,
                    186,
                    0
                ],
                "title": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PFCS: Prime Factorization Cache System for Deterministic Data\n  Relationship Discovery"
                },
                "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"
                },
                "authors": [
                    {
                        "name": "Duy Le"
                    }
                ],
                "author_detail": {
                    "name": "Duy Le"
                },
                "author": "Duy Le",
                "arxiv_comment": "6 pages, 3 figures, 3 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06483v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06483v3",
                "updated": "2025-07-05T01:08:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    5,
                    1,
                    8,
                    40,
                    5,
                    186,
                    0
                ],
                "published": "2024-06-10T17:22:17Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    17,
                    22,
                    17,
                    0,
                    162,
                    0
                ],
                "title": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Taxonomy and Comparative Analysis of IPv4 Identifier Selection\n  Correctness, Security, and Performance"
                },
                "summary": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The battle for a more secure Internet is waged on many fronts, including the\nmost basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an\nIPv4 header field as old as the Internet with an equally long history as an\nexploited side channel for scanning network properties, inferring off-path\nconnections, and poisoning DNS caches. This article taxonomizes the 25-year\nhistory of IPID-based exploits and the corresponding changes to IPID selection\nmethods. By mathematically analyzing these methods' correctness and security\nand empirically evaluating their performance, we reveal recommendations for\nbest practice as well as shortcomings of current operating system\nimplementations, emphasizing the value of systematic evaluations in network\nsecurity."
                },
                "authors": [
                    {
                        "name": "Joshua J. Daymude"
                    },
                    {
                        "name": "Antonio M. Espinoza"
                    },
                    {
                        "name": "Sean Bergen"
                    },
                    {
                        "name": "Benjamin Mixon-Baca"
                    },
                    {
                        "name": "Jeffrey Knockel"
                    },
                    {
                        "name": "Jedidiah R. Crandall"
                    }
                ],
                "author_detail": {
                    "name": "Jedidiah R. Crandall"
                },
                "author": "Jedidiah R. Crandall",
                "arxiv_comment": "36 pages, 11 figures, 2 tables, 1 algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06483v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06483v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v1",
                "updated": "2025-07-04T21:09:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. Khn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. Khn"
                },
                "author": "Martin J. Khn",
                "arxiv_comment": "29 pages, 10 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03445v1",
                "updated": "2025-07-04T10:01:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T10:01:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    1,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Quantum Algorithm for the Fixed-Radius Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Algorithm for the Fixed-Radius Neighbor Search"
                },
                "summary": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The neighbor search is a computationally demanding problem, usually both\ntime- and memory-consuming. The main problem of this kind of algorithms is the\nlong execution time due to cache misses. In this work, we propose a quantum\nalgorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the\nfixed-point version of Grover's algorithm. We derive an efficient circuit for\nsolving the FRANS with linear query complexity with the number of particles\n$N$. The quantum circuit returns the list of all the neighbors' pairs within\nthe fixed radius, together with their distance, avoiding the slow down given by\ncache miss. We explicitly write the Grover's operator and analyze its gate\ncomplexity. The whole algorithm has complexity of\n$\\mathcal{O}(M^{\\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is\nthe number of neighboring pairs, and uses $\\mathcal{O}(\\log N)$ number of\nqubits. By employing extra ancilla qubits the depth of the circuit can be\nbrought down to $\\mathcal{O}(N\\log N)$ at the cost of $\\mathcal{O}(N)$ qubits\nfor unstructured dataset, or $\\mathcal{O}(\\text{poly}(\\log N))$ qubits for\nstructured datasets. Finally we assess the resilience of the model to the\nreadout error, suggesting an error correction-free strategy to check the\naccuracy of the results."
                },
                "authors": [
                    {
                        "name": "Luca Cappelli"
                    },
                    {
                        "name": "Claudio Sanavio"
                    },
                    {
                        "name": "Alessandro Andrea Zecchi"
                    },
                    {
                        "name": "Giuseppe Murante"
                    },
                    {
                        "name": "Sauro Succi"
                    }
                ],
                "author_detail": {
                    "name": "Sauro Succi"
                },
                "author": "Sauro Succi",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03396v1",
                "updated": "2025-07-04T09:03:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T09:03:18Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    9,
                    3,
                    18,
                    4,
                    185,
                    0
                ],
                "title": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical investigation of the effect of high voltage frequency on the\n  density of RONS species in the air atmospheric pressure gas discharge"
                },
                "summary": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few decades, studies in various fields of plasma technology have\nexpanded and its application in different processes has increased. Therefore,\nthe achievement of a desirable and practical plasma with specific\ncharacteristics is of particular importance. The frequency of the applied\nvoltage is one of the important factors that play a role in the physical and\nchemical characteristics. In this research, changes in the density of active\nspecies produced in an electrical discharge using a dielectric barrier and air\nworking gas have been investigated from a frequency of 500 Hz to 500 kHz, and\nby applying a constant voltage of 2 kV, have been investigated. For this\npurpose, 87 different reactions with specific collision cross-sections were\ndefined in COMSOL Multiphysics. Other parameters, including current-voltage\nwaveform, electric field, and species densitywere evaluated. The results show\nthat under completely identical conditions, the electron temperature\ndistribution changes with increasing applied frequency, and the density of\nreactive oxygen and nitrogen species RONS decreases, but O shows an increasing\ntrend. It should be noted that the simulation results are in good agreement\nwith previous experimental and simulation reports. These results offer valuable\ninsights into optimizing plasma parameters for different applications,\npotentially resulting in better treatment outcomes across a range of\ntherapeutic domains."
                },
                "authors": [
                    {
                        "name": "Fariborz Momtazzadeh"
                    },
                    {
                        "name": "Farshad Sohbatzadeh"
                    },
                    {
                        "name": "Hamed Soltani Ahmadi"
                    },
                    {
                        "name": "Ramin Mehrabifard"
                    }
                ],
                "author_detail": {
                    "name": "Ramin Mehrabifard"
                },
                "author": "Ramin Mehrabifard",
                "arxiv_doi": "10.1007/S40042-025-01392-9.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/S40042-025-01392-9.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.03396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v3",
                "updated": "2025-07-04T06:49:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    31,
                    4,
                    185,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token\n  Selection"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value vector information on\ntop of attention-based eviction scores. Additionally, CAOTE can act as a\nmeta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "14 pages, 3 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15431v3",
                "updated": "2025-07-04T06:36:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    36,
                    38,
                    4,
                    185,
                    0
                ],
                "published": "2025-05-21T12:11:53Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    12,
                    11,
                    53,
                    2,
                    141,
                    0
                ],
                "title": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-TurboS: Advancing Large Language Models through\n  Mamba-Transformer Synergy and Adaptive Chain-of-Thought"
                },
                "summary": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,\na novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It\nsynergistically combines Mamba's long-sequence processing efficiency with\nTransformer's superior contextual understanding. Hunyuan-TurboS features an\nadaptive long-short chain-of-thought (CoT) mechanism, dynamically switching\nbetween rapid responses for simple queries and deep \"thinking\" modes for\ncomplex problems, optimizing computational resources. Architecturally, this 56B\nactivated (560B total) parameter model employs 128 layers (Mamba2, Attention,\nFFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear\ncomplexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE\nstructure. Pre-trained on 16T high-quality tokens, it supports a 256K context\nlength and is the first industry-deployed large-scale Mamba model. Our\ncomprehensive post-training strategy enhances capabilities via Supervised\nFine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,\nMulti-round Deliberation Learning for iterative improvement, and a two-stage\nLarge-scale Reinforcement Learning process targeting STEM and general\ninstruction-following. Evaluations show strong performance: overall top 7 rank\non LMSYS Chatbot Arena with a score of 1356, outperforming leading models like\nGemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves\nan average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances\nhigh performance and efficiency, offering substantial capabilities at lower\ninference costs than many reasoning models, establishing a new paradigm for\nefficient large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Tencent Hunyuan Team"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Botong Zhou"
                    },
                    {
                        "name": "Can Xu"
                    },
                    {
                        "name": "Chayse Zhou"
                    },
                    {
                        "name": "ChenChen Zhang"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Chenhao Wang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Guanwei Zhang"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Haipeng Luo"
                    },
                    {
                        "name": "Han Hu"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Lan Jiang"
                    },
                    {
                        "name": "Lixin Liu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Peiqi Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Qianbiao Xiang"
                    },
                    {
                        "name": "Qibin Liu"
                    },
                    {
                        "name": "Qingfeng Sun"
                    },
                    {
                        "name": "Richard Guo"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Tian Zhang"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Weidong Han"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Weijin Zhou"
                    },
                    {
                        "name": "Weikang Wang"
                    },
                    {
                        "name": "Wesleye Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yang Du"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Yulong Wang"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "ZhenXiang Yan"
                    },
                    {
                        "name": "Zheng Fang"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Zhuoyu Li"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Alex Yan"
                    },
                    {
                        "name": "Ande Liang"
                    },
                    {
                        "name": "Baitong Liu"
                    },
                    {
                        "name": "Beiping Pan"
                    },
                    {
                        "name": "Bin Xing"
                    },
                    {
                        "name": "Binghong Wu"
                    },
                    {
                        "name": "Bingxin Qu"
                    },
                    {
                        "name": "Bolin Ni"
                    },
                    {
                        "name": "Boyu Wu"
                    },
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chengjun Liu"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Chiyu Wang"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Daisy Yi"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Fanyang Lu"
                    },
                    {
                        "name": "Fei Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Guanghua Yu"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Guohua Wang"
                    },
                    {
                        "name": "Haisheng Lin"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Hao Lu"
                    },
                    {
                        "name": "Haoqing Jiang"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Haotian Zhu"
                    },
                    {
                        "name": "Huangjin Dai"
                    },
                    {
                        "name": "Huankui Chen"
                    },
                    {
                        "name": "Huawen Feng"
                    },
                    {
                        "name": "Huihui Cai"
                    },
                    {
                        "name": "Huxin Peng"
                    },
                    {
                        "name": "Jackson Lv"
                    },
                    {
                        "name": "Jiacheng Shi"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Jiangtao Guan"
                    },
                    {
                        "name": "Jianing Xu"
                    },
                    {
                        "name": "Jianwei Cai"
                    },
                    {
                        "name": "Jiarong Zhang"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jieneng Yang"
                    },
                    {
                        "name": "Jihong Zhang"
                    },
                    {
                        "name": "Jin lv"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Jinxing Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Juntao Guo"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Li Liu"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Liya Zhan"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Long Xu"
                    },
                    {
                        "name": "Mao Zheng"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Nanli Chen"
                    },
                    {
                        "name": "Peirui Chen"
                    },
                    {
                        "name": "Peng He"
                    },
                    {
                        "name": "Pengju Pan"
                    },
                    {
                        "name": "Pengzhi Wei"
                    },
                    {
                        "name": "Qi Yang"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Ruixu Zhou"
                    },
                    {
                        "name": "Shaofeng Zhang"
                    },
                    {
                        "name": "Sheng Zhang"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shuaishuai Chang"
                    },
                    {
                        "name": "Shulin Liu"
                    },
                    {
                        "name": "SiQi Wang"
                    },
                    {
                        "name": "Songjia Feng"
                    },
                    {
                        "name": "Songling Yuan"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Tianjiao Lang"
                    },
                    {
                        "name": "Tongkai Li"
                    },
                    {
                        "name": "Wei Deng"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Weigang Zhang"
                    },
                    {
                        "name": "Weixuan Sun"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Wenzhi Sun"
                    },
                    {
                        "name": "Wenzhuo Jia"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Xiangyu He"
                    },
                    {
                        "name": "Xianshun Ren"
                    },
                    {
                        "name": "XiaoYing Zhu"
                    },
                    {
                        "name": "Xiaolong Guo"
                    },
                    {
                        "name": "Xiaoxue Li"
                    },
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Xican Lu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Xinyu Guan"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xudong Gao"
                    },
                    {
                        "name": "Xun Luo"
                    },
                    {
                        "name": "Xuxiang Qi"
                    },
                    {
                        "name": "Yangkun Chen"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Yanling Xiao"
                    },
                    {
                        "name": "Yantao Mai"
                    },
                    {
                        "name": "Yanze Chen"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Yeting Yang"
                    },
                    {
                        "name": "YiFan Song"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Yijiao Zhu"
                    },
                    {
                        "name": "Yinhe Wu"
                    },
                    {
                        "name": "Yixian Liu"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuanjun Cai"
                    },
                    {
                        "name": "Yuanlin Tu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Yuhui Hu"
                    },
                    {
                        "name": "Yujin Lin"
                    },
                    {
                        "name": "Yun Yang"
                    },
                    {
                        "name": "Yunhao Wang"
                    },
                    {
                        "name": "Yusong Zhang"
                    },
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Zhan Yu"
                    },
                    {
                        "name": "Zhaoliang Yang"
                    },
                    {
                        "name": "Zhe Zhao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Zhenyu Huang"
                    },
                    {
                        "name": "Zhiguang Liu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    },
                    {
                        "name": "Zhiqing Kui"
                    },
                    {
                        "name": "Zhiyin Zeng"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Zhuo Han"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Zigang Geng"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Ziyan Tang"
                    },
                    {
                        "name": "Ziyuan Zhu"
                    },
                    {
                        "name": "Zonglei Zhu"
                    },
                    {
                        "name": "Zhijiang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijiang Xu"
                },
                "author": "Zhijiang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03231v1",
                "updated": "2025-07-04T00:16:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "published": "2025-07-04T00:16:15Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    0,
                    16,
                    15,
                    4,
                    185,
                    0
                ],
                "title": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust and Efficient Embedded Convex Optimization through First-Order\n  Adaptive Caching"
                },
                "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community."
                },
                "authors": [
                    {
                        "name": "Ishaan Mahajan"
                    },
                    {
                        "name": "Brian Plancher"
                    }
                ],
                "author_detail": {
                    "name": "Brian Plancher"
                },
                "author": "Brian Plancher",
                "arxiv_comment": "Accepted to IROS 2025, 7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03153v1",
                "updated": "2025-07-03T20:20:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T20:20:33Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    20,
                    33,
                    3,
                    184,
                    0
                ],
                "title": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference"
                },
                "summary": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling inference for large language models (LLMs) is increasingly\nconstrained by limited GPU memory, especially due to growing key-value (KV)\ncaches required for long-context generation. While existing approaches offload\nKV caches to CPU memory or apply sparse attention to reduce GPU load, they\noften underutilize CPU compute resources and compromise accuracy. We present\nHGCA, a hybrid CPU-GPU attention mechanism that enables scalable,\nhigh-throughput LLM inference with near-full attention quality. HGCA performs\ndense attention on recently generated KV entries retained in GPU memory and\nparallel sparse attention on selected, salient KV entries in CPU memory. The\nattention outputs are efficiently merged using log-sum-exp fusion, minimizing\nPCIe transfer overhead. HGCA also introduces a finegrained, per-head\nsparsification strategy optimized for CPU execution, preserving contextual\nrelevance while reducing computation. Our implementation seamlessly integrates\ninto existing LLM frameworks without requiring model retraining. Experiments\nacross diverse models and workloads show that HGCA achieves superior\nscalability, supports longer sequences and larger batch sizes, and outperforms\nexisting sparse attention baselines in both performance and accuracy -- all on\ncommodity GPU hardware."
                },
                "authors": [
                    {
                        "name": "Weishu Deng"
                    },
                    {
                        "name": "Yujie Yang"
                    },
                    {
                        "name": "Peiran Du"
                    },
                    {
                        "name": "Lingfeng Xiang"
                    },
                    {
                        "name": "Zhen Lin"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Song Jiang"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Jia Rao"
                    }
                ],
                "author_detail": {
                    "name": "Jia Rao"
                },
                "author": "Jia Rao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02860v1",
                "updated": "2025-07-03T17:59:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T17:59:54Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    59,
                    54,
                    3,
                    184,
                    0
                ],
                "title": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less is Enough: Training-Free Video Diffusion Acceleration via\n  Runtime-Adaptive Caching"
                },
                "summary": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video generation models have demonstrated remarkable performance, yet their\nbroader adoption remains constrained by slow inference speeds and substantial\ncomputational costs, primarily due to the iterative nature of the denoising\nprocess. Addressing this bottleneck is essential for democratizing advanced\nvideo synthesis technologies and enabling their integration into real-world\napplications. This work proposes EasyCache, a training-free acceleration\nframework for video diffusion models. EasyCache introduces a lightweight,\nruntime-adaptive caching mechanism that dynamically reuses previously computed\ntransformation vectors, avoiding redundant computations during inference.\nUnlike prior approaches, EasyCache requires no offline profiling,\npre-computation, or extensive parameter tuning. We conduct comprehensive\nstudies on various large-scale video generation models, including OpenSora,\nWan2.1, and HunyuanVideo. Our method achieves leading acceleration performance,\nreducing inference time by up to 2.1-3.3$\\times$ compared to the original\nbaselines while maintaining high visual fidelity with a significant up to 36%\nPSNR improvement compared to the previous SOTA method. This improvement makes\nour EasyCache a efficient and highly accessible solution for high-quality video\ngeneration in both research and practical applications. The code is available\nat https://github.com/H-EmbodVis/EasyCache."
                },
                "authors": [
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Dingkang Liang"
                    },
                    {
                        "name": "Kaijin Chen"
                    },
                    {
                        "name": "Tianrui Feng"
                    },
                    {
                        "name": "Xiwu Chen"
                    },
                    {
                        "name": "Hongkai Lin"
                    },
                    {
                        "name": "Yikang Ding"
                    },
                    {
                        "name": "Feiyang Tan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Xiang Bai"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Bai"
                },
                "author": "Xiang Bai",
                "arxiv_comment": "The code is made available at\n  https://github.com/H-EmbodVis/EasyCache. Project page:\n  https://h-embodvis.github.io/EasyCache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04789v2",
                "updated": "2025-07-03T17:11:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    17,
                    11,
                    28,
                    3,
                    184,
                    0
                ],
                "published": "2023-12-08T02:03:55Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    2,
                    3,
                    55,
                    4,
                    342,
                    0
                ],
                "title": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System"
                },
                "summary": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern workloads are demanding increasingly larger memory capacity. Compute\nExpress Link (CXL)-based memory tiering has emerged as a promising solution for\naddressing this problem by utilizing traditional DRAM alongside slow-tier CXL\nmemory devices. We analyze prior tiering systems and observe two challenges for\nhigh-performance memory tiering: adapting to skewed but dynamically varying\ndata hotness distributions while minimizing memory and cache overhead due to\ntiering.\n  To address these challenges, we propose HybridTier, an adaptive and\nlightweight tiering system for CXL memory. HybridTier tracks both long-term\ndata access frequency and short-term access momentum \\emph{simultaneously} to\naccurately capture and adapt to shifting hotness distributions. HybridTier\nreduces the metadata memory overhead by tracking data accesses\n\\emph{probabilistically}, obtaining higher memory efficiency by trading off a\nsmall amount of tracking inaccuracy that has a negligible impact on application\nperformance. To reduce cache overhead, HybridTier uses lightweight data\nstructures that optimize for data locality to track data hotness. Our\nevaluations show that HybridTier outperforms prior systems by up to $91\\%$\n($19\\%$ geomean), incurring $2.0-7.8\\times$ less memory overhead and\n$1.7-3.5\\times$ less cache misses."
                },
                "authors": [
                    {
                        "name": "Kevin Song"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Jishen Zhao"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Gennady Pekhimenko"
                    }
                ],
                "author_detail": {
                    "name": "Gennady Pekhimenko"
                },
                "author": "Gennady Pekhimenko",
                "arxiv_doi": "10.1145/3676642.3736119",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736119",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.04789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Appears in the Proceedings of the 30th ACM International Conference\n  on Architectural Support for Programming Languages and Operating Systems,\n  Volume 3 (ASPLOS 25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v3",
                "updated": "2025-07-03T16:06:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    16,
                    6,
                    35,
                    3,
                    184,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v1",
                "updated": "2025-07-03T14:20:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v3",
                "updated": "2025-07-03T08:22:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    22,
                    27,
                    3,
                    184,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02397v1",
                "updated": "2025-07-03T07:49:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T07:49:18Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    7,
                    49,
                    18,
                    3,
                    184,
                    0
                ],
                "title": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Reconstruction of Terahertz-driven Subcycle Electron Emission\n  Dynamics"
                },
                "summary": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While field-driven electron emission is theoretically understood down to the\nsubcycle regime, its direct experimental temporal characterization using\nlong-wavelength terahertz (THz) fields remains elusive. Here, by driving a\ngraphite tip with phase-stable quasi-single-cycle THz pulses, we reveal\ndistinct subcycle electron emission dynamics including: (1) At a\ncarrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz\nfield strength, characteristic of subcycle emission; (2) At the opposite CEP,\ndominant deceleration fields generate stationary low-energy peaks. Crucially,\nwe develop a pump-probe-free, direct reconstruction method extracting electron\npulse profiles solely from measured energy spectra, obtaining durations from\n97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved\nsimulations further reveal a 71.2% modulation in the cutoff energy and a\nnear-total (99.7%) suppression of the emission current. This work not only\nvalidates the Fowler-Nordheim model under THz excitation but also establishes a\ngeneral framework for the direct temporal characterization of subcycle electron\nemission, opening pathways for precise electron control in ultrafast electron\nsources and lightwave nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Jiakang Mao"
                    },
                    {
                        "name": "Yushan Zeng"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Liwei Song"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ruxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruxin Li"
                },
                "author": "Ruxin Li",
                "arxiv_comment": "16 pages, 5 figures, references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22618v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22618v3",
                "updated": "2025-07-03T04:51:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    4,
                    51,
                    5,
                    3,
                    184,
                    0
                ],
                "published": "2025-05-28T17:39:15Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    39,
                    15,
                    2,
                    148,
                    0
                ],
                "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding"
                },
                "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."
                },
                "authors": [
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Shuchen Xue"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22618v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22618v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02227v1",
                "updated": "2025-07-03T01:22:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "published": "2025-07-03T01:22:57Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    1,
                    22,
                    57,
                    3,
                    184,
                    0
                ],
                "title": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhysicsCorrect: A Training-Free Approach for Stable Neural PDE\n  Simulations"
                },
                "summary": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks have emerged as powerful surrogates for solving partial\ndifferential equations (PDEs), offering significant computational speedups over\ntraditional methods. However, these models suffer from a critical limitation:\nerror accumulation during long-term rollouts, where small inaccuracies compound\nexponentially, eventually causing complete divergence from physically valid\nsolutions. We present PhysicsCorrect, a training-free correction framework that\nenforces PDE consistency at each prediction step by formulating correction as a\nlinearized inverse problem based on PDE residuals. Our key innovation is an\nefficient caching strategy that precomputes the Jacobian and its pseudoinverse\nduring an offline warm-up phase, reducing computational overhead by two orders\nof magnitude compared to standard correction approaches. Across three\nrepresentative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and\nthe chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction\nerrors by up to 100x while adding negligible inference time (under 5\\%). The\nframework integrates seamlessly with diverse architectures including Fourier\nNeural Operators, UNets, and Vision Transformers, effectively transforming\nunstable neural surrogates into reliable simulation tools that bridge the gap\nbetween deep learning's computational efficiency and the physical fidelity\ndemanded by practical scientific applications."
                },
                "authors": [
                    {
                        "name": "Xinquan Huang"
                    },
                    {
                        "name": "Paris Perdikaris"
                    }
                ],
                "author_detail": {
                    "name": "Paris Perdikaris"
                },
                "author": "Paris Perdikaris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01652v1",
                "updated": "2025-07-02T12:27:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T12:27:06Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    12,
                    27,
                    6,
                    2,
                    183,
                    0
                ],
                "title": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Linear Complexity: A Spatial-Aware\n  Decay Perspective"
                },
                "summary": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have garnered significant attention in image\ngeneration for their ability to effectively capture both local and global\nstructures within visual data. However, prevalent AR models predominantly rely\non the transformer architectures, which are beset by quadratic computational\ncomplexity concerning input sequence length and substantial memory overhead due\nto the necessity of maintaining key-value caches. Although linear attention\nmechanisms have successfully reduced this burden in language models, our\ninitial experiments reveal that they significantly degrade image generation\nquality because of their inability to capture critical long-range dependencies\nin visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a\nnovel attention mechanism that explicitly preserves genuine 2D spatial\nrelationships within the flattened image sequences by computing\nposition-dependent decay factors based on true 2D spatial location rather than\n1D sequence positions. Based on this mechanism, we present LASADGen, an\nautoregressive image generator that enables selective attention to relevant\nspatial contexts with linear complexity. Experiments on ImageNet show LASADGen\nachieves state-of-the-art image generation performance and computational\nefficiency, bridging the gap between linear attention's efficiency and spatial\nunderstanding needed for high-quality generation."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Jinxing Zhou"
                    },
                    {
                        "name": "Hui Deng"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Bin Fan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10318v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10318v4",
                "updated": "2025-07-02T10:16:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    10,
                    16,
                    58,
                    2,
                    183,
                    0
                ],
                "published": "2022-12-20T15:09:30Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    15,
                    9,
                    30,
                    1,
                    354,
                    0
                ],
                "title": "Learned-Database Systems Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned-Database Systems Security"
                },
                "summary": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A learned database system uses machine learning (ML) internally to improve\nperformance. We can expect such systems to be vulnerable to some adversarial-ML\nattacks. Often, the learned component is shared between mutually-distrusting\nusers or processes, much like microarchitectural resources such as caches,\npotentially giving rise to highly-realistic attacker models. However, compared\nto attacks on other ML-based systems, attackers face a level of indirection as\nthey cannot interact directly with the learned model. Additionally, the\ndifference between the attack surface of learned and non-learned versions of\nthe same system is often subtle. These factors obfuscate the de-facto risks\nthat the incorporation of ML carries. We analyze the root causes of\npotentially-increased attack surface in learned database systems and develop a\nframework for identifying vulnerabilities that stem from the use of ML. We\napply our framework to a broad set of learned components currently being\nexplored in the database community. To empirically validate the vulnerabilities\nsurfaced by our framework, we choose 3 of them and implement and evaluate\nexploits against these. We show that the use of ML cause leakage of past\nqueries in a database, enable a poisoning attack that causes exponential memory\nblowup in an index structure and crashes it in seconds, and enable index users\nto snoop on each others' key distributions by timing queries over their own\nkeys. We find that adversarial ML is an universal threat against learned\ncomponents in database systems, point to open research gaps in our\nunderstanding of learned-systems security, and conclude by discussing\nmitigations, while noting that data leakage is inherent in systems whose\nlearned component is shared between multiple parties."
                },
                "authors": [
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Jin Peng Zhou"
                    },
                    {
                        "name": "Thorsten Eisenhofer"
                    },
                    {
                        "name": "Paul Grubbs"
                    },
                    {
                        "name": "Nicolas Papernot"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Papernot"
                },
                "author": "Nicolas Papernot",
                "arxiv_comment": "Accepted at TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10318v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10318v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01465v1",
                "updated": "2025-07-02T08:24:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T08:24:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    24,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "A new efficient RPKI Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new efficient RPKI Design"
                },
                "summary": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Public Key Infrastructure (RPKI) is a critical security mechanism\nfor BGP, but the complexity of its architecture is a growing concern as its\nadoption scales. Current RPKI design heavily reuses legacy PKI components, such\nas X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols,\nall these introduce excessive cryptographic validation, redundant metadata, and\ninefficiencies in both storage and processing. We show that these design\nchoices, although based on established standards, create significant\nperformance bottlenecks, increase the vulnerability surface, and hinder\nscalability for wide-scale Internet deployment.\n  In this paper, we perform the first systematic analysis of the root causes of\ncomplexity in RPKI's design and experimentally quantify their real-world\nimpact. We show that over 70% of validation time in RPKI relying parties is\nspent on certificate parsing and signature verification, much of it\nunnecessary. Building on this insight, we introduce the improved RPKI (iRPKI),\na backwards-compatible redesign that preserves all security guarantees while\nsubstantially reducing protocol overhead. iRPKI eliminates EE-certificates and\nROA signatures, merges revocation and integrity objects, replaces verbose\nencodings with Protobuf, and restructures repository metadata for more\nefficient access. We experimentally demonstrate that our implementation of\niRPKI in the Routinator validator achieves a 20x speed-up of processing time,\n18x improvement of bandwidth requirements and 8x reduction in cache memory\nfootprint, while also eliminating classes of vulnerabilities that have led to\nat least 10 vulnerabilities in RPKI software. iRPKI significantly increases the\nfeasibility of deploying RPKI at scale in the Internet, and especially in\nconstrained environments. Our design may be deployed incrementally without\nimpacting existing operations."
                },
                "authors": [
                    {
                        "name": "Haya Schulmann"
                    },
                    {
                        "name": "Niklas Vogel"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Vogel"
                },
                "author": "Niklas Vogel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01438v1",
                "updated": "2025-07-02T07:47:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T07:47:28Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    7,
                    47,
                    28,
                    2,
                    183,
                    0
                ],
                "title": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices"
                },
                "summary": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant attention due to their\nversatility across a wide array of applications. Fine-tuning LLMs with\nparameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these\nmodels to efficiently adapt to downstream tasks without extensive retraining.\nDeploying fine-tuned LLMs on multi-tenant edge devices offers substantial\nbenefits, such as reduced latency, enhanced privacy, and personalized\nresponses. However, serving LLMs efficiently on resource-constrained edge\ndevices presents critical challenges, including the complexity of adapter\nselection for different tasks and memory overhead from frequent adapter\nswapping. Moreover, given the multiple requests in multi-tenant settings,\nprocessing requests sequentially results in underutilization of computational\nresources and increased latency. This paper introduces EdgeLoRA, an efficient\nsystem for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA\nincorporates three key innovations: (1) an adaptive adapter selection mechanism\nto streamline the adapter configuration process; (2) heterogeneous memory\nmanagement, leveraging intelligent adapter caching and pooling to mitigate\nmemory operation overhead; and (3) batch LoRA inference, enabling efficient\nbatch processing to significantly reduce computational latency. Comprehensive\nevaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly\noutperforms the status quo (i.e., llama.cpp) in terms of both latency and\nthroughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times\nboost in throughput. Even more impressively, it can serve several orders of\nmagnitude more adapters simultaneously. These results highlight EdgeLoRA's\npotential to transform edge deployment of LLMs in multi-tenant scenarios,\noffering a scalable and efficient solution for resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Zheyu Shen"
                    },
                    {
                        "name": "Yexiao He"
                    },
                    {
                        "name": "Ziyao Wang"
                    },
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Guoheng Sun"
                    },
                    {
                        "name": "Wanghao Ye"
                    },
                    {
                        "name": "Ang Li"
                    }
                ],
                "author_detail": {
                    "name": "Ang Li"
                },
                "author": "Ang Li",
                "arxiv_doi": "10.1145/3711875.3729141",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711875.3729141",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.01438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02006v1",
                "updated": "2025-07-02T00:35:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "published": "2025-07-02T00:35:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    0,
                    35,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design"
                },
                "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Shakya Jayakody"
                    },
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01216v1",
                "updated": "2025-07-01T22:27:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T22:27:21Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    22,
                    27,
                    21,
                    1,
                    182,
                    0
                ],
                "title": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile\n  Device via Additive Side-Tuning"
                },
                "summary": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a huge gap between numerous intriguing applications fostered by\non-device large language model (LLM) fine-tuning (FT) from fresh mobile data\nand the limited resources of a mobile device. While existing server-assisted\nmethods (e.g., split learning or side-tuning) may enable LLM FT on the local\nmobile device, they suffer from heavy communication burdens of activation\ntransmissions, and may disclose data, labels or fine-tuned models to the\nserver. To address those issues, we develop PAE MobiLLM, a privacy-aware and\nefficient LLM FT method which can be deployed on the mobile device via\nserver-assisted additive side-tuning. To further accelerate FT convergence and\nimprove computing efficiency, PAE MobiLLM integrates activation caching on the\nserver side, which allows the server to reuse historical activations and saves\nthe mobile device from repeatedly computing forward passes for the recurring\ndata samples. Besides, to reduce communication cost, PAE MobiLLM develops a\none-token (i.e., ``pivot'' token) activation shortcut that transmits only a\nsingle activation dimension instead of full activation matrices to guide the\nside network tuning. Last but not least, PAE MobiLLM introduces the additive\nadapter side-network design which makes the server train the adapter modules\nbased on device-defined prediction differences rather than raw ground-truth\nlabels. In this way, the server can only assist device-defined side-network\ncomputing, and learn nothing about data, labels or fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Xingke Yang"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Zhiyi Wan"
                    },
                    {
                        "name": "Sicong Li"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Xiaoqi Qi"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Tomoaki Ohtsuki"
                    },
                    {
                        "name": "Xin Fu"
                    },
                    {
                        "name": "Miao Pan"
                    }
                ],
                "author_detail": {
                    "name": "Miao Pan"
                },
                "author": "Miao Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01154v1",
                "updated": "2025-07-01T19:28:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T19:28:37Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    19,
                    28,
                    37,
                    1,
                    182,
                    0
                ],
                "title": "FlashDP: Private Training Large Language Models with Efficient DP-SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDP: Private Training Large Language Models with Efficient DP-SGD"
                },
                "summary": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) increasingly underpin technological\nadvancements, the privacy of their training data emerges as a critical concern.\nDifferential Privacy (DP) serves as a rigorous mechanism to protect this data,\nyet its integration via Differentially Private Stochastic Gradient Descent\n(DP-SGD) introduces substantial challenges, primarily due to the complexities\nof per-sample gradient clipping. Current explicit methods, such as Opacus,\nnecessitate extensive storage for per-sample gradients, significantly inflating\nmemory requirements. Conversely, implicit methods like GhostClip reduce storage\nneeds by recalculating gradients multiple times, which leads to inefficiencies\ndue to redundant computations. This paper introduces FlashDP, an innovative\ncache-friendly per-layer DP-SGD that consolidates necessary operations into a\nsingle task, calculating gradients only once in a fused manner. This approach\nnot only diminishes memory movement by up to \\textbf{50\\%} but also cuts down\nredundant computations by \\textbf{20\\%}, compared to previous methods.\nConsequently, FlashDP does not increase memory demands and achieves a\n\\textbf{90\\%} throughput compared to the Non-DP method on a four-A100 system\nduring the pre-training of the Llama-13B model, while maintaining parity with\nstandard per-layer clipped DP-SGD in terms of accuracy. These advancements\nestablish FlashDP as a pivotal development for efficient and privacy-preserving\ntraining of LLMs. FlashDP's code has been open-sourced in\nhttps://github.com/kaustpradalab/flashdp."
                },
                "authors": [
                    {
                        "name": "Liangyu Wang"
                    },
                    {
                        "name": "Junxiao Wang"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Zihang Xiang"
                    },
                    {
                        "name": "David E. Keyes"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v1",
                "updated": "2025-07-01T16:36:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00797v1",
                "updated": "2025-07-01T14:30:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T14:30:31Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    30,
                    31,
                    1,
                    182,
                    0
                ],
                "title": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction\n  and Dataflow-flexible Accelerator"
                },
                "summary": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language processing tasks but\npose significant computational and memory challenges for edge deployment due to\ntheir intensive resource demands. This work addresses the efficiency of LLM\ninference by algorithm-hardware-dataflow tri-optimizations. We propose a novel\nvoting-based KV cache eviction algorithm, balancing hardware efficiency and\nalgorithm accuracy by adaptively identifying unimportant kv vectors. From a\ndataflow perspective, we introduce a flexible-product dataflow and a runtime\nreconfigurable PE array for matrix-vector multiplication. The proposed approach\neffectively handles the diverse dimensional requirements and solves the\nchallenges of incrementally varying sequence lengths. Additionally, an\nelement-serial scheduling scheme is proposed for nonlinear operations, such as\nsoftmax and layer normalization (layernorm). Results demonstrate a substantial\nreduction in latency, accompanied by a significant decrease in hardware\ncomplexity, from O(N) to O(1). The proposed solution is realized in a\ncustom-designed accelerator, VEDA, which outperforms existing hardware\nplatforms. This research represents a significant advancement in LLM inference\non resource-constrained edge devices, facilitating real-time processing,\nenhancing data privacy, and enabling model customization."
                },
                "authors": [
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongxiang Fan"
                    },
                    {
                        "name": "Haroon Waris"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Jianfei Jiang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Guanghui He"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui He"
                },
                "author": "Guanghui He",
                "arxiv_comment": "DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00727v1",
                "updated": "2025-07-01T13:17:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T13:17:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    17,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "On Hierarchical Coded Caching with Offline Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Hierarchical Coded Caching with Offline Users"
                },
                "summary": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a two-layer hierarchical network in which some users are\noffline during the content delivery phase. A two-layer hierarchical network\nconsists of a single server connected to multiple cache-aided mirror sites, and\neach mirror site is connected to a distinct set of cache-aided users. A scheme\nfor such a hierarchical system with offline users has been proposed recently\nbut considered a special case where all mirror caches have zero memory, which\nis a significant limitation. We propose an array known as a hierarchical\nhotplug placement delivery array (HHPDA), which describes the placement and\ndelivery phases of a coded caching scheme for a general two-layer hierarchical\nnetwork with offline users. Further, we construct a class of HHPDAs using\ncombinatorial t-designs."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A short version of this is accepted for presentation in 2025 IEEE\n  Information Theory Workshop; 8 pages, one figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00716v1",
                "updated": "2025-07-01T12:51:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:51:09Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    51,
                    9,
                    1,
                    182,
                    0
                ],
                "title": "Accelerating Loading WebGraphs in ParaGrapher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Loading WebGraphs in ParaGrapher"
                },
                "summary": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaGrapher is a graph loading API and library that enables graph processing\nframeworks to load large-scale compressed graphs with minimal overhead. This\ncapability accelerates the design and implementation of new high-performance\ngraph algorithms and their evaluation on a wide range of graphs and across\ndifferent frameworks. However, our previous study identified two major\nlimitations in ParaGrapher: inefficient utilization of high-bandwidth storage\nand reduced decompression bandwidth due to increased compression ratios. To\naddress these limitations, we present two optimizations for ParaGrapher in this\npaper. To improve storage utilization, particularly for high-bandwidth storage,\nwe introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE\n(Filesystem in User Space). PG-Fuse optimizes storage access by increasing the\nsize of requested blocks, reducing the number of calls to the underlying\nfilesystem, and caching the received blocks in memory for future calls. To\nimprove the decompression bandwidth, we introduce CompBin, a compact binary\nrepresentation of the CSR format. CompBin facilitates direct accesses to\nneighbors while preventing storage usage for unused bytes. Our evaluation on 12\nreal-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse\nand CompBin achieve up to 7.6 and 21.8 times speedup, respectively."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Koohi Esfahani"
                },
                "author": "Mohsen Koohi Esfahani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00715v1",
                "updated": "2025-07-01T12:42:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T12:42:06Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    42,
                    6,
                    1,
                    182,
                    0
                ],
                "title": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EARN: Efficient Inference Acceleration for LLM-based Generative\n  Recommendation by Register Tokens"
                },
                "summary": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-based generative recommendation (LLMRec) has achieved\nnotable success, but it suffers from high inference latency due to massive\ncomputational overhead and memory pressure of KV Cache. Existing KV Cache\nreduction methods face critical limitations: cache compression offers marginal\nacceleration given recommendation tasks' short decoding steps, while prompt\ncompression risks discarding vital interaction history. Through systematic\nanalysis of attention patterns in LLMRec, we uncover two pivotal insights: 1)\nlayer-wise attention sparsity inversion where early layers retain dense\ninformative patterns while later layers exhibit high redundancy, and 2) dual\nattention sinks phenomenon where attention scores concentrate on both head and\ntail tokens of input sequences. Motivated by these insights, we propose EARN,\nan efficient inference framework that leverages the early layers to compress\ninformation into register tokens placed at the input sequence boundaries, then\nfocuses solely on these tokens in the subsequent layers. Extensive experiments\non three datasets, two LLMRec methods and two LLM architectures demonstrate\nEARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction\nwith better accuracy than the general finetuning approach. Our work bridges the\nefficiency-effectiveness gap in LLMRec, offering practical deployment\nadvantages for industrial scenarios."
                },
                "authors": [
                    {
                        "name": "Chaoqun Yang"
                    },
                    {
                        "name": "Xinyu Lin"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yongqi Li"
                    },
                    {
                        "name": "Teng Sun"
                    },
                    {
                        "name": "Xianjing Han"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted by KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00614v1",
                "updated": "2025-07-01T09:47:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T09:47:38Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    47,
                    38,
                    1,
                    182,
                    0
                ],
                "title": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural, dielectric, and ferroelectric characteristics of the\n  low-temperature sintered 65PMN-35PT sample for electroceramic applications"
                },
                "summary": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A single-phase 65PMN-35PT ceramic was synthesized at a relatively low\ntemperature (875 oC) using a modified columbite method. X-ray diffraction\nanalysis confirmed the single-phase formation of perovskite 65PMN-35PT with a\ntetragonal structure. Morphological studies indicated that the sample consisted\nof small grains with a size of about 2 micro-m. The dielectric properties of\nthe material demonstrate its relaxor behavior near the ferroelectric transition\ntemperature, TC = 457 K. The saturation and remnant polarization values of\napproximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically\npoled sample. Additionally, the poling induced a negative internal electric\nfield of about -0.2 kV cm-1 was detected due to the presence of ferroelectric\nnano-grains in this bulk 65PMN-35PT sample. These observed characteristics of\nthe pyrochlore-free 65PMN-35PT ceramic are similar to those of its\nsingle-crystal counterpart."
                },
                "authors": [
                    {
                        "name": "B. Ramachandran"
                    },
                    {
                        "name": "N. Sudarshan"
                    },
                    {
                        "name": "G. Mangamma"
                    },
                    {
                        "name": "M. S. Ramachandra Rao"
                    }
                ],
                "author_detail": {
                    "name": "M. S. Ramachandra Rao"
                },
                "author": "M. S. Ramachandra Rao",
                "arxiv_doi": "10.1007/s10832-025-00423-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10832-025-00423-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.00614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 7 figures, 1 Table and Accepted for publication in Journal\n  of Electroceramics",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00462v1",
                "updated": "2025-07-01T06:22:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "published": "2025-07-01T06:22:00Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    22,
                    0,
                    1,
                    182,
                    0
                ],
                "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of All Test Samples: Mean-Shift Guided\n  Test-Time Adaptation"
                },
                "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training."
                },
                "authors": [
                    {
                        "name": "Jizhou Han"
                    },
                    {
                        "name": "Chenhao Ding"
                    },
                    {
                        "name": "SongLin Dong"
                    },
                    {
                        "name": "Yuhang He"
                    },
                    {
                        "name": "Xinyuan Gao"
                    },
                    {
                        "name": "Yihong Gong"
                    }
                ],
                "author_detail": {
                    "name": "Yihong Gong"
                },
                "author": "Yihong Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01988v1",
                "updated": "2025-06-28T13:02:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T13:02:17Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    13,
                    2,
                    17,
                    5,
                    179,
                    0
                ],
                "title": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers"
                },
                "summary": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models outpace the capabilities of single processors, interconnects\nacross chips have become a critical enabler for scalable computing. These\nprocessors exchange massive amounts of data at cache-line granularity,\nprompting the adoption of new interconnect protocols like CXL, NVLink, and\nUALink, designed for high bandwidth and small payloads. However, the increasing\ntransfer rates of these protocols heighten susceptibility to errors. While\nmechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction\n(FEC) are standard for reliable data transmission, scaling chip interconnects\nto multi-node configurations introduces new challenges, particularly in\nmanaging silently dropped flits in switching devices. This paper introduces\nImplicit Sequence Number (ISN), a novel mechanism that ensures precise flit\ndrop detection and in-order delivery without adding header overhead.\nAdditionally, we propose Reliability Extended Link (RXL), an extension of CXL\nthat incorporates ISN to support scalable, reliable multi-node interconnects\nwhile maintaining compatibility with the existing flit structure. By elevating\nCRC to a transport-layer mechanism for end-to-end data and sequence integrity,\nand relying on FEC for link-layer error correction and detection, RXL delivers\nrobust reliability and scalability without compromising bandwidth efficiency."
                },
                "authors": [
                    {
                        "name": "Giyong Jung"
                    },
                    {
                        "name": "Saeid Gorgin"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungrae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jungrae Kim"
                },
                "author": "Jungrae Kim",
                "arxiv_comment": "12 pages, 8 figures. This paper is accepted for [2025 The\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis (SC)]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Sren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Sren Stobbe"
                },
                "author": "Sren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.07106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07106v1",
                "updated": "2025-07-09T17:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    47,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:59:47Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    47,
                    2,
                    190,
                    0
                ],
                "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor"
                },
                "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/."
                },
                "authors": [
                    {
                        "name": "Vatsal Agarwal"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    },
                    {
                        "name": "Gefen Kohavi"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Daniel Ulbricht"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "Website: see https://vatsalag99.github.io/mustafar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07104v1",
                "updated": "2025-07-09T17:59:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    4,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:59:04Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    4,
                    2,
                    190,
                    0
                ],
                "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models"
                },
                "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD."
                },
                "authors": [
                    {
                        "name": "Tiezheng Zhang"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Yu-cheng Chou"
                    },
                    {
                        "name": "Jieneng Chen"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Junfei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Junfei Xiao"
                },
                "author": "Junfei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17295v3",
                "updated": "2025-07-09T17:37:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    37,
                    23,
                    2,
                    190,
                    0
                ],
                "published": "2024-06-25T05:45:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    5,
                    45,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "Less can be more for predicting properties with large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less can be more for predicting properties with large language models"
                },
                "summary": "Predicting properties from coordinate-category data -- sets of vectors paired\nwith categorical information -- is fundamental to computational science. In\nmaterials science, this challenge manifests as predicting properties like\nformation energies or elastic moduli from crystal structures comprising atomic\npositions (vectors) and element types (categorical information). While large\nlanguage models (LLMs) have increasingly been applied to such tasks, with\nresearchers encoding structural data as text, optimal strategies for achieving\nreliable predictions remain elusive. Here, we report fundamental limitations in\nLLM's ability to learn from coordinate information in coordinate-category data.\nThrough systematic experiments using synthetic datasets with tunable coordinate\nand category contributions, combined with a comprehensive benchmarking\nframework (MatText) spanning multiple representations and model scales, we find\nthat LLMs consistently fail to capture coordinate information while excelling\nat category patterns. This geometric blindness persists regardless of model\nsize (up to 70B parameters), dataset scale (up to 2M structures), or text\nrepresentation strategy. Our findings suggest immediate practical implications:\nfor materials property prediction tasks dominated by structural effects,\nspecialized geometric architectures consistently outperform LLMs by significant\nmargins, as evidenced by a clear \"GNN-LM wall\" in performance benchmarks. Based\non our analysis, we provide concrete guidelines for architecture selection in\nscientific machine learning, while highlighting the critical importance of\nunderstanding model inductive biases when tackling scientific prediction\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting properties from coordinate-category data -- sets of vectors paired\nwith categorical information -- is fundamental to computational science. In\nmaterials science, this challenge manifests as predicting properties like\nformation energies or elastic moduli from crystal structures comprising atomic\npositions (vectors) and element types (categorical information). While large\nlanguage models (LLMs) have increasingly been applied to such tasks, with\nresearchers encoding structural data as text, optimal strategies for achieving\nreliable predictions remain elusive. Here, we report fundamental limitations in\nLLM's ability to learn from coordinate information in coordinate-category data.\nThrough systematic experiments using synthetic datasets with tunable coordinate\nand category contributions, combined with a comprehensive benchmarking\nframework (MatText) spanning multiple representations and model scales, we find\nthat LLMs consistently fail to capture coordinate information while excelling\nat category patterns. This geometric blindness persists regardless of model\nsize (up to 70B parameters), dataset scale (up to 2M structures), or text\nrepresentation strategy. Our findings suggest immediate practical implications:\nfor materials property prediction tasks dominated by structural effects,\nspecialized geometric architectures consistently outperform LLMs by significant\nmargins, as evidenced by a clear \"GNN-LM wall\" in performance benchmarks. Based\non our analysis, we provide concrete guidelines for architecture selection in\nscientific machine learning, while highlighting the critical importance of\nunderstanding model inductive biases when tackling scientific prediction\nproblems."
                },
                "authors": [
                    {
                        "name": "Nawaf Alampara"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Maik Jablonka"
                },
                "author": "Kevin Maik Jablonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07077v1",
                "updated": "2025-07-09T17:35:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    35,
                    58,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:35:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    35,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "Reading a Ruler in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reading a Ruler in the Wild"
                },
                "summary": "Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo."
                },
                "authors": [
                    {
                        "name": "Yimu Pan"
                    },
                    {
                        "name": "Manas Mehta"
                    },
                    {
                        "name": "Gwen Sincerbeaux"
                    },
                    {
                        "name": "Jeffery A. Goldstein"
                    },
                    {
                        "name": "Alison D. Gernand"
                    },
                    {
                        "name": "James Z. Wang"
                    }
                ],
                "author_detail": {
                    "name": "James Z. Wang"
                },
                "author": "James Z. Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04133v3",
                "updated": "2025-07-09T17:33:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    33,
                    49,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-04T16:26:11Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    11,
                    2,
                    155,
                    0
                ],
                "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems"
                },
                "summary": "Agentic AI systems, built upon large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligence, autonomy,\ncollaboration, and decision-making across enterprise and societal domains. This\nreview presents a structured analysis of \\textbf{Trust, Risk, and Security\nManagement (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems\n(AMAS). We begin by examining the conceptual foundations of Agentic AI and\nhighlight its architectural distinctions from traditional AI agents. We then\nadapt and extend the AI TRiSM framework for Agentic AI, structured around four\nkey pillars: Explainability, ModelOps, Security, Privacy and Governance, each\ncontextualized to the challenges of multi-agent LLM systems. A novel risk\ntaxonomy is proposed to capture the unique threats and vulnerabilities of\nAgentic AI, ranging from coordination failures to prompt-based adversarial\nmanipulation. To support practical assessment in Agentic AI works, we introduce\ntwo novel metrics: the Component Synergy Score (CSS), which quantifies the\nquality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE),\nwhich evaluates the efficiency of tool use within agent workflows. We further\ndiscuss strategies for improving explainability in Agentic AI , as well as\napproaches to enhancing security and privacy through encryption, adversarial\nrobustness, and regulatory compliance. The review concludes with a research\nroadmap for the responsible development and deployment of Agentic AI, outlining\ncritical directions to align emerging systems with TRiSM principles for safe,\ntransparent, and accountable operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems, built upon large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligence, autonomy,\ncollaboration, and decision-making across enterprise and societal domains. This\nreview presents a structured analysis of \\textbf{Trust, Risk, and Security\nManagement (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems\n(AMAS). We begin by examining the conceptual foundations of Agentic AI and\nhighlight its architectural distinctions from traditional AI agents. We then\nadapt and extend the AI TRiSM framework for Agentic AI, structured around four\nkey pillars: Explainability, ModelOps, Security, Privacy and Governance, each\ncontextualized to the challenges of multi-agent LLM systems. A novel risk\ntaxonomy is proposed to capture the unique threats and vulnerabilities of\nAgentic AI, ranging from coordination failures to prompt-based adversarial\nmanipulation. To support practical assessment in Agentic AI works, we introduce\ntwo novel metrics: the Component Synergy Score (CSS), which quantifies the\nquality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE),\nwhich evaluates the efficiency of tool use within agent workflows. We further\ndiscuss strategies for improving explainability in Agentic AI , as well as\napproaches to enhancing security and privacy through encryption, adversarial\nrobustness, and regulatory compliance. The review concludes with a research\nroadmap for the responsible development and deployment of Agentic AI, outlining\ncritical directions to align emerging systems with TRiSM principles for safe,\ntransparent, and accountable operation."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Emmanouilidis"
                },
                "author": "Christos Emmanouilidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12446v2",
                "updated": "2025-07-09T17:31:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    31,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-18T02:27:23Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    27,
                    23,
                    1,
                    49,
                    0
                ],
                "title": "Multi-Attribute Steering of Language Models via Targeted Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Attribute Steering of Language Models via Targeted Intervention"
                },
                "summary": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline)."
                },
                "authors": [
                    {
                        "name": "Duy Nguyen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "ACL 2025 camera-ready, code link:\n  https://github.com/duykhuongnguyen/MAT-Steer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07067v1",
                "updated": "2025-07-09T17:27:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    27,
                    51,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:27:51Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    27,
                    51,
                    2,
                    190,
                    0
                ],
                "title": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided\n  Telecommunication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided\n  Telecommunication Networks"
                },
                "summary": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference."
                },
                "authors": [
                    {
                        "name": "Clement Ruah"
                    },
                    {
                        "name": "Houssem Sifaou"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Bashir M. Al-Hashimi"
                    }
                ],
                "author_detail": {
                    "name": "Bashir M. Al-Hashimi"
                },
                "author": "Bashir M. Al-Hashimi",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07064v1",
                "updated": "2025-07-09T17:26:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    26,
                    10,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:26:10Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    26,
                    10,
                    2,
                    190,
                    0
                ],
                "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning"
                },
                "summary": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec"
                },
                "authors": [
                    {
                        "name": "Shanle Zheng"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08268v3",
                "updated": "2025-07-09T17:25:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    25,
                    55,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-11T10:35:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCFO: Long Context and Long Form Output Dataset and Benchmarking"
                },
                "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6)."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-juss"
                    },
                    {
                        "name": "Pierre Andrews"
                    },
                    {
                        "name": "Mariano Coria Meglioli"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "David Dale"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Alexandre Mourachko"
                    },
                    {
                        "name": "Eduardo Snchez"
                    },
                    {
                        "name": "Holger Schwenk"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Carleigh Wood"
                    }
                ],
                "author_detail": {
                    "name": "Carleigh Wood"
                },
                "author": "Carleigh Wood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01735v2",
                "updated": "2025-07-09T17:19:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    19,
                    50,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-02T16:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    46,
                    38,
                    2,
                    276,
                    0
                ],
                "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits"
                },
                "summary": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling."
                },
                "authors": [
                    {
                        "name": "Duy Nguyen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "28 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02505v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02505v2",
                "updated": "2025-07-09T17:13:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    13,
                    26,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-04T11:16:46Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    11,
                    16,
                    46,
                    1,
                    63,
                    0
                ],
                "title": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment"
                },
                "summary": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, domain-agnostic, and intuitive for human users to guide\nagent interactions in 3D environments. Specifically, we propose a novel\ncross-view goal alignment framework that allows users to specify target objects\nusing segmentation masks from their camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x compared to ROCKET-1. We\nshow that ROCKET-2 can directly interpret goals from human camera views,\nenabling better human-agent interaction. Remarkably, ROCKET-2 demonstrates\nzero-shot generalization capabilities: despite being trained exclusively on the\nMinecraft dataset, it can adapt and generalize to other 3D environments like\nDoom, DMLab, and Unreal through a simple action space mapping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, domain-agnostic, and intuitive for human users to guide\nagent interactions in 3D environments. Specifically, we propose a novel\ncross-view goal alignment framework that allows users to specify target objects\nusing segmentation masks from their camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x compared to ROCKET-1. We\nshow that ROCKET-2 can directly interpret goals from human camera views,\nenabling better human-agent interaction. Remarkably, ROCKET-2 demonstrates\nzero-shot generalization capabilities: despite being trained exclusively on the\nMinecraft dataset, it can adapt and generalize to other 3D environments like\nDoom, DMLab, and Unreal through a simple action space mapping."
                },
                "authors": [
                    {
                        "name": "Shaofei Cai"
                    },
                    {
                        "name": "Zhancun Mu"
                    },
                    {
                        "name": "Anji Liu"
                    },
                    {
                        "name": "Yitao Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yitao Liang"
                },
                "author": "Yitao Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02505v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02505v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17538v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17538v7",
                "updated": "2025-07-09T17:11:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    11,
                    15,
                    2,
                    190,
                    0
                ],
                "published": "2024-09-26T04:56:49Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    4,
                    56,
                    49,
                    3,
                    270,
                    0
                ],
                "title": "Low-Rank Adaptation Secretly Imitates Differentially Private SGD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation Secretly Imitates Differentially Private SGD"
                },
                "summary": "As pre-trained language models grow in size, full fine-tuning their\nparameters on task adaptation data becomes increasingly impractical. To address\nthis challenge, some methods for low-rank adaptation of language models have\nbeen proposed, e.g. LoRA, which incorporates trainable low-rank decomposition\nmatrices into only some parameters of the pre-trained model, called adapters.\nThis approach significantly reduces the number of trainable parameters compared\nto fine-tuning all parameters or adapters. In this work, we look at low-rank\nadaptation method from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA is equivalent to fine-tuning adapters with\nnoisy batch gradients - just like what DPSGD algorithm does. We also quantify\nthe variance of the injected noise as a decreasing function of adaptation rank.\nBy establishing a Berry-Esseen type bound on the total variation distance\nbetween the injected noise distribution and a Gaussian noise distribution with\nthe same variance, we show that the dynamics of low-rank adaptation is very\nclose to when DPSGD is performed w.r.t the adapters. Following our theoretical\nfindings and approved by our experimental results, we show that low-rank\nadaptation provides robustness to membership inference attacks w.r.t the\nfine-tuning data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As pre-trained language models grow in size, full fine-tuning their\nparameters on task adaptation data becomes increasingly impractical. To address\nthis challenge, some methods for low-rank adaptation of language models have\nbeen proposed, e.g. LoRA, which incorporates trainable low-rank decomposition\nmatrices into only some parameters of the pre-trained model, called adapters.\nThis approach significantly reduces the number of trainable parameters compared\nto fine-tuning all parameters or adapters. In this work, we look at low-rank\nadaptation method from the lens of data privacy. We show theoretically that the\nlow-rank adaptation used in LoRA is equivalent to fine-tuning adapters with\nnoisy batch gradients - just like what DPSGD algorithm does. We also quantify\nthe variance of the injected noise as a decreasing function of adaptation rank.\nBy establishing a Berry-Esseen type bound on the total variation distance\nbetween the injected noise distribution and a Gaussian noise distribution with\nthe same variance, we show that the dynamics of low-rank adaptation is very\nclose to when DPSGD is performed w.r.t the adapters. Following our theoretical\nfindings and approved by our experimental results, we show that low-rank\nadaptation provides robustness to membership inference attacks w.r.t the\nfine-tuning data."
                },
                "authors": [
                    {
                        "name": "Saber Malekmohammadi"
                    },
                    {
                        "name": "Golnoosh Farnadi"
                    }
                ],
                "author_detail": {
                    "name": "Golnoosh Farnadi"
                },
                "author": "Golnoosh Farnadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17538v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17538v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07048v1",
                "updated": "2025-07-09T17:10:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    10,
                    33,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:10:33Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    10,
                    33,
                    2,
                    190,
                    0
                ],
                "title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark\n  Enriched with Contextual Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark\n  Enriched with Contextual Metadata"
                },
                "summary": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis."
                },
                "authors": [
                    {
                        "name": "Bruce Coburn"
                    },
                    {
                        "name": "Jiangpeng He"
                    },
                    {
                        "name": "Megan E. Rollo"
                    },
                    {
                        "name": "Satvinder S. Dhaliwal"
                    },
                    {
                        "name": "Deborah A. Kerr"
                    },
                    {
                        "name": "Fengqing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Fengqing Zhu"
                },
                "author": "Fengqing Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07045v1",
                "updated": "2025-07-09T17:07:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    7,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:07:39Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    7,
                    39,
                    2,
                    190,
                    0
                ],
                "title": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient\n  Design Framework for Individual and SME LLM Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient\n  Design Framework for Individual and SME LLM Usage"
                },
                "summary": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources."
                },
                "authors": [
                    {
                        "name": "Ugur Ari"
                    }
                ],
                "author_detail": {
                    "name": "Ugur Ari"
                },
                "author": "Ugur Ari",
                "arxiv_doi": "10.5281/zenodo.1234567 10.5281/zenodo.1234567 10.5281/zenodo.1234567",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.1234567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.1234567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.1234567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 5 tables. Includes comparative experimental results across\n  OpenAI, Anthropic, DeepSeek, and Gemini LLMs",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07031v1",
                "updated": "2025-07-09T17:03:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    3,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:03:21Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    3,
                    21,
                    2,
                    190,
                    0
                ],
                "title": "ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel\n  Proof Accumulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel\n  Proof Accumulation"
                },
                "summary": "As AI models become ubiquitous in our daily lives, there has been an\nincreasing demand for transparency in ML services. However, the model owner\ndoes not want to reveal the weights, as they are considered trade secrets. To\nsolve this problem, researchers have turned to zero-knowledge proofs of ML\nmodel inference. These proofs convince the user that the ML model output is\ncorrect, without revealing the weights of the model to the user. Past work on\nthese provers can be placed into two categories. The first method compiles the\nML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The\nsecond method uses custom cryptographic protocols designed only for a specific\nclass of models. Unfortunately, the first method is highly inefficient, making\nit impractical for the large models used today, and the second method does not\ngeneralize well, making it difficult to update in the rapidly changing field of\nmachine learning. To solve this, we propose ZKTorch, an open source end-to-end\nproving system that compiles ML models into base cryptographic operations\ncalled basic blocks, each proved using specialized protocols. ZKTorch is built\non top of a novel parallel extension to the Mira accumulation scheme, enabling\nsuccinct proofs with minimal accumulation overhead. These contributions allow\nZKTorch to achieve at least a $3\\times$ reduction in the proof size compared to\nspecialized protocols and up to a $6\\times$ speedup in proving time over a\ngeneral-purpose ZKML framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI models become ubiquitous in our daily lives, there has been an\nincreasing demand for transparency in ML services. However, the model owner\ndoes not want to reveal the weights, as they are considered trade secrets. To\nsolve this problem, researchers have turned to zero-knowledge proofs of ML\nmodel inference. These proofs convince the user that the ML model output is\ncorrect, without revealing the weights of the model to the user. Past work on\nthese provers can be placed into two categories. The first method compiles the\nML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The\nsecond method uses custom cryptographic protocols designed only for a specific\nclass of models. Unfortunately, the first method is highly inefficient, making\nit impractical for the large models used today, and the second method does not\ngeneralize well, making it difficult to update in the rapidly changing field of\nmachine learning. To solve this, we propose ZKTorch, an open source end-to-end\nproving system that compiles ML models into base cryptographic operations\ncalled basic blocks, each proved using specialized protocols. ZKTorch is built\non top of a novel parallel extension to the Mira accumulation scheme, enabling\nsuccinct proofs with minimal accumulation overhead. These contributions allow\nZKTorch to achieve at least a $3\\times$ reduction in the proof size compared to\nspecialized protocols and up to a $6\\times$ speedup in proving time over a\ngeneral-purpose ZKML framework."
                },
                "authors": [
                    {
                        "name": "Bing-Jyue Chen"
                    },
                    {
                        "name": "Lilia Tang"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07024v1",
                "updated": "2025-07-09T16:54:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    54,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:54:21Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    54,
                    21,
                    2,
                    190,
                    0
                ],
                "title": "FlexOlmo: Open Language Models for Flexible Data Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexOlmo: Open Language Models for Flexible Data Use"
                },
                "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Akshita Bhagia"
                    },
                    {
                        "name": "Kevin Farhat"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Pete Walsh"
                    },
                    {
                        "name": "Jacob Morrison"
                    },
                    {
                        "name": "Dustin Schwenk"
                    },
                    {
                        "name": "Shayne Longpre"
                    },
                    {
                        "name": "Jake Poznanski"
                    },
                    {
                        "name": "Allyson Ettinger"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Margaret Li"
                    },
                    {
                        "name": "Dirk Groeneveld"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Wen-tau Yih"
                    },
                    {
                        "name": "Luca Soldaini"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Sewon Min"
                    }
                ],
                "author_detail": {
                    "name": "Sewon Min"
                },
                "author": "Sewon Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07017v1",
                "updated": "2025-07-09T16:45:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    45,
                    48,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:45:48Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    45,
                    48,
                    2,
                    190,
                    0
                ],
                "title": "First Return, Entropy-Eliciting Explore",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Return, Entropy-Eliciting Explore"
                },
                "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration."
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Taoran Liang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zejun Ma"
                },
                "author": "Zejun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05261v2",
                "updated": "2025-07-09T16:40:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    40,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-18T23:17:29Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    23,
                    17,
                    29,
                    2,
                    169,
                    0
                ],
                "title": "TokenShapley: Token Level Context Attribution with Shapley Value",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenShapley: Token Level Context Attribution with Shapley Value"
                },
                "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy."
                },
                "authors": [
                    {
                        "name": "Yingtai Xiao"
                    },
                    {
                        "name": "Yuqing Zhu"
                    },
                    {
                        "name": "Sirat Samyoun"
                    },
                    {
                        "name": "Wanrong Zhang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "Jian Du"
                    }
                ],
                "author_detail": {
                    "name": "Jian Du"
                },
                "author": "Jian Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19710v2",
                "updated": "2025-07-09T16:39:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    39,
                    17,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-28T12:04:33Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    12,
                    4,
                    33,
                    0,
                    118,
                    0
                ],
                "title": "PhyloProfile v2: Scalable Exploration of Multilayered Phylogenetic\n  Profiles via Dimensionality Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhyloProfile v2: Scalable Exploration of Multilayered Phylogenetic\n  Profiles via Dimensionality Reduction"
                },
                "summary": "Phylogenetic profiles - presence-absence patterns of genes across taxa - are\nrich information sources for inferring the evolutionary history of genes and\ngene families. When aggregated across many genes, these profiles can reveal\ncoevolutionary patterns, supporting the prediction of gene functions and\ninteractions. With rapidly growing numbers of sequenced genomes, phylogenetic\nprofiles now routinely encompass thousands of genes and taxa. Existing software\nfall short in enabling interactive visualization, exploration, and analysis of\nsuch large datasets. We present PhyloProfile v2, a comprehensive overhaul of\nthe original PhyloProfile software. This new version introduces major\nperformance improvements along with novel features designed for more efficient\ndata exploration. Notably, PhyloProfile v2 integrates dimensionality reduction\ntechniques to visualize phylogenetic profiles in interactive 2D or 3D space,\noffering an intuitive overview even for massive datasets. Furthermore, the\nplatform enables seamless transitions from large-scale analyses - spanning\nmillions of orthology relationships - to detailed comparisons of protein\nfeature architectures between specific orthologs. PhyloProfile v2 thus provides\na versatile and scalable solution for evolutionary and functional genomics\nresearch. PhyloProfile v2 is available as an R package at Bioconductor\nhttps://doi.org/doi:10.18129/B9.bioc.PhyloProfile. The open-source code and\ndocumentation are provided under MIT license at\nhttps://github.com/BIONF/PhyloProfile",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phylogenetic profiles - presence-absence patterns of genes across taxa - are\nrich information sources for inferring the evolutionary history of genes and\ngene families. When aggregated across many genes, these profiles can reveal\ncoevolutionary patterns, supporting the prediction of gene functions and\ninteractions. With rapidly growing numbers of sequenced genomes, phylogenetic\nprofiles now routinely encompass thousands of genes and taxa. Existing software\nfall short in enabling interactive visualization, exploration, and analysis of\nsuch large datasets. We present PhyloProfile v2, a comprehensive overhaul of\nthe original PhyloProfile software. This new version introduces major\nperformance improvements along with novel features designed for more efficient\ndata exploration. Notably, PhyloProfile v2 integrates dimensionality reduction\ntechniques to visualize phylogenetic profiles in interactive 2D or 3D space,\noffering an intuitive overview even for massive datasets. Furthermore, the\nplatform enables seamless transitions from large-scale analyses - spanning\nmillions of orthology relationships - to detailed comparisons of protein\nfeature architectures between specific orthologs. PhyloProfile v2 thus provides\na versatile and scalable solution for evolutionary and functional genomics\nresearch. PhyloProfile v2 is available as an R package at Bioconductor\nhttps://doi.org/doi:10.18129/B9.bioc.PhyloProfile. The open-source code and\ndocumentation are provided under MIT license at\nhttps://github.com/BIONF/PhyloProfile"
                },
                "authors": [
                    {
                        "name": "Vinh Tran"
                    },
                    {
                        "name": "Ingo Ebersberger"
                    }
                ],
                "author_detail": {
                    "name": "Ingo Ebersberger"
                },
                "author": "Ingo Ebersberger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01945v2",
                "updated": "2025-07-09T16:30:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    30,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-02T17:55:50Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    17,
                    55,
                    50,
                    2,
                    183,
                    0
                ],
                "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory"
                },
                "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/."
                },
                "authors": [
                    {
                        "name": "Nan Chen"
                    },
                    {
                        "name": "Mengqi Huang"
                    },
                    {
                        "name": "Yihao Meng"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03772v2",
                "updated": "2025-07-09T16:28:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    28,
                    55,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-04T18:45:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    18,
                    45,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Skewed Score: A statistical framework to assess autograders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skewed Score: A statistical framework to assess autograders"
                },
                "summary": "The evaluation of large language model (LLM) outputs is increasingly\nperformed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or\nautograders. While autograders offer a scalable alternative to human\nevaluation, they have shown mixed reliability and may exhibit systematic\nbiases, depending on response type, scoring methodology, domain specificity, or\nother factors. Here we propose a statistical framework based on Bayesian\ngeneralised linear models (GLMs) that enables researchers to simultaneously\nassess their autograders while addressing their primary research questions\n(e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores\nor pairwise preferences) as a function of properties of the grader (e.g., human\nvs. autograder) and the evaluated item (e.g., response length or the LLM that\ngenerated it), allowing for explicit quantification of scoring differences and\npotential biases within a unified framework. In addition, our method can be\nused to augment traditional metrics such as inter-rater agreement, by providing\nuncertainty estimates and clarifying sources of disagreement. Overall, this\napproach contributes to more robust and interpretable use of autograders in LLM\nevaluation, enabling both performance analysis and bias detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language model (LLM) outputs is increasingly\nperformed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or\nautograders. While autograders offer a scalable alternative to human\nevaluation, they have shown mixed reliability and may exhibit systematic\nbiases, depending on response type, scoring methodology, domain specificity, or\nother factors. Here we propose a statistical framework based on Bayesian\ngeneralised linear models (GLMs) that enables researchers to simultaneously\nassess their autograders while addressing their primary research questions\n(e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores\nor pairwise preferences) as a function of properties of the grader (e.g., human\nvs. autograder) and the evaluated item (e.g., response length or the LLM that\ngenerated it), allowing for explicit quantification of scoring differences and\npotential biases within a unified framework. In addition, our method can be\nused to augment traditional metrics such as inter-rater agreement, by providing\nuncertainty estimates and clarifying sources of disagreement. Overall, this\napproach contributes to more robust and interpretable use of autograders in LLM\nevaluation, enabling both performance analysis and bias detection."
                },
                "authors": [
                    {
                        "name": "Magda Dubois"
                    },
                    {
                        "name": "Harry Coppock"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Timo Flesch"
                    },
                    {
                        "name": "Lennart Luettgau"
                    },
                    {
                        "name": "Cozmin Ududec"
                    }
                ],
                "author_detail": {
                    "name": "Cozmin Ududec"
                },
                "author": "Cozmin Ududec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06999v1",
                "updated": "2025-07-09T16:25:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    25,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:25:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    25,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs"
                },
                "summary": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility."
                },
                "authors": [
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Yuyang Dong"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06993v1",
                "updated": "2025-07-09T16:18:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    18,
                    9,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:18:09Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    18,
                    9,
                    2,
                    190,
                    0
                ],
                "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation"
                },
                "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response."
                },
                "authors": [
                    {
                        "name": "Jieren Deng"
                    },
                    {
                        "name": "Aleksandar Cvetkovic"
                    },
                    {
                        "name": "Pak Kiu Chung"
                    },
                    {
                        "name": "Dragomir Yankov"
                    },
                    {
                        "name": "Chiqun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiqun Zhang"
                },
                "author": "Chiqun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06992v1",
                "updated": "2025-07-09T16:15:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    15,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:15:38Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    15,
                    38,
                    2,
                    190,
                    0
                ],
                "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology\n  Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology\n  Report Generation"
                },
                "summary": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation."
                },
                "authors": [
                    {
                        "name": "Qilong Xing"
                    },
                    {
                        "name": "Zikai Song"
                    },
                    {
                        "name": "Youjia Zhang"
                    },
                    {
                        "name": "Na Feng"
                    },
                    {
                        "name": "Junqing Yu"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "arxiv_comment": "MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12112v3",
                "updated": "2025-07-09T16:13:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    13,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-15T23:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    23,
                    20,
                    54,
                    1,
                    289,
                    0
                ],
                "title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming"
                },
                "summary": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp."
                },
                "authors": [
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "57 pages, 25 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06986v1",
                "updated": "2025-07-09T16:08:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    8,
                    58,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:08:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    8,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "BarkBeetle: Stealing Decision Tree Models with Fault Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BarkBeetle: Stealing Decision Tree Models with Fault Injection"
                },
                "summary": "Machine learning models, particularly decision trees (DTs), are widely\nadopted across various domains due to their interpretability and efficiency.\nHowever, as ML models become increasingly integrated into privacy-sensitive\napplications, concerns about their confidentiality have grown, particularly in\nlight of emerging threats such as model extraction and fault injection attacks.\nAssessing the vulnerability of DTs under such attacks is therefore important.\nIn this work, we present BarkBeetle, a novel attack that leverages fault\ninjection to extract internal structural information of DT models. BarkBeetle\nemploys a bottom-up recovery strategy that uses targeted fault injection at\nspecific nodes to efficiently infer feature splits and threshold values. Our\nproof-of-concept implementation demonstrates that BarkBeetle requires\nsignificantly fewer queries and recovers more structural information compared\nto prior approaches, when evaluated on DTs trained with public UCI datasets. To\nvalidate its practical feasibility, we implement BarkBeetle on a Raspberry Pi\nRP2350 board and perform fault injections using the Faultier voltage glitching\ntool. As BarkBeetle targets general DT models, we also provide an in-depth\ndiscussion on its applicability to a broader range of tree-based applications,\nincluding data stream classification, DT variants, and cryptography schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models, particularly decision trees (DTs), are widely\nadopted across various domains due to their interpretability and efficiency.\nHowever, as ML models become increasingly integrated into privacy-sensitive\napplications, concerns about their confidentiality have grown, particularly in\nlight of emerging threats such as model extraction and fault injection attacks.\nAssessing the vulnerability of DTs under such attacks is therefore important.\nIn this work, we present BarkBeetle, a novel attack that leverages fault\ninjection to extract internal structural information of DT models. BarkBeetle\nemploys a bottom-up recovery strategy that uses targeted fault injection at\nspecific nodes to efficiently infer feature splits and threshold values. Our\nproof-of-concept implementation demonstrates that BarkBeetle requires\nsignificantly fewer queries and recovers more structural information compared\nto prior approaches, when evaluated on DTs trained with public UCI datasets. To\nvalidate its practical feasibility, we implement BarkBeetle on a Raspberry Pi\nRP2350 board and perform fault injections using the Faultier voltage glitching\ntool. As BarkBeetle targets general DT models, we also provide an in-depth\ndiscussion on its applicability to a broader range of tree-based applications,\nincluding data stream classification, DT variants, and cryptography schemes."
                },
                "authors": [
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Jonas Sander"
                    },
                    {
                        "name": "Minmin Jiang"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    },
                    {
                        "name": "David Oswald"
                    }
                ],
                "author_detail": {
                    "name": "David Oswald"
                },
                "author": "David Oswald",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06980v1",
                "updated": "2025-07-09T16:07:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    7,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:07:20Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    7,
                    20,
                    2,
                    190,
                    0
                ],
                "title": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code\n  Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability."
                },
                "authors": [
                    {
                        "name": "Binquan Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiwen Luo"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06976v1",
                "updated": "2025-07-09T16:05:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    5,
                    25,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:05:25Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    5,
                    25,
                    2,
                    190,
                    0
                ],
                "title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via\n  Joint LiDAR-Based 3D Object Detection and Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via\n  Joint LiDAR-Based 3D Object Detection and Denoising"
                },
                "summary": "While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles."
                },
                "authors": [
                    {
                        "name": "Sven Teufel"
                    },
                    {
                        "name": "Dominique Mayer"
                    },
                    {
                        "name": "Jrg Gamerdinger"
                    },
                    {
                        "name": "Oliver Bringmann"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Bringmann"
                },
                "author": "Oliver Bringmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06969v1",
                "updated": "2025-07-09T15:59:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    59,
                    30,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:59:30Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    59,
                    30,
                    2,
                    190,
                    0
                ],
                "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction\n  Risks in Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction\n  Risks in Differential Privacy"
                },
                "summary": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk."
                },
                "authors": [
                    {
                        "name": "Bogdan Kulynych"
                    },
                    {
                        "name": "Juan Felipe Gomez"
                    },
                    {
                        "name": "Georgios Kaissis"
                    },
                    {
                        "name": "Jamie Hayes"
                    },
                    {
                        "name": "Borja Balle"
                    },
                    {
                        "name": "Flavio du Pin Calmon"
                    },
                    {
                        "name": "Jean Louis Raisaro"
                    }
                ],
                "author_detail": {
                    "name": "Jean Louis Raisaro"
                },
                "author": "Jean Louis Raisaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06964v2",
                "updated": "2025-07-10T05:41:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    5,
                    41,
                    21,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T15:52:34Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    52,
                    34,
                    2,
                    190,
                    0
                ],
                "title": "IdentityByDescentDispersal.jl: Inferring dispersal rates with\n  identity-by-descent blocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IdentityByDescentDispersal.jl: Inferring dispersal rates with\n  identity-by-descent blocks"
                },
                "summary": "The population density and per-generation dispersal rate of a population are\ncentral parameters in the study of evolution and ecology. The distribution of\nrecent coalescent events between individuals in space can be used to estimate\nsuch quantities through the distribution of identity-by-descent (IBD) blocks.\nAn IBD block is defined as a segment of DNA that has been inherited by a pair\nof individuals from a common ancestor without being broken by recombination. We\nintroduce IdentityByDescentDispersal.jl, a Julia package for estimating\neffective population densities and dispersal rates from observed spatial\npatterns of IBD shared blocks. It implements the inference scheme proposed by\nRingbauer, Coop, and Barton (2017). The package provides a user-friendly\ninterface, supports efficient gradient-based optimization and accommodates\narbitrary user-defined demographic models through numerical integration. This\nsoftware aims to encourage a wider audience to utilize spatial genetic data for\nestimating dispersal rates, thereby motivating further research and expanding\nits applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The population density and per-generation dispersal rate of a population are\ncentral parameters in the study of evolution and ecology. The distribution of\nrecent coalescent events between individuals in space can be used to estimate\nsuch quantities through the distribution of identity-by-descent (IBD) blocks.\nAn IBD block is defined as a segment of DNA that has been inherited by a pair\nof individuals from a common ancestor without being broken by recombination. We\nintroduce IdentityByDescentDispersal.jl, a Julia package for estimating\neffective population densities and dispersal rates from observed spatial\npatterns of IBD shared blocks. It implements the inference scheme proposed by\nRingbauer, Coop, and Barton (2017). The package provides a user-friendly\ninterface, supports efficient gradient-based optimization and accommodates\narbitrary user-defined demographic models through numerical integration. This\nsoftware aims to encourage a wider audience to utilize spatial genetic data for\nestimating dispersal rates, thereby motivating further research and expanding\nits applications."
                },
                "authors": [
                    {
                        "name": "Francisco Campuzano Jimnez"
                    },
                    {
                        "name": "Arthur Zwaenepoel"
                    },
                    {
                        "name": "Els Lea R De Keyzer"
                    },
                    {
                        "name": "Hannes Svardal"
                    }
                ],
                "author_detail": {
                    "name": "Hannes Svardal"
                },
                "author": "Hannes Svardal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04394v2",
                "updated": "2025-07-09T15:49:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    49,
                    27,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-05T18:09:41Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    9,
                    41,
                    3,
                    340,
                    0
                ],
                "title": "Bayesian Quantum Amplitude Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantum Amplitude Estimation"
                },
                "summary": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present BAE, a problem-tailored and noise-aware Bayesian algorithm for\nquantum amplitude estimation. In a fault tolerant scenario, BAE is capable of\nsaturating the Heisenberg limit; if device noise is present, BAE can\ndynamically characterize it and self-adapt. We further propose aBAE, an\nannealed variant of BAE drawing on methods from statistical inference, to\nenhance robustness. Our proposals are parallelizable in both quantum and\nclassical components, offer tools for fast noise model assessment, and can\nleverage preexisting information. Additionally, they accommodate experimental\nlimitations and preferred cost trade-offs. We propose a robust benchmark for\namplitude estimation algorithms and use it to test BAE against other\napproaches, demonstrating its competitive performance in both noisy and\nnoiseless scenarios. In both cases, it achieves lower error than any other\nalgorithm as a function of the cost. In the presence of decoherence, it is\ncapable of learning when other algorithms fail."
                },
                "authors": [
                    {
                        "name": "Alexandra Rama"
                    },
                    {
                        "name": "Luis Paulo Santos"
                    }
                ],
                "author_detail": {
                    "name": "Luis Paulo Santos"
                },
                "author": "Luis Paulo Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06961v1",
                "updated": "2025-07-09T15:46:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    46,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:46:39Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    46,
                    39,
                    2,
                    190,
                    0
                ],
                "title": "Off-Policy Evaluation Under Nonignorable Missing Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Policy Evaluation Under Nonignorable Missing Data"
                },
                "summary": "Off-Policy Evaluation (OPE) aims to estimate the value of a target policy\nusing offline data collected from potentially different policies. In real-world\napplications, however, logged data often suffers from missingness. While OPE\nhas been extensively studied in the literature, a theoretical understanding of\nhow missing data affects OPE results remains unclear. In this paper, we\ninvestigate OPE in the presence of monotone missingness and theoretically\ndemonstrate that the value estimates remain unbiased under ignorable\nmissingness but can be biased under nonignorable (informative) missingness. To\nretain the consistency of value estimation, we propose an inverse probability\nweighted value estimator and conduct statistical inference to quantify the\nuncertainty of the estimates. Through a series of numerical experiments, we\nempirically demonstrate that our proposed estimator yields a more reliable\nvalue inference under missing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Policy Evaluation (OPE) aims to estimate the value of a target policy\nusing offline data collected from potentially different policies. In real-world\napplications, however, logged data often suffers from missingness. While OPE\nhas been extensively studied in the literature, a theoretical understanding of\nhow missing data affects OPE results remains unclear. In this paper, we\ninvestigate OPE in the presence of monotone missingness and theoretically\ndemonstrate that the value estimates remain unbiased under ignorable\nmissingness but can be biased under nonignorable (informative) missingness. To\nretain the consistency of value estimation, we propose an inverse probability\nweighted value estimator and conduct statistical inference to quantify the\nuncertainty of the estimates. Through a series of numerical experiments, we\nempirically demonstrate that our proposed estimator yields a more reliable\nvalue inference under missing data."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Wenbin Lu"
                    },
                    {
                        "name": "Rui Song"
                    }
                ],
                "author_detail": {
                    "name": "Rui Song"
                },
                "author": "Rui Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22675v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22675v3",
                "updated": "2025-07-09T15:42:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    42,
                    31,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-27T22:58:37Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    22,
                    58,
                    37,
                    4,
                    178,
                    0
                ],
                "title": "Bayesian Invariance Modeling of Multi-Environment Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Invariance Modeling of Multi-Environment Data"
                },
                "summary": "Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from\nmultiple environments to identify invariant features - those with a stable\npredictive relationship to the outcome. Such features support generalization to\nnew environments and help reveal causal mechanisms. Previous methods have\nprimarily tackled this problem through hypothesis testing or regularized\noptimization. Here we develop Bayesian Invariant Prediction (BIP), a\nprobabilistic model for invariant prediction. BIP encodes the indices of\ninvariant features as a latent variable and recover them by posterior\ninference. Under the assumptions of Peters et al. [2016], the BIP posterior\ntargets the true invariant features. We prove that the posterior is consistent\nand that greater environment heterogeneity leads to faster posterior\ncontraction. To handle many features, we design an efficient variational\napproximation called VI-BIP. In simulations and real data, we find that BIP and\nVI-BIP are more accurate and scalable than existing methods for invariant\nprediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from\nmultiple environments to identify invariant features - those with a stable\npredictive relationship to the outcome. Such features support generalization to\nnew environments and help reveal causal mechanisms. Previous methods have\nprimarily tackled this problem through hypothesis testing or regularized\noptimization. Here we develop Bayesian Invariant Prediction (BIP), a\nprobabilistic model for invariant prediction. BIP encodes the indices of\ninvariant features as a latent variable and recover them by posterior\ninference. Under the assumptions of Peters et al. [2016], the BIP posterior\ntargets the true invariant features. We prove that the posterior is consistent\nand that greater environment heterogeneity leads to faster posterior\ncontraction. To handle many features, we design an efficient variational\napproximation called VI-BIP. In simulations and real data, we find that BIP and\nVI-BIP are more accurate and scalable than existing methods for invariant\nprediction."
                },
                "authors": [
                    {
                        "name": "Luhuan Wu"
                    },
                    {
                        "name": "Mingzhang Yin"
                    },
                    {
                        "name": "Yixin Wang"
                    },
                    {
                        "name": "John P. Cunningham"
                    },
                    {
                        "name": "David M. Blei"
                    }
                ],
                "author_detail": {
                    "name": "David M. Blei"
                },
                "author": "David M. Blei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22675v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22675v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06956v1",
                "updated": "2025-07-09T15:39:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    39,
                    17,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:39:17Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    39,
                    17,
                    2,
                    190,
                    0
                ],
                "title": "Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level"
                },
                "summary": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed."
                },
                "authors": [
                    {
                        "name": "Sezen Perin"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Qutub Sha Syed"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Aleksei Kuvshinov"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Kay-Ulrich Scholl"
                    }
                ],
                "author_detail": {
                    "name": "Kay-Ulrich Scholl"
                },
                "author": "Kay-Ulrich Scholl",
                "arxiv_comment": "Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06941v1",
                "updated": "2025-07-09T15:22:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    22,
                    17,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:22:17Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    22,
                    17,
                    2,
                    190,
                    0
                ],
                "title": "Calibration of Quantum Devices via Robust Statistical Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibration of Quantum Devices via Robust Statistical Methods"
                },
                "summary": "Bayesian inference is a widely used technique for real-time characterization\nof quantum systems. It excels in experimental characterization in the low data\nregime, and when the measurements have degrees of freedom. A decisive factor\nfor its performance is the numerical representation of the Bayesian probability\ndistributions. In this work, we explore advanced statistical methods for this\npurpose, and numerically analyze their performance against the state-of-the-art\nin quantum parameter learning. In particular, we consider sequential importance\nresampling, tempered likelihood estimation, Markov Chain Monte Carlo, random\nwalk Metropolis (RWM), Hamiltonian Monte Carlo (HMC) and variants (stochastic\ngradients with and without friction, energy conserving subsampling), block\npseudo-marginal Metropolis-Hastings with subsampling, hybrid HMC-RWM\napproaches, and Gaussian rejection filtering. We demonstrate advantages of\nthese approaches over existing ones, namely robustness under multi-modality and\nhigh dimensionality. We apply these algorithms to the calibration of\nsuperconducting qubits from IBMQ, surpassing the standard quantum limit and\nachieving better results than Qiskit's default tools. In Hahn echo and Ramsey\nexperiments, we reduce the uncertainty by factors of 10 and 3 respectively,\nwithout increasing the number of measurements; conversely, we match the\nperformance of Qiskit's methods while using up to to 99.5% less experimental\ndata. We additionally investigate the roles of adaptivity, dataset ordering and\nheuristics in quantum characterization. Our findings have applications in\nchallenging quantum characterization tasks, namely learning the dynamics of\nopen quantum systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference is a widely used technique for real-time characterization\nof quantum systems. It excels in experimental characterization in the low data\nregime, and when the measurements have degrees of freedom. A decisive factor\nfor its performance is the numerical representation of the Bayesian probability\ndistributions. In this work, we explore advanced statistical methods for this\npurpose, and numerically analyze their performance against the state-of-the-art\nin quantum parameter learning. In particular, we consider sequential importance\nresampling, tempered likelihood estimation, Markov Chain Monte Carlo, random\nwalk Metropolis (RWM), Hamiltonian Monte Carlo (HMC) and variants (stochastic\ngradients with and without friction, energy conserving subsampling), block\npseudo-marginal Metropolis-Hastings with subsampling, hybrid HMC-RWM\napproaches, and Gaussian rejection filtering. We demonstrate advantages of\nthese approaches over existing ones, namely robustness under multi-modality and\nhigh dimensionality. We apply these algorithms to the calibration of\nsuperconducting qubits from IBMQ, surpassing the standard quantum limit and\nachieving better results than Qiskit's default tools. In Hahn echo and Ramsey\nexperiments, we reduce the uncertainty by factors of 10 and 3 respectively,\nwithout increasing the number of measurements; conversely, we match the\nperformance of Qiskit's methods while using up to to 99.5% less experimental\ndata. We additionally investigate the roles of adaptivity, dataset ordering and\nheuristics in quantum characterization. Our findings have applications in\nchallenging quantum characterization tasks, namely learning the dynamics of\nopen quantum systems."
                },
                "authors": [
                    {
                        "name": "Alexandra Rama"
                    },
                    {
                        "name": "Raffaele Santagati"
                    },
                    {
                        "name": "Nathan Wiebe"
                    }
                ],
                "author_detail": {
                    "name": "Nathan Wiebe"
                },
                "author": "Nathan Wiebe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06939v1",
                "updated": "2025-07-09T15:21:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    21,
                    22,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:21:22Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    21,
                    22,
                    2,
                    190,
                    0
                ],
                "title": "Sound Interval-Based Synthesis for Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound Interval-Based Synthesis for Probabilistic Programs"
                },
                "summary": "Probabilistic programming has become a standard practice to model stochastic\nevents and learn about the behavior of nature in different scientific contexts,\nranging from Genetics and Ecology to Linguistics and Psychology. However,\ndomain practitioners (such as biologists) also need to be experts in statistics\nin order to select which probabilistic model is suitable for a given particular\nproblem, relying then on probabilistic inference engines such as Stan, Pyro or\nEdward to fine-tune the parameters of that particular model. Probabilistic\nProgramming would be more useful if the model selection is made automatic,\nwithout requiring statistics expertise from the end user. Automatically\nselecting the model is challenging because of the large search space of\nprobabilistic programs needed to be explored, because the fact that most of\nthat search space contains invalid programs, and because invalid programs may\nonly be detected in some executions, due to its probabilistic nature. We\npropose a type system to statically reject invalid probabilistic programs, a\ntype-directed synthesis algorithm that guarantees that generated programs are\ntype-safe by construction, and an heuristic search procedure to handle the vast\nsearch space. We collect a number of probabilistic programs from the\nliterature, and use them to compare our method with both a type-agnostic random\nsearch, and a data-guided method from the literature (DaPPer). Our results show\nthat our technique both outperforms random search and DaPPer, specially on more\ncomplex programs. This drastic performance difference in synthesis allows for\nfast sampling of programs and enables techniques that previously suffered from\nthe complexity of synthesis, such as Genetic Programming, to be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic programming has become a standard practice to model stochastic\nevents and learn about the behavior of nature in different scientific contexts,\nranging from Genetics and Ecology to Linguistics and Psychology. However,\ndomain practitioners (such as biologists) also need to be experts in statistics\nin order to select which probabilistic model is suitable for a given particular\nproblem, relying then on probabilistic inference engines such as Stan, Pyro or\nEdward to fine-tune the parameters of that particular model. Probabilistic\nProgramming would be more useful if the model selection is made automatic,\nwithout requiring statistics expertise from the end user. Automatically\nselecting the model is challenging because of the large search space of\nprobabilistic programs needed to be explored, because the fact that most of\nthat search space contains invalid programs, and because invalid programs may\nonly be detected in some executions, due to its probabilistic nature. We\npropose a type system to statically reject invalid probabilistic programs, a\ntype-directed synthesis algorithm that guarantees that generated programs are\ntype-safe by construction, and an heuristic search procedure to handle the vast\nsearch space. We collect a number of probabilistic programs from the\nliterature, and use them to compare our method with both a type-agnostic random\nsearch, and a data-guided method from the literature (DaPPer). Our results show\nthat our technique both outperforms random search and DaPPer, specially on more\ncomplex programs. This drastic performance difference in synthesis allows for\nfast sampling of programs and enables techniques that previously suffered from\nthe complexity of synthesis, such as Genetic Programming, to be applied."
                },
                "authors": [
                    {
                        "name": "Guilherme Espada"
                    },
                    {
                        "name": "Alcides Fonseca"
                    }
                ],
                "author_detail": {
                    "name": "Alcides Fonseca"
                },
                "author": "Alcides Fonseca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06938v1",
                "updated": "2025-07-09T15:20:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    20,
                    2,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:20:02Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    20,
                    2,
                    2,
                    190,
                    0
                ],
                "title": "Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian\n  Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian\n  Processes"
                },
                "summary": "Multivariate Gaussian processes (GPs) offer a powerful probabilistic\nframework to represent complex interdependent phenomena. They pose, however,\nsignificant computational challenges in high-dimensional settings, which\nfrequently arise in spatial-temporal applications. We present DALIA, a highly\nscalable framework for performing Bayesian inference tasks on spatio-temporal\nmultivariate GPs, based on the methodology of integrated nested Laplace\napproximations. Our approach relies on a sparse inverse covariance matrix\nformulation of the GP, puts forward a GPU-accelerated block-dense approach, and\nintroduces a hierarchical, triple-layer, distributed memory parallel scheme. We\nshowcase weak scaling performance surpassing the state-of-the-art by two orders\nof magnitude on a model whose parameter space is 8$\\times$ larger and measure\nstrong scaling speedups of three orders of magnitude when running on 496 GH200\nsuperchips on the Alps supercomputer. Applying DALIA to air pollution data from\nnorthern Italy over 48 days, we showcase refined spatial resolutions over the\naggregated pollutant measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate Gaussian processes (GPs) offer a powerful probabilistic\nframework to represent complex interdependent phenomena. They pose, however,\nsignificant computational challenges in high-dimensional settings, which\nfrequently arise in spatial-temporal applications. We present DALIA, a highly\nscalable framework for performing Bayesian inference tasks on spatio-temporal\nmultivariate GPs, based on the methodology of integrated nested Laplace\napproximations. Our approach relies on a sparse inverse covariance matrix\nformulation of the GP, puts forward a GPU-accelerated block-dense approach, and\nintroduces a hierarchical, triple-layer, distributed memory parallel scheme. We\nshowcase weak scaling performance surpassing the state-of-the-art by two orders\nof magnitude on a model whose parameter space is 8$\\times$ larger and measure\nstrong scaling speedups of three orders of magnitude when running on 496 GH200\nsuperchips on the Alps supercomputer. Applying DALIA to air pollution data from\nnorthern Italy over 48 days, we showcase refined spatial resolutions over the\naggregated pollutant measurements."
                },
                "authors": [
                    {
                        "name": "Lisa Gaedke-Merzhuser"
                    },
                    {
                        "name": "Vincent Maillou"
                    },
                    {
                        "name": "Fernando Rodriguez Avellaneda"
                    },
                    {
                        "name": "Olaf Schenk"
                    },
                    {
                        "name": "Mathieu Luisier"
                    },
                    {
                        "name": "Paula Moraga"
                    },
                    {
                        "name": "Alexandros Nikolaos Ziogas"
                    },
                    {
                        "name": "Hvard Rue"
                    }
                ],
                "author_detail": {
                    "name": "Hvard Rue"
                },
                "author": "Hvard Rue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 68W15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3; G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18497v2",
                "updated": "2025-07-09T15:14:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    14,
                    46,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-24T15:28:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "Neuron-Level Differentiation of Memorization and Generalization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-Level Differentiation of Memorization and Generalization in Large\n  Language Models"
                },
                "summary": "We investigate how Large Language Models (LLMs) distinguish between\nmemorization and generalization at the neuron level. Through carefully designed\ntasks, we identify distinct neuron subsets responsible for each behavior.\nExperiments on both a GPT-2 model trained from scratch and a pretrained\nLLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level\nspecialization. We further demonstrate that inference-time interventions on\nthese neurons can steer the model's behavior toward memorization or\ngeneralization. To assess robustness, we evaluate intra-task and inter-task\nconsistency, confirming that these neuron-behavior associations reflect\ngeneralizable patterns rather than dataset-specific artifacts. Our findings\nreveal modular structure in LLMs and enable controlling memorization and\ngeneralization behaviors at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how Large Language Models (LLMs) distinguish between\nmemorization and generalization at the neuron level. Through carefully designed\ntasks, we identify distinct neuron subsets responsible for each behavior.\nExperiments on both a GPT-2 model trained from scratch and a pretrained\nLLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level\nspecialization. We further demonstrate that inference-time interventions on\nthese neurons can steer the model's behavior toward memorization or\ngeneralization. To assess robustness, we evaluate intra-task and inter-task\nconsistency, confirming that these neuron-behavior associations reflect\ngeneralizable patterns rather than dataset-specific artifacts. Our findings\nreveal modular structure in LLMs and enable controlling memorization and\ngeneralization behaviors at inference time."
                },
                "authors": [
                    {
                        "name": "Ko-Wei Huang"
                    },
                    {
                        "name": "Yi-Fu Fu"
                    },
                    {
                        "name": "Ching-Yu Tsai"
                    },
                    {
                        "name": "Yu-Chieh Tu"
                    },
                    {
                        "name": "Tzu-Ling Cheng"
                    },
                    {
                        "name": "Cheng-Yu Lin"
                    },
                    {
                        "name": "Yi-Ting Yang"
                    },
                    {
                        "name": "Heng-Yi Liu"
                    },
                    {
                        "name": "Keng-Te Liao"
                    },
                    {
                        "name": "Da-Cheng Juan"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09567v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09567v4",
                "updated": "2025-07-09T15:13:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    13,
                    24,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-12T17:35:03Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    17,
                    35,
                    3,
                    2,
                    71,
                    0
                ],
                "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning\n  Large Language Models"
                },
                "summary": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning with large language models (RLLMs), such as\nOpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in\ncomplex domains like mathematics and coding. A central factor in their success\nlies in the application of long chain-of-thought (Long CoT) characteristics,\nwhich enhance reasoning abilities and enable the solution of intricate\nproblems. However, despite these developments, a comprehensive survey on Long\nCoT is still lacking, limiting our understanding of its distinctions from\ntraditional short chain-of-thought (Short CoT) and complicating ongoing debates\non issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks\nto fill this gap by offering a unified perspective on Long CoT. (1) We first\ndistinguish Long CoT from Short CoT and introduce a novel taxonomy to\ncategorize current reasoning paradigms. (2) Next, we explore the key\ncharacteristics of Long CoT: deep reasoning, extensive exploration, and\nfeasible reflection, which enable models to handle more complex tasks and\nproduce more efficient, coherent outcomes compared to the shallower Short CoT.\n(3) We then investigate key phenomena such as the emergence of Long CoT with\nthese characteristics, including overthinking, and inference-time scaling,\noffering insights into how these processes manifest in practice. (4) Finally,\nwe identify significant research gaps and highlight promising future\ndirections, including the integration of multi-modal reasoning, efficiency\nimprovements, and enhanced knowledge frameworks. By providing a structured\noverview, this survey aims to inspire future research and further the\ndevelopment of logical reasoning in artificial intelligence."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Te Gao"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Paper are available at https://long-cot.github.io/, and Github are\n  available at\n  https://github.com/LightChen233/Awesome-Long-Chain-of-Thought-Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09567v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09567v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23463v2",
                "updated": "2025-07-09T15:10:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    10,
                    56,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-30T02:03:23Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    2,
                    3,
                    23,
                    0,
                    181,
                    0
                ],
                "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Keep and What to Drop: Adaptive Table Filtering Framework"
                },
                "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks."
                },
                "authors": [
                    {
                        "name": "WonJune Jang"
                    }
                ],
                "author_detail": {
                    "name": "WonJune Jang"
                },
                "author": "WonJune Jang",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06921v1",
                "updated": "2025-07-09T14:58:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    58,
                    54,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:58:54Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    58,
                    54,
                    2,
                    190,
                    0
                ],
                "title": "Distribution-free inference for LightGBM and GLM with Tweedie loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distribution-free inference for LightGBM and GLM with Tweedie loss"
                },
                "summary": "Prediction uncertainty quantification is a key research topic in recent years\nscientific and business problems. In insurance industries\n(\\cite{parodi2023pricing}), assessing the range of possible claim costs for\nindividual drivers improves premium pricing accuracy. It also enables insurers\nto manage risk more effectively by accounting for uncertainty in accident\nlikelihood and severity. In the presence of covariates, a variety of\nregression-type models are often used for modeling insurance claims, ranging\nfrom relatively simple generalized linear models (GLMs) to regularized GLMs to\ngradient boosting models (GBMs). Conformal predictive inference has arisen as a\npopular distribution-free approach for quantifying predictive uncertainty under\nrelatively weak assumptions of exchangeability, and has been well studied under\nthe classic linear regression setting. In this work, we propose new\nnon-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized\nTweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal\nprediction performance with these non-conformity measures in insurance claims\ndata. Our simulation results favor the use of locally weighted Pearson\nresiduals for LightGBM over other methods considered, as the resulting\nintervals maintained the nominal coverage with the smallest average width.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction uncertainty quantification is a key research topic in recent years\nscientific and business problems. In insurance industries\n(\\cite{parodi2023pricing}), assessing the range of possible claim costs for\nindividual drivers improves premium pricing accuracy. It also enables insurers\nto manage risk more effectively by accounting for uncertainty in accident\nlikelihood and severity. In the presence of covariates, a variety of\nregression-type models are often used for modeling insurance claims, ranging\nfrom relatively simple generalized linear models (GLMs) to regularized GLMs to\ngradient boosting models (GBMs). Conformal predictive inference has arisen as a\npopular distribution-free approach for quantifying predictive uncertainty under\nrelatively weak assumptions of exchangeability, and has been well studied under\nthe classic linear regression setting. In this work, we propose new\nnon-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized\nTweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal\nprediction performance with these non-conformity measures in insurance claims\ndata. Our simulation results favor the use of locally weighted Pearson\nresiduals for LightGBM over other methods considered, as the resulting\nintervals maintained the nominal coverage with the smallest average width."
                },
                "authors": [
                    {
                        "name": "Alokesh Manna"
                    },
                    {
                        "name": "Aditya Vikram Sett"
                    },
                    {
                        "name": "Dipak K. Dey"
                    },
                    {
                        "name": "Yuwen Gu"
                    },
                    {
                        "name": "Elizabeth D. Schifano"
                    },
                    {
                        "name": "Jichao He"
                    }
                ],
                "author_detail": {
                    "name": "Jichao He"
                },
                "author": "Jichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Application to insurance data, Methodology",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06920v2",
                "updated": "2025-07-10T03:12:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    12,
                    9,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T14:58:47Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    58,
                    47,
                    2,
                    190,
                    0
                ],
                "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing"
                },
                "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration."
                },
                "authors": [
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Maosong Cao"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Minnan Luo"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16434v2",
                "updated": "2025-07-09T14:47:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    50,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-19T16:13:53Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    16,
                    13,
                    53,
                    3,
                    170,
                    0
                ],
                "title": "Going beyond $S_8$: fast inference of the matter power spectrum from\n  weak-lensing surveys",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going beyond $S_8$: fast inference of the matter power spectrum from\n  weak-lensing surveys"
                },
                "summary": "Weak lensing surveys are often summarized by constraints on the derived\nparameter ${S_8\\equiv\\sigma_8\\sqrt{\\Omega_{\\rm m}/0.3}}$, obscuring the rich\nscale and redshift information encoded in the data, and limiting our ability to\nidentify the origin of any tensions with $\\Lambda$CDM predictions from the\ncosmic microwave background. In this work, we introduce a fast and flexible\nframework to extract the scale-dependent matter power spectrum $P(k, z)$ from\ncosmic shear and CMB lensing measurements, parameterizing deviations from the\nPlanck $\\Lambda$CDM prediction as a free function $\\alpha(k)$. Using public\ndata from DES Y3, KiDS-1000, HSC Y3, and ACT DR6, we constrain $\\alpha(k)$ with\nfast Hamiltonian Monte Carlo inference, employing multipoles up to $\\ell_{\\rm\nmax}\\sim2000$ for the galaxy lensing surveys. Our results show a consistent\n15-30% suppression in the matter power spectrum at intermediate scales ($k \\sim\n0.1-1{\\rm Mpc}^{-1}$) in galaxy-lensing data relative to a Planck $\\Lambda$CDM\nprediction with a CDM-only (no baryonic feedback) power spectrum, with combined\ntensions reaching up to $4\\sigma$. This is under a fixed cosmology and with\nanalytic marginalization over shear and redshift calibration uncertainties. In\ncontrast, ACT CMB lensing is consistent with $\\Lambda$CDM at ${k\\lesssim 0.1\n{\\rm Mpc}^{-1}}$. We validate our method using mock data, quantify consistency\nbetween datasets, and demonstrate how the resulting $\\alpha(k)$ likelihoods can\nbe used to test specific models for the power spectrum. All code, data\nproducts, and derived likelihoods are publicly released. Our results highlight\nthe importance of reporting lensing constraints on $P(k, z)$ and pave the way\nfor model-agnostic test of growth of structure with upcoming surveys such as\nLSST, Euclid, and Roman.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weak lensing surveys are often summarized by constraints on the derived\nparameter ${S_8\\equiv\\sigma_8\\sqrt{\\Omega_{\\rm m}/0.3}}$, obscuring the rich\nscale and redshift information encoded in the data, and limiting our ability to\nidentify the origin of any tensions with $\\Lambda$CDM predictions from the\ncosmic microwave background. In this work, we introduce a fast and flexible\nframework to extract the scale-dependent matter power spectrum $P(k, z)$ from\ncosmic shear and CMB lensing measurements, parameterizing deviations from the\nPlanck $\\Lambda$CDM prediction as a free function $\\alpha(k)$. Using public\ndata from DES Y3, KiDS-1000, HSC Y3, and ACT DR6, we constrain $\\alpha(k)$ with\nfast Hamiltonian Monte Carlo inference, employing multipoles up to $\\ell_{\\rm\nmax}\\sim2000$ for the galaxy lensing surveys. Our results show a consistent\n15-30% suppression in the matter power spectrum at intermediate scales ($k \\sim\n0.1-1{\\rm Mpc}^{-1}$) in galaxy-lensing data relative to a Planck $\\Lambda$CDM\nprediction with a CDM-only (no baryonic feedback) power spectrum, with combined\ntensions reaching up to $4\\sigma$. This is under a fixed cosmology and with\nanalytic marginalization over shear and redshift calibration uncertainties. In\ncontrast, ACT CMB lensing is consistent with $\\Lambda$CDM at ${k\\lesssim 0.1\n{\\rm Mpc}^{-1}}$. We validate our method using mock data, quantify consistency\nbetween datasets, and demonstrate how the resulting $\\alpha(k)$ likelihoods can\nbe used to test specific models for the power spectrum. All code, data\nproducts, and derived likelihoods are publicly released. Our results highlight\nthe importance of reporting lensing constraints on $P(k, z)$ and pave the way\nfor model-agnostic test of growth of structure with upcoming surveys such as\nLSST, Euclid, and Roman."
                },
                "authors": [
                    {
                        "name": "Cyrille Doux"
                    },
                    {
                        "name": "Tanvi Karwal"
                    }
                ],
                "author_detail": {
                    "name": "Tanvi Karwal"
                },
                "author": "Tanvi Karwal",
                "arxiv_comment": "The code, data, intermediate and final results, as well as a\n  reproducible notebook for further model testing, are available in the GitHub\n  repository at https://github.com/xuod/cls2pk . Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06910v1",
                "updated": "2025-07-09T14:47:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:47:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in\n  Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in\n  Dialogues"
                },
                "summary": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task."
                },
                "authors": [
                    {
                        "name": "Fareya Ikram"
                    },
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06909v1",
                "updated": "2025-07-09T14:47:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    0,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:47:00Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    0,
                    2,
                    190,
                    0
                ],
                "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction"
                },
                "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Diancheng Shui"
                    },
                    {
                        "name": "Zhiguang Han"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "Accepted by NLPCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05167v3",
                "updated": "2025-07-09T14:35:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    35,
                    23,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-07T18:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoLiMa: Long-Context Evaluation Beyond Literal Matching"
                },
                "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 13 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 11 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nEven models enhanced with reasoning capabilities or CoT prompting struggle to\nmaintain performance in long contexts. We publicly release the dataset and\nevaluation code at https://github.com/adobe-research/NoLiMa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 13 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 11 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nEven models enhanced with reasoning capabilities or CoT prompting struggle to\nmaintain performance in long contexts. We publicly release the dataset and\nevaluation code at https://github.com/adobe-research/NoLiMa."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06892v2",
                "updated": "2025-07-10T13:42:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    13,
                    42,
                    4,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T14:29:45Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    29,
                    45,
                    2,
                    190,
                    0
                ],
                "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model"
                },
                "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc."
                },
                "authors": [
                    {
                        "name": "Jing Liang"
                    },
                    {
                        "name": "Hongyao Tang"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Jinyi Liu"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "Preliminary version, v2, added more details and corrected some minor\n  mistakes. Project page: https://anitaleungxx.github.io/ReMix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20573v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20573v3",
                "updated": "2025-07-10T08:40:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    8,
                    40,
                    9,
                    3,
                    191,
                    0
                ],
                "published": "2025-06-25T16:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    16,
                    7,
                    59,
                    2,
                    176,
                    0
                ],
                "title": "LARP: Learner-Agnostic Robust Data Prefiltering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LARP: Learner-Agnostic Robust Data Prefiltering"
                },
                "summary": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread availability of large public datasets is a key factor behind\nthe recent successes of statistical inference and machine learning methods.\nHowever, these datasets often contain some low-quality or contaminated data, to\nwhich many learning procedures are sensitive. Therefore, the question of\nwhether and how public datasets should be prefiltered to facilitate accurate\ndownstream learning arises. On a technical level this requires the construction\nof principled data prefiltering methods which are learner-agnostic robust, in\nthe sense of provably protecting a set of pre-specified downstream learners\nfrom corrupted data. In this work, we formalize the problem of Learner-Agnostic\nRobust data Prefiltering (LARP), which aims at finding prefiltering procedures\nthat minimize a worst-case loss over a pre-specified set of learners. We first\ninstantiate our framework in the context of scalar mean estimation with Huber\nestimators under the Huber data contamination model. We provide a hardness\nresult on a specific problem instance and analyze several natural prefiltering\nprocedures. Our theoretical results indicate that performing LARP on a\nheterogeneous set of learners leads to some loss in model performance compared\nto the alternative of prefiltering data for each learner/use-case individually.\nWe explore the resulting utility loss and its dependence on the problem\nparameters via extensive experiments on real-world image and tabular data,\nobserving statistically significant reduction in utility. Finally, we model the\ntrade-off between the utility drop and the cost of repeated (learner-specific)\nprefiltering within a game-theoretic framework and showcase benefits of LARP\nfor large datasets."
                },
                "authors": [
                    {
                        "name": "Kristian Minchev"
                    },
                    {
                        "name": "Dimitar Iliev Dimitrov"
                    },
                    {
                        "name": "Nikola Konstantinov"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Konstantinov"
                },
                "author": "Nikola Konstantinov",
                "arxiv_comment": "Presented at ICML 2025 Workshop on DataWorld: Unifying Data Curation\n  Frameworks Across Domains",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20573v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20573v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06864v1",
                "updated": "2025-07-09T14:05:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    5,
                    13,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:05:13Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    5,
                    13,
                    2,
                    190,
                    0
                ],
                "title": "Toward Neurodivergent-Aware Productivity: A Systems and AI-Based\n  Human-in-the-Loop Framework for ADHD-Affected Professionals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Neurodivergent-Aware Productivity: A Systems and AI-Based\n  Human-in-the-Loop Framework for ADHD-Affected Professionals"
                },
                "summary": "Digital work environments in IT and knowledge-based sectors demand high\nlevels of attention management, task juggling, and self-regulation. For adults\nwith ADHD, these settings often amplify challenges such as time blindness,\ndigital distraction, emotional reactivity, and executive dysfunction. These\nindividuals prefer low-touch, easy-to-use interventions for daily tasks.\nConventional productivity tools often fail to support the cognitive variability\nand overload experienced by neurodivergent professionals. This paper presents a\nframework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and\nprivacy-first adaptive agents to support ADHD-affected users. The assistant\nsenses tab usage, application focus, and inactivity using on-device ML. These\ncues are used to infer attention states and deliver nudges, reflective prompts,\nor accountability-based presence (body doubling) that aid regulation without\ndisruption. Technically grounded in AI, the approach views attention as shaped\nby dynamic feedback loops. The result is a replicable model for adaptive,\ninclusive support tools in high-distraction work environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital work environments in IT and knowledge-based sectors demand high\nlevels of attention management, task juggling, and self-regulation. For adults\nwith ADHD, these settings often amplify challenges such as time blindness,\ndigital distraction, emotional reactivity, and executive dysfunction. These\nindividuals prefer low-touch, easy-to-use interventions for daily tasks.\nConventional productivity tools often fail to support the cognitive variability\nand overload experienced by neurodivergent professionals. This paper presents a\nframework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and\nprivacy-first adaptive agents to support ADHD-affected users. The assistant\nsenses tab usage, application focus, and inactivity using on-device ML. These\ncues are used to infer attention states and deliver nudges, reflective prompts,\nor accountability-based presence (body doubling) that aid regulation without\ndisruption. Technically grounded in AI, the approach views attention as shaped\nby dynamic feedback loops. The result is a replicable model for adaptive,\ninclusive support tools in high-distraction work environments."
                },
                "authors": [
                    {
                        "name": "Raghavendra Deshmukh"
                    }
                ],
                "author_detail": {
                    "name": "Raghavendra Deshmukh"
                },
                "author": "Raghavendra Deshmukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06857v1",
                "updated": "2025-07-09T13:58:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    58,
                    51,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:58:51Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    58,
                    51,
                    2,
                    190,
                    0
                ],
                "title": "Nonparametric Bayesian Inference for Stochastic Reaction-Diffusion\n  Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric Bayesian Inference for Stochastic Reaction-Diffusion\n  Equations"
                },
                "summary": "We consider the Bayesian nonparametric estimation of a nonlinear reaction\nfunction in a reaction-diffusion stochastic partial differential equation\n(SPDE). The likelihood is well-defined and tractable by the\ninfinite-dimensional Girsanov theorem, and the posterior distribution is\nanalysed in the growing domain asymptotic. Based on a Gaussian wavelet prior,\nthe contraction of the posterior distribution around the truth at the minimax\noptimal rate is proved. The analysis of the posterior distribution is\ncomplemented by a semiparametric Bernstein--von Mises theorem. The proofs rely\non the sub-Gaussian concentration of spatio-temporal averages of\ntransformations of the SPDE, which is derived by combining the Clark-Ocone\nformula with bounds for the derivatives of the (marginal) densities of the\nSPDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the Bayesian nonparametric estimation of a nonlinear reaction\nfunction in a reaction-diffusion stochastic partial differential equation\n(SPDE). The likelihood is well-defined and tractable by the\ninfinite-dimensional Girsanov theorem, and the posterior distribution is\nanalysed in the growing domain asymptotic. Based on a Gaussian wavelet prior,\nthe contraction of the posterior distribution around the truth at the minimax\noptimal rate is proved. The analysis of the posterior distribution is\ncomplemented by a semiparametric Bernstein--von Mises theorem. The proofs rely\non the sub-Gaussian concentration of spatio-temporal averages of\ntransformations of the SPDE, which is derived by combining the Clark-Ocone\nformula with bounds for the derivatives of the (marginal) densities of the\nSPDE."
                },
                "authors": [
                    {
                        "name": "Randolf Altmeyer"
                    },
                    {
                        "name": "Sascha Gaudlitz"
                    }
                ],
                "author_detail": {
                    "name": "Sascha Gaudlitz"
                },
                "author": "Sascha Gaudlitz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62G20, 60H15, secondary 60H07, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04204v2",
                "updated": "2025-07-09T13:58:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    58,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-05T15:18:55Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    15,
                    18,
                    55,
                    5,
                    95,
                    0
                ],
                "title": "Adaptive Elicitation of Latent Information Using Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Elicitation of Latent Information Using Natural Language"
                },
                "summary": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings."
                },
                "authors": [
                    {
                        "name": "Jimmy Wang"
                    },
                    {
                        "name": "Thomas Zollo"
                    },
                    {
                        "name": "Richard Zemel"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06854v1",
                "updated": "2025-07-09T13:58:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    58,
                    7,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:58:07Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    58,
                    7,
                    2,
                    190,
                    0
                ],
                "title": "Proof-Theoretic Functional Completeness for the Connexive Logic C",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proof-Theoretic Functional Completeness for the Connexive Logic C"
                },
                "summary": "We show the functional completeness for the connectives of the non-trivial\nnegation inconsistent logic C by using a well-established method implementing\npurely proof-theoretic notions only. Firstly, given that C contains a strong\nnegation, expressing a notion of direct refutation, the proof needs to be\napplied in a bilateralist way in that not only higher-order rule schemata for\nproofs but also for refutations need to be considered. Secondly, given that C\nis a connexive logic we need to take a connexive understanding of inference as\na basis, leading to a different conception of (higher-order) refutation than is\nusually employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show the functional completeness for the connectives of the non-trivial\nnegation inconsistent logic C by using a well-established method implementing\npurely proof-theoretic notions only. Firstly, given that C contains a strong\nnegation, expressing a notion of direct refutation, the proof needs to be\napplied in a bilateralist way in that not only higher-order rule schemata for\nproofs but also for refutations need to be considered. Secondly, given that C\nis a connexive logic we need to take a connexive understanding of inference as\na basis, leading to a different conception of (higher-order) refutation than is\nusually employed."
                },
                "authors": [
                    {
                        "name": "Sara Ayhan"
                    },
                    {
                        "name": "Hrafn Valtr Oddsson"
                    }
                ],
                "author_detail": {
                    "name": "Hrafn Valtr Oddsson"
                },
                "author": "Hrafn Valtr Oddsson",
                "arxiv_doi": "10.1007/s11225-025-10200-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11225-025-10200-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.06854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "For published version, see https://rdcu.be/evpKb",
                "arxiv_journal_ref": "Studia Logica (Published Online: 08 July 2025)",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06853v1",
                "updated": "2025-07-09T13:57:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    57,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:57:20Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    57,
                    20,
                    2,
                    190,
                    0
                ],
                "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models"
                },
                "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation."
                },
                "authors": [
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Yu Rong"
                    },
                    {
                        "name": "Tingyang Xu"
                    },
                    {
                        "name": "Zhenyi Zhong"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Pengju Wang"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v2",
                "updated": "2025-07-10T15:18:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    15,
                    18,
                    20,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06849v1",
                "updated": "2025-07-09T13:54:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    47,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:54:47Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    47,
                    2,
                    190,
                    0
                ],
                "title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural\n  Network Digital Predistortion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural\n  Network Digital Predistortion"
                },
                "summary": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD."
                },
                "authors": [
                    {
                        "name": "Yizhuo Wu"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Chang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Chang Gao"
                },
                "author": "Chang Gao",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03345v2",
                "updated": "2025-07-09T13:53:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    53,
                    54,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-05T10:15:12Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    10,
                    15,
                    12,
                    2,
                    64,
                    0
                ],
                "title": "Perturbation theory for post-Newtonian neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbation theory for post-Newtonian neutron stars"
                },
                "summary": "Neutron stars are compact, relativistic bodies that host several extremes of\nmodern physics. An exciting development in recent years has been the\nopportunity to probe this exotic physics by observing compact-binary\ncoalescences using sensitive gravitational-wave and electromagnetic\ninstruments. To maximise the science inferred from these measurements, we\nrequire models that accurately represent the physics. In this study, we\nconsider the post-Newtonian approximation to general relativity for the\nmodelling of neutron-star dynamics, with a particular view to model dynamical\ntides at the late stages of binary inspiral. We develop the post-Newtonian\nperturbation equations for a non-rotating star and show that the perturbation\nproblem is Hermitian and therefore derives from a fundamental Lagrangian.\nEstablishing this Lagrangian system leads to a conserved symplectic product and\ncanonical energy for the perturbations. We determine the orthogonality\ncondition for the post-Newtonian oscillation modes, which in turn forms the\nfoundation of a mode-sum representation often used for dynamical tides.\nFinally, we demonstrate that the perturbation formulation is unique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron stars are compact, relativistic bodies that host several extremes of\nmodern physics. An exciting development in recent years has been the\nopportunity to probe this exotic physics by observing compact-binary\ncoalescences using sensitive gravitational-wave and electromagnetic\ninstruments. To maximise the science inferred from these measurements, we\nrequire models that accurately represent the physics. In this study, we\nconsider the post-Newtonian approximation to general relativity for the\nmodelling of neutron-star dynamics, with a particular view to model dynamical\ntides at the late stages of binary inspiral. We develop the post-Newtonian\nperturbation equations for a non-rotating star and show that the perturbation\nproblem is Hermitian and therefore derives from a fundamental Lagrangian.\nEstablishing this Lagrangian system leads to a conserved symplectic product and\ncanonical energy for the perturbations. We determine the orthogonality\ncondition for the post-Newtonian oscillation modes, which in turn forms the\nfoundation of a mode-sum representation often used for dynamical tides.\nFinally, we demonstrate that the perturbation formulation is unique."
                },
                "authors": [
                    {
                        "name": "Fabian Gittins"
                    },
                    {
                        "name": "Nils Andersson"
                    },
                    {
                        "name": "Shanshan Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shanshan Yin"
                },
                "author": "Shanshan Yin",
                "arxiv_doi": "10.1088/1361-6382/ade83f",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6382/ade83f",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.03345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages. Accepted for publication in Class. Quantum Gravity",
                "arxiv_journal_ref": "Class. Quantum Gravity 42, 135014 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06848v1",
                "updated": "2025-07-09T13:53:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    53,
                    34,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:53:34Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    53,
                    34,
                    2,
                    190,
                    0
                ],
                "title": "Know Your Attention Maps: Class-specific Token Masking for Weakly\n  Supervised Semantic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Know Your Attention Maps: Class-specific Token Masking for Weakly\n  Supervised Semantic Segmentation"
                },
                "summary": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data."
                },
                "authors": [
                    {
                        "name": "Joelle Hanna"
                    },
                    {
                        "name": "Damian Borth"
                    }
                ],
                "author_detail": {
                    "name": "Damian Borth"
                },
                "author": "Damian Borth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15530v3",
                "updated": "2025-07-09T13:49:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    49,
                    37,
                    2,
                    190,
                    0
                ],
                "published": "2025-01-26T13:49:21Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    13,
                    49,
                    21,
                    6,
                    26,
                    0
                ],
                "title": "Constraints on fast radio burst population from the first CHIME/FRB\n  catalog with the Hierarchical Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraints on fast radio burst population from the first CHIME/FRB\n  catalog with the Hierarchical Bayesian Inference"
                },
                "summary": "Fast Radio Bursts (FRBs) have emerged as one of the most dynamic areas of\nresearch in astronomy and cosmology. Despite increasing number of FRBs have\nbeen reported, the exact origin of FRBs remains elusive. Investigating the\nintrinsic redshift distributions of FRBs could provide valuable insights into\ntheir possible origins and enhance the power of FRBs as a cosmological probe.\nIn this paper, we propose a hierarchical Bayesian inference approach combining\nwith several viable models to investigate the redshift distribution of the\nCHIME/FRB catalog 1. By utilizing this method, we aim to uncover the underlying\npatterns and characteristics of the FRB population, i.e. intrinsic redshift\ndistribution of FRB. Taking uncertainties within the observational data and\nselection effects into consideration, we obtained that the redshift\ndistribution of FRBs is significantly delayed with respect to that of the star\nformation history.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Radio Bursts (FRBs) have emerged as one of the most dynamic areas of\nresearch in astronomy and cosmology. Despite increasing number of FRBs have\nbeen reported, the exact origin of FRBs remains elusive. Investigating the\nintrinsic redshift distributions of FRBs could provide valuable insights into\ntheir possible origins and enhance the power of FRBs as a cosmological probe.\nIn this paper, we propose a hierarchical Bayesian inference approach combining\nwith several viable models to investigate the redshift distribution of the\nCHIME/FRB catalog 1. By utilizing this method, we aim to uncover the underlying\npatterns and characteristics of the FRB population, i.e. intrinsic redshift\ndistribution of FRB. Taking uncertainties within the observational data and\nselection effects into consideration, we obtained that the redshift\ndistribution of FRBs is significantly delayed with respect to that of the star\nformation history."
                },
                "authors": [
                    {
                        "name": "Huan Zhou"
                    },
                    {
                        "name": "Zhengxiang Li"
                    },
                    {
                        "name": "Zong-Hong Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Zong-Hong Zhu"
                },
                "author": "Zong-Hong Zhu",
                "arxiv_comment": "7 pages, 3 figures, 2 tables, Accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02579v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02579v3",
                "updated": "2025-07-09T13:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    45,
                    7,
                    2,
                    190,
                    0
                ],
                "published": "2025-05-05T11:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    30,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning"
                },
                "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including competing objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the fine-tuning to improve efficiency and\nflexibility. Our method is the first to aggregate the hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text classification models to score the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including competing objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the fine-tuning to improve efficiency and\nflexibility. Our method is the first to aggregate the hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text classification models to score the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."
                },
                "authors": [
                    {
                        "name": "Lingxiao Kong"
                    },
                    {
                        "name": "Cong Yang"
                    },
                    {
                        "name": "Susanne Neufang"
                    },
                    {
                        "name": "Oya Deniz Beyan"
                    },
                    {
                        "name": "Zeyd Boukhers"
                    }
                ],
                "author_detail": {
                    "name": "Zeyd Boukhers"
                },
                "author": "Zeyd Boukhers",
                "arxiv_comment": "14 pages, 9 figures, accepted by the SIGDIAL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02579v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02579v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06841v1",
                "updated": "2025-07-09T13:41:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    41,
                    52,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:41:52Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    41,
                    52,
                    2,
                    190,
                    0
                ],
                "title": "Measuring cosmic baryon density with FRB and GW data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring cosmic baryon density with FRB and GW data"
                },
                "summary": "The dispersion measure (DM) analysis of extragalactic fast radio bursts\n(FRBs) has accounted for all cosmic baryons but remains limited by systematic\nuncertainties from various parameter degeneracies. We show that the prominent\ndegeneracy between the baryon density ($\\Omega_{\\rm b}$) and the Hubble\nconstant ($H_0$) can be disentangled through an independent $H_0$ value\nuniquely inferred from the absolute luminosity distance measurements of\ngravitational-wave (GW) standard sirens. By combining $104$ localized FRBs with\n$47$ GW events, we obtain a robust, late-Universe measurement of \\ob\n$=0.0488\\pm0.0064$ ($1\\sigma$), in concordance with early-Universe observations\nof CMB + BBN. Notably, incorporating GW data helps not only avoid biases\ninduced by the $H_0$ tension, but also mitigate those from the parameters of\ndiffuse baryon fraction and DM distribution models. Although the current\nprecision ($\\sim 13\\%$) is limited by sample size, the growing detections of\nboth FRBs and GWs will make their synergy a powerful probe of low-redshift\ncosmology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dispersion measure (DM) analysis of extragalactic fast radio bursts\n(FRBs) has accounted for all cosmic baryons but remains limited by systematic\nuncertainties from various parameter degeneracies. We show that the prominent\ndegeneracy between the baryon density ($\\Omega_{\\rm b}$) and the Hubble\nconstant ($H_0$) can be disentangled through an independent $H_0$ value\nuniquely inferred from the absolute luminosity distance measurements of\ngravitational-wave (GW) standard sirens. By combining $104$ localized FRBs with\n$47$ GW events, we obtain a robust, late-Universe measurement of \\ob\n$=0.0488\\pm0.0064$ ($1\\sigma$), in concordance with early-Universe observations\nof CMB + BBN. Notably, incorporating GW data helps not only avoid biases\ninduced by the $H_0$ tension, but also mitigate those from the parameters of\ndiffuse baryon fraction and DM distribution models. Although the current\nprecision ($\\sim 13\\%$) is limited by sample size, the growing detections of\nboth FRBs and GWs will make their synergy a powerful probe of low-redshift\ncosmology."
                },
                "authors": [
                    {
                        "name": "Ji-Guo Zhang"
                    },
                    {
                        "name": "Ji-Yu Song"
                    },
                    {
                        "name": "Ze-Wei Zhao"
                    },
                    {
                        "name": "Wan-Peng Sun"
                    },
                    {
                        "name": "Jing-Fei Zhang"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06838v2",
                "updated": "2025-07-10T01:36:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    36,
                    33,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T13:35:36Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    35,
                    36,
                    2,
                    190,
                    0
                ],
                "title": "Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation"
                },
                "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR"
                },
                "authors": [
                    {
                        "name": "Dahyun Lee"
                    },
                    {
                        "name": "Yongrae Jo"
                    },
                    {
                        "name": "Haeju Park"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "Accepted to ACL 2025 main (Oral Presentation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06829v1",
                "updated": "2025-07-09T13:28:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    28,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:28:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    28,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal\n  Semantic Entropy-Guided Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal\n  Semantic Entropy-Guided Framework"
                },
                "summary": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy..."
                },
                "authors": [
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zexuan Qiu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "arxiv_comment": "13 pages, 5 fiures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11703v2",
                "updated": "2025-07-09T13:26:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    26,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-17T11:40:48Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    40,
                    48,
                    0,
                    48,
                    0
                ],
                "title": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation"
                },
                "summary": "Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepository https://github.com/YuY-2001/C-MQCIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepository https://github.com/YuY-2001/C-MQCIC."
                },
                "authors": [
                    {
                        "name": "Guangya Yu"
                    },
                    {
                        "name": "Yanhao Li"
                    },
                    {
                        "name": "Zongying Jiang"
                    },
                    {
                        "name": "Yuxiong Jin"
                    },
                    {
                        "name": "Li Dai"
                    },
                    {
                        "name": "Yupian Lin"
                    },
                    {
                        "name": "Ruihui Hou"
                    },
                    {
                        "name": "Weiyan Zhang"
                    },
                    {
                        "name": "Yongqi Fan"
                    },
                    {
                        "name": "Qi Ye"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Tong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Tong Ruan"
                },
                "author": "Tong Ruan",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15167v2",
                "updated": "2025-07-09T13:20:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    20,
                    45,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-18T06:28:22Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    28,
                    22,
                    2,
                    169,
                    0
                ],
                "title": "LLM Agent for Hyper-Parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agent for Hyper-Parameter Optimization"
                },
                "summary": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters optimization\napproaches for Warm-Start Particles Swarm Optimization with Crossover and\nMutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial\nvehicle (UAV) trajectory and communication, are primarily heuristic-based,\nexhibiting low levels of automation and improvable performance. In this paper,\nwe design an Large Language Model (LLM) agent for automatic\nhyper-parameters-tuning, where an iterative framework and Model Context\nProtocol (MCP) are applied. In particular, the LLM agent is first set up via a\nprofile, which specifies the boundary of hyper-parameters, task objective,\nterminal condition, conservative or aggressive strategy of optimizing\nhyper-parameters, and LLM configurations. Then, the LLM agent iteratively\ninvokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the\nloop based on the terminal condition and returns an optimized set of\nhyperparameters. Our experiment results show that the minimal sum-rate achieved\nby hyper-parameters generated via our LLM agent is significantly higher than\nthose by both human heuristics and random generation methods. This indicates\nthat an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in\nseeking high-performance hyper-parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters optimization\napproaches for Warm-Start Particles Swarm Optimization with Crossover and\nMutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial\nvehicle (UAV) trajectory and communication, are primarily heuristic-based,\nexhibiting low levels of automation and improvable performance. In this paper,\nwe design an Large Language Model (LLM) agent for automatic\nhyper-parameters-tuning, where an iterative framework and Model Context\nProtocol (MCP) are applied. In particular, the LLM agent is first set up via a\nprofile, which specifies the boundary of hyper-parameters, task objective,\nterminal condition, conservative or aggressive strategy of optimizing\nhyper-parameters, and LLM configurations. Then, the LLM agent iteratively\ninvokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the\nloop based on the terminal condition and returns an optimized set of\nhyperparameters. Our experiment results show that the minimal sum-rate achieved\nby hyper-parameters generated via our LLM agent is significantly higher than\nthose by both human heuristics and random generation methods. This indicates\nthat an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in\nseeking high-performance hyper-parameters."
                },
                "authors": [
                    {
                        "name": "Wanzhe Wang"
                    },
                    {
                        "name": "Jianqiu Peng"
                    },
                    {
                        "name": "Menghao Hu"
                    },
                    {
                        "name": "Weihuang Zhong"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Wanli Ni"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ni"
                },
                "author": "Wanli Ni",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03933v2",
                "updated": "2025-07-09T13:14:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    14,
                    29,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-05T07:36:49Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    7,
                    36,
                    49,
                    5,
                    186,
                    0
                ],
                "title": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual\n  Large Language Models"
                },
                "summary": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to offload the task of writing itself\nto these technologies, enabling them to change our linguistic ecosystem more\ndirectly. While they provide us quick access to information and impressively\nfluent output, beneath their apparent sophistication lies a subtle, more\ninsidious threat: the gradual decline and loss of linguistic diversity. With\nthis opinion piece, I explore how model collapse, with a particular focus on\ntranslation technology, can lead to the loss of linguistic forms, grammatical\nfeatures, and cultural nuance. Model collapse refers to the eventual\nconsequence of self-consuming training loops, where models reinforce their own\nbiases and lose linguistic diversity. Drawing on recent work in Computer\nVision, Natural Language Processing (NLP) and Machine Translation (MT), I argue\nthat the tails of our linguistic distributions are vanishing, and with them,\nthe narratives and identities they carry. This is a call to resist linguistic\nflattening and to reimagine NLP as a field that encourages, values and protects\nexpressive multilingual lexical and linguistic diversity and creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to offload the task of writing itself\nto these technologies, enabling them to change our linguistic ecosystem more\ndirectly. While they provide us quick access to information and impressively\nfluent output, beneath their apparent sophistication lies a subtle, more\ninsidious threat: the gradual decline and loss of linguistic diversity. With\nthis opinion piece, I explore how model collapse, with a particular focus on\ntranslation technology, can lead to the loss of linguistic forms, grammatical\nfeatures, and cultural nuance. Model collapse refers to the eventual\nconsequence of self-consuming training loops, where models reinforce their own\nbiases and lose linguistic diversity. Drawing on recent work in Computer\nVision, Natural Language Processing (NLP) and Machine Translation (MT), I argue\nthat the tails of our linguistic distributions are vanishing, and with them,\nthe narratives and identities they carry. This is a call to resist linguistic\nflattening and to reimagine NLP as a field that encourages, values and protects\nexpressive multilingual lexical and linguistic diversity and creativity."
                },
                "authors": [
                    {
                        "name": "Eva Vanmassenhove"
                    }
                ],
                "author_detail": {
                    "name": "Eva Vanmassenhove"
                },
                "author": "Eva Vanmassenhove",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09347v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09347v3",
                "updated": "2025-07-09T13:09:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    9,
                    13,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-12T12:49:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "arxiv_comment": "9 pages, ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09347v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09347v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2007.14245v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2007.14245v4",
                "updated": "2025-07-09T13:07:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    7,
                    0,
                    2,
                    190,
                    0
                ],
                "published": "2020-07-11T21:43:20Z",
                "published_parsed": [
                    2020,
                    7,
                    11,
                    21,
                    43,
                    20,
                    5,
                    193,
                    0
                ],
                "title": "Bayesian Multi-Scale Neural Network for Crowd Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Multi-Scale Neural Network for Crowd Counting"
                },
                "summary": "Crowd counting is a challenging yet critical task in computer vision with\napplications ranging from public safety to urban planning. Recent advances\nusing Convolutional Neural Networks (CNNs) that estimate density maps have\nshown significant success. However, accurately counting individuals in highly\ncongested scenes remains an open problem due to severe occlusions, scale\nvariations, and perspective distortions, where people appear at drastically\ndifferent sizes across the image. In this work, we propose a novel deep\nlearning architecture that effectively addresses these challenges. Our network\nintegrates a ResNet-based feature extractor for capturing rich hierarchical\nrepresentations, followed by a downsampling block employing dilated\nconvolutions to preserve spatial resolution while expanding the receptive\nfield. An upsampling block using transposed convolutions reconstructs the\nhigh-resolution density map. Central to our architecture is a novel\nPerspective-aware Aggregation Module (PAM) designed to enhance robustness to\nscale and perspective variations by adaptively aggregating multi-scale\ncontextual information. We detail the training procedure, including the loss\nfunctions and optimization strategies used. Our method is evaluated on three\nwidely used benchmark datasets using Mean Absolute Error (MAE) and Mean Squared\nError (MSE) as evaluation metrics. Experimental results demonstrate that our\nmodel achieves superior performance compared to existing state-of-the-art\nmethods. Additionally, we incorporate principled Bayesian inference techniques\nto provide uncertainty estimates along with the crowd count predictions,\noffering a measure of confidence in the model's outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crowd counting is a challenging yet critical task in computer vision with\napplications ranging from public safety to urban planning. Recent advances\nusing Convolutional Neural Networks (CNNs) that estimate density maps have\nshown significant success. However, accurately counting individuals in highly\ncongested scenes remains an open problem due to severe occlusions, scale\nvariations, and perspective distortions, where people appear at drastically\ndifferent sizes across the image. In this work, we propose a novel deep\nlearning architecture that effectively addresses these challenges. Our network\nintegrates a ResNet-based feature extractor for capturing rich hierarchical\nrepresentations, followed by a downsampling block employing dilated\nconvolutions to preserve spatial resolution while expanding the receptive\nfield. An upsampling block using transposed convolutions reconstructs the\nhigh-resolution density map. Central to our architecture is a novel\nPerspective-aware Aggregation Module (PAM) designed to enhance robustness to\nscale and perspective variations by adaptively aggregating multi-scale\ncontextual information. We detail the training procedure, including the loss\nfunctions and optimization strategies used. Our method is evaluated on three\nwidely used benchmark datasets using Mean Absolute Error (MAE) and Mean Squared\nError (MSE) as evaluation metrics. Experimental results demonstrate that our\nmodel achieves superior performance compared to existing state-of-the-art\nmethods. Additionally, we incorporate principled Bayesian inference techniques\nto provide uncertainty estimates along with the crowd count predictions,\noffering a measure of confidence in the model's outputs."
                },
                "authors": [
                    {
                        "name": "Abhinav Sagar"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Sagar"
                },
                "author": "Abhinav Sagar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2007.14245v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2007.14245v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06803v1",
                "updated": "2025-07-09T12:44:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    44,
                    49,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T12:44:49Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    44,
                    49,
                    2,
                    190,
                    0
                ],
                "title": "Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams"
                },
                "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only."
                },
                "authors": [
                    {
                        "name": "Matthew Anderson Hendricks"
                    },
                    {
                        "name": "Alice Cicirello"
                    }
                ],
                "author_detail": {
                    "name": "Alice Cicirello"
                },
                "author": "Alice Cicirello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04417v2",
                "updated": "2025-07-09T12:33:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    33,
                    51,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-06T15:13:31Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    13,
                    31,
                    6,
                    187,
                    0
                ],
                "title": "Neural Networks for Tamed Milstein Approximation of SDEs with Additive\n  Symmetric Jump Noise Driven by a Poisson Random Measure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Networks for Tamed Milstein Approximation of SDEs with Additive\n  Symmetric Jump Noise Driven by a Poisson Random Measure"
                },
                "summary": "This work aims to estimate the drift and diffusion functions in stochastic\ndifferential equations (SDEs) driven by a particular class of L\\'evy processes\nwith finite jump intensity, using neural networks. We propose a framework that\nintegrates the Tamed-Milstein scheme with neural networks employed as\nnon-parametric function approximators. Estimation is carried out in a\nnon-parametric fashion for the drift function $f: \\mathbb{Z} \\to \\mathbb{R}$,\nthe diffusion coefficient $g: \\mathbb{Z} \\to \\mathbb{R}$. The model of interest\nis given by \\[ dX(t) = \\xi + f(X(t))\\, dt + g(X(t))\\, dW_t + \\gamma\n\\int_{\\mathbb{Z}} z\\, N(dt,dz), \\] where $W_t$ is a standard Brownian motion,\nand $N(dt,dz)$ is a Poisson random measure on $(\\mathbb{R}_{+} \\times\n\\mathbb{Z}$, $\\mathcal{B} (\\mathbb{R}_{+}) \\otimes \\mathcal{Z}$, $\\lambda(\n\\Lambda \\otimes v))$, with $\\lambda, \\gamma > 0$, $\\Lambda$ being the Lebesgue\nmeasure on $\\mathbb{R}_{+}$, and $v$ a finite measure on the measurable space\n$(\\mathbb{Z}, \\mathcal{Z})$. Neural networks are used as non-parametric\nfunction approximators, enabling the modeling of complex nonlinear dynamics\nwithout assuming restrictive functional forms. The proposed methodology\nconstitutes a flexible alternative for inference in systems with\nstate-dependent noise and discontinuities driven by L\\'evy processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work aims to estimate the drift and diffusion functions in stochastic\ndifferential equations (SDEs) driven by a particular class of L\\'evy processes\nwith finite jump intensity, using neural networks. We propose a framework that\nintegrates the Tamed-Milstein scheme with neural networks employed as\nnon-parametric function approximators. Estimation is carried out in a\nnon-parametric fashion for the drift function $f: \\mathbb{Z} \\to \\mathbb{R}$,\nthe diffusion coefficient $g: \\mathbb{Z} \\to \\mathbb{R}$. The model of interest\nis given by \\[ dX(t) = \\xi + f(X(t))\\, dt + g(X(t))\\, dW_t + \\gamma\n\\int_{\\mathbb{Z}} z\\, N(dt,dz), \\] where $W_t$ is a standard Brownian motion,\nand $N(dt,dz)$ is a Poisson random measure on $(\\mathbb{R}_{+} \\times\n\\mathbb{Z}$, $\\mathcal{B} (\\mathbb{R}_{+}) \\otimes \\mathcal{Z}$, $\\lambda(\n\\Lambda \\otimes v))$, with $\\lambda, \\gamma > 0$, $\\Lambda$ being the Lebesgue\nmeasure on $\\mathbb{R}_{+}$, and $v$ a finite measure on the measurable space\n$(\\mathbb{Z}, \\mathcal{Z})$. Neural networks are used as non-parametric\nfunction approximators, enabling the modeling of complex nonlinear dynamics\nwithout assuming restrictive functional forms. The proposed methodology\nconstitutes a flexible alternative for inference in systems with\nstate-dependent noise and discontinuities driven by L\\'evy processes."
                },
                "authors": [
                    {
                        "name": "Jose-Hermenegildo Ramirez-Gonzalez"
                    },
                    {
                        "name": "Ying Sun"
                    }
                ],
                "author_detail": {
                    "name": "Ying Sun"
                },
                "author": "Ying Sun",
                "arxiv_comment": "14 pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60H10, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06795v2",
                "updated": "2025-07-10T07:05:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    7,
                    5,
                    41,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T12:30:42Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    30,
                    42,
                    2,
                    190,
                    0
                ],
                "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining"
                },
                "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment."
                },
                "authors": [
                    {
                        "name": "Seonwu Kim"
                    },
                    {
                        "name": "Yohan Na"
                    },
                    {
                        "name": "Kihun Kim"
                    },
                    {
                        "name": "Hanhee Cho"
                    },
                    {
                        "name": "Geun Lim"
                    },
                    {
                        "name": "Mintae Kim"
                    },
                    {
                        "name": "Seongik Park"
                    },
                    {
                        "name": "Ki Hyun Kim"
                    },
                    {
                        "name": "Youngsub Han"
                    },
                    {
                        "name": "Byoung-Ki Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Ki Jeon"
                },
                "author": "Byoung-Ki Jeon",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16903v2",
                "updated": "2025-07-09T12:13:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    13,
                    12,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-24T06:57:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    57,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of\n  In-the-wild LLM Jailbreak Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of\n  In-the-wild LLM Jailbreak Methods"
                },
                "summary": "Despite the growing interest in jailbreak methods as an effective red-teaming\ntool for building safe and responsible large language models (LLMs), flawed\nevaluation system designs have led to significant discrepancies in their\neffectiveness assessments. We conduct a systematic measurement study based on\n37 jailbreak studies since 2022, focusing on both the methods and the\nevaluation systems they employ. We find that existing evaluation systems lack\ncase-specific criteria, resulting in misleading conclusions about their\neffectiveness and safety implications. This paper advocates a shift to a more\nnuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel\nbenchmark comprising a curated harmful question dataset, detailed case-by-case\nevaluation guidelines and an evaluation system integrated with these guidelines\n-- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate\nmeasurements of jailbreak performance, enabling meaningful comparisons across\nmethods and uncovering new insights overlooked in previous evaluations.\nGuidedEval reduces inter-evaluator variance by at least 76.03\\%. Furthermore,\nwe observe that incorporating guidelines can enhance the effectiveness of\njailbreak methods themselves, offering new insights into both attack strategies\nand evaluation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing interest in jailbreak methods as an effective red-teaming\ntool for building safe and responsible large language models (LLMs), flawed\nevaluation system designs have led to significant discrepancies in their\neffectiveness assessments. We conduct a systematic measurement study based on\n37 jailbreak studies since 2022, focusing on both the methods and the\nevaluation systems they employ. We find that existing evaluation systems lack\ncase-specific criteria, resulting in misleading conclusions about their\neffectiveness and safety implications. This paper advocates a shift to a more\nnuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel\nbenchmark comprising a curated harmful question dataset, detailed case-by-case\nevaluation guidelines and an evaluation system integrated with these guidelines\n-- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate\nmeasurements of jailbreak performance, enabling meaningful comparisons across\nmethods and uncovering new insights overlooked in previous evaluations.\nGuidedEval reduces inter-evaluator variance by at least 76.03\\%. Furthermore,\nwe observe that incorporating guidelines can enhance the effectiveness of\njailbreak methods themselves, offering new insights into both attack strategies\nand evaluation paradigms."
                },
                "authors": [
                    {
                        "name": "Ruixuan Huang"
                    },
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06780v1",
                "updated": "2025-07-09T12:11:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    11,
                    27,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T12:11:27Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    11,
                    27,
                    2,
                    190,
                    0
                ],
                "title": "Learning safe, constrained policies via imitation learning: Connection\n  to Probabilistic Inference and a Naive Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning safe, constrained policies via imitation learning: Connection\n  to Probabilistic Inference and a Naive Algorithm"
                },
                "summary": "This article introduces an imitation learning method for learning maximum\nentropy policies that comply with constraints demonstrated by expert\ntrajectories executing a task. The formulation of the method takes advantage of\nresults connecting performance to bounds for the KL-divergence between\ndemonstrated and learned policies, and its objective is rigorously justified\nthrough a connection to a probabilistic inference framework for reinforcement\nlearning, incorporating the reinforcement learning objective and the objective\nto abide by constraints in an entropy maximization setting. The proposed\nalgorithm optimizes the learning objective with dual gradient descent,\nsupporting effective and stable training. Experiments show that the proposed\nmethod can learn effective policy models for constraints-abiding behaviour, in\nsettings with multiple constraints of different types, accommodating different\nmodalities of demonstrated behaviour, and with abilities to generalize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces an imitation learning method for learning maximum\nentropy policies that comply with constraints demonstrated by expert\ntrajectories executing a task. The formulation of the method takes advantage of\nresults connecting performance to bounds for the KL-divergence between\ndemonstrated and learned policies, and its objective is rigorously justified\nthrough a connection to a probabilistic inference framework for reinforcement\nlearning, incorporating the reinforcement learning objective and the objective\nto abide by constraints in an entropy maximization setting. The proposed\nalgorithm optimizes the learning objective with dual gradient descent,\nsupporting effective and stable training. Experiments show that the proposed\nmethod can learn effective policy models for constraints-abiding behaviour, in\nsettings with multiple constraints of different types, accommodating different\nmodalities of demonstrated behaviour, and with abilities to generalize."
                },
                "authors": [
                    {
                        "name": "George Papadopoulos"
                    },
                    {
                        "name": "George A. Vouros"
                    }
                ],
                "author_detail": {
                    "name": "George A. Vouros"
                },
                "author": "George A. Vouros",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06776v1",
                "updated": "2025-07-09T12:05:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    5,
                    31,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T12:05:31Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    5,
                    31,
                    2,
                    190,
                    0
                ],
                "title": "Bayesian Generalized Nonlinear Models Offer Basis Free SINDy With Model\n  Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Generalized Nonlinear Models Offer Basis Free SINDy With Model\n  Uncertainty"
                },
                "summary": "Sparse Identification of Nonlinear Dynamics (SINDy) has become a standard\nmethodology for inferring governing equations of dynamical systems from\nobserved data using statistical modeling. However, classical SINDy approaches\nrely on predefined libraries of candidate functions to model nonlinearities,\nwhich limits flexibility and excludes robust uncertainty quantification. This\npaper proposes Bayesian Generalized Nonlinear Models (BGNLMs) as a principled\nalternative for more flexible statistical modeling. BGNLMs employ\nspike-and-slab priors combined with binary inclusion indicators to\nautomatically discover relevant nonlinearities without predefined basis\nfunctions. Moreover, BGNLMs quantify uncertainty in selected bases and final\nmodel predictions, enabling robust exploration of the model space. In this\npaper, the BGNLM framework is applied to several three-dimensional (3D) SINDy\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Identification of Nonlinear Dynamics (SINDy) has become a standard\nmethodology for inferring governing equations of dynamical systems from\nobserved data using statistical modeling. However, classical SINDy approaches\nrely on predefined libraries of candidate functions to model nonlinearities,\nwhich limits flexibility and excludes robust uncertainty quantification. This\npaper proposes Bayesian Generalized Nonlinear Models (BGNLMs) as a principled\nalternative for more flexible statistical modeling. BGNLMs employ\nspike-and-slab priors combined with binary inclusion indicators to\nautomatically discover relevant nonlinearities without predefined basis\nfunctions. Moreover, BGNLMs quantify uncertainty in selected bases and final\nmodel predictions, enabling robust exploration of the model space. In this\npaper, the BGNLM framework is applied to several three-dimensional (3D) SINDy\nproblems."
                },
                "authors": [
                    {
                        "name": "Aliaksandr Hubin"
                    }
                ],
                "author_detail": {
                    "name": "Aliaksandr Hubin"
                },
                "author": "Aliaksandr Hubin",
                "arxiv_comment": "Published in Proceedings of the 39th International Workshop on\n  Statistical Modelling,\n  https://iwsm2025.ie/wp-content/uploads/2025/07/IWSM2025_Limerick_Proceedings.pdf",
                "arxiv_journal_ref": "https://iwsm2025.ie/wp-content/uploads/2025/07/IWSM2025_Limerick_Proceedings.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62-02, 62-09, 62F07, 62F15, 62J12, 62J05, 62J99, 62M05, 05A16,\n  60J22, 92D20, 90C27, 90C59",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.2; G.1.6; G.2.1; G.3; I.2.0; I.2.6; I.2.8; I.5.1; I.6; I.6.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20668v2",
                "updated": "2025-07-09T12:04:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    4,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-05-27T03:35:54Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    3,
                    35,
                    54,
                    1,
                    147,
                    0
                ],
                "title": "Eigenstructure inference for high-dimensional covariance with\n  generalized shrinkage inverse-Wishart prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigenstructure inference for high-dimensional covariance with\n  generalized shrinkage inverse-Wishart prior"
                },
                "summary": "In multivariate statistics, estimating the covariance matrix is essential for\nunderstanding the interdependence among variables. In high-dimensional\nsettings, where the number of covariates increases with the sample size, it is\nwell known that the eigenstructure of the sample covariance matrix is\ninconsistent. The inverse-Wishart prior, a standard choice for covariance\nestimation in Bayesian inference, also suffers from posterior inconsistency. To\naddress the issue of eigenvalue dispersion in high-dimensional settings, the\nshrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its\nconceptual appeal and empirical success, the asymptotic justification for the\nSIW prior has remained limited. In this paper, we propose a generalized\nshrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance\nmodeling. By extending the SIW framework, the gSIW prior accommodates a broader\nclass of prior distributions and facilitates the derivation of theoretical\nproperties under specific parameter choices. In particular, under the spiked\ncovariance assumption, we establish the asymptotic behavior of the posterior\ndistribution for both eigenvalues and eigenvectors by directly evaluating the\nposterior expectations for two sets of parameter choices. This direct\nevaluation provides insights into the large-sample behavior of the posterior\nthat cannot be obtained through general posterior asymptotic theorems. Finally,\nsimulation studies illustrate that the proposed prior provides accurate\nestimation of the eigenstructure, particularly for spiked eigenvalues,\nachieving narrower credible intervals and higher coverage probabilities\ncompared to existing methods. For spiked eigenvectors, the performance is\ngenerally comparable to that of competing approaches, including the sample\ncovariance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multivariate statistics, estimating the covariance matrix is essential for\nunderstanding the interdependence among variables. In high-dimensional\nsettings, where the number of covariates increases with the sample size, it is\nwell known that the eigenstructure of the sample covariance matrix is\ninconsistent. The inverse-Wishart prior, a standard choice for covariance\nestimation in Bayesian inference, also suffers from posterior inconsistency. To\naddress the issue of eigenvalue dispersion in high-dimensional settings, the\nshrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its\nconceptual appeal and empirical success, the asymptotic justification for the\nSIW prior has remained limited. In this paper, we propose a generalized\nshrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance\nmodeling. By extending the SIW framework, the gSIW prior accommodates a broader\nclass of prior distributions and facilitates the derivation of theoretical\nproperties under specific parameter choices. In particular, under the spiked\ncovariance assumption, we establish the asymptotic behavior of the posterior\ndistribution for both eigenvalues and eigenvectors by directly evaluating the\nposterior expectations for two sets of parameter choices. This direct\nevaluation provides insights into the large-sample behavior of the posterior\nthat cannot be obtained through general posterior asymptotic theorems. Finally,\nsimulation studies illustrate that the proposed prior provides accurate\nestimation of the eigenstructure, particularly for spiked eigenvalues,\nachieving narrower credible intervals and higher coverage probabilities\ncompared to existing methods. For spiked eigenvectors, the performance is\ngenerally comparable to that of competing approaches, including the sample\ncovariance."
                },
                "authors": [
                    {
                        "name": "Seongmin Kim"
                    },
                    {
                        "name": "Kwangmin Lee"
                    },
                    {
                        "name": "Sewon Park"
                    },
                    {
                        "name": "Jaeyong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeyong Lee"
                },
                "author": "Jaeyong Lee",
                "arxiv_comment": "51 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12, 62H12 (Primary) 62F15, 60B20 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06774v1",
                "updated": "2025-07-09T12:03:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    3,
                    6,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T12:03:06Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    3,
                    6,
                    2,
                    190,
                    0
                ],
                "title": "Checklist Engineering Empowers Multilingual LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checklist Engineering Empowers Multilingual LLM Judges"
                },
                "summary": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghiasvand Mohammadkhani"
                    },
                    {
                        "name": "Hamid Beigy"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Beigy"
                },
                "author": "Hamid Beigy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04446v2",
                "updated": "2025-07-09T11:52:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    52,
                    25,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-06T16:13:33Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    16,
                    13,
                    33,
                    6,
                    187,
                    0
                ],
                "title": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient\n  LLM Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient\n  LLM Jailbreaking"
                },
                "summary": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety."
                },
                "authors": [
                    {
                        "name": "Tim Beyer"
                    },
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gnnemann"
                },
                "author": "Stephan Gnnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06762v1",
                "updated": "2025-07-09T11:38:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    38,
                    53,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T11:38:53Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    38,
                    53,
                    2,
                    190,
                    0
                ],
                "title": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation"
                },
                "summary": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos."
                },
                "authors": [
                    {
                        "name": "Nathalia Barbosa"
                    },
                    {
                        "name": "Paulo Borba"
                    },
                    {
                        "name": "Luson Da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Luson Da Silva"
                },
                "arxiv_affiliation": "Polytechnique Montreal, Canad",
                "author": "Luson Da Silva",
                "arxiv_comment": "Comments: 11 pages, in Portuguese language. 3 figures. Submitted to\n  SAST 2025 (X Simp\\'osio Brasileiro de Teste de Software Sistem\\'atico e\n  Automatizado)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03498v2",
                "updated": "2025-07-09T11:30:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    30,
                    58,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-04T11:52:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    11,
                    52,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Reinforcement Learning-based Feature Generation Algorithm for Scientific\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning-based Feature Generation Algorithm for Scientific\n  Data"
                },
                "summary": "Feature generation (FG) aims to enhance the prediction potential of original\ndata by constructing high-order feature combinations and removing redundant\nfeatures. It is a key preprocessing step for tabular scientific data to improve\ndownstream machine-learning model performance. Traditional methods face the\nfollowing two challenges when dealing with the feature generation of scientific\ndata: First, the effective construction of high-order feature combinations in\nscientific data necessitates profound and extensive domain-specific expertise.\nSecondly, as the order of feature combinations increases, the search space\nexpands exponentially, imposing prohibitive human labor consumption.\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\nopened novel avenues for automating feature generation processes. Inspired by\nthat, this paper revisits the conventional feature generation workflow and\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\nthe iterative exploration stage, multi-agents will construct mathematical\ntransformation equations collaboratively, synthesize and identify feature\ncombinations ex-hibiting high information content, and leverage a reinforcement\nlearning mechanism to evolve their strategies. Upon completing the exploration\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\nevaluate the generated features of each significant model performance\nbreakthrough. Experimental results and case studies consistently demonstrate\nthat the MAFG framework effectively automates the feature generation process\nand significantly enhances various downstream scientific data mining tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature generation (FG) aims to enhance the prediction potential of original\ndata by constructing high-order feature combinations and removing redundant\nfeatures. It is a key preprocessing step for tabular scientific data to improve\ndownstream machine-learning model performance. Traditional methods face the\nfollowing two challenges when dealing with the feature generation of scientific\ndata: First, the effective construction of high-order feature combinations in\nscientific data necessitates profound and extensive domain-specific expertise.\nSecondly, as the order of feature combinations increases, the search space\nexpands exponentially, imposing prohibitive human labor consumption.\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\nopened novel avenues for automating feature generation processes. Inspired by\nthat, this paper revisits the conventional feature generation workflow and\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\nthe iterative exploration stage, multi-agents will construct mathematical\ntransformation equations collaboratively, synthesize and identify feature\ncombinations ex-hibiting high information content, and leverage a reinforcement\nlearning mechanism to evolve their strategies. Upon completing the exploration\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\nevaluate the generated features of each significant model performance\nbreakthrough. Experimental results and case studies consistently demonstrate\nthat the MAFG framework effectively automates the feature generation process\nand significantly enhances various downstream scientific data mining tasks."
                },
                "authors": [
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Junfeng Zhou"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "12 pages, in Chinese language, accepted by Journal of Computer\n  Research and Development",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08724v2",
                "updated": "2025-07-09T11:28:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    28,
                    46,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-10T23:54:41Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    23,
                    54,
                    41,
                    0,
                    69,
                    0
                ],
                "title": "Direct Flow Simulations with Implicit Neural Representation of Complex\n  Geometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Flow Simulations with Implicit Neural Representation of Complex\n  Geometry"
                },
                "summary": "Implicit neural representations have emerged as a powerful approach for\nencoding complex geometries as continuous functions. These implicit models are\nwidely used in computer vision and 3D content creation, but their integration\ninto scientific computing workflows, such as finite element or finite volume\nsimulations, remains limited. One reason is that conventional simulation\npipelines require explicit geometric inputs (meshes), forcing INR-based shapes\nto be converted to meshes--a step that introduces approximation errors,\ncomputational overhead, and significant manual effort. Immersed boundary\nmethods partially alleviate this issue by allowing simulations on background\ngrids without body-fitted meshes. However, they still require an explicit\nboundary description and can suffer from numerical artifacts, such as sliver\ncut cells. The shifted boundary method (SBM) eliminates the need for explicit\ngeometry by using grid-aligned surrogate boundaries, making it inherently\ncompatible with implicit shape representations. Here, we present a framework\nthat directly couples neural implicit geometries with SBM to perform\nhigh-fidelity fluid flow simulations without any intermediate mesh generation.\nBy leveraging neural network inference, our approach computes the surrogate\nboundary and distance vectors required by SBM on-the-fly directly from the INR,\nthus completely bypassing traditional geometry processing. We demonstrate this\napproach on canonical 2D and 3D flow benchmarks (lid-driven cavity flows) and\ncomplex geometries (gyroids, the Stanford bunny, and AI-generated shapes),\nachieving simulation accuracy comparable to conventional mesh-based methods.\nThis work highlights a novel pathway for integrating AI-driven geometric\nrepresentations into computational physics, establishing INRs as a versatile\nand scalable tool for simulations and removing a long-standing bottleneck in\ngeometry handling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations have emerged as a powerful approach for\nencoding complex geometries as continuous functions. These implicit models are\nwidely used in computer vision and 3D content creation, but their integration\ninto scientific computing workflows, such as finite element or finite volume\nsimulations, remains limited. One reason is that conventional simulation\npipelines require explicit geometric inputs (meshes), forcing INR-based shapes\nto be converted to meshes--a step that introduces approximation errors,\ncomputational overhead, and significant manual effort. Immersed boundary\nmethods partially alleviate this issue by allowing simulations on background\ngrids without body-fitted meshes. However, they still require an explicit\nboundary description and can suffer from numerical artifacts, such as sliver\ncut cells. The shifted boundary method (SBM) eliminates the need for explicit\ngeometry by using grid-aligned surrogate boundaries, making it inherently\ncompatible with implicit shape representations. Here, we present a framework\nthat directly couples neural implicit geometries with SBM to perform\nhigh-fidelity fluid flow simulations without any intermediate mesh generation.\nBy leveraging neural network inference, our approach computes the surrogate\nboundary and distance vectors required by SBM on-the-fly directly from the INR,\nthus completely bypassing traditional geometry processing. We demonstrate this\napproach on canonical 2D and 3D flow benchmarks (lid-driven cavity flows) and\ncomplex geometries (gyroids, the Stanford bunny, and AI-generated shapes),\nachieving simulation accuracy comparable to conventional mesh-based methods.\nThis work highlights a novel pathway for integrating AI-driven geometric\nrepresentations into computational physics, establishing INRs as a versatile\nand scalable tool for simulations and removing a long-standing bottleneck in\ngeometry handling."
                },
                "authors": [
                    {
                        "name": "Samundra Karki"
                    },
                    {
                        "name": "Mehdi Shadkah"
                    },
                    {
                        "name": "Cheng-Hau Yang"
                    },
                    {
                        "name": "Aditya Balu"
                    },
                    {
                        "name": "Guglielmo Scovazzi"
                    },
                    {
                        "name": "Adarsh Krishnamurthy"
                    },
                    {
                        "name": "Baskar Ganapathysubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Baskar Ganapathysubramanian"
                },
                "author": "Baskar Ganapathysubramanian",
                "arxiv_comment": "32 pages,29 figures, Supplement at end",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05558v2",
                "updated": "2025-07-09T11:25:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    25,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T00:45:26Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    45,
                    26,
                    1,
                    189,
                    0
                ],
                "title": "AI Agent Smart Contract Exploit Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agent Smart Contract Exploit Generation"
                },
                "summary": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a \\$6000 exploit\nvalue, while defenders require \\$60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a \\$6000 exploit\nvalue, while defenders require \\$60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense."
                },
                "authors": [
                    {
                        "name": "Arthur Gervais"
                    },
                    {
                        "name": "Liyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Liyi Zhou"
                },
                "author": "Liyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06747v1",
                "updated": "2025-07-09T11:02:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    2,
                    46,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T11:02:46Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    2,
                    46,
                    2,
                    190,
                    0
                ],
                "title": "LOVON: Legged Open-Vocabulary Object Navigator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOVON: Legged Open-Vocabulary Object Navigator"
                },
                "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON."
                },
                "authors": [
                    {
                        "name": "Daojie Peng"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "arxiv_comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03785v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03785v3",
                "updated": "2025-07-09T10:58:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    58,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-04T09:46:43Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    46,
                    43,
                    2,
                    155,
                    0
                ],
                "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons"
                },
                "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring."
                },
                "authors": [
                    {
                        "name": "Isik Baran Sandan"
                    },
                    {
                        "name": "Tu Anh Dinh"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "Accepted to GEM @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03785v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03785v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06742v1",
                "updated": "2025-07-09T10:56:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    56,
                    32,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:56:32Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    56,
                    32,
                    2,
                    190,
                    0
                ],
                "title": "PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI"
                },
                "summary": "Ethical hacking today relies on highly skilled practitioners executing\ncomplex sequences of commands, which is inherently time-consuming, difficult to\nscale, and prone to human error. To help mitigate these limitations, we\npreviously introduced 'PenTest++', an AI-augmented system combining automation\nwith generative AI supporting ethical hacking workflows. However, a key\nlimitation of PenTest++ was its lack of support for privilege escalation, a\ncrucial element of ethical hacking. In this paper we present 'PenTest2.0', a\nsubstantial evolution of PenTest++ supporting automated privilege escalation\ndriven entirely by Large Language Model reasoning. It also incorporates several\nsignificant enhancements: 'Retrieval-Augmented Generation', including both\none-line and offline modes; 'Chain-of-Thought' prompting for intermediate\nreasoning; persistent 'PenTest Task Trees' to track goal progression across\nturns; and the optional integration of human-authored hints. We describe how it\noperates, present a proof-of-concept prototype, and discuss its benefits and\nlimitations. We also describe application of the system to a controlled Linux\ntarget, showing it can carry out multi-turn, adaptive privilege escalation. We\nexplain the rationale behind its core design choices, and provide comprehensive\ntesting results and cost analysis. Our findings indicate that 'PenTest2.0'\nrepresents a meaningful step toward practical, scalable, AI-automated\npenetration testing, whilst highlighting the shortcomings of generative AI\nsystems, particularly their sensitivity to prompt structure, execution context,\nand semantic drift, reinforcing the need for further research and refinement in\nthis emerging space.\n  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM\n(Large Language Model), HITL (Human-in-the-Loop)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical hacking today relies on highly skilled practitioners executing\ncomplex sequences of commands, which is inherently time-consuming, difficult to\nscale, and prone to human error. To help mitigate these limitations, we\npreviously introduced 'PenTest++', an AI-augmented system combining automation\nwith generative AI supporting ethical hacking workflows. However, a key\nlimitation of PenTest++ was its lack of support for privilege escalation, a\ncrucial element of ethical hacking. In this paper we present 'PenTest2.0', a\nsubstantial evolution of PenTest++ supporting automated privilege escalation\ndriven entirely by Large Language Model reasoning. It also incorporates several\nsignificant enhancements: 'Retrieval-Augmented Generation', including both\none-line and offline modes; 'Chain-of-Thought' prompting for intermediate\nreasoning; persistent 'PenTest Task Trees' to track goal progression across\nturns; and the optional integration of human-authored hints. We describe how it\noperates, present a proof-of-concept prototype, and discuss its benefits and\nlimitations. We also describe application of the system to a controlled Linux\ntarget, showing it can carry out multi-turn, adaptive privilege escalation. We\nexplain the rationale behind its core design choices, and provide comprehensive\ntesting results and cost analysis. Our findings indicate that 'PenTest2.0'\nrepresents a meaningful step toward practical, scalable, AI-automated\npenetration testing, whilst highlighting the shortcomings of generative AI\nsystems, particularly their sensitivity to prompt structure, execution context,\nand semantic drift, reinforcing the need for further research and refinement in\nthis emerging space.\n  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM\n(Large Language Model), HITL (Human-in-the-Loop)"
                },
                "authors": [
                    {
                        "name": "Haitham S. Al-Sinani"
                    },
                    {
                        "name": "Chris J. Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Mitchell"
                },
                "author": "Chris J. Mitchell",
                "arxiv_comment": "45 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14541v2",
                "updated": "2025-07-09T10:55:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    55,
                    6,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-20T13:20:19Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    20,
                    19,
                    3,
                    51,
                    0
                ],
                "title": "LLM-based User Profile Management for Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based User Profile Management for Recommender System"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations."
                },
                "authors": [
                    {
                        "name": "Seunghwan Bang"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Accepted GENNEXT@SIGIR'25 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06739v1",
                "updated": "2025-07-09T10:53:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:53:05Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    53,
                    5,
                    2,
                    190,
                    0
                ],
                "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold"
                },
                "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes."
                },
                "authors": [
                    {
                        "name": "Zishen Huang"
                    },
                    {
                        "name": "Chunyu Yang"
                    },
                    {
                        "name": "Mengyuan Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengyuan Ren"
                },
                "author": "Mengyuan Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06732v1",
                "updated": "2025-07-09T10:45:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    45,
                    50,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:45:50Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    45,
                    50,
                    2,
                    190,
                    0
                ],
                "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation"
                },
                "summary": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency."
                },
                "authors": [
                    {
                        "name": "Sobhan Asasi"
                    },
                    {
                        "name": "Mohamed Ilyes Lakhal"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in SLTAT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11289v2",
                "updated": "2025-07-09T10:32:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    32,
                    0,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-12T20:46:41Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    46,
                    41,
                    3,
                    163,
                    0
                ],
                "title": "Inferring Quantum Network Topologies using Genetic Optimisation of\n  Indirect Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Quantum Network Topologies using Genetic Optimisation of\n  Indirect Measurements"
                },
                "summary": "The characterisation of quantum networks is fundamental to understanding how\nenergy and information propagates through complex systems, with applications in\ncontrol, communication, error mitigation and energy transfer. In this work, we\nexplore the use of external probes to infer the network topology in the context\nof continuous-time quantum walks, where a single excitation traverses the\nnetwork with a pattern strongly influenced by its topology. The probes act as\ndecay channels for the excitation, and can be interpreted as performing an\nindirect measurement on the network dynamics. By making use of a Genetic\nOptimisation algorithm, we demonstrate that the data collected by the probes\ncan be used to successfully reconstruct the topology of any quantum network\nwith high success rates, where performance is limited only by computational\nresources for large network sizes. Moreover, we show that increasing the number\nof probes significantly simplifies the reconstruction task, revealing a\ntradeoff between the number of probes and the required computational power.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The characterisation of quantum networks is fundamental to understanding how\nenergy and information propagates through complex systems, with applications in\ncontrol, communication, error mitigation and energy transfer. In this work, we\nexplore the use of external probes to infer the network topology in the context\nof continuous-time quantum walks, where a single excitation traverses the\nnetwork with a pattern strongly influenced by its topology. The probes act as\ndecay channels for the excitation, and can be interpreted as performing an\nindirect measurement on the network dynamics. By making use of a Genetic\nOptimisation algorithm, we demonstrate that the data collected by the probes\ncan be used to successfully reconstruct the topology of any quantum network\nwith high success rates, where performance is limited only by computational\nresources for large network sizes. Moreover, we show that increasing the number\nof probes significantly simplifies the reconstruction task, revealing a\ntradeoff between the number of probes and the required computational power."
                },
                "authors": [
                    {
                        "name": "Conall J. Campbell"
                    },
                    {
                        "name": "Matthew Mackinnon"
                    },
                    {
                        "name": "Mauro Paternostro"
                    },
                    {
                        "name": "Diana A. Chisholm"
                    }
                ],
                "author_detail": {
                    "name": "Diana A. Chisholm"
                },
                "author": "Diana A. Chisholm",
                "arxiv_comment": "9 pages, 8 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06722v1",
                "updated": "2025-07-09T10:30:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    30,
                    9,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:30:09Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    30,
                    9,
                    2,
                    190,
                    0
                ],
                "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effect of Uncertainty on Layer-wise Inference Dynamics"
                },
                "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference."
                },
                "authors": [
                    {
                        "name": "Sunwoo Kim"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Accepted to Actionable Interpretability Workshop - ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06719v1",
                "updated": "2025-07-09T10:20:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    20,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:20:38Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    20,
                    38,
                    2,
                    190,
                    0
                ],
                "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for\n  Open-Vocabulary 3D Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for\n  Open-Vocabulary 3D Visual Grounding"
                },
                "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability."
                },
                "authors": [
                    {
                        "name": "Zhenyang Liu"
                    },
                    {
                        "name": "Sixiao Zheng"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Cairong Zhao"
                    },
                    {
                        "name": "Longfei Liang"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14661v2",
                "updated": "2025-07-09T10:14:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    14,
                    26,
                    2,
                    190,
                    0
                ],
                "published": "2024-01-26T05:50:58Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    5,
                    50,
                    58,
                    4,
                    26,
                    0
                ],
                "title": "From Blurry to Brilliant Detection: YOLO-Based Aerial Object Detection\n  with Super Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Blurry to Brilliant Detection: YOLO-Based Aerial Object Detection\n  with Super Resolution"
                },
                "summary": "Aerial object detection presents challenges from small object sizes, high\ndensity clustering, and image quality degradation from distance and motion\nblur. These factors create an information bottleneck where limited pixel\nrepresentation cannot encode sufficient discriminative features. B2BDet\naddresses this with a two-stage framework that applies domain-specific\nsuper-resolution during inference, followed by detection using an enhanced\nYOLOv5 architecture. Unlike training-time super-resolution approaches that\nenhance learned representations, our method recovers visual information from\neach input image. The approach combines aerial-optimized SRGAN fine-tuning with\narchitectural innovations including an Efficient Attention Module (EAM) and\nCross-Layer Feature Pyramid Network (CLFPN). Evaluation across four aerial\ndatasets shows performance gains, with VisDrone achieving 52.5% mAP using only\n27.7M parameters. Ablation studies show that super-resolution preprocessing\ncontributes +2.6% mAP improvement while architectural enhancements add +2.9%,\nyielding +5.5% total improvement over baseline YOLOv5. The method achieves\ncomputational efficiency with 53.8% parameter reduction compared to recent\napproaches while achieving strong small object detection performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerial object detection presents challenges from small object sizes, high\ndensity clustering, and image quality degradation from distance and motion\nblur. These factors create an information bottleneck where limited pixel\nrepresentation cannot encode sufficient discriminative features. B2BDet\naddresses this with a two-stage framework that applies domain-specific\nsuper-resolution during inference, followed by detection using an enhanced\nYOLOv5 architecture. Unlike training-time super-resolution approaches that\nenhance learned representations, our method recovers visual information from\neach input image. The approach combines aerial-optimized SRGAN fine-tuning with\narchitectural innovations including an Efficient Attention Module (EAM) and\nCross-Layer Feature Pyramid Network (CLFPN). Evaluation across four aerial\ndatasets shows performance gains, with VisDrone achieving 52.5% mAP using only\n27.7M parameters. Ablation studies show that super-resolution preprocessing\ncontributes +2.6% mAP improvement while architectural enhancements add +2.9%,\nyielding +5.5% total improvement over baseline YOLOv5. The method achieves\ncomputational efficiency with 53.8% parameter reduction compared to recent\napproaches while achieving strong small object detection performance."
                },
                "authors": [
                    {
                        "name": "Ragib Amin Nihal"
                    },
                    {
                        "name": "Benjamin Yen"
                    },
                    {
                        "name": "Takeshi Ashizawa"
                    },
                    {
                        "name": "Katsutoshi Itoyama"
                    },
                    {
                        "name": "Kazuhiro Nakadai"
                    }
                ],
                "author_detail": {
                    "name": "Kazuhiro Nakadai"
                },
                "author": "Kazuhiro Nakadai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06715v1",
                "updated": "2025-07-09T10:13:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    13,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:13:38Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    13,
                    38,
                    2,
                    190,
                    0
                ],
                "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs"
                },
                "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust."
                },
                "authors": [
                    {
                        "name": "Garapati Keerthana"
                    },
                    {
                        "name": "Manik Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Manik Gupta"
                },
                "author": "Manik Gupta",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03635v2",
                "updated": "2025-07-09T10:08:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    8,
                    34,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-04T17:57:22Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    57,
                    22,
                    4,
                    94,
                    0
                ],
                "title": "Do Larger Language Models Imply Better Generalization? A Pretraining\n  Scaling Law for Implicit Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Larger Language Models Imply Better Generalization? A Pretraining\n  Scaling Law for Implicit Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Shawn Tan"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Yikang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yikang Shen"
                },
                "author": "Yikang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16525v2",
                "updated": "2025-07-09T10:04:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    4,
                    6,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-21T08:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    8,
                    2,
                    58,
                    5,
                    356,
                    0
                ],
                "title": "One Size Does Not Fit All: Investigating Efficacy of Perplexity in\n  Detecting LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Size Does Not Fit All: Investigating Efficacy of Perplexity in\n  Detecting LLM-Generated Code"
                },
                "summary": "Large language model-generated code (LLMgCode) has become increasingly common\nin software development. So far LLMgCode has more quality issues than\nhuman-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a\ncode change, while the change is signed by only human developers, without being\ncarefully examined. Many automated methods have been proposed to detect\nLLMgCode from HaCode, in which the perplexity-based method (PERPLEXITY for\nshort) is the state-of-the-art method. However, the efficacy evaluation of\nPERPLEXITY has focused on detection accuracy. Yet it is unclear whether\nPERPLEXITY is good enough in a wider range of realistic evaluation settings. To\nthis end, we carry out a family of experiments to compare PERPLEXITY against\nfeature- and pre-training-based methods from three perspectives: detection\naccuracy, detection speed, and generalization capability. The experimental\nresults show that PERPLEXITY has the best generalization capability while\nhaving limited detection accuracy and detection speed. Based on that, we\ndiscuss the strengths and limitations of PERPLEXITY, e.g., PERPLEXITY is\nunsuitable for high-level programming languages. Finally, we provide\nrecommendations to improve PERPLEXITY and apply it in practice. As the first\nlarge-scale investigation on detecting LLMgCode from HaCode, this article\nprovides a wide range of findings for future improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-generated code (LLMgCode) has become increasingly common\nin software development. So far LLMgCode has more quality issues than\nhuman-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a\ncode change, while the change is signed by only human developers, without being\ncarefully examined. Many automated methods have been proposed to detect\nLLMgCode from HaCode, in which the perplexity-based method (PERPLEXITY for\nshort) is the state-of-the-art method. However, the efficacy evaluation of\nPERPLEXITY has focused on detection accuracy. Yet it is unclear whether\nPERPLEXITY is good enough in a wider range of realistic evaluation settings. To\nthis end, we carry out a family of experiments to compare PERPLEXITY against\nfeature- and pre-training-based methods from three perspectives: detection\naccuracy, detection speed, and generalization capability. The experimental\nresults show that PERPLEXITY has the best generalization capability while\nhaving limited detection accuracy and detection speed. Based on that, we\ndiscuss the strengths and limitations of PERPLEXITY, e.g., PERPLEXITY is\nunsuitable for high-level programming languages. Finally, we provide\nrecommendations to improve PERPLEXITY and apply it in practice. As the first\nlarge-scale investigation on detecting LLMgCode from HaCode, this article\nprovides a wide range of findings for future improvement."
                },
                "authors": [
                    {
                        "name": "Jinwei Xu"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Yanjing Yang"
                    },
                    {
                        "name": "Lanxin Yang"
                    },
                    {
                        "name": "Zeru Cheng"
                    },
                    {
                        "name": "Jun Lyu"
                    },
                    {
                        "name": "Bohan Liu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Alberto Bacchelli"
                    },
                    {
                        "name": "Yin Kia Chiam"
                    },
                    {
                        "name": "Thiam Kian Chiew"
                    }
                ],
                "author_detail": {
                    "name": "Thiam Kian Chiew"
                },
                "author": "Thiam Kian Chiew",
                "arxiv_comment": "This article has been accepted by ACM Transactions on Software\n  Engineering and Methodology (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04320v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04320v3",
                "updated": "2025-07-09T10:00:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    0,
                    4,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-06T01:37:50Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    1,
                    37,
                    50,
                    6,
                    96,
                    0
                ],
                "title": "Causal Inference Isn't Special: Why It's Just Another Prediction Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Inference Isn't Special: Why It's Just Another Prediction Problem"
                },
                "summary": "Causal inference is often portrayed as fundamentally distinct from predictive\nmodeling, with its own terminology, goals, and intellectual challenges. But at\nits core, causal inference is simply a structured instance of prediction under\ndistribution shift. In both cases, we begin with labeled data from a source\ndomain and seek to generalize to a target domain where outcomes are not\nobserved. The key difference is that in causal inference, the labels --\npotential outcomes -- are selectively observed based on treatment assignment,\nintroducing bias that must be addressed through assumptions. This perspective\nreframes causal estimation as a familiar generalization problem and highlights\nhow techniques from predictive modeling, such as reweighting and domain\nadaptation, apply directly to causal tasks. It also clarifies that causal\nassumptions are not uniquely strong -- they are simply more explicit. By\nviewing causal inference through the lens of prediction, we demystify its\nlogic, connect it to familiar tools, and make it more accessible to\npractitioners and educators alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is often portrayed as fundamentally distinct from predictive\nmodeling, with its own terminology, goals, and intellectual challenges. But at\nits core, causal inference is simply a structured instance of prediction under\ndistribution shift. In both cases, we begin with labeled data from a source\ndomain and seek to generalize to a target domain where outcomes are not\nobserved. The key difference is that in causal inference, the labels --\npotential outcomes -- are selectively observed based on treatment assignment,\nintroducing bias that must be addressed through assumptions. This perspective\nreframes causal estimation as a familiar generalization problem and highlights\nhow techniques from predictive modeling, such as reweighting and domain\nadaptation, apply directly to causal tasks. It also clarifies that causal\nassumptions are not uniquely strong -- they are simply more explicit. By\nviewing causal inference through the lens of prediction, we demystify its\nlogic, connect it to familiar tools, and make it more accessible to\npractitioners and educators alike."
                },
                "authors": [
                    {
                        "name": "Carlos Fernndez-Lora"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Fernndez-Lora"
                },
                "author": "Carlos Fernndez-Lora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04320v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04320v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07675v2",
                "updated": "2025-07-09T09:51:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    51,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-09T11:51:27Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    51,
                    27,
                    0,
                    160,
                    0
                ],
                "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents"
                },
                "summary": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle."
                },
                "authors": [
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Hanxu Yan"
                    },
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Yufei Li"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19962v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19962v3",
                "updated": "2025-07-09T09:30:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    30,
                    24,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-28T16:33:55Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    33,
                    55,
                    0,
                    118,
                    0
                ],
                "title": "Approximating neutron-star radii using gravitational-wave only\n  measurements with symbolic regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximating neutron-star radii using gravitational-wave only\n  measurements with symbolic regression"
                },
                "summary": "Gravitational waves emitted by binary neutron-star inspirals carry\ninformation on components' masses and tidal deformabilities, but not directly\nradii, which are measured by electromagnetic observations of neutron stars. To\nimprove the multi-messenger astronomy studies of neutron stars, an expression\nfor neutron-star radii as a function of gravitational-wave only data would be\nadvantageous, as it would allow to compare information from two different\nchannels. In order to do so, a symbolic regression method, pySR, is trained on\nTOV solutions to piecewise polytropic EOS input to discover an approximate\nsymbolic expression for the neutron-star radius as a function of\ngravitational-wave measurements only. The approximation is tested on piecewise\npolytropic EOS NS data, as well as on NS sequences based on selected realistic\n(non-polytropic) dense-matter theory EOSs, achieving consistent agreement\nbetween the ground truth values and the symbolic approximation for a broad\nrange of NS parameters covering current astrophysical observations, with\naverage radii differences of few hundred meters. Additionally, the\napproximation is applied to the GW170817 gravitational-wave mass and tidal\ndeformability posteriors, and compared to reported inferred radius\ndistributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves emitted by binary neutron-star inspirals carry\ninformation on components' masses and tidal deformabilities, but not directly\nradii, which are measured by electromagnetic observations of neutron stars. To\nimprove the multi-messenger astronomy studies of neutron stars, an expression\nfor neutron-star radii as a function of gravitational-wave only data would be\nadvantageous, as it would allow to compare information from two different\nchannels. In order to do so, a symbolic regression method, pySR, is trained on\nTOV solutions to piecewise polytropic EOS input to discover an approximate\nsymbolic expression for the neutron-star radius as a function of\ngravitational-wave measurements only. The approximation is tested on piecewise\npolytropic EOS NS data, as well as on NS sequences based on selected realistic\n(non-polytropic) dense-matter theory EOSs, achieving consistent agreement\nbetween the ground truth values and the symbolic approximation for a broad\nrange of NS parameters covering current astrophysical observations, with\naverage radii differences of few hundred meters. Additionally, the\napproximation is applied to the GW170817 gravitational-wave mass and tidal\ndeformability posteriors, and compared to reported inferred radius\ndistributions."
                },
                "authors": [
                    {
                        "name": "Micha Bejger"
                    }
                ],
                "author_detail": {
                    "name": "Micha Bejger"
                },
                "author": "Micha Bejger",
                "arxiv_doi": "10.1103/cv6n-xtsf",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/cv6n-xtsf",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.19962v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19962v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 6 figures, 2 tables; PRD accepted",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06687v1",
                "updated": "2025-07-09T09:30:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    30,
                    7,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T09:30:07Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    30,
                    7,
                    2,
                    190,
                    0
                ],
                "title": "StixelNExT++: Lightweight Monocular Scene Segmentation and\n  Representation for Collective Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StixelNExT++: Lightweight Monocular Scene Segmentation and\n  Representation for Collective Perception"
                },
                "summary": "This paper presents StixelNExT++, a novel approach to scene representation\nfor monocular perception systems. Building on the established Stixel\nrepresentation, our method infers 3D Stixels and enhances object segmentation\nby clustering smaller 3D Stixel units. The approach achieves high compression\nof scene information while remaining adaptable to point cloud and\nbird's-eye-view representations. Our lightweight neural network, trained on\nautomatically generated LiDAR-based ground truth, achieves real-time\nperformance with computation times as low as 10 ms per frame. Experimental\nresults on the Waymo dataset demonstrate competitive performance within a\n30-meter range, highlighting the potential of StixelNExT++ for collective\nperception in autonomous systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents StixelNExT++, a novel approach to scene representation\nfor monocular perception systems. Building on the established Stixel\nrepresentation, our method infers 3D Stixels and enhances object segmentation\nby clustering smaller 3D Stixel units. The approach achieves high compression\nof scene information while remaining adaptable to point cloud and\nbird's-eye-view representations. Our lightweight neural network, trained on\nautomatically generated LiDAR-based ground truth, achieves real-time\nperformance with computation times as low as 10 ms per frame. Experimental\nresults on the Waymo dataset demonstrate competitive performance within a\n30-meter range, highlighting the potential of StixelNExT++ for collective\nperception in autonomous systems."
                },
                "authors": [
                    {
                        "name": "Marcel Vosshans"
                    },
                    {
                        "name": "Omar Ait-Aider"
                    },
                    {
                        "name": "Youcef Mezouar"
                    },
                    {
                        "name": "Markus Enzweiler"
                    }
                ],
                "author_detail": {
                    "name": "Markus Enzweiler"
                },
                "author": "Markus Enzweiler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20232v2",
                "updated": "2025-07-09T09:12:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    12,
                    25,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-25T08:23:09Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    8,
                    23,
                    9,
                    2,
                    176,
                    0
                ],
                "title": "Tomography for Plasma Imaging: a Unifying Framework for Bayesian\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tomography for Plasma Imaging: a Unifying Framework for Bayesian\n  Inference"
                },
                "summary": "Plasma diagnostics often employ computerized tomography to estimate\nemissivity profiles from a finite, and often limited, number of line-integrated\nmeasurements. Decades of algorithmic refinement have brought considerable\nimprovements, and led to a variety of employed solutions. These often feature\nan underlying, common structure that is rarely acknowledged or investigated. In\nthis paper, we present a unifying perspective on sparse-view tomographic\nreconstructions for plasma imaging, highlighting how many inversion approaches\nreported in the literature can be naturally understood within a Bayesian\nframework. In this setting, statistical modelling of acquired data leads to a\nlikelihood term, while the assumed properties of the profile to be\nreconstructed are encoded within a prior term. Together, these terms yield the\nposterior distribution, which models all the available information on the\nprofile to be reconstructed. We show how credible reconstructions, uncertainty\nquantification and further statistical quantities of interest can be\nefficiently obtained from noisy tomographic data by means of a stochastic\ngradient flow algorithm targeting the posterior. This is demonstrated by\napplication to soft x-ray imaging at the TCV tokamak. We validate the proposed\nimaging pipeline on a large dataset of generated model phantoms, showing how\nposterior-based inference can be leveraged to perform principled statistical\nanalysis of quantities of interest. Finally, we address some of the inherent,\nand thus remaining, limitations of sparse-view tomography. All the\ncomputational routines used in this work are made available as open access\ncode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma diagnostics often employ computerized tomography to estimate\nemissivity profiles from a finite, and often limited, number of line-integrated\nmeasurements. Decades of algorithmic refinement have brought considerable\nimprovements, and led to a variety of employed solutions. These often feature\nan underlying, common structure that is rarely acknowledged or investigated. In\nthis paper, we present a unifying perspective on sparse-view tomographic\nreconstructions for plasma imaging, highlighting how many inversion approaches\nreported in the literature can be naturally understood within a Bayesian\nframework. In this setting, statistical modelling of acquired data leads to a\nlikelihood term, while the assumed properties of the profile to be\nreconstructed are encoded within a prior term. Together, these terms yield the\nposterior distribution, which models all the available information on the\nprofile to be reconstructed. We show how credible reconstructions, uncertainty\nquantification and further statistical quantities of interest can be\nefficiently obtained from noisy tomographic data by means of a stochastic\ngradient flow algorithm targeting the posterior. This is demonstrated by\napplication to soft x-ray imaging at the TCV tokamak. We validate the proposed\nimaging pipeline on a large dataset of generated model phantoms, showing how\nposterior-based inference can be leveraged to perform principled statistical\nanalysis of quantities of interest. Finally, we address some of the inherent,\nand thus remaining, limitations of sparse-view tomography. All the\ncomputational routines used in this work are made available as open access\ncode."
                },
                "authors": [
                    {
                        "name": "D. Hamm"
                    },
                    {
                        "name": "C. Theiler"
                    },
                    {
                        "name": "M. Simeoni"
                    },
                    {
                        "name": "B. P. Duval"
                    },
                    {
                        "name": "T. Debarre"
                    },
                    {
                        "name": "L. Simons"
                    },
                    {
                        "name": "J. R. Queralt"
                    }
                ],
                "author_detail": {
                    "name": "J. R. Queralt"
                },
                "author": "J. R. Queralt",
                "arxiv_comment": "Fixed broken links to software resources (references 60 and 61)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13217v2",
                "updated": "2025-07-09T08:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-06-19T04:59:09Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    4,
                    59,
                    9,
                    2,
                    171,
                    0
                ],
                "title": "Automating IRAC Analysis in Malaysian Contract Law using a\n  Semi-Structured Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating IRAC Analysis in Malaysian Contract Law using a\n  Semi-Structured Knowledge Base"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often\nlimited due to the unique legal terminologies and the necessity for highly\nspecialized knowledge. These limitations highlight the need for high-quality\ndata tailored for complex legal reasoning tasks. This paper introduces\nLegalSemi, a benchmark specifically curated for legal scenario analysis.\nLegalSemi comprises 54 legal scenarios, each rigorously annotated by legal\nexperts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)\nframework from Malaysian Contract Law. In addition, LegalSemi is accompanied by\na structured knowledge base (SKE). A series of experiments were conducted to\nassess the usefulness of LegalSemi for IRAC analysis. The experimental results\ndemonstrate the effectiveness of incorporating the SKE for issue\nidentification, rule retrieval, application and conclusion generation using\nfour different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often\nlimited due to the unique legal terminologies and the necessity for highly\nspecialized knowledge. These limitations highlight the need for high-quality\ndata tailored for complex legal reasoning tasks. This paper introduces\nLegalSemi, a benchmark specifically curated for legal scenario analysis.\nLegalSemi comprises 54 legal scenarios, each rigorously annotated by legal\nexperts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)\nframework from Malaysian Contract Law. In addition, LegalSemi is accompanied by\na structured knowledge base (SKE). A series of experiments were conducted to\nassess the usefulness of LegalSemi for IRAC analysis. The experimental results\ndemonstrate the effectiveness of incorporating the SKE for issue\nidentification, rule retrieval, application and conclusion generation using\nfour different LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoxi Kang"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Lay-Ki Soon"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Adnan Trakic"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Trakic"
                },
                "author": "Adnan Trakic",
                "arxiv_doi": "10.1007/s10506-025-09467-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10506-025-09467-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Artificial Intelligence and Law (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06656v1",
                "updated": "2025-07-09T08:40:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    40,
                    46,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T08:40:46Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    40,
                    46,
                    2,
                    190,
                    0
                ],
                "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Diffusion Model Stability for Image Restoration via Gradient\n  Management"
                },
                "summary": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}."
                },
                "authors": [
                    {
                        "name": "Hongjie Wu"
                    },
                    {
                        "name": "Mingqin Zhang"
                    },
                    {
                        "name": "Linchao He"
                    },
                    {
                        "name": "Ji-Zhe Zhou"
                    },
                    {
                        "name": "Jiancheng Lv"
                    }
                ],
                "author_detail": {
                    "name": "Jiancheng Lv"
                },
                "author": "Jiancheng Lv",
                "arxiv_comment": "Accepted to ACM Multimedia 2025. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.07106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07106v1",
                "updated": "2025-07-09T17:59:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    47,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:59:47Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    47,
                    2,
                    190,
                    0
                ],
                "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor"
                },
                "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/."
                },
                "authors": [
                    {
                        "name": "Vatsal Agarwal"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    },
                    {
                        "name": "Gefen Kohavi"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Daniel Ulbricht"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Shrivastava"
                },
                "author": "Abhinav Shrivastava",
                "arxiv_comment": "Website: see https://vatsalag99.github.io/mustafar/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07104v1",
                "updated": "2025-07-09T17:59:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    4,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:59:04Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    59,
                    4,
                    2,
                    190,
                    0
                ],
                "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation\n  from Diffusion Models"
                },
                "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD."
                },
                "authors": [
                    {
                        "name": "Tiezheng Zhang"
                    },
                    {
                        "name": "Yitong Li"
                    },
                    {
                        "name": "Yu-cheng Chou"
                    },
                    {
                        "name": "Jieneng Chen"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Chen Wei"
                    },
                    {
                        "name": "Junfei Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Junfei Xiao"
                },
                "author": "Junfei Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17295v3",
                "updated": "2025-07-09T17:37:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    37,
                    23,
                    2,
                    190,
                    0
                ],
                "published": "2024-06-25T05:45:07Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    5,
                    45,
                    7,
                    1,
                    177,
                    0
                ],
                "title": "Less can be more for predicting properties with large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Less can be more for predicting properties with large language models"
                },
                "summary": "Predicting properties from coordinate-category data -- sets of vectors paired\nwith categorical information -- is fundamental to computational science. In\nmaterials science, this challenge manifests as predicting properties like\nformation energies or elastic moduli from crystal structures comprising atomic\npositions (vectors) and element types (categorical information). While large\nlanguage models (LLMs) have increasingly been applied to such tasks, with\nresearchers encoding structural data as text, optimal strategies for achieving\nreliable predictions remain elusive. Here, we report fundamental limitations in\nLLM's ability to learn from coordinate information in coordinate-category data.\nThrough systematic experiments using synthetic datasets with tunable coordinate\nand category contributions, combined with a comprehensive benchmarking\nframework (MatText) spanning multiple representations and model scales, we find\nthat LLMs consistently fail to capture coordinate information while excelling\nat category patterns. This geometric blindness persists regardless of model\nsize (up to 70B parameters), dataset scale (up to 2M structures), or text\nrepresentation strategy. Our findings suggest immediate practical implications:\nfor materials property prediction tasks dominated by structural effects,\nspecialized geometric architectures consistently outperform LLMs by significant\nmargins, as evidenced by a clear \"GNN-LM wall\" in performance benchmarks. Based\non our analysis, we provide concrete guidelines for architecture selection in\nscientific machine learning, while highlighting the critical importance of\nunderstanding model inductive biases when tackling scientific prediction\nproblems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting properties from coordinate-category data -- sets of vectors paired\nwith categorical information -- is fundamental to computational science. In\nmaterials science, this challenge manifests as predicting properties like\nformation energies or elastic moduli from crystal structures comprising atomic\npositions (vectors) and element types (categorical information). While large\nlanguage models (LLMs) have increasingly been applied to such tasks, with\nresearchers encoding structural data as text, optimal strategies for achieving\nreliable predictions remain elusive. Here, we report fundamental limitations in\nLLM's ability to learn from coordinate information in coordinate-category data.\nThrough systematic experiments using synthetic datasets with tunable coordinate\nand category contributions, combined with a comprehensive benchmarking\nframework (MatText) spanning multiple representations and model scales, we find\nthat LLMs consistently fail to capture coordinate information while excelling\nat category patterns. This geometric blindness persists regardless of model\nsize (up to 70B parameters), dataset scale (up to 2M structures), or text\nrepresentation strategy. Our findings suggest immediate practical implications:\nfor materials property prediction tasks dominated by structural effects,\nspecialized geometric architectures consistently outperform LLMs by significant\nmargins, as evidenced by a clear \"GNN-LM wall\" in performance benchmarks. Based\non our analysis, we provide concrete guidelines for architecture selection in\nscientific machine learning, while highlighting the critical importance of\nunderstanding model inductive biases when tackling scientific prediction\nproblems."
                },
                "authors": [
                    {
                        "name": "Nawaf Alampara"
                    },
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Maik Jablonka"
                },
                "author": "Kevin Maik Jablonka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04133v3",
                "updated": "2025-07-09T17:33:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    33,
                    49,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-04T16:26:11Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    26,
                    11,
                    2,
                    155,
                    0
                ],
                "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management\n  in LLM-based Agentic Multi-Agent Systems"
                },
                "summary": "Agentic AI systems, built upon large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligence, autonomy,\ncollaboration, and decision-making across enterprise and societal domains. This\nreview presents a structured analysis of \\textbf{Trust, Risk, and Security\nManagement (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems\n(AMAS). We begin by examining the conceptual foundations of Agentic AI and\nhighlight its architectural distinctions from traditional AI agents. We then\nadapt and extend the AI TRiSM framework for Agentic AI, structured around four\nkey pillars: Explainability, ModelOps, Security, Privacy and Governance, each\ncontextualized to the challenges of multi-agent LLM systems. A novel risk\ntaxonomy is proposed to capture the unique threats and vulnerabilities of\nAgentic AI, ranging from coordination failures to prompt-based adversarial\nmanipulation. To support practical assessment in Agentic AI works, we introduce\ntwo novel metrics: the Component Synergy Score (CSS), which quantifies the\nquality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE),\nwhich evaluates the efficiency of tool use within agent workflows. We further\ndiscuss strategies for improving explainability in Agentic AI , as well as\napproaches to enhancing security and privacy through encryption, adversarial\nrobustness, and regulatory compliance. The review concludes with a research\nroadmap for the responsible development and deployment of Agentic AI, outlining\ncritical directions to align emerging systems with TRiSM principles for safe,\ntransparent, and accountable operation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic AI systems, built upon large language models (LLMs) and deployed in\nmulti-agent configurations, are redefining intelligence, autonomy,\ncollaboration, and decision-making across enterprise and societal domains. This\nreview presents a structured analysis of \\textbf{Trust, Risk, and Security\nManagement (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems\n(AMAS). We begin by examining the conceptual foundations of Agentic AI and\nhighlight its architectural distinctions from traditional AI agents. We then\nadapt and extend the AI TRiSM framework for Agentic AI, structured around four\nkey pillars: Explainability, ModelOps, Security, Privacy and Governance, each\ncontextualized to the challenges of multi-agent LLM systems. A novel risk\ntaxonomy is proposed to capture the unique threats and vulnerabilities of\nAgentic AI, ranging from coordination failures to prompt-based adversarial\nmanipulation. To support practical assessment in Agentic AI works, we introduce\ntwo novel metrics: the Component Synergy Score (CSS), which quantifies the\nquality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE),\nwhich evaluates the efficiency of tool use within agent workflows. We further\ndiscuss strategies for improving explainability in Agentic AI , as well as\napproaches to enhancing security and privacy through encryption, adversarial\nrobustness, and regulatory compliance. The review concludes with a research\nroadmap for the responsible development and deployment of Agentic AI, outlining\ncritical directions to align emerging systems with TRiSM principles for safe,\ntransparent, and accountable operation."
                },
                "authors": [
                    {
                        "name": "Shaina Raza"
                    },
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    },
                    {
                        "name": "Christos Emmanouilidis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Emmanouilidis"
                },
                "author": "Christos Emmanouilidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12446v2",
                "updated": "2025-07-09T17:31:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    31,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-18T02:27:23Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    27,
                    23,
                    1,
                    49,
                    0
                ],
                "title": "Multi-Attribute Steering of Language Models via Targeted Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Attribute Steering of Language Models via Targeted Intervention"
                },
                "summary": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time intervention (ITI) has emerged as a promising method for\nsteering large language model (LLM) behavior in a particular direction (e.g.,\nimproving helpfulness) by intervening on token representations without costly\nupdates to the LLM's parameters. However, existing ITI approaches fail to scale\nto multi-attribute settings with conflicts, such as enhancing helpfulness while\nalso reducing toxicity. To address this, we introduce Multi-Attribute Targeted\nSteering (MAT-Steer), a novel steering framework designed for selective\ntoken-level intervention across multiple attributes. MAT-Steer learns steering\nvectors using an alignment objective that shifts the model's internal\nrepresentations of undesirable outputs closer to those of desirable ones while\nenforcing sparsity and orthogonality among vectors for different attributes,\nthereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two\ndistinct settings: (i) on question answering (QA) tasks where we balance\nattributes like truthfulness, bias, and toxicity; (ii) on generative tasks\nwhere we simultaneously improve attributes like helpfulness, correctness, and\ncoherence. MAT-Steer outperforms existing ITI and parameter-efficient\nfine-tuning approaches across both task types (e.g., 3% average accuracy gain\nacross QA tasks and 55.82% win rate against the best ITI baseline)."
                },
                "authors": [
                    {
                        "name": "Duy Nguyen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "ACL 2025 camera-ready, code link:\n  https://github.com/duykhuongnguyen/MAT-Steer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07067v1",
                "updated": "2025-07-09T17:27:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    27,
                    51,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:27:51Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    27,
                    51,
                    2,
                    190,
                    0
                ],
                "title": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided\n  Telecommunication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided\n  Telecommunication Networks"
                },
                "summary": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference."
                },
                "authors": [
                    {
                        "name": "Clement Ruah"
                    },
                    {
                        "name": "Houssem Sifaou"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Bashir M. Al-Hashimi"
                    }
                ],
                "author_detail": {
                    "name": "Bashir M. Al-Hashimi"
                },
                "author": "Bashir M. Al-Hashimi",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07064v1",
                "updated": "2025-07-09T17:26:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    26,
                    10,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:26:10Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    26,
                    10,
                    2,
                    190,
                    0
                ],
                "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Parameter Efficiency in LLM-Based Recommendation through\n  Sophisticated Pruning"
                },
                "summary": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec"
                },
                "authors": [
                    {
                        "name": "Shanle Zheng"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Jizhi Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Fuli Feng"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08268v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08268v3",
                "updated": "2025-07-09T17:25:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    25,
                    55,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-11T10:35:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCFO: Long Context and Long Form Output Dataset and Benchmarking"
                },
                "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6)."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-juss"
                    },
                    {
                        "name": "Pierre Andrews"
                    },
                    {
                        "name": "Mariano Coria Meglioli"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "David Dale"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Alexandre Mourachko"
                    },
                    {
                        "name": "Eduardo Snchez"
                    },
                    {
                        "name": "Holger Schwenk"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Carleigh Wood"
                    }
                ],
                "author_detail": {
                    "name": "Carleigh Wood"
                },
                "author": "Carleigh Wood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08268v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08268v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01735v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01735v2",
                "updated": "2025-07-09T17:19:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    19,
                    50,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-02T16:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    46,
                    38,
                    2,
                    276,
                    0
                ],
                "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LASeR: Learning to Adaptively Select Reward Models with Multi-Armed\n  Bandits"
                },
                "summary": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but\nthe degree to which an RM specialized to one task (e.g. writing) generalizes to\nnew tasks (e.g. math) is often not known a priori, often making using only one\nfixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs\nsimultaneously can incur a prohibitively high computational cost and lead to\nconflicting signals from different RMs that may degrade performance. To address\nthese challenges, we introduce LASeR (Learning to Adaptively Select Rewards),\nwhich frames reward model selection as a multi-armed bandit problem,\nefficiently and iteratively training LLMs using multiple RMs by selecting the\nmost well-suited RM for each instance. On commonsense and math reasoning tasks,\nwe show that LASeR boosts iterative LLM training, improving the absolute\naverage accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of\nRM scores while also showing superior efficiency (e.g., a 2x speedup).\nMoreover, on WildChat (open-ended instruction-following tasks), LASeR leads to\na 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to\nlong-context generation, LASeR improves by 2.96 F1 points (avg.) on\nsingle-document QA tasks and 2.97 F1 points on few-shot learning over the RM\nscore ensemble baseline with best-of-n sampling."
                },
                "authors": [
                    {
                        "name": "Duy Nguyen"
                    },
                    {
                        "name": "Archiki Prasad"
                    },
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "28 pages; First two authors contributed equally. Code:\n  https://github.com/duykhuongnguyen/LASeR-MAB",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01735v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01735v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07048v1",
                "updated": "2025-07-09T17:10:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    10,
                    33,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:10:33Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    10,
                    33,
                    2,
                    190,
                    0
                ],
                "title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark\n  Enriched with Contextual Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark\n  Enriched with Contextual Metadata"
                },
                "summary": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis."
                },
                "authors": [
                    {
                        "name": "Bruce Coburn"
                    },
                    {
                        "name": "Jiangpeng He"
                    },
                    {
                        "name": "Megan E. Rollo"
                    },
                    {
                        "name": "Satvinder S. Dhaliwal"
                    },
                    {
                        "name": "Deborah A. Kerr"
                    },
                    {
                        "name": "Fengqing Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Fengqing Zhu"
                },
                "author": "Fengqing Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07045v1",
                "updated": "2025-07-09T17:07:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    7,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:07:39Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    7,
                    39,
                    2,
                    190,
                    0
                ],
                "title": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient\n  Design Framework for Individual and SME LLM Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient\n  Design Framework for Individual and SME LLM Usage"
                },
                "summary": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progression from traditional prompt engineering to a more rigorous\ndiscipline of prompt design marks a pivotal shift in human-LLM interaction. As\nLarge Language Models (LLMs) become increasingly embedded in mission-critical\napplications, there emerges a pressing need for frameworks that are not only\nexplicit and systematic but also minimal enough to remain practical and broadly\naccessible. While many existing approaches address prompt structuring through\nelaborate Domain-Specific Languages (DSLs) or multi-layered templates, such\nmethods can impose significant token and cognitive overhead, potentially\nconstraining the model's creative capacity. In this context, we propose the 5C\nPrompt Contract, a framework that distills prompt design into five intuitive\ncomponents: Character, Cause, Constraint, Contingency, and Calibration. This\nminimal cognitive schema explicitly integrates fallback and output optimization\ndirectives, fostering reliable, interpretable, and creatively flexible AI\ninteractions. Experimental results demonstrate that the 5C framework\nconsistently achieves superior input token efficiency while maintaining rich\nand consistent outputs across diverse LLM architectures (OpenAI, Anthropic,\nDeepSeek, and Gemini), making it particularly suited for individuals and\nSmall-to-Medium Enterprises (SMEs) with limited AI engineering resources."
                },
                "authors": [
                    {
                        "name": "Ugur Ari"
                    }
                ],
                "author_detail": {
                    "name": "Ugur Ari"
                },
                "author": "Ugur Ari",
                "arxiv_doi": "10.5281/zenodo.1234567 10.5281/zenodo.1234567 10.5281/zenodo.1234567",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.1234567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.1234567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.1234567",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.07045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 5 tables. Includes comparative experimental results across\n  OpenAI, Anthropic, DeepSeek, and Gemini LLMs",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07044v1",
                "updated": "2025-07-09T17:07:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    7,
                    26,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:07:26Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    7,
                    26,
                    2,
                    190,
                    0
                ],
                "title": "Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision\n  Transformer Accelerator with Silicon Photonics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision\n  Transformer Accelerator with Silicon Photonics"
                },
                "summary": "Vision Transformers (ViTs) have emerged as a powerful architecture for\ncomputer vision tasks due to their ability to model long-range dependencies and\nglobal contextual relationships. However, their substantial compute and memory\ndemands hinder efficient deployment in scenarios with strict energy and\nbandwidth limitations. In this work, we propose OptoViT, the first near-sensor,\nregion-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time\nand energy-efficient vision processing. Opto-ViT features a hybrid\nelectronic-photonic architecture, where the optical core handles\ncompute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting\nLasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and\nnormalization are executed electronically. To reduce redundant computation and\npatch processing, we introduce a lightweight Mask Generation Network (MGNet)\nthat identifies regions of interest in the current frame and prunes irrelevant\npatches before ViT encoding. We further co-optimize the ViT backbone using\nquantization-aware training and matrix decomposition tailored for photonic\nconstraints. Experiments across device fabrication, circuit and architecture\nco-design, to classification, detection, and video tasks demonstrate that\nOptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6%\naccuracy loss, while enabling scalable and efficient ViT deployment at the\nedge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have emerged as a powerful architecture for\ncomputer vision tasks due to their ability to model long-range dependencies and\nglobal contextual relationships. However, their substantial compute and memory\ndemands hinder efficient deployment in scenarios with strict energy and\nbandwidth limitations. In this work, we propose OptoViT, the first near-sensor,\nregion-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time\nand energy-efficient vision processing. Opto-ViT features a hybrid\nelectronic-photonic architecture, where the optical core handles\ncompute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting\nLasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and\nnormalization are executed electronically. To reduce redundant computation and\npatch processing, we introduce a lightweight Mask Generation Network (MGNet)\nthat identifies regions of interest in the current frame and prunes irrelevant\npatches before ViT encoding. We further co-optimize the ViT backbone using\nquantization-aware training and matrix decomposition tailored for photonic\nconstraints. Experiments across device fabrication, circuit and architecture\nco-design, to classification, detection, and video tasks demonstrate that\nOptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6%\naccuracy loss, while enabling scalable and efficient ViT deployment at the\nedge."
                },
                "authors": [
                    {
                        "name": "Mehrdad Morsali"
                    },
                    {
                        "name": "Chengwei Zhou"
                    },
                    {
                        "name": "Deniz Najafi"
                    },
                    {
                        "name": "Sreetama Sarkar"
                    },
                    {
                        "name": "Pietro Mercati"
                    },
                    {
                        "name": "Navid Khoshavi"
                    },
                    {
                        "name": "Peter Beerel"
                    },
                    {
                        "name": "Mahdi Nikdast"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Shaahin Angizi"
                    }
                ],
                "author_detail": {
                    "name": "Shaahin Angizi"
                },
                "author": "Shaahin Angizi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07033v1",
                "updated": "2025-07-09T17:03:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    3,
                    50,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T17:03:50Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    17,
                    3,
                    50,
                    2,
                    190,
                    0
                ],
                "title": "Self-Supervised Learning at the Edge: The Cost of Labeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Supervised Learning at the Edge: The Cost of Labeling"
                },
                "summary": "Contrastive learning (CL) has recently emerged as an alternative to\ntraditional supervised machine learning solutions by enabling rich\nrepresentations from unstructured and unlabeled data. However, CL and, more\nbroadly, self-supervised learning (SSL) methods often demand a large amount of\ndata and computational resources, posing challenges for deployment on\nresource-constrained edge devices. In this work, we explore the feasibility and\nefficiency of SSL techniques for edge-based learning, focusing on trade-offs\nbetween model performance and energy efficiency. In particular, we analyze how\ndifferent SSL techniques adapt to limited computational, data, and energy\nbudgets, evaluating their effectiveness in learning robust representations\nunder resource-constrained settings. Moreover, we also consider the energy\ncosts involved in labeling data and assess how semi-supervised learning may\nassist in reducing the overall energy consumed to train CL models. Through\nextensive experiments, we demonstrate that tailored SSL strategies can achieve\ncompetitive performance while reducing resource consumption by up to 4X,\nunderscoring their potential for energy-efficient learning at the edge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive learning (CL) has recently emerged as an alternative to\ntraditional supervised machine learning solutions by enabling rich\nrepresentations from unstructured and unlabeled data. However, CL and, more\nbroadly, self-supervised learning (SSL) methods often demand a large amount of\ndata and computational resources, posing challenges for deployment on\nresource-constrained edge devices. In this work, we explore the feasibility and\nefficiency of SSL techniques for edge-based learning, focusing on trade-offs\nbetween model performance and energy efficiency. In particular, we analyze how\ndifferent SSL techniques adapt to limited computational, data, and energy\nbudgets, evaluating their effectiveness in learning robust representations\nunder resource-constrained settings. Moreover, we also consider the energy\ncosts involved in labeling data and assess how semi-supervised learning may\nassist in reducing the overall energy consumed to train CL models. Through\nextensive experiments, we demonstrate that tailored SSL strategies can achieve\ncompetitive performance while reducing resource consumption by up to 4X,\nunderscoring their potential for energy-efficient learning at the edge."
                },
                "authors": [
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Fernanda Fam"
                    },
                    {
                        "name": "Asal Rangrazi"
                    },
                    {
                        "name": "Marco Miozzo"
                    },
                    {
                        "name": "Charalampos Kalalas"
                    },
                    {
                        "name": "Paolo Dini"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Dini"
                },
                "author": "Paolo Dini",
                "arxiv_comment": "Accepted for publication in IEEE MLSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07026v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07026v1",
                "updated": "2025-07-09T16:57:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    57,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:57:59Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    57,
                    59,
                    2,
                    190,
                    0
                ],
                "title": "Exploring Fairness Interventions in Open Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Fairness Interventions in Open Source Projects"
                },
                "summary": "The deployment of biased machine learning (ML) models has resulted in adverse\neffects in crucial sectors such as criminal justice and healthcare. To address\nthese challenges, a diverse range of machine learning fairness interventions\nhave been developed, aiming to mitigate bias and promote the creation of more\nequitable models. Despite the growing availability of these interventions,\ntheir adoption in real-world applications remains limited, with many\npractitioners unaware of their existence. To address this gap, we\nsystematically identified and compiled a dataset of 62 open source fairness\ninterventions and identified active ones. We conducted an in-depth analysis of\ntheir specifications and features to uncover considerations that may drive\npractitioner preference and to identify the software interventions actively\nmaintained in the open source ecosystem. Our findings indicate that 32% of\nthese interventions have been actively maintained within the past year, and 50%\nof them offer both bias detection and mitigation capabilities, mostly during\ninprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of biased machine learning (ML) models has resulted in adverse\neffects in crucial sectors such as criminal justice and healthcare. To address\nthese challenges, a diverse range of machine learning fairness interventions\nhave been developed, aiming to mitigate bias and promote the creation of more\nequitable models. Despite the growing availability of these interventions,\ntheir adoption in real-world applications remains limited, with many\npractitioners unaware of their existence. To address this gap, we\nsystematically identified and compiled a dataset of 62 open source fairness\ninterventions and identified active ones. We conducted an in-depth analysis of\ntheir specifications and features to uncover considerations that may drive\npractitioner preference and to identify the software interventions actively\nmaintained in the open source ecosystem. Our findings indicate that 32% of\nthese interventions have been actively maintained within the past year, and 50%\nof them offer both bias detection and mitigation capabilities, mostly during\ninprocessing."
                },
                "authors": [
                    {
                        "name": "Sadia Afrin Mim"
                    },
                    {
                        "name": "Fatema Tuz Zohra"
                    },
                    {
                        "name": "Justin Smith"
                    },
                    {
                        "name": "Brittany Johnson"
                    }
                ],
                "author_detail": {
                    "name": "Brittany Johnson"
                },
                "author": "Brittany Johnson",
                "arxiv_comment": "Revised version accepted at the 1st International Workshop on\n  Fairness in Software Systems(SANER 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07026v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07026v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07017v1",
                "updated": "2025-07-09T16:45:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    45,
                    48,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:45:48Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    45,
                    48,
                    2,
                    190,
                    0
                ],
                "title": "First Return, Entropy-Eliciting Explore",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First Return, Entropy-Eliciting Explore"
                },
                "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration."
                },
                "authors": [
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Taoran Liang"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Zejun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zejun Ma"
                },
                "author": "Zejun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05261v2",
                "updated": "2025-07-09T16:40:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    40,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-18T23:17:29Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    23,
                    17,
                    29,
                    2,
                    169,
                    0
                ],
                "title": "TokenShapley: Token Level Context Attribution with Shapley Value",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenShapley: Token Level Context Attribution with Shapley Value"
                },
                "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy."
                },
                "authors": [
                    {
                        "name": "Yingtai Xiao"
                    },
                    {
                        "name": "Yuqing Zhu"
                    },
                    {
                        "name": "Sirat Samyoun"
                    },
                    {
                        "name": "Wanrong Zhang"
                    },
                    {
                        "name": "Jiachen T. Wang"
                    },
                    {
                        "name": "Jian Du"
                    }
                ],
                "author_detail": {
                    "name": "Jian Du"
                },
                "author": "Jian Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03772v2",
                "updated": "2025-07-09T16:28:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    28,
                    55,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-04T18:45:10Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    18,
                    45,
                    10,
                    4,
                    185,
                    0
                ],
                "title": "Skewed Score: A statistical framework to assess autograders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skewed Score: A statistical framework to assess autograders"
                },
                "summary": "The evaluation of large language model (LLM) outputs is increasingly\nperformed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or\nautograders. While autograders offer a scalable alternative to human\nevaluation, they have shown mixed reliability and may exhibit systematic\nbiases, depending on response type, scoring methodology, domain specificity, or\nother factors. Here we propose a statistical framework based on Bayesian\ngeneralised linear models (GLMs) that enables researchers to simultaneously\nassess their autograders while addressing their primary research questions\n(e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores\nor pairwise preferences) as a function of properties of the grader (e.g., human\nvs. autograder) and the evaluated item (e.g., response length or the LLM that\ngenerated it), allowing for explicit quantification of scoring differences and\npotential biases within a unified framework. In addition, our method can be\nused to augment traditional metrics such as inter-rater agreement, by providing\nuncertainty estimates and clarifying sources of disagreement. Overall, this\napproach contributes to more robust and interpretable use of autograders in LLM\nevaluation, enabling both performance analysis and bias detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of large language model (LLM) outputs is increasingly\nperformed by other LLMs, a setup commonly known as \"LLM-as-a-judge\", or\nautograders. While autograders offer a scalable alternative to human\nevaluation, they have shown mixed reliability and may exhibit systematic\nbiases, depending on response type, scoring methodology, domain specificity, or\nother factors. Here we propose a statistical framework based on Bayesian\ngeneralised linear models (GLMs) that enables researchers to simultaneously\nassess their autograders while addressing their primary research questions\n(e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores\nor pairwise preferences) as a function of properties of the grader (e.g., human\nvs. autograder) and the evaluated item (e.g., response length or the LLM that\ngenerated it), allowing for explicit quantification of scoring differences and\npotential biases within a unified framework. In addition, our method can be\nused to augment traditional metrics such as inter-rater agreement, by providing\nuncertainty estimates and clarifying sources of disagreement. Overall, this\napproach contributes to more robust and interpretable use of autograders in LLM\nevaluation, enabling both performance analysis and bias detection."
                },
                "authors": [
                    {
                        "name": "Magda Dubois"
                    },
                    {
                        "name": "Harry Coppock"
                    },
                    {
                        "name": "Mario Giulianelli"
                    },
                    {
                        "name": "Timo Flesch"
                    },
                    {
                        "name": "Lennart Luettgau"
                    },
                    {
                        "name": "Cozmin Ududec"
                    }
                ],
                "author_detail": {
                    "name": "Cozmin Ududec"
                },
                "author": "Cozmin Ududec",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06999v1",
                "updated": "2025-07-09T16:25:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    25,
                    44,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:25:44Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    25,
                    44,
                    2,
                    190,
                    0
                ],
                "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning\n  in Multimodal LLMs"
                },
                "summary": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility."
                },
                "authors": [
                    {
                        "name": "Yahan Yu"
                    },
                    {
                        "name": "Yuyang Dong"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06993v1",
                "updated": "2025-07-09T16:18:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    18,
                    9,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:18:09Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    18,
                    9,
                    2,
                    190,
                    0
                ],
                "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced\n  Planning, Navigation, and Dynamic Adaptation"
                },
                "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response."
                },
                "authors": [
                    {
                        "name": "Jieren Deng"
                    },
                    {
                        "name": "Aleksandar Cvetkovic"
                    },
                    {
                        "name": "Pak Kiu Chung"
                    },
                    {
                        "name": "Dragomir Yankov"
                    },
                    {
                        "name": "Chiqun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiqun Zhang"
                },
                "author": "Chiqun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06992v1",
                "updated": "2025-07-09T16:15:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    15,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:15:38Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    15,
                    38,
                    2,
                    190,
                    0
                ],
                "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology\n  Report Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology\n  Report Generation"
                },
                "summary": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation."
                },
                "authors": [
                    {
                        "name": "Qilong Xing"
                    },
                    {
                        "name": "Zikai Song"
                    },
                    {
                        "name": "Youjia Zhang"
                    },
                    {
                        "name": "Na Feng"
                    },
                    {
                        "name": "Junqing Yu"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "arxiv_comment": "MICCAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12112v3",
                "updated": "2025-07-09T16:13:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    13,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2024-10-15T23:20:54Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    23,
                    20,
                    54,
                    1,
                    289,
                    0
                ],
                "title": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning Anything with Rigor: General-Purpose Zero-Shot Planning with\n  LLM-based Formalized Programming"
                },
                "summary": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have recently demonstrated strong\npotential in solving planning problems, there is a trade-off between\nflexibility and complexity. LLMs, as zero-shot planners themselves, are still\nnot capable of directly generating valid plans for complex planning problems\nsuch as multi-constraint or long-horizon tasks. On the other hand, many\nframeworks aiming to solve complex planning problems often rely on\ntask-specific preparatory efforts, such as task-specific in-context examples\nand pre-defined critics/verifiers, which limits their cross-task generalization\ncapability. In this paper, we tackle these challenges by observing that the\ncore of many planning problems lies in optimization problems: searching for the\noptimal solution (best plan) with goals subject to constraints (preconditions\nand effects of decisions). With LLMs' commonsense, reasoning, and programming\ncapabilities, this opens up the possibilities of a universal LLM-based approach\nto planning problems. Inspired by this observation, we propose LLMFP, a\ngeneral-purpose framework that leverages LLMs to capture key information from\nplanning problems and formally formulate and solve them as optimization\nproblems from scratch, with no task-specific examples needed. We apply LLMFP to\n9 planning problems, ranging from multi-constraint decision making to\nmulti-step planning problems, and demonstrate that LLMFP achieves on average\n83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet,\nsignificantly outperforming the best baseline (direct planning with OpenAI\no1-preview) with 37.6% and 40.7% improvements. We also validate components of\nLLMFP with ablation experiments and analyzed the underlying success and failure\nreasons. Project page: https://sites.google.com/view/llmfp."
                },
                "authors": [
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "57 pages, 25 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06980v1",
                "updated": "2025-07-09T16:07:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    7,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T16:07:20Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    16,
                    7,
                    20,
                    2,
                    190,
                    0
                ],
                "title": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They All Good? Evaluating the Quality of CoTs in LLM-based Code\n  Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance in code\ngeneration, particularly when augmented with chain-of-thought (CoT) prompting\ntechniques. They break down requirements into intermediate reasoning steps,\nwhich act as design rationales to guide LLMs in writing code like human\nprogrammers. Thus, the quality of these steps is crucial for ensuring the\ncorrectness and reliability of the generated code. However, little is known\nabout the quality of CoT generated by LLMs. To what extent can we trust the\nthoughts generated by LLMs? How good are they? This paper empirically explores\nthe external and internal factors of why LLMs generate unsatisfactory CoTs by\nanalyzing 1,023 failed code samples on two widely used code generation\nbenchmarks. We also evaluate their impact on code generation performance by\nanalyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting\nLLMs. Our study reveals three key findings: (1) External factors (53.60%), such\nas unclear requirements and lack of context, mainly affect CoT quality, while\ninternal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even\nwhen CoTs are correct, 18.5% of the generated code contains errors due to\ninstruction-following issues; conversely, 11.90% of correct code is paired with\nflawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when\ngiven detailed problem descriptions. These findings highlight key challenges in\nCoT-based code generation and suggest directions for improving LLM reasoning\nand reliability."
                },
                "authors": [
                    {
                        "name": "Binquan Zhang"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiwen Luo"
                    },
                    {
                        "name": "Yuxin Du"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Lin Shi"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shi"
                },
                "author": "Lin Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06956v1",
                "updated": "2025-07-09T15:39:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    39,
                    17,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T15:39:17Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    39,
                    17,
                    2,
                    190,
                    0
                ],
                "title": "Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the Robustness of Retrieval-Augmented Generation at the\n  Query Level"
                },
                "summary": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed."
                },
                "authors": [
                    {
                        "name": "Sezen Perin"
                    },
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Qutub Sha Syed"
                    },
                    {
                        "name": "Phillip Howard"
                    },
                    {
                        "name": "Aleksei Kuvshinov"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Kay-Ulrich Scholl"
                    }
                ],
                "author_detail": {
                    "name": "Kay-Ulrich Scholl"
                },
                "author": "Kay-Ulrich Scholl",
                "arxiv_comment": "Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18497v2",
                "updated": "2025-07-09T15:14:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    14,
                    46,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-24T15:28:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    15,
                    28,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "Neuron-Level Differentiation of Memorization and Generalization in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuron-Level Differentiation of Memorization and Generalization in Large\n  Language Models"
                },
                "summary": "We investigate how Large Language Models (LLMs) distinguish between\nmemorization and generalization at the neuron level. Through carefully designed\ntasks, we identify distinct neuron subsets responsible for each behavior.\nExperiments on both a GPT-2 model trained from scratch and a pretrained\nLLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level\nspecialization. We further demonstrate that inference-time interventions on\nthese neurons can steer the model's behavior toward memorization or\ngeneralization. To assess robustness, we evaluate intra-task and inter-task\nconsistency, confirming that these neuron-behavior associations reflect\ngeneralizable patterns rather than dataset-specific artifacts. Our findings\nreveal modular structure in LLMs and enable controlling memorization and\ngeneralization behaviors at inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate how Large Language Models (LLMs) distinguish between\nmemorization and generalization at the neuron level. Through carefully designed\ntasks, we identify distinct neuron subsets responsible for each behavior.\nExperiments on both a GPT-2 model trained from scratch and a pretrained\nLLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level\nspecialization. We further demonstrate that inference-time interventions on\nthese neurons can steer the model's behavior toward memorization or\ngeneralization. To assess robustness, we evaluate intra-task and inter-task\nconsistency, confirming that these neuron-behavior associations reflect\ngeneralizable patterns rather than dataset-specific artifacts. Our findings\nreveal modular structure in LLMs and enable controlling memorization and\ngeneralization behaviors at inference time."
                },
                "authors": [
                    {
                        "name": "Ko-Wei Huang"
                    },
                    {
                        "name": "Yi-Fu Fu"
                    },
                    {
                        "name": "Ching-Yu Tsai"
                    },
                    {
                        "name": "Yu-Chieh Tu"
                    },
                    {
                        "name": "Tzu-Ling Cheng"
                    },
                    {
                        "name": "Cheng-Yu Lin"
                    },
                    {
                        "name": "Yi-Ting Yang"
                    },
                    {
                        "name": "Heng-Yi Liu"
                    },
                    {
                        "name": "Keng-Te Liao"
                    },
                    {
                        "name": "Da-Cheng Juan"
                    },
                    {
                        "name": "Shou-De Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shou-De Lin"
                },
                "author": "Shou-De Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23463v2",
                "updated": "2025-07-09T15:10:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    15,
                    10,
                    56,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-30T02:03:23Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    2,
                    3,
                    23,
                    0,
                    181,
                    0
                ],
                "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What to Keep and What to Drop: Adaptive Table Filtering Framework"
                },
                "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by 70%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks."
                },
                "authors": [
                    {
                        "name": "WonJune Jang"
                    }
                ],
                "author_detail": {
                    "name": "WonJune Jang"
                },
                "author": "WonJune Jang",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06920v2",
                "updated": "2025-07-10T03:12:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    3,
                    12,
                    9,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T14:58:47Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    58,
                    47,
                    2,
                    190,
                    0
                ],
                "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing"
                },
                "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration."
                },
                "authors": [
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Maosong Cao"
                    },
                    {
                        "name": "Junnan Liu"
                    },
                    {
                        "name": "Wenwei Zhang"
                    },
                    {
                        "name": "Minnan Luo"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06911v1",
                "updated": "2025-07-09T14:49:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    49,
                    11,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:49:11Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    49,
                    11,
                    2,
                    190,
                    0
                ],
                "title": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G"
                },
                "summary": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability."
                },
                "authors": [
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Niloofar Mohamadi"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_comment": "Submitted to IEEE for publication, copyright may change without\n  notice. 8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06910v1",
                "updated": "2025-07-09T14:47:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:47:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in\n  Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in\n  Dialogues"
                },
                "summary": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task."
                },
                "authors": [
                    {
                        "name": "Fareya Ikram"
                    },
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06909v1",
                "updated": "2025-07-09T14:47:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    0,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:47:00Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    47,
                    0,
                    2,
                    190,
                    0
                ],
                "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal\n  Prediction"
                },
                "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP."
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Jiahuan Pei"
                    },
                    {
                        "name": "Diancheng Shui"
                    },
                    {
                        "name": "Zhiguang Han"
                    },
                    {
                        "name": "Xin Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "arxiv_comment": "Accepted by NLPCC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06904v1",
                "updated": "2025-07-09T14:43:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    43,
                    34,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:43:34Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    43,
                    34,
                    2,
                    190,
                    0
                ],
                "title": "Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA\n  Assisted Wireless Communication Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA\n  Assisted Wireless Communication Systems"
                },
                "summary": "To address the limitations of traditional reconfigurable intelligent surfaces\n(RIS) in spatial control capability, this paper introduces the concept of the\nfluid antenna system (FAS) and proposes a fluid simultaneously transmitting and\nreflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA)\nmulti-user communication system. In this system, each FSTAR-RIS element is\ncapable of flexible mobility and can dynamically adjust its position in\nresponse to environmental variations, thereby enabling simultaneous service to\nusers in both the transmission and reflection zones. This significantly\nenhances the system's spatial degrees of freedom (DoF) and service\nadaptability. To maximize the system's weighted sum-rate, we formulate a\nnon-convex optimization problem that jointly optimizes the base station\nbeamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the\nelement positions. An alternating optimization (AO) algorithm is developed,\nincorporating successive convex approximation (SCA), semi-definite relaxation\n(SDR), and majorization-minimization (MM) techniques. In particular, to address\nthe complex channel coupling introduced by the coexistence of direct and\nFSTAR-RIS paths, the MM framework is employed in the element position\noptimization subproblem, enabling an efficient iterative solution strategy.\nSimulation results validate that the proposed system achieves up to a 27%\nincrease in total sum rate compared to traditional STAR-RIS systems and\nrequires approximately 50% fewer RIS elements to attain the same performance,\nhighlighting its effectiveness for cost-efficient large-scale deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the limitations of traditional reconfigurable intelligent surfaces\n(RIS) in spatial control capability, this paper introduces the concept of the\nfluid antenna system (FAS) and proposes a fluid simultaneously transmitting and\nreflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA)\nmulti-user communication system. In this system, each FSTAR-RIS element is\ncapable of flexible mobility and can dynamically adjust its position in\nresponse to environmental variations, thereby enabling simultaneous service to\nusers in both the transmission and reflection zones. This significantly\nenhances the system's spatial degrees of freedom (DoF) and service\nadaptability. To maximize the system's weighted sum-rate, we formulate a\nnon-convex optimization problem that jointly optimizes the base station\nbeamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the\nelement positions. An alternating optimization (AO) algorithm is developed,\nincorporating successive convex approximation (SCA), semi-definite relaxation\n(SDR), and majorization-minimization (MM) techniques. In particular, to address\nthe complex channel coupling introduced by the coexistence of direct and\nFSTAR-RIS paths, the MM framework is employed in the element position\noptimization subproblem, enabling an efficient iterative solution strategy.\nSimulation results validate that the proposed system achieves up to a 27%\nincrease in total sum rate compared to traditional STAR-RIS systems and\nrequires approximately 50% fewer RIS elements to attain the same performance,\nhighlighting its effectiveness for cost-efficient large-scale deployment."
                },
                "authors": [
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Qu Luo"
                    },
                    {
                        "name": "Gaojie Chen"
                    },
                    {
                        "name": "Pei Xiao"
                    },
                    {
                        "name": "Ahmed Elzanaty"
                    },
                    {
                        "name": "Mohsen Khalily"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    }
                ],
                "author_detail": {
                    "name": "Rahim Tafazolli"
                },
                "author": "Rahim Tafazolli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05167v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05167v3",
                "updated": "2025-07-09T14:35:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    35,
                    23,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-07T18:49:46Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    18,
                    49,
                    46,
                    4,
                    38,
                    0
                ],
                "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoLiMa: Long-Context Evaluation Beyond Literal Matching"
                },
                "summary": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 13 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 11 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nEven models enhanced with reasoning capabilities or CoT prompting struggle to\nmaintain performance in long contexts. We publicly release the dataset and\nevaluation code at https://github.com/adobe-research/NoLiMa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) support long contexts ranging from 128K\nto 1M tokens. A popular method for evaluating these capabilities is the\nneedle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\"\n(relevant information) from a \"haystack\" (long irrelevant context). Extensions\nof this approach include increasing distractors, fact chaining, and in-context\nreasoning. However, in these benchmarks, models can exploit existing literal\nmatches between the needle and haystack to simplify the task. To address this,\nwe introduce NoLiMa, a benchmark extending NIAH with a carefully designed\nneedle set, where questions and needles have minimal lexical overlap, requiring\nmodels to infer latent associations to locate the needle within the haystack.\nWe evaluate 13 popular LLMs that claim to support contexts of at least 128K\ntokens. While they perform well in short contexts (<1K), performance degrades\nsignificantly as context length increases. At 32K, for instance, 11 models drop\nbelow 50% of their strong short-length baselines. Even GPT-4o, one of the\ntop-performing exceptions, experiences a reduction from an almost-perfect\nbaseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the\nincreased difficulty the attention mechanism faces in longer contexts when\nliteral matches are absent, making it harder to retrieve relevant information.\nEven models enhanced with reasoning capabilities or CoT prompting struggle to\nmaintain performance in long contexts. We publicly release the dataset and\nevaluation code at https://github.com/adobe-research/NoLiMa."
                },
                "authors": [
                    {
                        "name": "Ali Modarressi"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Seunghyun Yoon"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "Accepted at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05167v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05167v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06892v2",
                "updated": "2025-07-10T13:42:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    13,
                    42,
                    4,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T14:29:45Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    29,
                    45,
                    2,
                    190,
                    0
                ],
                "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning\n  for Large Language Model"
                },
                "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc."
                },
                "authors": [
                    {
                        "name": "Jing Liang"
                    },
                    {
                        "name": "Hongyao Tang"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Jinyi Liu"
                    },
                    {
                        "name": "Yan Zheng"
                    },
                    {
                        "name": "Shuyue Hu"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jianye Hao"
                    }
                ],
                "author_detail": {
                    "name": "Jianye Hao"
                },
                "author": "Jianye Hao",
                "arxiv_comment": "Preliminary version, v2, added more details and corrected some minor\n  mistakes. Project page: https://anitaleungxx.github.io/ReMix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06890v1",
                "updated": "2025-07-09T14:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    27,
                    40,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    27,
                    40,
                    2,
                    190,
                    0
                ],
                "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis\n  in Smart Microgrids Using Dual Fractional-Order Feature Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis\n  in Smart Microgrids Using Dual Fractional-Order Feature Analysis"
                },
                "summary": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Wang"
                },
                "author": "Yifan Wang",
                "arxiv_comment": "8 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06884v1",
                "updated": "2025-07-09T14:19:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    19,
                    58,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T14:19:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    14,
                    19,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "Toward a Full-Stack Co-Simulation Platform for Testing of Automated\n  Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Full-Stack Co-Simulation Platform for Testing of Automated\n  Driving Systems"
                },
                "summary": "Virtual testing has emerged as an effective approach to accelerate the\ndeployment of automated driving systems. Nevertheless, existing simulation\ntoolchains encounter difficulties in integrating rapid, automated scenario\ngeneration with simulation environments supporting advanced automated driving\ncapabilities. To address this limitation, a full-stack toolchain is presented,\nenabling automatic scenario generation from real-world datasets and efficient\nvalidation through a co-simulation platform based on CarMaker, ROS, and Apollo.\nThe simulation results demonstrate the effectiveness of the proposed toolchain.\nA demonstration video showcasing the toolchain is available at the provided\nlink: https://youtu.be/taJw_-CmSiY.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual testing has emerged as an effective approach to accelerate the\ndeployment of automated driving systems. Nevertheless, existing simulation\ntoolchains encounter difficulties in integrating rapid, automated scenario\ngeneration with simulation environments supporting advanced automated driving\ncapabilities. To address this limitation, a full-stack toolchain is presented,\nenabling automatic scenario generation from real-world datasets and efficient\nvalidation through a co-simulation platform based on CarMaker, ROS, and Apollo.\nThe simulation results demonstrate the effectiveness of the proposed toolchain.\nA demonstration video showcasing the toolchain is available at the provided\nlink: https://youtu.be/taJw_-CmSiY."
                },
                "authors": [
                    {
                        "name": "Dong Bi"
                    },
                    {
                        "name": "Yongqi Zhao"
                    },
                    {
                        "name": "Zhengguo Gu"
                    },
                    {
                        "name": "Tomislav Mihalj"
                    },
                    {
                        "name": "Jia Hu"
                    },
                    {
                        "name": "Arno Eichberger"
                    }
                ],
                "author_detail": {
                    "name": "Arno Eichberger"
                },
                "author": "Arno Eichberger",
                "arxiv_comment": "IEEE International Conference on Intelligent Transportation Systems\n  (ITSC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04204v2",
                "updated": "2025-07-09T13:58:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    58,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-05T15:18:55Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    15,
                    18,
                    55,
                    5,
                    95,
                    0
                ],
                "title": "Adaptive Elicitation of Latent Information Using Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Elicitation of Latent Information Using Natural Language"
                },
                "summary": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting information to reduce uncertainty about a latent entity is a\ncritical task in many application domains, e.g., assessing individual student\nlearning outcomes, diagnosing underlying diseases, or learning user\npreferences. Though natural language is a powerful medium for this purpose,\nlarge language models (LLMs) and existing fine-tuning algorithms lack\nmechanisms for strategically gathering information to refine their own\nunderstanding of the latent entity. To harness the generalization power and\nworld knowledge of LLMs in developing effective information-gathering\nstrategies, we propose an adaptive elicitation framework that actively reduces\nuncertainty on the latent entity. Since probabilistic modeling of an abstract\nlatent entity is difficult, our framework adopts a predictive view of\nuncertainty, using a meta-learned language model to simulate future\nobservations and enable scalable uncertainty quantification over complex\nnatural language. Through autoregressive forward simulation, our model\nquantifies how new questions reduce epistemic uncertainty, enabling the\ndevelopment of sophisticated information-gathering strategies to choose the\nmost informative next queries. In experiments on the 20 questions game, dynamic\nopinion polling, and adaptive student assessment, our method consistently\noutperforms baselines in identifying critical unknowns and improving downstream\npredictions, illustrating the promise of strategic information gathering in\nnatural language settings."
                },
                "authors": [
                    {
                        "name": "Jimmy Wang"
                    },
                    {
                        "name": "Thomas Zollo"
                    },
                    {
                        "name": "Richard Zemel"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v2",
                "updated": "2025-07-10T15:18:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    15,
                    18,
                    20,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02579v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02579v3",
                "updated": "2025-07-09T13:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    45,
                    7,
                    2,
                    190,
                    0
                ],
                "published": "2025-05-05T11:30:46Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    11,
                    30,
                    46,
                    0,
                    125,
                    0
                ],
                "title": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and\n  Flexible LLM Fine-Tuning"
                },
                "summary": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including competing objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the fine-tuning to improve efficiency and\nflexibility. Our method is the first to aggregate the hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text classification models to score the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) for large language model (LLM)\nfine-tuning show promise in addressing multi-objective tasks but still face\nsignificant challenges, including competing objective balancing, low training\nefficiency, poor scalability, and limited explainability. Leveraging ensemble\nlearning principles, we introduce an Ensemble Multi-Objective RL (EMORL)\nframework that fine-tunes multiple models with individual objectives while\noptimizing their aggregation after the fine-tuning to improve efficiency and\nflexibility. Our method is the first to aggregate the hidden states of\nindividual models, incorporating contextual information from multiple\nobjectives. This approach is supported by a hierarchical grid search algorithm\nthat identifies optimal weighted combinations. We evaluate EMORL on counselor\nreflection generation tasks, using text classification models to score the\ngenerations and provide rewards during RL fine-tuning. Through comprehensive\nexperiments on the PAIR and Psych8k datasets, we demonstrate the advantages of\nEMORL against existing baselines: significantly lower and more stable training\nconsumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds),\nimproved scalability and explainability, and comparable performance across\nmultiple objectives."
                },
                "authors": [
                    {
                        "name": "Lingxiao Kong"
                    },
                    {
                        "name": "Cong Yang"
                    },
                    {
                        "name": "Susanne Neufang"
                    },
                    {
                        "name": "Oya Deniz Beyan"
                    },
                    {
                        "name": "Zeyd Boukhers"
                    }
                ],
                "author_detail": {
                    "name": "Zeyd Boukhers"
                },
                "author": "Zeyd Boukhers",
                "arxiv_comment": "14 pages, 9 figures, accepted by the SIGDIAL 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02579v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02579v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06838v2",
                "updated": "2025-07-10T01:36:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    36,
                    33,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T13:35:36Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    35,
                    36,
                    2,
                    190,
                    0
                ],
                "title": "Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shifting from Ranking to Set Selection for Retrieval Augmented\n  Generation"
                },
                "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR"
                },
                "authors": [
                    {
                        "name": "Dahyun Lee"
                    },
                    {
                        "name": "Yongrae Jo"
                    },
                    {
                        "name": "Haeju Park"
                    },
                    {
                        "name": "Moontae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Moontae Lee"
                },
                "author": "Moontae Lee",
                "arxiv_comment": "Accepted to ACL 2025 main (Oral Presentation)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06833v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06833v1",
                "updated": "2025-07-09T13:31:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    31,
                    51,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:31:51Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    31,
                    51,
                    2,
                    190,
                    0
                ],
                "title": "Enhancing Environment Generalizability for Deep Learning-Based CSI\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Environment Generalizability for Deep Learning-Based CSI\n  Feedback"
                },
                "summary": "Accurate and low-overhead channel state information (CSI) feedback is\nessential to boost the capacity of frequency division duplex (FDD) massive\nmultiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback\nsignificantly outperforms conventional approaches. Nevertheless, current deep\nlearning-based CSI feedback algorithms exhibit limited generalizability to\nunseen environments, which obviously increases the deployment cost. In this\npaper, we first model the distribution shift of CSI across different\nenvironments, which is composed of the distribution shift of multipath\nstructure and a single-path. Then, EG-CsiNet is proposed as a novel CSI\nfeedback learning framework to enhance environment-generalizability.\nExplicitly, EG-CsiNet comprises the modules of multipath decoupling and\nfine-grained alignment, which can address the distribution shift of multipath\nstructure and a single path. Based on extensive simulations, the proposed\nEG-CsiNet can robustly enhance the generalizability in unseen environments\ncompared to the state-of-the-art, especially in challenging conditions with a\nsingle source environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and low-overhead channel state information (CSI) feedback is\nessential to boost the capacity of frequency division duplex (FDD) massive\nmultiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback\nsignificantly outperforms conventional approaches. Nevertheless, current deep\nlearning-based CSI feedback algorithms exhibit limited generalizability to\nunseen environments, which obviously increases the deployment cost. In this\npaper, we first model the distribution shift of CSI across different\nenvironments, which is composed of the distribution shift of multipath\nstructure and a single-path. Then, EG-CsiNet is proposed as a novel CSI\nfeedback learning framework to enhance environment-generalizability.\nExplicitly, EG-CsiNet comprises the modules of multipath decoupling and\nfine-grained alignment, which can address the distribution shift of multipath\nstructure and a single path. Based on extensive simulations, the proposed\nEG-CsiNet can robustly enhance the generalizability in unseen environments\ncompared to the state-of-the-art, especially in challenging conditions with a\nsingle source environment."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Shuangfeng Han"
                    },
                    {
                        "name": "Xiaoyun Wang"
                    },
                    {
                        "name": "Zhi Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Sun"
                },
                "author": "Zhi Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06833v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06833v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06829v1",
                "updated": "2025-07-09T13:28:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    28,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T13:28:35Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    28,
                    35,
                    2,
                    190,
                    0
                ],
                "title": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal\n  Semantic Entropy-Guided Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal\n  Semantic Entropy-Guided Framework"
                },
                "summary": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy..."
                },
                "authors": [
                    {
                        "name": "Zenan Xu"
                    },
                    {
                        "name": "Zexuan Qiu"
                    },
                    {
                        "name": "Guanhua Huang"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Kejiao Li"
                    },
                    {
                        "name": "Qi Yi"
                    },
                    {
                        "name": "Yuhao Jiang"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "arxiv_comment": "13 pages, 5 fiures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11703v2",
                "updated": "2025-07-09T13:26:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    26,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-17T11:40:48Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    40,
                    48,
                    0,
                    48,
                    0
                ],
                "title": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models\n  in Medical Quality Control Indicator Calculation"
                },
                "summary": "Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepository https://github.com/YuY-2001/C-MQCIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical quality control indicators are essential to assess the qualifications\nof healthcare institutions for medical services. With the impressive\nperformance of large language models (LLMs) like GPT-4 in the medical field,\nleveraging these technologies for the Medical Quality Control Indicator\nCalculation (MQCIC) presents a promising approach. In this work, (1) we\nintroduce a real-world task MQCIC and propose an open-source Chinese electronic\nmedical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances\nand 76 indicators. (2) We propose a semi-automatic method to enhance the rule\nrepresentation. Then we propose the Clinical Facts-based Inferential Rule\n(CF-IR) method that disentangles the clinical fact verification and inferential\nrule reasoning actions. (3) We conduct comprehensive experiments on 20\nrepresentative LLMs, covering general and medical models. Our findings reveal\nthat CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct\nan error analysis and investigate the capabilities of clinical fact\nverification and inferential rule reasoning, providing insights to improve\nperformance in the MQCIC further. The dataset and code is available in this\nrepository https://github.com/YuY-2001/C-MQCIC."
                },
                "authors": [
                    {
                        "name": "Guangya Yu"
                    },
                    {
                        "name": "Yanhao Li"
                    },
                    {
                        "name": "Zongying Jiang"
                    },
                    {
                        "name": "Yuxiong Jin"
                    },
                    {
                        "name": "Li Dai"
                    },
                    {
                        "name": "Yupian Lin"
                    },
                    {
                        "name": "Ruihui Hou"
                    },
                    {
                        "name": "Weiyan Zhang"
                    },
                    {
                        "name": "Yongqi Fan"
                    },
                    {
                        "name": "Qi Ye"
                    },
                    {
                        "name": "Jingping Liu"
                    },
                    {
                        "name": "Tong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Tong Ruan"
                },
                "author": "Tong Ruan",
                "arxiv_comment": "2025 ACL Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15167v2",
                "updated": "2025-07-09T13:20:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    20,
                    45,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-18T06:28:22Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    28,
                    22,
                    2,
                    169,
                    0
                ],
                "title": "LLM Agent for Hyper-Parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agent for Hyper-Parameter Optimization"
                },
                "summary": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters optimization\napproaches for Warm-Start Particles Swarm Optimization with Crossover and\nMutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial\nvehicle (UAV) trajectory and communication, are primarily heuristic-based,\nexhibiting low levels of automation and improvable performance. In this paper,\nwe design an Large Language Model (LLM) agent for automatic\nhyper-parameters-tuning, where an iterative framework and Model Context\nProtocol (MCP) are applied. In particular, the LLM agent is first set up via a\nprofile, which specifies the boundary of hyper-parameters, task objective,\nterminal condition, conservative or aggressive strategy of optimizing\nhyper-parameters, and LLM configurations. Then, the LLM agent iteratively\ninvokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the\nloop based on the terminal condition and returns an optimized set of\nhyperparameters. Our experiment results show that the minimal sum-rate achieved\nby hyper-parameters generated via our LLM agent is significantly higher than\nthose by both human heuristics and random generation methods. This indicates\nthat an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in\nseeking high-performance hyper-parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-parameters are essential and critical for the performance of\ncommunication algorithms. However, current hyper-parameters optimization\napproaches for Warm-Start Particles Swarm Optimization with Crossover and\nMutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial\nvehicle (UAV) trajectory and communication, are primarily heuristic-based,\nexhibiting low levels of automation and improvable performance. In this paper,\nwe design an Large Language Model (LLM) agent for automatic\nhyper-parameters-tuning, where an iterative framework and Model Context\nProtocol (MCP) are applied. In particular, the LLM agent is first set up via a\nprofile, which specifies the boundary of hyper-parameters, task objective,\nterminal condition, conservative or aggressive strategy of optimizing\nhyper-parameters, and LLM configurations. Then, the LLM agent iteratively\ninvokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the\nloop based on the terminal condition and returns an optimized set of\nhyperparameters. Our experiment results show that the minimal sum-rate achieved\nby hyper-parameters generated via our LLM agent is significantly higher than\nthose by both human heuristics and random generation methods. This indicates\nthat an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in\nseeking high-performance hyper-parameters."
                },
                "authors": [
                    {
                        "name": "Wanzhe Wang"
                    },
                    {
                        "name": "Jianqiu Peng"
                    },
                    {
                        "name": "Menghao Hu"
                    },
                    {
                        "name": "Weihuang Zhong"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Wanli Ni"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Ni"
                },
                "author": "Wanli Ni",
                "arxiv_comment": "6 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03933v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03933v2",
                "updated": "2025-07-09T13:14:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    14,
                    29,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-05T07:36:49Z",
                "published_parsed": [
                    2025,
                    7,
                    5,
                    7,
                    36,
                    49,
                    5,
                    186,
                    0
                ],
                "title": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Losing our Tail -- Again: On (Un)Natural Selection And Multilingual\n  Large Language Models"
                },
                "summary": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to offload the task of writing itself\nto these technologies, enabling them to change our linguistic ecosystem more\ndirectly. While they provide us quick access to information and impressively\nfluent output, beneath their apparent sophistication lies a subtle, more\ninsidious threat: the gradual decline and loss of linguistic diversity. With\nthis opinion piece, I explore how model collapse, with a particular focus on\ntranslation technology, can lead to the loss of linguistic forms, grammatical\nfeatures, and cultural nuance. Model collapse refers to the eventual\nconsequence of self-consuming training loops, where models reinforce their own\nbiases and lose linguistic diversity. Drawing on recent work in Computer\nVision, Natural Language Processing (NLP) and Machine Translation (MT), I argue\nthat the tails of our linguistic distributions are vanishing, and with them,\nthe narratives and identities they carry. This is a call to resist linguistic\nflattening and to reimagine NLP as a field that encourages, values and protects\nexpressive multilingual lexical and linguistic diversity and creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Large Language Models (LLMs) considerably changed how\ntechnologies can influence language. While previous technologies could mediate\nor assist humans, there is now a tendency to offload the task of writing itself\nto these technologies, enabling them to change our linguistic ecosystem more\ndirectly. While they provide us quick access to information and impressively\nfluent output, beneath their apparent sophistication lies a subtle, more\ninsidious threat: the gradual decline and loss of linguistic diversity. With\nthis opinion piece, I explore how model collapse, with a particular focus on\ntranslation technology, can lead to the loss of linguistic forms, grammatical\nfeatures, and cultural nuance. Model collapse refers to the eventual\nconsequence of self-consuming training loops, where models reinforce their own\nbiases and lose linguistic diversity. Drawing on recent work in Computer\nVision, Natural Language Processing (NLP) and Machine Translation (MT), I argue\nthat the tails of our linguistic distributions are vanishing, and with them,\nthe narratives and identities they carry. This is a call to resist linguistic\nflattening and to reimagine NLP as a field that encourages, values and protects\nexpressive multilingual lexical and linguistic diversity and creativity."
                },
                "authors": [
                    {
                        "name": "Eva Vanmassenhove"
                    }
                ],
                "author_detail": {
                    "name": "Eva Vanmassenhove"
                },
                "author": "Eva Vanmassenhove",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03933v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03933v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09347v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09347v3",
                "updated": "2025-07-09T13:09:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    9,
                    13,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-12T12:49:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "arxiv_comment": "9 pages, ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09347v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09347v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06803v1",
                "updated": "2025-07-09T12:44:49Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    44,
                    49,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T12:44:49Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    44,
                    49,
                    2,
                    190,
                    0
                ],
                "title": "Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text to model via SysML: Automated generation of dynamical system\n  computational models from unstructured natural language text via enhanced\n  System Modeling Language diagrams"
                },
                "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only."
                },
                "authors": [
                    {
                        "name": "Matthew Anderson Hendricks"
                    },
                    {
                        "name": "Alice Cicirello"
                    }
                ],
                "author_detail": {
                    "name": "Alice Cicirello"
                },
                "author": "Alice Cicirello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06795v2",
                "updated": "2025-07-10T07:05:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    7,
                    5,
                    41,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T12:30:42Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    30,
                    42,
                    2,
                    190,
                    0
                ],
                "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining"
                },
                "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment."
                },
                "authors": [
                    {
                        "name": "Seonwu Kim"
                    },
                    {
                        "name": "Yohan Na"
                    },
                    {
                        "name": "Kihun Kim"
                    },
                    {
                        "name": "Hanhee Cho"
                    },
                    {
                        "name": "Geun Lim"
                    },
                    {
                        "name": "Mintae Kim"
                    },
                    {
                        "name": "Seongik Park"
                    },
                    {
                        "name": "Ki Hyun Kim"
                    },
                    {
                        "name": "Youngsub Han"
                    },
                    {
                        "name": "Byoung-Ki Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Ki Jeon"
                },
                "author": "Byoung-Ki Jeon",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16903v2",
                "updated": "2025-07-09T12:13:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    13,
                    12,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-24T06:57:27Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    57,
                    27,
                    0,
                    55,
                    0
                ],
                "title": "GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of\n  In-the-wild LLM Jailbreak Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of\n  In-the-wild LLM Jailbreak Methods"
                },
                "summary": "Despite the growing interest in jailbreak methods as an effective red-teaming\ntool for building safe and responsible large language models (LLMs), flawed\nevaluation system designs have led to significant discrepancies in their\neffectiveness assessments. We conduct a systematic measurement study based on\n37 jailbreak studies since 2022, focusing on both the methods and the\nevaluation systems they employ. We find that existing evaluation systems lack\ncase-specific criteria, resulting in misleading conclusions about their\neffectiveness and safety implications. This paper advocates a shift to a more\nnuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel\nbenchmark comprising a curated harmful question dataset, detailed case-by-case\nevaluation guidelines and an evaluation system integrated with these guidelines\n-- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate\nmeasurements of jailbreak performance, enabling meaningful comparisons across\nmethods and uncovering new insights overlooked in previous evaluations.\nGuidedEval reduces inter-evaluator variance by at least 76.03\\%. Furthermore,\nwe observe that incorporating guidelines can enhance the effectiveness of\njailbreak methods themselves, offering new insights into both attack strategies\nand evaluation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing interest in jailbreak methods as an effective red-teaming\ntool for building safe and responsible large language models (LLMs), flawed\nevaluation system designs have led to significant discrepancies in their\neffectiveness assessments. We conduct a systematic measurement study based on\n37 jailbreak studies since 2022, focusing on both the methods and the\nevaluation systems they employ. We find that existing evaluation systems lack\ncase-specific criteria, resulting in misleading conclusions about their\neffectiveness and safety implications. This paper advocates a shift to a more\nnuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel\nbenchmark comprising a curated harmful question dataset, detailed case-by-case\nevaluation guidelines and an evaluation system integrated with these guidelines\n-- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate\nmeasurements of jailbreak performance, enabling meaningful comparisons across\nmethods and uncovering new insights overlooked in previous evaluations.\nGuidedEval reduces inter-evaluator variance by at least 76.03\\%. Furthermore,\nwe observe that incorporating guidelines can enhance the effectiveness of\njailbreak methods themselves, offering new insights into both attack strategies\nand evaluation paradigms."
                },
                "authors": [
                    {
                        "name": "Ruixuan Huang"
                    },
                    {
                        "name": "Xunguang Wang"
                    },
                    {
                        "name": "Zongjie Li"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Shuai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuai Wang"
                },
                "author": "Shuai Wang",
                "arxiv_comment": "Homepage: https://sproutnan.github.io/AI-Safety_Benchmark/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06774v1",
                "updated": "2025-07-09T12:03:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    3,
                    6,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T12:03:06Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    3,
                    6,
                    2,
                    190,
                    0
                ],
                "title": "Checklist Engineering Empowers Multilingual LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Checklist Engineering Empowers Multilingual LLM Judges"
                },
                "summary": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghiasvand Mohammadkhani"
                    },
                    {
                        "name": "Hamid Beigy"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Beigy"
                },
                "author": "Hamid Beigy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04446v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04446v2",
                "updated": "2025-07-09T11:52:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    52,
                    25,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-06T16:13:33Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    16,
                    13,
                    33,
                    6,
                    187,
                    0
                ],
                "title": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient\n  LLM Jailbreaking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tail-aware Adversarial Attacks: A Distributional Approach to Efficient\n  LLM Jailbreaking"
                },
                "summary": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To guarantee safe and robust deployment of large language models (LLMs) at\nscale, it is critical to accurately assess their adversarial robustness.\nExisting adversarial attacks typically target harmful responses in\nsingle-point, greedy generations, overlooking the inherently stochastic nature\nof LLMs. In this paper, we propose a novel framework for adversarial robustness\nevaluation that explicitly models the entire output distribution, including\ntail-risks, providing better estimates for model robustness at scale. By\ncasting the attack process as a resource allocation problem between\noptimization and sampling, we determine compute-optimal tradeoffs and show that\nintegrating sampling into existing attacks boosts ASR by up to 48% and improves\nefficiency by up to two orders of magnitude. Our framework also enables us to\nanalyze how different attack algorithms affect output harm distributions.\nSurprisingly, we find that most optimization strategies have little effect on\noutput harmfulness. Finally, we introduce a data-free proof-of-concept\nobjective based on entropy-maximization to demonstrate how our tail-aware\nperspective enables new optimization targets. Overall, our findings highlight\nthe importance of tail-aware attacks and evaluation protocols to accurately\nassess and strengthen LLM safety."
                },
                "authors": [
                    {
                        "name": "Tim Beyer"
                    },
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Leo Schwinn"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Gnnemann"
                },
                "author": "Stephan Gnnemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04446v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04446v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06762v1",
                "updated": "2025-07-09T11:38:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    38,
                    53,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T11:38:53Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    38,
                    53,
                    2,
                    190,
                    0
                ],
                "title": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation"
                },
                "summary": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic conflicts arise when a developer introduces changes to a codebase\nthat unintentionally affect the behavior of changes integrated in parallel by\nother developers. Traditional merge tools are unable to detect such conflicts,\nso complementary tools like SMAT have been proposed. SMAT relies on generating\nand executing unit tests: if a test fails on the base version, passes on a\ndeveloper's modified version, but fails again after merging with another\ndeveloper's changes, a semantic conflict is indicated. While SMAT is effective\nat detecting conflicts, it suffers from a high rate of false negatives, partly\ndue to the limitations of unit test generation tools such as Randoop and\nEvosuite. To investigate whether large language models (LLMs) can overcome\nthese limitations, we propose and integrate a new test generation tool based on\nCode Llama 70B into SMAT. We explore the model's ability to generate tests\nusing different interaction strategies, prompt contents, and parameter\nconfigurations. Our evaluation uses two samples: a benchmark with simpler\nsystems from related work, and a more significant sample based on complex,\nreal-world systems. We assess the effectiveness of the new SMAT extension in\ndetecting conflicts. Results indicate that, although LLM-based test generation\nremains challenging and computationally expensive in complex scenarios, there\nis promising potential for improving semantic conflict detection.\n  --\n  Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\\c{c}as em\numa base de c\\'odigo que afetam, de forma n~ao intencional, o comportamento de\naltera\\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas\ntradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso\nferramentas complementares como o SMAT foram propostas. O SMAT depende da\ngera\\c{c}~ao e execu\\c{c}~ao de testes de unidade: se um teste falha na vers~ao\nbase, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar\nap\\'os o merge com as mudan\\c{c}as de outro desenvolvedor, um conflito\nsem^antico \\'e identificado. Embora o SMAT seja eficaz na detec\\c{c}~ao de\nconflitos, apresenta alta taxa de falsos negativos, em parte devido \\`as\nlimita\\c{c}~oes das ferramentas de gera\\c{c}~ao de testes como Randoop e\nEvosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem\nsuperar essas limita\\c{c}~oes, propomos e integramos ao SMAT uma nova\nferramenta de gera\\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a\ncapacidade do modelo de gerar testes utilizando diferentes estrat\\'egias de\nintera\\c{c}~ao, conte\\'udos de prompts e configura\\c{c}~oes de par^ametros.\nNossa avalia\\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais\nsimples, usados em trabalhos relacionados, e uma amostra mais significativa\nbaseada em sistemas complexos e reais. Avaliamos a efic\\'acia da nova extens~ao\ndo SMAT na detec\\c{c}~ao de conflitos. Os resultados indicam que, embora a\ngera\\c{c}~ao de testes por LLM em cen\\'arios complexos ainda seja desafiadora e\ncustosa computacionalmente, h\\'a potencial promissor para aprimorar a\ndetec\\c{c}~ao de conflitos sem^anticos."
                },
                "authors": [
                    {
                        "name": "Nathalia Barbosa"
                    },
                    {
                        "name": "Paulo Borba"
                    },
                    {
                        "name": "Luson Da Silva"
                    }
                ],
                "author_detail": {
                    "name": "Luson Da Silva"
                },
                "arxiv_affiliation": "Polytechnique Montreal, Canad",
                "author": "Luson Da Silva",
                "arxiv_comment": "Comments: 11 pages, in Portuguese language. 3 figures. Submitted to\n  SAST 2025 (X Simp\\'osio Brasileiro de Teste de Software Sistem\\'atico e\n  Automatizado)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06761v1",
                "updated": "2025-07-09T11:38:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    38,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T11:38:20Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    38,
                    20,
                    2,
                    190,
                    0
                ],
                "title": "Finetuning Vision-Language Models as OCR Systems for Low-Resource\n  Languages: A Case Study of Manchu",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finetuning Vision-Language Models as OCR Systems for Low-Resource\n  Languages: A Case Study of Manchu"
                },
                "summary": "Manchu, a critically endangered language essential for understanding early\nmodern Eastern Eurasian history, lacks effective OCR systems that can handle\nreal-world historical documents. This study develops high-performing OCR\nsystems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,\nQwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using\nparameter-efficient training. LLaMA-3.2-11B achieved exceptional performance\nwith 98.3\\% word accuracy and 0.0024 character error rate on synthetic data,\nwhile crucially maintaining 93.1\\% accuracy on real-world handwritten\ndocuments. Comparative evaluation reveals substantial advantages over\ntraditional approaches: while a CRNN baseline achieved 99.8\\% synthetic\naccuracy, it suffered severe degradation to 72.5\\% on real documents. Our\napproach demonstrates effective synthetic-to-real domain transfer, providing a\ncost-effective solution deployable on accessible infrastructure. This work\nestablishes a transferable framework for endangered language OCR that removes\ntechnical and financial barriers in digital humanities, enabling historians and\nlinguists to process historical archives without specialized computing\nresources. Code and model weights are available at\nhttps://github.com/mic7ch1/ManchuAI-OCR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manchu, a critically endangered language essential for understanding early\nmodern Eastern Eurasian history, lacks effective OCR systems that can handle\nreal-world historical documents. This study develops high-performing OCR\nsystems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,\nQwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using\nparameter-efficient training. LLaMA-3.2-11B achieved exceptional performance\nwith 98.3\\% word accuracy and 0.0024 character error rate on synthetic data,\nwhile crucially maintaining 93.1\\% accuracy on real-world handwritten\ndocuments. Comparative evaluation reveals substantial advantages over\ntraditional approaches: while a CRNN baseline achieved 99.8\\% synthetic\naccuracy, it suffered severe degradation to 72.5\\% on real documents. Our\napproach demonstrates effective synthetic-to-real domain transfer, providing a\ncost-effective solution deployable on accessible infrastructure. This work\nestablishes a transferable framework for endangered language OCR that removes\ntechnical and financial barriers in digital humanities, enabling historians and\nlinguists to process historical archives without specialized computing\nresources. Code and model weights are available at\nhttps://github.com/mic7ch1/ManchuAI-OCR."
                },
                "authors": [
                    {
                        "name": "Yan Hon Michael Chung"
                    },
                    {
                        "name": "Donghyeok Choi"
                    }
                ],
                "author_detail": {
                    "name": "Donghyeok Choi"
                },
                "author": "Donghyeok Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03498v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03498v2",
                "updated": "2025-07-09T11:30:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    30,
                    58,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-04T11:52:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    11,
                    52,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Reinforcement Learning-based Feature Generation Algorithm for Scientific\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning-based Feature Generation Algorithm for Scientific\n  Data"
                },
                "summary": "Feature generation (FG) aims to enhance the prediction potential of original\ndata by constructing high-order feature combinations and removing redundant\nfeatures. It is a key preprocessing step for tabular scientific data to improve\ndownstream machine-learning model performance. Traditional methods face the\nfollowing two challenges when dealing with the feature generation of scientific\ndata: First, the effective construction of high-order feature combinations in\nscientific data necessitates profound and extensive domain-specific expertise.\nSecondly, as the order of feature combinations increases, the search space\nexpands exponentially, imposing prohibitive human labor consumption.\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\nopened novel avenues for automating feature generation processes. Inspired by\nthat, this paper revisits the conventional feature generation workflow and\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\nthe iterative exploration stage, multi-agents will construct mathematical\ntransformation equations collaboratively, synthesize and identify feature\ncombinations ex-hibiting high information content, and leverage a reinforcement\nlearning mechanism to evolve their strategies. Upon completing the exploration\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\nevaluate the generated features of each significant model performance\nbreakthrough. Experimental results and case studies consistently demonstrate\nthat the MAFG framework effectively automates the feature generation process\nand significantly enhances various downstream scientific data mining tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature generation (FG) aims to enhance the prediction potential of original\ndata by constructing high-order feature combinations and removing redundant\nfeatures. It is a key preprocessing step for tabular scientific data to improve\ndownstream machine-learning model performance. Traditional methods face the\nfollowing two challenges when dealing with the feature generation of scientific\ndata: First, the effective construction of high-order feature combinations in\nscientific data necessitates profound and extensive domain-specific expertise.\nSecondly, as the order of feature combinations increases, the search space\nexpands exponentially, imposing prohibitive human labor consumption.\nAdvancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have\nopened novel avenues for automating feature generation processes. Inspired by\nthat, this paper revisits the conventional feature generation workflow and\nproposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in\nthe iterative exploration stage, multi-agents will construct mathematical\ntransformation equations collaboratively, synthesize and identify feature\ncombinations ex-hibiting high information content, and leverage a reinforcement\nlearning mechanism to evolve their strategies. Upon completing the exploration\nphase, MAFG integrates the large language models (LLMs) to interpreta-tively\nevaluate the generated features of each significant model performance\nbreakthrough. Experimental results and case studies consistently demonstrate\nthat the MAFG framework effectively automates the feature generation process\nand significantly enhances various downstream scientific data mining tasks."
                },
                "authors": [
                    {
                        "name": "Meng Xiao"
                    },
                    {
                        "name": "Junfeng Zhou"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Zhou"
                },
                "author": "Yuanchun Zhou",
                "arxiv_comment": "12 pages, in Chinese language, accepted by Journal of Computer\n  Research and Development",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03498v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03498v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05558v2",
                "updated": "2025-07-09T11:25:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    25,
                    39,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T00:45:26Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    0,
                    45,
                    26,
                    1,
                    189,
                    0
                ],
                "title": "AI Agent Smart Contract Exploit Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agent Smart Contract Exploit Generation"
                },
                "summary": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a \\$6000 exploit\nvalue, while defenders require \\$60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present A1, an agentic execution driven system that transforms any LLM\ninto an end-to-end exploit generator. A1 has no hand-crafted heuristics and\nprovides the agent with six domain-specific tools that enable autonomous\nvulnerability discovery. The agent can flexibly leverage these tools to\nunderstand smart contract behavior, generate exploit strategies, test them on\nblockchain states, and refine approaches based on execution feedback. All\noutputs are concretely validated to eliminate false positives.\n  The evaluation across 36 real-world vulnerable contracts on Ethereum and\nBinance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the\nVERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional\nvulnerable contracts, with 5 cases occurring after the strongest model's\ntraining cutoff date. Across all 26 successful cases, A1 extracts up to 8.59\nmillion USD per case and 9.33 million USD total. Through 432 experiments across\nsix LLMs, we analyze iteration-wise performance showing diminishing returns\nwith average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations\n2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo\nanalysis of 19 historical attacks shows success probabilities of 85.9%-88.8%\nwithout detection delays.\n  We investigate whether an attacker or a defender benefits most from deploying\nA1 as a continuous on-chain scanning system. Our model shows that OpenAI's\no3-pro maintains profitability up to a 30.0 days scanning delay at 0.100%\nvulnerability incidence rates, while faster models require >=1.000% rates to\nbreak-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability\nrates, attackers achieve an on-chain scanning profitability at a \\$6000 exploit\nvalue, while defenders require \\$60000, raising fundamental questions about\nwhether AI agents inevitably favor exploitation over defense."
                },
                "authors": [
                    {
                        "name": "Arthur Gervais"
                    },
                    {
                        "name": "Liyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Liyi Zhou"
                },
                "author": "Liyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06750v1",
                "updated": "2025-07-09T11:15:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    15,
                    19,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T11:15:19Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    15,
                    19,
                    2,
                    190,
                    0
                ],
                "title": "Distributed Fault-Tolerant Multi-Robot Cooperative Localization in\n  Adversarial Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Fault-Tolerant Multi-Robot Cooperative Localization in\n  Adversarial Environments"
                },
                "summary": "In multi-robot systems (MRS), cooperative localization is a crucial task for\nenhancing system robustness and scalability, especially in GPS-denied or\ncommunication-limited environments. However, adversarial attacks, such as\nsensor manipulation, and communication jamming, pose significant challenges to\nthe performance of traditional localization methods. In this paper, we propose\na novel distributed fault-tolerant cooperative localization framework to\nenhance resilience against sensor and communication disruptions in adversarial\nenvironments. We introduce an adaptive event-triggered communication strategy\nthat dynamically adjusts communication thresholds based on real-time sensing\nand communication quality. This strategy ensures optimal performance even in\nthe presence of sensor degradation or communication failure. Furthermore, we\nconduct a rigorous analysis of the convergence and stability properties of the\nproposed algorithm, demonstrating its resilience against bounded adversarial\nzones and maintaining accurate state estimation. Robotarium-based experiment\nresults show that our proposed algorithm significantly outperforms traditional\nmethods in terms of localization accuracy and communication efficiency,\nparticularly in adversarial settings. Our approach offers improved scalability,\nreliability, and fault tolerance for MRS, making it suitable for large-scale\ndeployments in real-world, challenging environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-robot systems (MRS), cooperative localization is a crucial task for\nenhancing system robustness and scalability, especially in GPS-denied or\ncommunication-limited environments. However, adversarial attacks, such as\nsensor manipulation, and communication jamming, pose significant challenges to\nthe performance of traditional localization methods. In this paper, we propose\na novel distributed fault-tolerant cooperative localization framework to\nenhance resilience against sensor and communication disruptions in adversarial\nenvironments. We introduce an adaptive event-triggered communication strategy\nthat dynamically adjusts communication thresholds based on real-time sensing\nand communication quality. This strategy ensures optimal performance even in\nthe presence of sensor degradation or communication failure. Furthermore, we\nconduct a rigorous analysis of the convergence and stability properties of the\nproposed algorithm, demonstrating its resilience against bounded adversarial\nzones and maintaining accurate state estimation. Robotarium-based experiment\nresults show that our proposed algorithm significantly outperforms traditional\nmethods in terms of localization accuracy and communication efficiency,\nparticularly in adversarial settings. Our approach offers improved scalability,\nreliability, and fault tolerance for MRS, making it suitable for large-scale\ndeployments in real-world, challenging environments."
                },
                "authors": [
                    {
                        "name": "Tohid Kargar Tasooji"
                    },
                    {
                        "name": "Ramviyas Parasuraman"
                    }
                ],
                "author_detail": {
                    "name": "Ramviyas Parasuraman"
                },
                "author": "Ramviyas Parasuraman",
                "arxiv_comment": "Accepted to IROS 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06747v1",
                "updated": "2025-07-09T11:02:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    2,
                    46,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T11:02:46Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    11,
                    2,
                    46,
                    2,
                    190,
                    0
                ],
                "title": "LOVON: Legged Open-Vocabulary Object Navigator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOVON: Legged Open-Vocabulary Object Navigator"
                },
                "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON."
                },
                "authors": [
                    {
                        "name": "Daojie Peng"
                    },
                    {
                        "name": "Jiahang Cao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Jun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Jun Ma"
                },
                "author": "Jun Ma",
                "arxiv_comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03785v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03785v3",
                "updated": "2025-07-09T10:58:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    58,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-04T09:46:43Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    46,
                    43,
                    2,
                    155,
                    0
                ],
                "title": "Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knockout LLM Assessment: Using Large Language Models for Evaluations\n  through Iterative Pairwise Comparisons"
                },
                "summary": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown to be effective evaluators across\nvarious domains such as machine translations or the scientific domain. Current\nLLM-as-a-Judge approaches rely mostly on individual assessments or a single\nround of pairwise assessments, preventing the judge LLM from developing a\nglobal ranking perspective. To address this, we present Knockout Assessment, an\nLLM-asa Judge method using a knockout tournament system with iterative pairwise\ncomparisons. Experiments across three LLMs on two datasets show that knockout\nassessment improves scoring accuracy, increasing Pearson correlation with\nexpert evaluations by 0.07 on average for university-level exam scoring and\nmachine translation evaluations, aligning LLM assessments more closely with\nhuman scoring."
                },
                "authors": [
                    {
                        "name": "Isik Baran Sandan"
                    },
                    {
                        "name": "Tu Anh Dinh"
                    },
                    {
                        "name": "Jan Niehues"
                    }
                ],
                "author_detail": {
                    "name": "Jan Niehues"
                },
                "author": "Jan Niehues",
                "arxiv_comment": "Accepted to GEM @ ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03785v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03785v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06742v1",
                "updated": "2025-07-09T10:56:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    56,
                    32,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:56:32Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    56,
                    32,
                    2,
                    190,
                    0
                ],
                "title": "PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI"
                },
                "summary": "Ethical hacking today relies on highly skilled practitioners executing\ncomplex sequences of commands, which is inherently time-consuming, difficult to\nscale, and prone to human error. To help mitigate these limitations, we\npreviously introduced 'PenTest++', an AI-augmented system combining automation\nwith generative AI supporting ethical hacking workflows. However, a key\nlimitation of PenTest++ was its lack of support for privilege escalation, a\ncrucial element of ethical hacking. In this paper we present 'PenTest2.0', a\nsubstantial evolution of PenTest++ supporting automated privilege escalation\ndriven entirely by Large Language Model reasoning. It also incorporates several\nsignificant enhancements: 'Retrieval-Augmented Generation', including both\none-line and offline modes; 'Chain-of-Thought' prompting for intermediate\nreasoning; persistent 'PenTest Task Trees' to track goal progression across\nturns; and the optional integration of human-authored hints. We describe how it\noperates, present a proof-of-concept prototype, and discuss its benefits and\nlimitations. We also describe application of the system to a controlled Linux\ntarget, showing it can carry out multi-turn, adaptive privilege escalation. We\nexplain the rationale behind its core design choices, and provide comprehensive\ntesting results and cost analysis. Our findings indicate that 'PenTest2.0'\nrepresents a meaningful step toward practical, scalable, AI-automated\npenetration testing, whilst highlighting the shortcomings of generative AI\nsystems, particularly their sensitivity to prompt structure, execution context,\nand semantic drift, reinforcing the need for further research and refinement in\nthis emerging space.\n  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM\n(Large Language Model), HITL (Human-in-the-Loop)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ethical hacking today relies on highly skilled practitioners executing\ncomplex sequences of commands, which is inherently time-consuming, difficult to\nscale, and prone to human error. To help mitigate these limitations, we\npreviously introduced 'PenTest++', an AI-augmented system combining automation\nwith generative AI supporting ethical hacking workflows. However, a key\nlimitation of PenTest++ was its lack of support for privilege escalation, a\ncrucial element of ethical hacking. In this paper we present 'PenTest2.0', a\nsubstantial evolution of PenTest++ supporting automated privilege escalation\ndriven entirely by Large Language Model reasoning. It also incorporates several\nsignificant enhancements: 'Retrieval-Augmented Generation', including both\none-line and offline modes; 'Chain-of-Thought' prompting for intermediate\nreasoning; persistent 'PenTest Task Trees' to track goal progression across\nturns; and the optional integration of human-authored hints. We describe how it\noperates, present a proof-of-concept prototype, and discuss its benefits and\nlimitations. We also describe application of the system to a controlled Linux\ntarget, showing it can carry out multi-turn, adaptive privilege escalation. We\nexplain the rationale behind its core design choices, and provide comprehensive\ntesting results and cost analysis. Our findings indicate that 'PenTest2.0'\nrepresents a meaningful step toward practical, scalable, AI-automated\npenetration testing, whilst highlighting the shortcomings of generative AI\nsystems, particularly their sensitivity to prompt structure, execution context,\nand semantic drift, reinforcing the need for further research and refinement in\nthis emerging space.\n  Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM\n(Large Language Model), HITL (Human-in-the-Loop)"
                },
                "authors": [
                    {
                        "name": "Haitham S. Al-Sinani"
                    },
                    {
                        "name": "Chris J. Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "Chris J. Mitchell"
                },
                "author": "Chris J. Mitchell",
                "arxiv_comment": "45 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14541v2",
                "updated": "2025-07-09T10:55:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    55,
                    6,
                    2,
                    190,
                    0
                ],
                "published": "2025-02-20T13:20:19Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    13,
                    20,
                    19,
                    3,
                    51,
                    0
                ],
                "title": "LLM-based User Profile Management for Recommender System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based User Profile Management for Recommender System"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened new\nopportunities in recommender systems by enabling zero-shot recommendation\nwithout conventional training. Despite their potential, most existing works\nrely solely on users' purchase histories, leaving significant room for\nimprovement by incorporating user-generated textual data, such as reviews and\nproduct descriptions. Addressing this gap, we propose PURE, a novel LLM-based\nrecommendation framework that builds and maintains evolving user profiles by\nsystematically extracting and summarizing key information from user reviews.\nPURE consists of three core components: a Review Extractor for identifying user\npreferences and key product features, a Profile Updater for refining and\nupdating user profiles, and a Recommender for generating personalized\nrecommendations using the most current profile. To evaluate PURE, we introduce\na continuous sequential recommendation task that reflects real-world scenarios\nby adding reviews over time and updating predictions incrementally. Our\nexperimental results on Amazon datasets demonstrate that PURE outperforms\nexisting LLM-based methods, effectively leveraging long-term user information\nwhile managing token limitations."
                },
                "authors": [
                    {
                        "name": "Seunghwan Bang"
                    },
                    {
                        "name": "Hwanjun Song"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjun Song"
                },
                "author": "Hwanjun Song",
                "arxiv_comment": "Accepted GENNEXT@SIGIR'25 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06735v1",
                "updated": "2025-07-09T10:48:00Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    48,
                    0,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:48:00Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    48,
                    0,
                    2,
                    190,
                    0
                ],
                "title": "Residual Prior-driven Frequency-aware Network for Image Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual Prior-driven Frequency-aware Network for Image Fusion"
                },
                "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task."
                },
                "authors": [
                    {
                        "name": "Guan Zheng"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Wenhua Qian"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Runzhuo Ma"
                    }
                ],
                "author_detail": {
                    "name": "Runzhuo Ma"
                },
                "author": "Runzhuo Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06732v1",
                "updated": "2025-07-09T10:45:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    45,
                    50,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:45:50Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    45,
                    50,
                    2,
                    190,
                    0
                ],
                "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation"
                },
                "summary": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency."
                },
                "authors": [
                    {
                        "name": "Sobhan Asasi"
                    },
                    {
                        "name": "Mohamed Ilyes Lakhal"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in SLTAT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06722v1",
                "updated": "2025-07-09T10:30:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    30,
                    9,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:30:09Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    30,
                    9,
                    2,
                    190,
                    0
                ],
                "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effect of Uncertainty on Layer-wise Inference Dynamics"
                },
                "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference."
                },
                "authors": [
                    {
                        "name": "Sunwoo Kim"
                    },
                    {
                        "name": "Haneul Yoo"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "arxiv_comment": "Accepted to Actionable Interpretability Workshop - ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06719v1",
                "updated": "2025-07-09T10:20:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    20,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:20:38Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    20,
                    38,
                    2,
                    190,
                    0
                ],
                "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for\n  Open-Vocabulary 3D Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for\n  Open-Vocabulary 3D Visual Grounding"
                },
                "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability."
                },
                "authors": [
                    {
                        "name": "Zhenyang Liu"
                    },
                    {
                        "name": "Sixiao Zheng"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Cairong Zhao"
                    },
                    {
                        "name": "Longfei Liang"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06715v1",
                "updated": "2025-07-09T10:13:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    13,
                    38,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:13:38Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    13,
                    38,
                    2,
                    190,
                    0
                ],
                "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and\n  Context Aware Text Generation with LLMs"
                },
                "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust."
                },
                "authors": [
                    {
                        "name": "Garapati Keerthana"
                    },
                    {
                        "name": "Manik Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Manik Gupta"
                },
                "author": "Manik Gupta",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03635v2",
                "updated": "2025-07-09T10:08:34Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    8,
                    34,
                    2,
                    190,
                    0
                ],
                "published": "2025-04-04T17:57:22Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    57,
                    22,
                    4,
                    94,
                    0
                ],
                "title": "Do Larger Language Models Imply Better Generalization? A Pretraining\n  Scaling Law for Implicit Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Larger Language Models Imply Better Generalization? A Pretraining\n  Scaling Law for Implicit Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks requiring complex reasoning. However, the effects of\nscaling on their reasoning abilities remain insufficiently understood. In this\npaper, we introduce a synthetic multihop reasoning environment designed to\nclosely replicate the structure and distribution of real-world large-scale\nknowledge graphs. Our reasoning task involves completing missing edges in the\ngraph, which requires advanced multi-hop reasoning and mimics real-world\nreasoning scenarios. To evaluate this, we pretrain language models (LMs) from\nscratch solely on triples from the incomplete graph and assess their ability to\ninfer the missing edges. Interestingly, we observe that overparameterization\ncan impair reasoning performance due to excessive memorization. We investigate\ndifferent factors that affect this U-shaped loss curve, including graph\nstructure, model size, and training steps. To predict the optimal model size\nfor a specific knowledge graph, we find an empirical scaling that linearly maps\nthe knowledge graph search entropy to the optimal model size. This work\nprovides new insights into the relationship between scaling and reasoning in\nLLMs, shedding light on possible ways to optimize their performance for\nreasoning tasks."
                },
                "authors": [
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Shawn Tan"
                    },
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Yikang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yikang Shen"
                },
                "author": "Yikang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06710v1",
                "updated": "2025-07-09T10:08:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    8,
                    15,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T10:08:15Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    8,
                    15,
                    2,
                    190,
                    0
                ],
                "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning"
                },
                "summary": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%."
                },
                "authors": [
                    {
                        "name": "Zhenyang Liu"
                    },
                    {
                        "name": "Yikai Wang"
                    },
                    {
                        "name": "Kuanning Wang"
                    },
                    {
                        "name": "Longfei Liang"
                    },
                    {
                        "name": "Xiangyang Xue"
                    },
                    {
                        "name": "Yanwei Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanwei Fu"
                },
                "author": "Yanwei Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16525v2",
                "updated": "2025-07-09T10:04:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    10,
                    4,
                    6,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-21T08:02:58Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    8,
                    2,
                    58,
                    5,
                    356,
                    0
                ],
                "title": "One Size Does Not Fit All: Investigating Efficacy of Perplexity in\n  Detecting LLM-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Size Does Not Fit All: Investigating Efficacy of Perplexity in\n  Detecting LLM-Generated Code"
                },
                "summary": "Large language model-generated code (LLMgCode) has become increasingly common\nin software development. So far LLMgCode has more quality issues than\nhuman-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a\ncode change, while the change is signed by only human developers, without being\ncarefully examined. Many automated methods have been proposed to detect\nLLMgCode from HaCode, in which the perplexity-based method (PERPLEXITY for\nshort) is the state-of-the-art method. However, the efficacy evaluation of\nPERPLEXITY has focused on detection accuracy. Yet it is unclear whether\nPERPLEXITY is good enough in a wider range of realistic evaluation settings. To\nthis end, we carry out a family of experiments to compare PERPLEXITY against\nfeature- and pre-training-based methods from three perspectives: detection\naccuracy, detection speed, and generalization capability. The experimental\nresults show that PERPLEXITY has the best generalization capability while\nhaving limited detection accuracy and detection speed. Based on that, we\ndiscuss the strengths and limitations of PERPLEXITY, e.g., PERPLEXITY is\nunsuitable for high-level programming languages. Finally, we provide\nrecommendations to improve PERPLEXITY and apply it in practice. As the first\nlarge-scale investigation on detecting LLMgCode from HaCode, this article\nprovides a wide range of findings for future improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model-generated code (LLMgCode) has become increasingly common\nin software development. So far LLMgCode has more quality issues than\nhuman-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a\ncode change, while the change is signed by only human developers, without being\ncarefully examined. Many automated methods have been proposed to detect\nLLMgCode from HaCode, in which the perplexity-based method (PERPLEXITY for\nshort) is the state-of-the-art method. However, the efficacy evaluation of\nPERPLEXITY has focused on detection accuracy. Yet it is unclear whether\nPERPLEXITY is good enough in a wider range of realistic evaluation settings. To\nthis end, we carry out a family of experiments to compare PERPLEXITY against\nfeature- and pre-training-based methods from three perspectives: detection\naccuracy, detection speed, and generalization capability. The experimental\nresults show that PERPLEXITY has the best generalization capability while\nhaving limited detection accuracy and detection speed. Based on that, we\ndiscuss the strengths and limitations of PERPLEXITY, e.g., PERPLEXITY is\nunsuitable for high-level programming languages. Finally, we provide\nrecommendations to improve PERPLEXITY and apply it in practice. As the first\nlarge-scale investigation on detecting LLMgCode from HaCode, this article\nprovides a wide range of findings for future improvement."
                },
                "authors": [
                    {
                        "name": "Jinwei Xu"
                    },
                    {
                        "name": "He Zhang"
                    },
                    {
                        "name": "Yanjing Yang"
                    },
                    {
                        "name": "Lanxin Yang"
                    },
                    {
                        "name": "Zeru Cheng"
                    },
                    {
                        "name": "Jun Lyu"
                    },
                    {
                        "name": "Bohan Liu"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Alberto Bacchelli"
                    },
                    {
                        "name": "Yin Kia Chiam"
                    },
                    {
                        "name": "Thiam Kian Chiew"
                    }
                ],
                "author_detail": {
                    "name": "Thiam Kian Chiew"
                },
                "author": "Thiam Kian Chiew",
                "arxiv_comment": "This article has been accepted by ACM Transactions on Software\n  Engineering and Methodology (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07675v2",
                "updated": "2025-07-09T09:51:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    51,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-09T11:51:27Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    51,
                    27,
                    0,
                    160,
                    0
                ],
                "title": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUITE: A Query Rewrite System Beyond Rules with LLM Agents"
                },
                "summary": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query rewrite transforms SQL queries into semantically equivalent forms that\nrun more efficiently. Existing approaches mainly rely on predefined rewrite\nrules, but they handle a limited subset of queries and can cause performance\nregressions. This limitation stems from three challenges of rule-based query\nrewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite\nrules do not generalize to new query patterns, and (3) some rewrite techniques\ncannot be expressed as fixed rules. Motivated by the fact that human experts\nexhibit significantly better rewrite ability but suffer from scalability, and\nLarge Language Models (LLMs) have demonstrated nearly human-level semantic and\nreasoning abilities, we propose a new approach of using LLMs to rewrite SQL\nqueries beyond rules. Due to the hallucination problems in LLMs, directly\napplying LLMs often leads to nonequivalent and suboptimal queries. To address\nthis issue, we propose QUITE (query rewrite), a training-free and\nfeedback-aware system based on LLM agents that rewrites SQL queries into\nsemantically equivalent forms with significantly better performance, covering a\nbroader range of query patterns and rewrite strategies compared to rule-based\nmethods. Firstly, we design a multi-agent framework controlled by a finite\nstate machine (FSM) to equip LLMs with the ability to use external tools and\nenhance the rewrite process with real-time database feedback. Secondly, we\ndevelop a rewrite middleware to enhance the ability of LLMs to generate\noptimized query equivalents. Finally, we employ a novel hint injection\ntechnique to improve execution plans for rewritten queries. Extensive\nexperiments show that QUITE reduces query execution time by up to 35.8% over\nstate-of-the-art approaches and produces 24.1% more rewrites than prior\nmethods, covering query cases that earlier systems did not handle."
                },
                "authors": [
                    {
                        "name": "Yuyang Song"
                    },
                    {
                        "name": "Hanxu Yan"
                    },
                    {
                        "name": "Jiale Lao"
                    },
                    {
                        "name": "Yibo Wang"
                    },
                    {
                        "name": "Yufei Li"
                    },
                    {
                        "name": "Yuanchun Zhou"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Mingjie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Mingjie Tang"
                },
                "author": "Mingjie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06671v1",
                "updated": "2025-07-09T09:00:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    0,
                    52,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T09:00:52Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    9,
                    0,
                    52,
                    2,
                    190,
                    0
                ],
                "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for\n  3D Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for\n  3D Gaussian Splatting"
                },
                "summary": "3D Gaussian splatting has become a prominent technique for representing and\nrendering complex 3D scenes, due to its high fidelity and speed advantages.\nHowever, the growing demand for large-scale models calls for effective\ncompression to reduce memory and computation costs, especially on mobile and\nedge devices with limited resources. Existing compression methods effectively\nreduce 3D Gaussian parameters but often require extensive retraining or\nfine-tuning, lacking flexibility under varying compression constraints.\n  In this paper, we introduce FlexGaussian, a flexible and cost-effective\nmethod that combines mixed-precision quantization with attribute-discriminative\npruning for training-free 3D Gaussian compression. FlexGaussian eliminates the\nneed for retraining and adapts easily to diverse compression targets.\nEvaluation results show that FlexGaussian achieves up to 96.4% compression\nwhile maintaining high rendering quality (<1 dB drop in PSNR), and is\ndeployable on mobile devices. FlexGaussian delivers high compression ratios\nwithin seconds, being 1.7-2.1x faster than state-of-the-art training-free\nmethods and 10-100x faster than training-involved approaches. The code is being\nprepared and will be released soon at:\nhttps://github.com/Supercomputing-System-AI-Lab/FlexGaussian",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian splatting has become a prominent technique for representing and\nrendering complex 3D scenes, due to its high fidelity and speed advantages.\nHowever, the growing demand for large-scale models calls for effective\ncompression to reduce memory and computation costs, especially on mobile and\nedge devices with limited resources. Existing compression methods effectively\nreduce 3D Gaussian parameters but often require extensive retraining or\nfine-tuning, lacking flexibility under varying compression constraints.\n  In this paper, we introduce FlexGaussian, a flexible and cost-effective\nmethod that combines mixed-precision quantization with attribute-discriminative\npruning for training-free 3D Gaussian compression. FlexGaussian eliminates the\nneed for retraining and adapts easily to diverse compression targets.\nEvaluation results show that FlexGaussian achieves up to 96.4% compression\nwhile maintaining high rendering quality (<1 dB drop in PSNR), and is\ndeployable on mobile devices. FlexGaussian delivers high compression ratios\nwithin seconds, being 1.7-2.1x faster than state-of-the-art training-free\nmethods and 10-100x faster than training-involved approaches. The code is being\nprepared and will be released soon at:\nhttps://github.com/Supercomputing-System-AI-Lab/FlexGaussian"
                },
                "authors": [
                    {
                        "name": "Boyuan Tian"
                    },
                    {
                        "name": "Qizhe Gao"
                    },
                    {
                        "name": "Siran Xianyu"
                    },
                    {
                        "name": "Xiaotong Cui"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "To appear at ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04969v2",
                "updated": "2025-07-09T08:59:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    59,
                    36,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-07T13:12:28Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    12,
                    28,
                    0,
                    188,
                    0
                ],
                "title": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for\n  Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for\n  Serverless Computing"
                },
                "summary": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems."
                },
                "authors": [
                    {
                        "name": "Chanh Nguyen"
                    },
                    {
                        "name": "Erik Elmroth"
                    },
                    {
                        "name": "Monowar Bhuyan"
                    }
                ],
                "author_detail": {
                    "name": "Monowar Bhuyan"
                },
                "author": "Monowar Bhuyan",
                "arxiv_comment": "12 pages, 6 figures, Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16407v2",
                "updated": "2025-07-09T08:56:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    56,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-05-22T08:59:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    8,
                    59,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robust Longitudinal-lateral Look-ahead Pursuit Path-Following Control:\n  Fast Finite-Time Stability Guarantee",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Longitudinal-lateral Look-ahead Pursuit Path-Following Control:\n  Fast Finite-Time Stability Guarantee"
                },
                "summary": "This paper addresses the challenging problem of robust path-following for\nfixed-wing unmanned aerial vehicles (UAVs) in complex environments with bounded\nexternal disturbances and non-smooth predefined paths. Due to the unique\naerodynamic characteristics and flight constraints of fixed-wing UAVs,\nachieving accurate and fast stable path following remains difficult, especially\nin low-altitude mountainous terrains, urban landscapes, and under wind\ndisturbances. Most existing path-following guidance laws often struggle to\nensure fast stabilization under unknown bounded disturbances while maintaining\nsufficient robustness, and there is a lack of research on optimizing robustness\nfor non-smooth paths under flight constraints. This paper addresses these\nissues by proposing a constraints-based robust path-following controller.\nFirstly, from the perspective of global random attractor, we innovatively\nintroduce robustness metrics that quantify both the exponential convergence\nrate and the range of the ultimate attractor set. Secondly, building on these\nmetrics, we develop a robust longitudinal-lateral look-ahead pursuit (RLLP)\nguidance law for fixed-wing UAVs, specifically considering the flight path\nangle and track angle under external disturbances. Thirdly, we also derive an\noptimized version (Optimal-RLLP) to enhance the robustness metrics, and\nelaborate on the sufficient conditions for fast finite-time stability, ensuring\nthe guidance law achieves finite-time stability and robustness with reduced\nsensitivity to constrained uncertainties. The simulation results validate the\nproposed guidance law's feasibility, optimality and robustness under\natmospheric disturbances using a high-fidelity simulation platform and provide\nkey principle for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenging problem of robust path-following for\nfixed-wing unmanned aerial vehicles (UAVs) in complex environments with bounded\nexternal disturbances and non-smooth predefined paths. Due to the unique\naerodynamic characteristics and flight constraints of fixed-wing UAVs,\nachieving accurate and fast stable path following remains difficult, especially\nin low-altitude mountainous terrains, urban landscapes, and under wind\ndisturbances. Most existing path-following guidance laws often struggle to\nensure fast stabilization under unknown bounded disturbances while maintaining\nsufficient robustness, and there is a lack of research on optimizing robustness\nfor non-smooth paths under flight constraints. This paper addresses these\nissues by proposing a constraints-based robust path-following controller.\nFirstly, from the perspective of global random attractor, we innovatively\nintroduce robustness metrics that quantify both the exponential convergence\nrate and the range of the ultimate attractor set. Secondly, building on these\nmetrics, we develop a robust longitudinal-lateral look-ahead pursuit (RLLP)\nguidance law for fixed-wing UAVs, specifically considering the flight path\nangle and track angle under external disturbances. Thirdly, we also derive an\noptimized version (Optimal-RLLP) to enhance the robustness metrics, and\nelaborate on the sufficient conditions for fast finite-time stability, ensuring\nthe guidance law achieves finite-time stability and robustness with reduced\nsensitivity to constrained uncertainties. The simulation results validate the\nproposed guidance law's feasibility, optimality and robustness under\natmospheric disturbances using a high-fidelity simulation platform and provide\nkey principle for practical deployment."
                },
                "authors": [
                    {
                        "name": "Zimao Sheng"
                    },
                    {
                        "name": "Hong'an Yang"
                    },
                    {
                        "name": "Shuxiang Yang"
                    },
                    {
                        "name": "Zirui Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Yu"
                },
                "author": "Zirui Yu",
                "arxiv_comment": "21 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16029v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16029v3",
                "updated": "2025-07-09T08:51:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    51,
                    13,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-20T10:52:50Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    10,
                    52,
                    50,
                    3,
                    79,
                    0
                ],
                "title": "iDynamics: A Novel Framework for Evaluating Microservice Scheduling\n  Policies under Controllable Dynamics in Cloud-Edge Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "iDynamics: A Novel Framework for Evaluating Microservice Scheduling\n  Policies under Controllable Dynamics in Cloud-Edge Continuum"
                },
                "summary": "Designing and evaluating microservice scheduling policies is challenging,\nparticularly under dynamic conditions such as complex call-graph dependencies\nand varying cross-node networking conditions. Moreover, deploying such systems\nin real-world cloud-edge environments to evaluate scheduling strategies is\noften impractical due to complexity, cost, and limited accessibility. This\nhighlights the need for an emulation framework that can faithfully emulate the\ncharacteristics of the cloud-edge continuum. These characteristics include\ndynamic topology changes, latency-sensitive service chains, and varying\nnetworking conditions, all of which must be accurately modeled for meaningful\nevaluation. In this work, iDynamics addresses these challenges by providing a\nconfigurable and extensible framework that captures the essential dynamics of\nrunning microservice applications in cloud-edge environments, enabling\nsystematic development and testing of microservice scheduling strategies. The\nframework comprises modular components, such as the Graph Dynamics Analyzer,\nNetworking Dynamics Manager, and Scheduling Policy Extender. This enables\nfine-grained environmental control and facilitates systematic comparisons of\ndifferent scheduling strategies. Extensive experiments on a real cloud-edge\ntestbed demonstrate that iDynamics effectively captures diverse dynamic\nscenarios encountered in microservice deployments, offering a robust solution\nfor designing and evaluating different policies under realistic and\ncontrollable conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and evaluating microservice scheduling policies is challenging,\nparticularly under dynamic conditions such as complex call-graph dependencies\nand varying cross-node networking conditions. Moreover, deploying such systems\nin real-world cloud-edge environments to evaluate scheduling strategies is\noften impractical due to complexity, cost, and limited accessibility. This\nhighlights the need for an emulation framework that can faithfully emulate the\ncharacteristics of the cloud-edge continuum. These characteristics include\ndynamic topology changes, latency-sensitive service chains, and varying\nnetworking conditions, all of which must be accurately modeled for meaningful\nevaluation. In this work, iDynamics addresses these challenges by providing a\nconfigurable and extensible framework that captures the essential dynamics of\nrunning microservice applications in cloud-edge environments, enabling\nsystematic development and testing of microservice scheduling strategies. The\nframework comprises modular components, such as the Graph Dynamics Analyzer,\nNetworking Dynamics Manager, and Scheduling Policy Extender. This enables\nfine-grained environmental control and facilitates systematic comparisons of\ndifferent scheduling strategies. Extensive experiments on a real cloud-edge\ntestbed demonstrate that iDynamics effectively captures diverse dynamic\nscenarios encountered in microservice deployments, offering a robust solution\nfor designing and evaluating different policies under realistic and\ncontrollable conditions."
                },
                "authors": [
                    {
                        "name": "Ming Chen"
                    },
                    {
                        "name": "Muhammed Tawfiqul Islam"
                    },
                    {
                        "name": "Maria Rodriguez Read"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "arxiv_comment": "14 pages, 10 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16029v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16029v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13217v2",
                "updated": "2025-07-09T08:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2024-06-19T04:59:09Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    4,
                    59,
                    9,
                    2,
                    171,
                    0
                ],
                "title": "Automating IRAC Analysis in Malaysian Contract Law using a\n  Semi-Structured Knowledge Base",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating IRAC Analysis in Malaysian Contract Law using a\n  Semi-Structured Knowledge Base"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often\nlimited due to the unique legal terminologies and the necessity for highly\nspecialized knowledge. These limitations highlight the need for high-quality\ndata tailored for complex legal reasoning tasks. This paper introduces\nLegalSemi, a benchmark specifically curated for legal scenario analysis.\nLegalSemi comprises 54 legal scenarios, each rigorously annotated by legal\nexperts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)\nframework from Malaysian Contract Law. In addition, LegalSemi is accompanied by\na structured knowledge base (SKE). A series of experiments were conducted to\nassess the usefulness of LegalSemi for IRAC analysis. The experimental results\ndemonstrate the effectiveness of incorporating the SKE for issue\nidentification, rule retrieval, application and conclusion generation using\nfour different LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in legal reasoning is often\nlimited due to the unique legal terminologies and the necessity for highly\nspecialized knowledge. These limitations highlight the need for high-quality\ndata tailored for complex legal reasoning tasks. This paper introduces\nLegalSemi, a benchmark specifically curated for legal scenario analysis.\nLegalSemi comprises 54 legal scenarios, each rigorously annotated by legal\nexperts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion)\nframework from Malaysian Contract Law. In addition, LegalSemi is accompanied by\na structured knowledge base (SKE). A series of experiments were conducted to\nassess the usefulness of LegalSemi for IRAC analysis. The experimental results\ndemonstrate the effectiveness of incorporating the SKE for issue\nidentification, rule retrieval, application and conclusion generation using\nfour different LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaoxi Kang"
                    },
                    {
                        "name": "Lizhen Qu"
                    },
                    {
                        "name": "Lay-Ki Soon"
                    },
                    {
                        "name": "Zhuang Li"
                    },
                    {
                        "name": "Adnan Trakic"
                    }
                ],
                "author_detail": {
                    "name": "Adnan Trakic"
                },
                "author": "Adnan Trakic",
                "arxiv_doi": "10.1007/s10506-025-09467-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10506-025-09467-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Artificial Intelligence and Law (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.01786v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.01786v2",
                "updated": "2025-07-09T08:46:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    46,
                    58,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-02T15:12:43Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    15,
                    12,
                    43,
                    2,
                    183,
                    0
                ],
                "title": "Probing and Steering Evaluation Awareness of Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing and Steering Evaluation Awareness of Language Models"
                },
                "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception."
                },
                "authors": [
                    {
                        "name": "Jord Nguyen"
                    },
                    {
                        "name": "Khiem Hoang"
                    },
                    {
                        "name": "Carlo Leonardo Attubato"
                    },
                    {
                        "name": "Felix Hofsttter"
                    }
                ],
                "author_detail": {
                    "name": "Felix Hofsttter"
                },
                "author": "Felix Hofsttter",
                "arxiv_comment": "Actionable Interpretability Workshop (Poster) and Workshop on\n  Technical AI Governance (Poster) at ICML 2025, Vancouver, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.01786v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.01786v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03088v2",
                "updated": "2025-07-09T08:26:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    26,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-05T01:04:45Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    1,
                    4,
                    45,
                    2,
                    64,
                    0
                ],
                "title": "AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for\n  Segment Anything Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for\n  Segment Anything Model"
                },
                "summary": "The Segment Anything Model (SAM) has demonstrated strong versatility across\nvarious visual tasks. However, its large storage requirements and high\ncomputational cost pose challenges for practical deployment. Post-training\nquantization (PTQ) has emerged as an effective strategy for efficient\ndeployment, but we identify two key challenges in SAM that hinder the\neffectiveness of existing PTQ methods: the heavy-tailed and skewed distribution\nof post-GELU activations, and significant inter-channel variation in linear\nprojection activations. To address these challenges, we propose AHCPTQ, an\naccurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces\nhardware-compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU\nactivations, employing log2 quantization for dense small values and uniform\nquantization for sparse large values to enhance quantization resolution.\nAdditionally, AHCPTQ incorporates Channel-Aware Grouping (CAG) to mitigate\ninter-channel variation by progressively clustering activation channels with\nsimilar distributions, enabling them to share quantization parameters and\nimproving hardware efficiency. The combination of HLUQ and CAG not only\nenhances quantization effectiveness but also ensures compatibility with\nefficient hardware execution. For instance, under the W4A4 configuration on the\nSAM-L model, AHCPTQ achieves 36.6% mAP on instance segmentation with the DINO\ndetector, while achieving a 7.89x speedup and 8.64x energy efficiency over its\nfloating-point counterpart in FPGA implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has demonstrated strong versatility across\nvarious visual tasks. However, its large storage requirements and high\ncomputational cost pose challenges for practical deployment. Post-training\nquantization (PTQ) has emerged as an effective strategy for efficient\ndeployment, but we identify two key challenges in SAM that hinder the\neffectiveness of existing PTQ methods: the heavy-tailed and skewed distribution\nof post-GELU activations, and significant inter-channel variation in linear\nprojection activations. To address these challenges, we propose AHCPTQ, an\naccurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces\nhardware-compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU\nactivations, employing log2 quantization for dense small values and uniform\nquantization for sparse large values to enhance quantization resolution.\nAdditionally, AHCPTQ incorporates Channel-Aware Grouping (CAG) to mitigate\ninter-channel variation by progressively clustering activation channels with\nsimilar distributions, enabling them to share quantization parameters and\nimproving hardware efficiency. The combination of HLUQ and CAG not only\nenhances quantization effectiveness but also ensures compatibility with\nefficient hardware execution. For instance, under the W4A4 configuration on the\nSAM-L model, AHCPTQ achieves 36.6% mAP on instance segmentation with the DINO\ndetector, while achieving a 7.89x speedup and 8.64x energy efficiency over its\nfloating-point counterpart in FPGA implementation."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Yunshan Zhong"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18951v2",
                "updated": "2025-07-09T08:22:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    22,
                    28,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-23T09:41:37Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    9,
                    41,
                    37,
                    0,
                    174,
                    0
                ],
                "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications"
                },
                "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/"
                },
                "authors": [
                    {
                        "name": "Jinyang Li"
                    },
                    {
                        "name": "Xiaolong Li"
                    },
                    {
                        "name": "Ge Qu"
                    },
                    {
                        "name": "Per Jacobsson"
                    },
                    {
                        "name": "Bowen Qin"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Nan Huo"
                    },
                    {
                        "name": "Xiaohan Xu"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Ziwei Tang"
                    },
                    {
                        "name": "Yuanshuai Li"
                    },
                    {
                        "name": "Florensia Widjaja"
                    },
                    {
                        "name": "Xintong Zhu"
                    },
                    {
                        "name": "Feige Zhou"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Yannis Papakonstantinou"
                    },
                    {
                        "name": "Fatma Ozcan"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Reynold Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Reynold Cheng"
                },
                "author": "Reynold Cheng",
                "arxiv_comment": "26 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04670v3",
                "updated": "2025-07-09T08:04:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    4,
                    21,
                    2,
                    190,
                    0
                ],
                "published": "2025-01-08T18:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    30,
                    53,
                    2,
                    8,
                    0
                ],
                "title": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are They the Same? Exploring Visual Correspondence Shortcomings of\n  Multimodal LLMs"
                },
                "summary": "Recent advancements in multimodal large language models (MLLM) have shown a\nstrong ability in visual perception, reasoning abilities, and vision-language\nunderstanding. However, the visual matching ability of MLLMs is rarely studied,\ndespite finding the visual correspondence of objects is essential in computer\nvision. Our research reveals that the matching capabilities in recent MLLMs\nstill exhibit systematic shortcomings, even with current strong MLLMs models,\nGPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM)\nbenchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is\nbuilt from 15 open-source datasets and Internet videos with manual annotation.\nWe categorize the data samples of MMVM benchmark into eight aspects based on\nthe required cues and capabilities to more comprehensively evaluate and analyze\ncurrent MLLMs. In addition, we have designed an automatic annotation pipeline\nto generate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. To our knowledge, this is the first visual corresponding\ndataset and benchmark for the MLLM community. Finally, we present CoLVA, a\nnovel contrastive MLLM with two novel technical designs: fine-grained vision\nexpert with object-level contrastive learning and instruction augmentation\nstrategy. The former learns instance discriminative tokens, while the latter\nfurther improves instruction following ability. CoLVA-InternVL2-4B achieves an\noverall accuracy (OA) of 49.80\\% on the MMVM benchmark, surpassing GPT-4o and\nthe best open-source MLLM, Qwen2VL-72B, by 7.15\\% and 11.72\\% OA, respectively.\nThese results demonstrate the effectiveness of our MMVM SFT dataset and our\nnovel technical designs. Code, benchmark, dataset, and models will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal large language models (MLLM) have shown a\nstrong ability in visual perception, reasoning abilities, and vision-language\nunderstanding. However, the visual matching ability of MLLMs is rarely studied,\ndespite finding the visual correspondence of objects is essential in computer\nvision. Our research reveals that the matching capabilities in recent MLLMs\nstill exhibit systematic shortcomings, even with current strong MLLMs models,\nGPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM)\nbenchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is\nbuilt from 15 open-source datasets and Internet videos with manual annotation.\nWe categorize the data samples of MMVM benchmark into eight aspects based on\nthe required cues and capabilities to more comprehensively evaluate and analyze\ncurrent MLLMs. In addition, we have designed an automatic annotation pipeline\nto generate the MMVM SFT dataset, including 220K visual matching data with\nreasoning annotation. To our knowledge, this is the first visual corresponding\ndataset and benchmark for the MLLM community. Finally, we present CoLVA, a\nnovel contrastive MLLM with two novel technical designs: fine-grained vision\nexpert with object-level contrastive learning and instruction augmentation\nstrategy. The former learns instance discriminative tokens, while the latter\nfurther improves instruction following ability. CoLVA-InternVL2-4B achieves an\noverall accuracy (OA) of 49.80\\% on the MMVM benchmark, surpassing GPT-4o and\nthe best open-source MLLM, Qwen2VL-72B, by 7.15\\% and 11.72\\% OA, respectively.\nThese results demonstrate the effectiveness of our MMVM SFT dataset and our\nnovel technical designs. Code, benchmark, dataset, and models will be released."
                },
                "authors": [
                    {
                        "name": "Yikang Zhou"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Shilin Xu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Shunping Ji"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Xiangtai Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangtai Li"
                },
                "author": "Xiangtai Li",
                "arxiv_comment": "Accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02966v2",
                "updated": "2025-07-09T08:02:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    8,
                    2,
                    8,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-30T14:42:49Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    42,
                    49,
                    0,
                    181,
                    0
                ],
                "title": "PBa-LLM: Privacy- and Bias-aware NLP using Named-Entity Recognition\n  (NER)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBa-LLM: Privacy- and Bias-aware NLP using Named-Entity Recognition\n  (NER)"
                },
                "summary": "The use of Natural Language Processing (NLP) in highstakes AI-based\napplications has increased significantly in recent years, especially since the\nemergence of Large Language Models (LLMs). However, despite their strong\nperformance, LLMs introduce important legal/ ethical concerns, particularly\nregarding privacy, data protection, and transparency. Due to these concerns,\nthis work explores the use of Named- Entity Recognition (NER) to facilitate the\nprivacy-preserving training (or adaptation) of LLMs. We propose a framework\nthat uses NER technologies to anonymize sensitive information in text data,\nsuch as personal identities or geographic locations. An evaluation of the\nproposed privacy-preserving learning framework was conducted to measure its\nimpact on user privacy and system performance in a particular high-stakes and\nsensitive setup: AI-based resume scoring for recruitment processes. The study\ninvolved two language models (BERT and RoBERTa) and six anonymization\nalgorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)\napplied to a database of 24,000 candidate profiles. The findings indicate that\nthe proposed privacy preservation techniques effectively maintain system\nperformance while playing a critical role in safeguarding candidate\nconfidentiality, thus promoting trust in the experimented scenario. On top of\nthe proposed privacy-preserving approach, we also experiment applying an\nexisting approach that reduces the gender bias in LLMs, thus finally obtaining\nour proposed Privacyand Bias-aware LLMs (PBa-LLMs). Note that the proposed\nPBa-LLMs have been evaluated in a particular setup (resume scoring), but are\ngenerally applicable to any other LLM-based AI application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Natural Language Processing (NLP) in highstakes AI-based\napplications has increased significantly in recent years, especially since the\nemergence of Large Language Models (LLMs). However, despite their strong\nperformance, LLMs introduce important legal/ ethical concerns, particularly\nregarding privacy, data protection, and transparency. Due to these concerns,\nthis work explores the use of Named- Entity Recognition (NER) to facilitate the\nprivacy-preserving training (or adaptation) of LLMs. We propose a framework\nthat uses NER technologies to anonymize sensitive information in text data,\nsuch as personal identities or geographic locations. An evaluation of the\nproposed privacy-preserving learning framework was conducted to measure its\nimpact on user privacy and system performance in a particular high-stakes and\nsensitive setup: AI-based resume scoring for recruitment processes. The study\ninvolved two language models (BERT and RoBERTa) and six anonymization\nalgorithms (based on Presidio, FLAIR, BERT, and different versions of GPT)\napplied to a database of 24,000 candidate profiles. The findings indicate that\nthe proposed privacy preservation techniques effectively maintain system\nperformance while playing a critical role in safeguarding candidate\nconfidentiality, thus promoting trust in the experimented scenario. On top of\nthe proposed privacy-preserving approach, we also experiment applying an\nexisting approach that reduces the gender bias in LLMs, thus finally obtaining\nour proposed Privacyand Bias-aware LLMs (PBa-LLMs). Note that the proposed\nPBa-LLMs have been evaluated in a particular setup (resume scoring), but are\ngenerally applicable to any other LLM-based AI application."
                },
                "authors": [
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    },
                    {
                        "name": "Alejandro Penna"
                    },
                    {
                        "name": "Miguel Lopez-Duran"
                    },
                    {
                        "name": "Francisco Jurado"
                    },
                    {
                        "name": "Alvaro Ortigosa"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Ortigosa"
                },
                "author": "Alvaro Ortigosa",
                "arxiv_comment": "Presented at AAAI Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI) 2025, Philadelphia, PA, USA, March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06623v1",
                "updated": "2025-07-09T07:50:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    50,
                    55,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T07:50:55Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    50,
                    55,
                    2,
                    190,
                    0
                ],
                "title": "Expediting data extraction using a large language model (LLM) and\n  scoping review protocol: a methodological study within a complex scoping\n  review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expediting data extraction using a large language model (LLM) and\n  scoping review protocol: a methodological study within a complex scoping\n  review"
                },
                "summary": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting."
                },
                "authors": [
                    {
                        "name": "James Stewart-Evans"
                    },
                    {
                        "name": "Emma Wilson"
                    },
                    {
                        "name": "Tessa Langley"
                    },
                    {
                        "name": "Andrew Prayle"
                    },
                    {
                        "name": "Angela Hands"
                    },
                    {
                        "name": "Karen Exley"
                    },
                    {
                        "name": "Jo Leonardi-Bee"
                    }
                ],
                "author_detail": {
                    "name": "Jo Leonardi-Bee"
                },
                "author": "Jo Leonardi-Bee",
                "arxiv_comment": "44 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06622v1",
                "updated": "2025-07-09T07:49:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    49,
                    55,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T07:49:55Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    49,
                    55,
                    2,
                    190,
                    0
                ],
                "title": "FuDoBa: Fusing Document and Knowledge Graph-based Representations with\n  Bayesian Optimisation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuDoBa: Fusing Document and Knowledge Graph-based Representations with\n  Bayesian Optimisation"
                },
                "summary": "Building on the success of Large Language Models (LLMs), LLM-based\nrepresentations have dominated the document representation landscape, achieving\ngreat performance on the document embedding benchmarks. However, the\nhigh-dimensional, computationally expensive embeddings from LLMs tend to be\neither too generic or inefficient for domain-specific applications. To address\nthese limitations, we introduce FuDoBa a Bayesian optimisation-based method\nthat integrates LLM-based embeddings with domain-specific structured knowledge,\nsourced both locally and from external repositories like WikiData. This fusion\nproduces low-dimensional, task-relevant representations while reducing training\ncomplexity and yielding interpretable early-fusion weights for enhanced\nclassification performance. We demonstrate the effectiveness of our approach on\nsix datasets in two domains, showing that when paired with robust AutoML-based\nclassifiers, our proposed representation learning approach performs on par\nwith, or surpasses, those produced solely by the proprietary LLM-based\nembedding baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the success of Large Language Models (LLMs), LLM-based\nrepresentations have dominated the document representation landscape, achieving\ngreat performance on the document embedding benchmarks. However, the\nhigh-dimensional, computationally expensive embeddings from LLMs tend to be\neither too generic or inefficient for domain-specific applications. To address\nthese limitations, we introduce FuDoBa a Bayesian optimisation-based method\nthat integrates LLM-based embeddings with domain-specific structured knowledge,\nsourced both locally and from external repositories like WikiData. This fusion\nproduces low-dimensional, task-relevant representations while reducing training\ncomplexity and yielding interpretable early-fusion weights for enhanced\nclassification performance. We demonstrate the effectiveness of our approach on\nsix datasets in two domains, showing that when paired with robust AutoML-based\nclassifiers, our proposed representation learning approach performs on par\nwith, or surpasses, those produced solely by the proprietary LLM-based\nembedding baselines."
                },
                "authors": [
                    {
                        "name": "Boshko Koloski"
                    },
                    {
                        "name": "Senja Pollak"
                    },
                    {
                        "name": "Roberto Navigli"
                    },
                    {
                        "name": "Bla krlj"
                    }
                ],
                "author_detail": {
                    "name": "Bla krlj"
                },
                "author": "Bla krlj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v2",
                "updated": "2025-07-09T07:47:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    47,
                    59,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Safety Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Safety Inference Scaling"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "Previous title: \"Saffron-1: Towards an Inference Scaling Paradigm for\n  LLM Safety Assurance\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23132v2",
                "updated": "2025-07-09T07:30:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    30,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-03-29T15:52:08Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    15,
                    52,
                    8,
                    5,
                    88,
                    0
                ],
                "title": "LAURA: LLM-Assisted UAV Routing for AoI Minimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAURA: LLM-Assisted UAV Routing for AoI Minimization"
                },
                "summary": "With the rapid growth of the low-altitude economy, there is increasing demand\nfor real-time data collection using UAV-assisted wireless sensor networks. This\npaper investigates the problem of minimizing the age of information (AoI) in\nUAV-assisted wireless sensor networks by optimizing the UAV flight routing. We\nformulate the AoI minimization task and propose a large language model\n(LLM)-assisted UAV routing algorithm (LAURA). LAURA employs an LLM as\nintelligent crossover operators within an evolutionary optimization framework\nto efficiently explore the solution space. Simulation results show that LAURA\noutperforms benchmark methods in reducing the maximum AoI, especially in\nscenarios with a large number of sensor nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of the low-altitude economy, there is increasing demand\nfor real-time data collection using UAV-assisted wireless sensor networks. This\npaper investigates the problem of minimizing the age of information (AoI) in\nUAV-assisted wireless sensor networks by optimizing the UAV flight routing. We\nformulate the AoI minimization task and propose a large language model\n(LLM)-assisted UAV routing algorithm (LAURA). LAURA employs an LLM as\nintelligent crossover operators within an evolutionary optimization framework\nto efficiently explore the solution space. Simulation results show that LAURA\noutperforms benchmark methods in reducing the maximum AoI, especially in\nscenarios with a large number of sensor nodes."
                },
                "authors": [
                    {
                        "name": "Bisheng Wei"
                    },
                    {
                        "name": "Ruichen Zhang"
                    },
                    {
                        "name": "Ruihong Jiang"
                    },
                    {
                        "name": "Mugen Peng"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06612v1",
                "updated": "2025-07-09T07:29:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    29,
                    41,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T07:29:41Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    29,
                    41,
                    2,
                    190,
                    0
                ],
                "title": "Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization\n  to Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization\n  to Estimation"
                },
                "summary": "Cell-free integrated sensing and communication (ISAC) systems have emerged as\na promising paradigm for sixth-generation (6G) networks, enabling simultaneous\nhigh-rate data transmission and high-precision radar sensing through\ncooperative distributed access points (APs). Fully exploiting these\ncapabilities requires a unified design that bridges system-level optimization\nwith multi-target parameter estimation. This paper proposes an end-to-end graph\nlearning approach to close this gap, modeling the entire cell-free ISAC network\nas a heterogeneous graph to jointly design the AP mode selection, user\nassociation, precoding, and echo signal processing for multi-target position\nand velocity estimation. In particular, we propose two novel heterogeneous\ngraph learning frameworks: a dynamic graph learning framework and a lightweight\nmirror-based graph attention network (mirror-GAT) framework. The dynamic graph\nlearning framework employs structural and temporal attention mechanisms\nintegrated with a three-dimensional convolutional neural network (3D-CNN),\nenabling superior performance and robustness in cell-free ISAC environments.\nConversely, the mirror-GAT framework significantly reduces computational\ncomplexity and signaling overhead through a bi-level iterative structure with\nshare adjacency. Simulation results validate that both proposed\ngraph-learning-based frameworks achieve significant improvements in\nmulti-target position and velocity estimation accuracy compared to conventional\nheuristic and optimization-based designs. Particularly, the mirror-GAT\nframework demonstrates substantial reductions in computational time and\nsignaling overhead, underscoring its suitability for practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free integrated sensing and communication (ISAC) systems have emerged as\na promising paradigm for sixth-generation (6G) networks, enabling simultaneous\nhigh-rate data transmission and high-precision radar sensing through\ncooperative distributed access points (APs). Fully exploiting these\ncapabilities requires a unified design that bridges system-level optimization\nwith multi-target parameter estimation. This paper proposes an end-to-end graph\nlearning approach to close this gap, modeling the entire cell-free ISAC network\nas a heterogeneous graph to jointly design the AP mode selection, user\nassociation, precoding, and echo signal processing for multi-target position\nand velocity estimation. In particular, we propose two novel heterogeneous\ngraph learning frameworks: a dynamic graph learning framework and a lightweight\nmirror-based graph attention network (mirror-GAT) framework. The dynamic graph\nlearning framework employs structural and temporal attention mechanisms\nintegrated with a three-dimensional convolutional neural network (3D-CNN),\nenabling superior performance and robustness in cell-free ISAC environments.\nConversely, the mirror-GAT framework significantly reduces computational\ncomplexity and signaling overhead through a bi-level iterative structure with\nshare adjacency. Simulation results validate that both proposed\ngraph-learning-based frameworks achieve significant improvements in\nmulti-target position and velocity estimation accuracy compared to conventional\nheuristic and optimization-based designs. Particularly, the mirror-GAT\nframework demonstrates substantial reductions in computational time and\nsignaling overhead, underscoring its suitability for practical deployments."
                },
                "authors": [
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Rang Liu"
                    },
                    {
                        "name": "Qian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Liu"
                },
                "author": "Qian Liu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06608v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06608v2",
                "updated": "2025-07-10T15:48:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    10,
                    15,
                    48,
                    42,
                    3,
                    191,
                    0
                ],
                "published": "2025-07-09T07:27:18Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    27,
                    18,
                    2,
                    190,
                    0
                ],
                "title": "Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient\n  GPU Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient\n  GPU Sharing"
                },
                "summary": "Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs."
                },
                "authors": [
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Colin Cai"
                    },
                    {
                        "name": "Junjia Du"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06608v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06608v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06602v1",
                "updated": "2025-07-09T07:22:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    22,
                    22,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T07:22:22Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    22,
                    22,
                    2,
                    190,
                    0
                ],
                "title": "Generalization in Reinforcement Learning for Radio Access Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalization in Reinforcement Learning for Radio Access Networks"
                },
                "summary": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) encodes cell\ntopology and node attributes via attention-based graph representations; (ii)\napplies domain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) encodes cell\ntopology and node attributes via attention-based graph representations; (ii)\napplies domain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent."
                },
                "authors": [
                    {
                        "name": "Burak Demirel"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Cristian Tatino"
                    },
                    {
                        "name": "Pablo Soldati"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Soldati"
                },
                "author": "Pablo Soldati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06057v2",
                "updated": "2025-07-09T07:06:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    6,
                    36,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-08T14:59:46Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    14,
                    59,
                    46,
                    1,
                    189,
                    0
                ],
                "title": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large\n  Language Models"
                },
                "summary": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models - C32B, S32B, R32B - from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in reasoning for large language models (LLMs) have lead to\nsignificant performance improvements for LLMs in various fields such as\nmathematics and programming. However, research applying these advances to the\nfinancial domain, where considerable domain-specific knowledge is necessary to\ncomplete tasks, remains limited. To address this gap, we introduce FEVO\n(Financial Evolution), a multi-stage enhancement framework developed to enhance\nLLM performance in the financial domain. FEVO systemically enhances LLM\nperformance by using continued pre-training (CPT) to expand financial domain\nknowledge, supervised fine-tuning (SFT) to instill structured, elaborate\nreasoning patterns, and reinforcement learning (RL) to further integrate the\nexpanded financial domain knowledge with the learned structured reasoning. To\nensure effective and efficient training, we leverage frontier reasoning models\nand rule-based filtering to curate FEVO-Train, high-quality datasets\nspecifically designed for the different post-training phases. Using our\nframework, we train the FEVO series of models - C32B, S32B, R32B - from\nQwen2.5-32B and evaluate them on seven benchmarks to assess financial and\ngeneral capabilities, with results showing that FEVO-R32B achieves\nstate-of-the-art performance on five financial benchmarks against much larger\nmodels as well as specialist models. More significantly, FEVO-R32B demonstrates\nmarkedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct\nusing only RL), thus validating the effectiveness of financial domain knowledge\nexpansion and structured, logical reasoning distillation"
                },
                "authors": [
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Yalu Ouyang"
                    },
                    {
                        "name": "Hangfei Xu"
                    },
                    {
                        "name": "Ziqi Jia"
                    },
                    {
                        "name": "Panpan Li"
                    },
                    {
                        "name": "Shengzhao Wen"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Yanpeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yanpeng Wang"
                },
                "author": "Yanpeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03160v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03160v3",
                "updated": "2025-07-09T06:49:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    49,
                    35,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-03T20:32:36Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    20,
                    32,
                    36,
                    3,
                    184,
                    0
                ],
                "title": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Small Language Models for Code Generation: An Empirical Study\n  with Benchmarks"
                },
                "summary": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advancements of Small Language Models (SLMs) have opened new\npossibilities for efficient code generation. SLMs offer lightweight and\ncost-effective alternatives to Large Language Models (LLMs), making them\nattractive for use in resource-constrained environments. However, empirical\nunderstanding of SLMs, particularly their capabilities, limitations, and\nperformance trade-offs in code generation remains limited. This study presents\na comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B\nto 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP,\nMercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three\ndimensions: i) functional correctness of generated code, ii) computational\nefficiency and iii) performance across multiple programming languages. The\nfindings of this study reveal that several compact SLMs achieve competitive\nresults while maintaining a balance between performance and efficiency, making\nthem viable for deployment in resource-constrained environments. However,\nachieving further improvements in accuracy requires switching to larger models.\nThese models generally outperform their smaller counterparts, but they require\nmuch more computational power. We observe that for 10% performance\nimprovements, models can require nearly a 4x increase in VRAM consumption,\nhighlighting a trade-off between effectiveness and scalability. Besides, the\nmultilingual performance analysis reveals that SLMs tend to perform better in\nlanguages such as Python, Java, and PHP, while exhibiting relatively weaker\nperformance in Go, C++, and Ruby. However, statistical analysis suggests these\ndifferences are not significant, indicating a generalizability of SLMs across\nprogramming languages. Based on the findings, this work provides insights into\nthe design and selection of SLMs for real-world code generation tasks."
                },
                "authors": [
                    {
                        "name": "Md Mahade Hasan"
                    },
                    {
                        "name": "Muhammad Waseem"
                    },
                    {
                        "name": "Kai-Kristian Kemell"
                    },
                    {
                        "name": "Jussi Rasku"
                    },
                    {
                        "name": "Juha Ala-Rantala"
                    },
                    {
                        "name": "Pekka Abrahamsson"
                    }
                ],
                "author_detail": {
                    "name": "Pekka Abrahamsson"
                },
                "author": "Pekka Abrahamsson",
                "arxiv_comment": "17 pages, 10 Tables, 57 figures. Includes benchmarks and multilingual\n  evaluation. Submitted to the Journal of Systems and Software",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03160v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03160v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06587v1",
                "updated": "2025-07-09T06:37:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    37,
                    12,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T06:37:12Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    37,
                    12,
                    2,
                    190,
                    0
                ],
                "title": "Illuminating the Future: Nanophotonics for Future Green Technologies,\n  Precision Healthcare, and Optical Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Illuminating the Future: Nanophotonics for Future Green Technologies,\n  Precision Healthcare, and Optical Computing"
                },
                "summary": "Nanophotonics, an interdisciplinary field merging nanotechnology and\nphotonics, has enabled transformative advancements across diverse sectors\nincluding green energy, biomedicine, and optical computing. This review\ncomprehensively examines recent progress in nanophotonic principles and\napplications, highlighting key innovations in material design, device\nengineering, and system integration. In renewable energy, nanophotonic allows\nlight-trapping nanostructures and spectral control in perovskite solar cells,\nconcentrating solar power, and thermophotovoltaics. That have significantly\nenhanced solar conversion efficiencies, approaching theoretical limits. For\nbiosensing, nanophotonic platforms achieve unprecedented sensitivity in\ndetecting biomolecules, pathogens, and pollutants, enabling real-time\ndiagnostics and environmental monitoring. Medical applications leverage\ntailored light-matter interactions for precision photothermal therapy,\nimage-guided surgery, and early disease detection. Furthermore, nanophotonics\nunderpins next-generation optical neural networks and neuromorphic computing,\noffering ultra-fast, energy-efficient alternatives to von Neumann\narchitectures. Despite rapid growth, challenges in scalability, fabrication\ncosts, and material stability persist. Future advancements will rely on novel\nmaterials, AI-driven design optimization, and multidisciplinary approaches to\nenable scalable, low-cost deployment. This review summarizes recent progress\nand highlights future trends, including novel material systems,\nmultidisciplinary approaches, and enhanced computational capabilities, to pave\nthe way for transformative applications in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nanophotonics, an interdisciplinary field merging nanotechnology and\nphotonics, has enabled transformative advancements across diverse sectors\nincluding green energy, biomedicine, and optical computing. This review\ncomprehensively examines recent progress in nanophotonic principles and\napplications, highlighting key innovations in material design, device\nengineering, and system integration. In renewable energy, nanophotonic allows\nlight-trapping nanostructures and spectral control in perovskite solar cells,\nconcentrating solar power, and thermophotovoltaics. That have significantly\nenhanced solar conversion efficiencies, approaching theoretical limits. For\nbiosensing, nanophotonic platforms achieve unprecedented sensitivity in\ndetecting biomolecules, pathogens, and pollutants, enabling real-time\ndiagnostics and environmental monitoring. Medical applications leverage\ntailored light-matter interactions for precision photothermal therapy,\nimage-guided surgery, and early disease detection. Furthermore, nanophotonics\nunderpins next-generation optical neural networks and neuromorphic computing,\noffering ultra-fast, energy-efficient alternatives to von Neumann\narchitectures. Despite rapid growth, challenges in scalability, fabrication\ncosts, and material stability persist. Future advancements will rely on novel\nmaterials, AI-driven design optimization, and multidisciplinary approaches to\nenable scalable, low-cost deployment. This review summarizes recent progress\nand highlights future trends, including novel material systems,\nmultidisciplinary approaches, and enhanced computational capabilities, to pave\nthe way for transformative applications in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Osama M. Halawa"
                    },
                    {
                        "name": "Esraa Ahmed"
                    },
                    {
                        "name": "Malk M. Abdelrazek"
                    },
                    {
                        "name": "Yasser M. Nagy"
                    },
                    {
                        "name": "Omar A. M. Abdelraouf"
                    }
                ],
                "author_detail": {
                    "name": "Omar A. M. Abdelraouf"
                },
                "author": "Omar A. M. Abdelraouf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15628v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15628v5",
                "updated": "2025-07-09T06:33:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    33,
                    14,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-20T07:35:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    35,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Can Input Attributions Explain Inductive Reasoning in In-Context\n  Learning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Input Attributions Explain Inductive Reasoning in In-Context\n  Learning?"
                },
                "summary": "Interpreting the internal process of neural models has long been a challenge.\nThis challenge remains relevant in the era of large language models (LLMs) and\nin-context learning (ICL); for example, ICL poses a new issue of interpreting\nwhich example in the few-shot examples contributed to identifying/solving the\ntask. To this end, in this paper, we design synthetic diagnostic tasks of\ninductive reasoning, inspired by the generalization tests typically adopted in\npsycholinguistics. Here, most in-context examples are ambiguous w.r.t. their\nunderlying rule, and one critical example disambiguates it. The question is\nwhether conventional input attribution (IA) methods can track such a reasoning\nprocess, i.e., identify the influential example, in ICL. Our experiments\nprovide several practical findings; for example, a certain simple IA method\nworks the best, and the larger the model, the generally harder it is to\ninterpret the ICL with gradient-based IA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting the internal process of neural models has long been a challenge.\nThis challenge remains relevant in the era of large language models (LLMs) and\nin-context learning (ICL); for example, ICL poses a new issue of interpreting\nwhich example in the few-shot examples contributed to identifying/solving the\ntask. To this end, in this paper, we design synthetic diagnostic tasks of\ninductive reasoning, inspired by the generalization tests typically adopted in\npsycholinguistics. Here, most in-context examples are ambiguous w.r.t. their\nunderlying rule, and one critical example disambiguates it. The question is\nwhether conventional input attribution (IA) methods can track such a reasoning\nprocess, i.e., identify the influential example, in ICL. Our experiments\nprovide several practical findings; for example, a certain simple IA method\nworks the best, and the larger the model, the generally harder it is to\ninterpret the ICL with gradient-based IA methods."
                },
                "authors": [
                    {
                        "name": "Mengyu Ye"
                    },
                    {
                        "name": "Tatsuki Kuribayashi"
                    },
                    {
                        "name": "Goro Kobayashi"
                    },
                    {
                        "name": "Jun Suzuki"
                    }
                ],
                "author_detail": {
                    "name": "Jun Suzuki"
                },
                "author": "Jun Suzuki",
                "arxiv_comment": "Findings of ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15628v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15628v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11111v2",
                "updated": "2025-07-09T06:18:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    18,
                    33,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-08T16:20:12Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    20,
                    12,
                    6,
                    159,
                    0
                ],
                "title": "Evaluating and Improving Robustness in Large Language Models: A Survey\n  and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving Robustness in Large Language Models: A Survey\n  and Future Directions"
                },
                "summary": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity."
                },
                "authors": [
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Le Wu"
                    },
                    {
                        "name": "Kui Yu"
                    },
                    {
                        "name": "Guangyi Lv"
                    },
                    {
                        "name": "Dacao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dacao Zhang"
                },
                "author": "Dacao Zhang",
                "arxiv_comment": "33 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06581v1",
                "updated": "2025-07-09T06:15:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    15,
                    20,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T06:15:20Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    15,
                    20,
                    2,
                    190,
                    0
                ],
                "title": "Airway Segmentation Network for Enhanced Tubular Feature Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Airway Segmentation Network for Enhanced Tubular Feature Extraction"
                },
                "summary": "Manual annotation of airway regions in computed tomography images is a\ntime-consuming and expertise-dependent task. Automatic airway segmentation is\ntherefore a prerequisite for enabling rapid bronchoscopic navigation and the\nclinical deployment of bronchoscopic robotic systems. Although convolutional\nneural network methods have gained considerable attention in airway\nsegmentation, the unique tree-like structure of airways poses challenges for\nconventional and deformable convolutions, which often fail to focus on fine\nairway structures, leading to missed segments and discontinuities. To address\nthis issue, this study proposes a novel tubular feature extraction network,\nnamed TfeNet. TfeNet introduces a novel direction-aware convolution operation\nthat first applies spatial rotation transformations to adjust the sampling\npositions of linear convolution kernels. The deformed kernels are then\nrepresented as line segments or polylines in 3D space. Furthermore, a tubular\nfeature fusion module (TFFM) is designed based on asymmetric convolution and\nresidual connection strategies, enhancing the network's focus on subtle airway\nstructures. Extensive experiments conducted on one public dataset and two\ndatasets used in airway segmentation challenges demonstrate that the proposed\nTfeNet achieves more accuracy and continuous airway structure predictions\ncompared with existing methods. In particular, TfeNet achieves the highest\noverall score of 94.95% on the current largest airway segmentation dataset,\nAirway Tree Modeling(ATM22), and demonstrates advanced performance on the lung\nfibrosis dataset(AIIB23). The code is available at\nhttps://github.com/QibiaoWu/TfeNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manual annotation of airway regions in computed tomography images is a\ntime-consuming and expertise-dependent task. Automatic airway segmentation is\ntherefore a prerequisite for enabling rapid bronchoscopic navigation and the\nclinical deployment of bronchoscopic robotic systems. Although convolutional\nneural network methods have gained considerable attention in airway\nsegmentation, the unique tree-like structure of airways poses challenges for\nconventional and deformable convolutions, which often fail to focus on fine\nairway structures, leading to missed segments and discontinuities. To address\nthis issue, this study proposes a novel tubular feature extraction network,\nnamed TfeNet. TfeNet introduces a novel direction-aware convolution operation\nthat first applies spatial rotation transformations to adjust the sampling\npositions of linear convolution kernels. The deformed kernels are then\nrepresented as line segments or polylines in 3D space. Furthermore, a tubular\nfeature fusion module (TFFM) is designed based on asymmetric convolution and\nresidual connection strategies, enhancing the network's focus on subtle airway\nstructures. Extensive experiments conducted on one public dataset and two\ndatasets used in airway segmentation challenges demonstrate that the proposed\nTfeNet achieves more accuracy and continuous airway structure predictions\ncompared with existing methods. In particular, TfeNet achieves the highest\noverall score of 94.95% on the current largest airway segmentation dataset,\nAirway Tree Modeling(ATM22), and demonstrates advanced performance on the lung\nfibrosis dataset(AIIB23). The code is available at\nhttps://github.com/QibiaoWu/TfeNet."
                },
                "authors": [
                    {
                        "name": "Qibiao Wu"
                    },
                    {
                        "name": "Yagang Wang"
                    },
                    {
                        "name": "Qian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Zhang"
                },
                "author": "Qian Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06573v1",
                "updated": "2025-07-09T06:05:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    5,
                    28,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T06:05:28Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    6,
                    5,
                    28,
                    2,
                    190,
                    0
                ],
                "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via\n  Progressive Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via\n  Progressive Optimization"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently advanced\nthe reasoning capabilities of large language models (LLMs). While prior work\nhas emphasized algorithmic design, data curation, and reward shaping, we\ninvestigate RLVR from a sample-centric perspective and introduce LPPO\n(Learning-Progress and Prefix-guided Optimization), a framework of progressive\noptimization techniques. Our work addresses a critical question: how to best\nleverage a small set of trusted, high-quality demonstrations, rather than\nsimply scaling up data volume. First, motivated by how hints aid human\nproblem-solving, we propose prefix-guided sampling, an online data augmentation\nmethod that incorporates partial solution prefixes from expert demonstrations\nto guide the policy, particularly for challenging instances. Second, inspired\nby how humans focus on important questions aligned with their current\ncapabilities, we introduce learning-progress weighting, a dynamic strategy that\nadjusts each training sample's influence based on model progression. We\nestimate sample-level learning progress via an exponential moving average of\nper-sample pass rates, promoting samples that foster learning and\nde-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks\ndemonstrate that our methods outperform strong baselines, yielding faster\nconvergence and a higher performance ceiling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has recently advanced\nthe reasoning capabilities of large language models (LLMs). While prior work\nhas emphasized algorithmic design, data curation, and reward shaping, we\ninvestigate RLVR from a sample-centric perspective and introduce LPPO\n(Learning-Progress and Prefix-guided Optimization), a framework of progressive\noptimization techniques. Our work addresses a critical question: how to best\nleverage a small set of trusted, high-quality demonstrations, rather than\nsimply scaling up data volume. First, motivated by how hints aid human\nproblem-solving, we propose prefix-guided sampling, an online data augmentation\nmethod that incorporates partial solution prefixes from expert demonstrations\nto guide the policy, particularly for challenging instances. Second, inspired\nby how humans focus on important questions aligned with their current\ncapabilities, we introduce learning-progress weighting, a dynamic strategy that\nadjusts each training sample's influence based on model progression. We\nestimate sample-level learning progress via an exponential moving average of\nper-sample pass rates, promoting samples that foster learning and\nde-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks\ndemonstrate that our methods outperform strong baselines, yielding faster\nconvergence and a higher performance ceiling."
                },
                "authors": [
                    {
                        "name": "Xinjie Chen"
                    },
                    {
                        "name": "Minpeng Liao"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Chengxi Li"
                    },
                    {
                        "name": "Biao Fu"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Xinggao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinggao Liu"
                },
                "author": "Xinggao Liu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12538v2",
                "updated": "2025-07-09T05:59:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    59,
                    31,
                    2,
                    190,
                    0
                ],
                "published": "2024-09-19T07:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    7,
                    54,
                    29,
                    3,
                    263,
                    0
                ],
                "title": "PersonaFlow: Designing LLM-Simulated Expert Perspectives for Enhanced\n  Research Ideation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersonaFlow: Designing LLM-Simulated Expert Perspectives for Enhanced\n  Research Ideation"
                },
                "summary": "Generating interdisciplinary research ideas requires diverse domain\nexpertise, but access to timely feedback is often limited by the availability\nof experts. In this paper, we introduce PersonaFlow, a novel system designed to\nprovide multiple perspectives by using LLMs to simulate domain-specific\nexperts. Our user studies showed that the new design 1) increased the perceived\nrelevance and creativity of ideated research directions, and 2) promoted users'\ncritical thinking activities (e.g., interpretation, analysis, evaluation,\ninference, and self-regulation), without increasing their perceived cognitive\nload. Moreover, users' ability to customize expert profiles significantly\nimproved their sense of agency, which can potentially mitigate their\nover-reliance on AI. This work contributes to the design of intelligent systems\nthat augment creativity and collaboration, and provides design implications of\nusing customizable AI-simulated personas in domains within and beyond research\nideation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating interdisciplinary research ideas requires diverse domain\nexpertise, but access to timely feedback is often limited by the availability\nof experts. In this paper, we introduce PersonaFlow, a novel system designed to\nprovide multiple perspectives by using LLMs to simulate domain-specific\nexperts. Our user studies showed that the new design 1) increased the perceived\nrelevance and creativity of ideated research directions, and 2) promoted users'\ncritical thinking activities (e.g., interpretation, analysis, evaluation,\ninference, and self-regulation), without increasing their perceived cognitive\nload. Moreover, users' ability to customize expert profiles significantly\nimproved their sense of agency, which can potentially mitigate their\nover-reliance on AI. This work contributes to the design of intelligent systems\nthat augment creativity and collaboration, and provides design implications of\nusing customizable AI-simulated personas in domains within and beyond research\nideation."
                },
                "authors": [
                    {
                        "name": "Yiren Liu"
                    },
                    {
                        "name": "Pranav Sharma"
                    },
                    {
                        "name": "Mehul Jitendra Oswal"
                    },
                    {
                        "name": "Haijun Xia"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "arxiv_doi": "10.1145/3715336.3735789",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3715336.3735789",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to DIS2025",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06569v1",
                "updated": "2025-07-09T05:47:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    47,
                    26,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:47:26Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    47,
                    26,
                    2,
                    190,
                    0
                ],
                "title": "Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted\n  Binary Cross-Entropy for Enhanced Edge Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted\n  Binary Cross-Entropy for Enhanced Edge Detection"
                },
                "summary": "Edge detection (ED) remains a fundamental task in computer vision, yet its\nperformance is often hindered by the ambiguous nature of non-edge pixels near\nobject boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss\ntreats all non-edge pixels uniformly, overlooking the structural nuances around\nedges and often resulting in blurred predictions. In this paper, we propose the\nEdge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides\npixels into three categories, edge, boundary, and texture, and assigns each a\ndistinct supervisory weight. This tri-class formulation enables more structured\nlearning by guiding the model to focus on both edge precision and contextual\nboundary localization. We theoretically show that the EBT loss generalizes the\nWBCE loss, with the latter becoming a limit case. Extensive experiments across\nmultiple benchmarks demonstrate the superiority of the EBT loss both\nquantitatively and perceptually. Furthermore, the consistent use of unified\nhyperparameters across all models and datasets, along with robustness to their\nmoderate variations, indicates that the EBT loss requires minimal fine-tuning\nand is easily deployable in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge detection (ED) remains a fundamental task in computer vision, yet its\nperformance is often hindered by the ambiguous nature of non-edge pixels near\nobject boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss\ntreats all non-edge pixels uniformly, overlooking the structural nuances around\nedges and often resulting in blurred predictions. In this paper, we propose the\nEdge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides\npixels into three categories, edge, boundary, and texture, and assigns each a\ndistinct supervisory weight. This tri-class formulation enables more structured\nlearning by guiding the model to focus on both edge precision and contextual\nboundary localization. We theoretically show that the EBT loss generalizes the\nWBCE loss, with the latter becoming a limit case. Extensive experiments across\nmultiple benchmarks demonstrate the superiority of the EBT loss both\nquantitatively and perceptually. Furthermore, the consistent use of unified\nhyperparameters across all models and datasets, along with robustness to their\nmoderate variations, indicates that the EBT loss requires minimal fine-tuning\nand is easily deployable in practice."
                },
                "authors": [
                    {
                        "name": "Hao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Shu"
                },
                "author": "Hao Shu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06567v1",
                "updated": "2025-07-09T05:43:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:43:43Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    43,
                    43,
                    2,
                    190,
                    0
                ],
                "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed\n  Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09073v3",
                "updated": "2025-07-09T05:40:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    40,
                    56,
                    2,
                    190,
                    0
                ],
                "published": "2024-11-13T22:56:00Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    22,
                    56,
                    0,
                    2,
                    318,
                    0
                ],
                "title": "CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models\n  through Reinforcement Learning with AI Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models\n  through Reinforcement Learning with AI Feedback"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks but struggle with code-mixed (or code-switched) language\nunderstanding. For example, prior work benchmarking the performance of\nmultilingual LLMs on code-mixed translation tasks has demonstrated that current\nstate-of-the-art multilingual LLMs are ineffective in dealing with code-mixed\nlanguages. However, the question of how to improve the capability of\nmultilingual LLMs to handle code-mixed language has not received any attention\nto date. In this paper, we tackle this research gap by proposing CHAI, a novel\ngeneral-purpose framework for improving the ability of multilingual LLMs to\nhandle code-mixed languages. CHAI relies on three novel contributions made in\nthis paper. First, we explore the ability of LLMs to provide accurate\nannotations for code-mixed translation tasks. Second, we leverage this ability\nof LLMs as annotators to generate preference data for code-mixed translation\ntasks at scale, which are then used within a reinforcement learning from AI\nfeedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks.\nThird, we conduct a rigorous experimental evaluation across various real-world\ndatasets and settings. Our analysis shows that CHAI-powered LLMs outperform\nstate-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated\nby human annotators) in code-mixed translation tasks. This work represents a\nfirst step towards developing more inclusive code-mixed LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks but struggle with code-mixed (or code-switched) language\nunderstanding. For example, prior work benchmarking the performance of\nmultilingual LLMs on code-mixed translation tasks has demonstrated that current\nstate-of-the-art multilingual LLMs are ineffective in dealing with code-mixed\nlanguages. However, the question of how to improve the capability of\nmultilingual LLMs to handle code-mixed language has not received any attention\nto date. In this paper, we tackle this research gap by proposing CHAI, a novel\ngeneral-purpose framework for improving the ability of multilingual LLMs to\nhandle code-mixed languages. CHAI relies on three novel contributions made in\nthis paper. First, we explore the ability of LLMs to provide accurate\nannotations for code-mixed translation tasks. Second, we leverage this ability\nof LLMs as annotators to generate preference data for code-mixed translation\ntasks at scale, which are then used within a reinforcement learning from AI\nfeedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks.\nThird, we conduct a rigorous experimental evaluation across various real-world\ndatasets and settings. Our analysis shows that CHAI-powered LLMs outperform\nstate-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated\nby human annotators) in code-mixed translation tasks. This work represents a\nfirst step towards developing more inclusive code-mixed LLMs."
                },
                "authors": [
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Aditya Majumdar"
                    },
                    {
                        "name": "Amulya Yadav"
                    }
                ],
                "author_detail": {
                    "name": "Amulya Yadav"
                },
                "author": "Amulya Yadav",
                "arxiv_comment": "full draft v2: 8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06565v1",
                "updated": "2025-07-09T05:39:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    39,
                    56,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:39:56Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    39,
                    56,
                    2,
                    190,
                    0
                ],
                "title": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge\n  Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge\n  Production"
                },
                "summary": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest."
                },
                "authors": [
                    {
                        "name": "Juan B. Gutirrez"
                    }
                ],
                "author_detail": {
                    "name": "Juan B. Gutirrez"
                },
                "author": "Juan B. Gutirrez",
                "arxiv_comment": "27 pages, 3 figures, 4 tables, 1 algorithm, 28 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.11; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06564v1",
                "updated": "2025-07-09T05:38:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    38,
                    32,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:38:32Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    38,
                    32,
                    2,
                    190,
                    0
                ],
                "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in\n  Urban Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in\n  Urban Environments"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments."
                },
                "authors": [
                    {
                        "name": "Tianshun Li"
                    },
                    {
                        "name": "Tianyi Huai"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yichun Gao"
                    },
                    {
                        "name": "Haoang Li"
                    },
                    {
                        "name": "Xinhu Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Xinhu Zheng"
                },
                "author": "Xinhu Zheng",
                "arxiv_comment": "8 pages, 9 figures, has been accepted by IROS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10422v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10422v4",
                "updated": "2025-07-09T05:24:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    24,
                    56,
                    2,
                    190,
                    0
                ],
                "published": "2024-12-10T11:03:49Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    11,
                    3,
                    49,
                    1,
                    345,
                    0
                ],
                "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPrep: Natural Language Question-Aware Data Preparation with a\n  Multi-Agent Framework"
                },
                "summary": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-ware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multiagent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-ofClauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Answering natural language (NL) questions about tables, known as Tabular\nQuestion Answering (TQA), is crucial because it allows users to quickly and\nefficiently extract meaningful insights from structured data, effectively\nbridging the gap between human language and machine-readable formats. Many of\nthese tables are derived from web sources or real-world scenarios, which\nrequire meticulous data preparation (or data prep) to ensure accurate\nresponses. However, preparing such tables for NL questions introduces new\nrequirements that extend beyond traditional data preparation. This\nquestion-ware data preparation involves specific tasks such as column\nderivation and filtering tailored to particular questions, as well as\nquestion-aware value normalization or conversion, highlighting the need for a\nmore nuanced approach in this context. Because each of the above tasks is\nunique, a single model (or agent) may not perform effectively across all\nscenarios. In this paper, we propose AutoPrep, a large language model\n(LLM)-based multiagent framework that leverages the strengths of multiple\nagents, each specialized in a certain type of data prep, ensuring more accurate\nand contextually relevant responses. Given an NL question over a table,\nAutoPrep performs data prep through three key components. Planner: Determines a\nlogical plan, outlining a sequence of high-level operations. Programmer:\nTranslates this logical plan into a physical plan by generating the\ncorresponding low-level code. Executor: Executes the generated code to process\nthe table. To support this multi-agent framework, we design a novel\nChain-ofClauses reasoning mechanism for high-level operation suggestion, and a\ntool-augmented method for low-level code generation."
                },
                "authors": [
                    {
                        "name": "Meihao Fan"
                    },
                    {
                        "name": "Ju Fan"
                    },
                    {
                        "name": "Nan Tang"
                    },
                    {
                        "name": "Lei Cao"
                    },
                    {
                        "name": "Guoliang Li"
                    },
                    {
                        "name": "Xiaoyong Du"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyong Du"
                },
                "author": "Xiaoyong Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10422v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10422v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06554v1",
                "updated": "2025-07-09T05:13:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    13,
                    9,
                    2,
                    190,
                    0
                ],
                "published": "2025-07-09T05:13:09Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    13,
                    9,
                    2,
                    190,
                    0
                ],
                "title": "SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth\n  Generation for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth\n  Generation for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large\nLanguage Models (LLMs), where the effectiveness of the retriever largely\ndetermines the overall response quality of RAG systems. Retrievers encompass a\nmultitude of hyperparameters that significantly impact performance outcomes and\ndemonstrate sensitivity to specific applications. Nevertheless, hyperparameter\noptimization entails prohibitively high computational expenses. Existing\nevaluation methods suffer from either prohibitive costs or disconnection from\ndomain-specific scenarios. This paper proposes SEARA (Subset sampling\nEvaluation for Automatic Retriever Assessment), which addresses evaluation data\nchallenges through subset sampling techniques and achieves robust automated\nretriever evaluation by minimal retrieval facts extraction and comprehensive\nretrieval metrics. Based on real user queries, this method enables fully\nautomated retriever evaluation at low cost, thereby obtaining optimal retriever\nfor specific business scenarios. We validate our method across classic RAG\napplications in rednote, including knowledge-based Q&A system and\nretrieval-based travel assistant, successfully obtaining scenario-specific\noptimal retrievers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large\nLanguage Models (LLMs), where the effectiveness of the retriever largely\ndetermines the overall response quality of RAG systems. Retrievers encompass a\nmultitude of hyperparameters that significantly impact performance outcomes and\ndemonstrate sensitivity to specific applications. Nevertheless, hyperparameter\noptimization entails prohibitively high computational expenses. Existing\nevaluation methods suffer from either prohibitive costs or disconnection from\ndomain-specific scenarios. This paper proposes SEARA (Subset sampling\nEvaluation for Automatic Retriever Assessment), which addresses evaluation data\nchallenges through subset sampling techniques and achieves robust automated\nretriever evaluation by minimal retrieval facts extraction and comprehensive\nretrieval metrics. Based on real user queries, this method enables fully\nautomated retriever evaluation at low cost, thereby obtaining optimal retriever\nfor specific business scenarios. We validate our method across classic RAG\napplications in rednote, including knowledge-based Q&A system and\nretrieval-based travel assistant, successfully obtaining scenario-specific\noptimal retrievers."
                },
                "authors": [
                    {
                        "name": "Zou Yuheng"
                    },
                    {
                        "name": "Wang Yiran"
                    },
                    {
                        "name": "Tian Yuzhu"
                    },
                    {
                        "name": "Zhu Min"
                    },
                    {
                        "name": "Huang Yanhua"
                    }
                ],
                "author_detail": {
                    "name": "Huang Yanhua"
                },
                "author": "Huang Yanhua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19030v2",
                "updated": "2025-07-09T05:10:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    9,
                    5,
                    10,
                    3,
                    2,
                    190,
                    0
                ],
                "published": "2025-06-23T18:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    18,
                    37,
                    55,
                    0,
                    174,
                    0
                ],
                "title": "WiLLM: an Open Framework for LLM Services over Wireless Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiLLM: an Open Framework for LLM Services over Wireless Systems"
                },
                "summary": "Large Language Model (LLM) services fundamentally differ from traditional\nDeep Neural Network (DNN) applications in wireless networks. We identify three\ncritical distinctions: (1) unlike traditional DNNs with unidirectional data\nflows, LLM's multimodal interactions create bidirectional heavy loads with\ncontrasting bottlenecks, requiring direction-aware resource scheduling; (2)\nwhile traditional DNNs exhibit fixed computational patterns, LLM's highly\nvariable inference times interact complexly with network slicing, causing\ndynamic bottleneck migration; and (3) in contrast to predictable DNN traffic,\nLLM's token streams demonstrate unprecedented burstiness and state\ndependencies. These insights motivate WiLLM, the first open-source framework,\nimplemented as a wireless platform, for LLM service research. Built on\nOpenAirInterface, WiLLM introduces several technical innovations: dynamic slice\ncompatibility, universal UE compatibility through application-layer tunneling,\nmulti-UE multi-slice scheduling, dual-mode resource allocation, and cross-layer\nAPIs. In addition, WiLLM eliminates the need for specialized wireless\nexpertise, enabling researchers and developers to experiment with LLM services\nover realistic cellular networks. We demonstrate the platform's capabilities\nthrough a smart glasses case study and provide a comprehensive dataset of \\~1.6\nmillion synchronized measurements. The complete system, dataset, and appendix\nare available at https://openwillm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) services fundamentally differ from traditional\nDeep Neural Network (DNN) applications in wireless networks. We identify three\ncritical distinctions: (1) unlike traditional DNNs with unidirectional data\nflows, LLM's multimodal interactions create bidirectional heavy loads with\ncontrasting bottlenecks, requiring direction-aware resource scheduling; (2)\nwhile traditional DNNs exhibit fixed computational patterns, LLM's highly\nvariable inference times interact complexly with network slicing, causing\ndynamic bottleneck migration; and (3) in contrast to predictable DNN traffic,\nLLM's token streams demonstrate unprecedented burstiness and state\ndependencies. These insights motivate WiLLM, the first open-source framework,\nimplemented as a wireless platform, for LLM service research. Built on\nOpenAirInterface, WiLLM introduces several technical innovations: dynamic slice\ncompatibility, universal UE compatibility through application-layer tunneling,\nmulti-UE multi-slice scheduling, dual-mode resource allocation, and cross-layer\nAPIs. In addition, WiLLM eliminates the need for specialized wireless\nexpertise, enabling researchers and developers to experiment with LLM services\nover realistic cellular networks. We demonstrate the platform's capabilities\nthrough a smart glasses case study and provide a comprehensive dataset of \\~1.6\nmillion synchronized measurements. The complete system, dataset, and appendix\nare available at https://openwillm.github.io."
                },
                "authors": [
                    {
                        "name": "Boyi Liu"
                    },
                    {
                        "name": "Yongguang Lu"
                    },
                    {
                        "name": "Jianguo Zhao"
                    },
                    {
                        "name": "Qiang Yang"
                    },
                    {
                        "name": "Wen Wu"
                    },
                    {
                        "name": "Lin Chen"
                    },
                    {
                        "name": "Jagmohan Chauhan"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]