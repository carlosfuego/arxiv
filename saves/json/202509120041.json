[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v3",
                "updated": "2025-09-09T13:30:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    13,
                    30,
                    17,
                    1,
                    252,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07379v1",
                "updated": "2025-09-09T04:00:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-09T04:00:43Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    4,
                    0,
                    43,
                    1,
                    252,
                    0
                ],
                "title": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for\n  Efficient MoE LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\na wide range of deep learning tasks. Mixture of Experts (MoE) further enhances\ntheir capabilities by increasing model width through sparsely activated expert\nbranches, which keeps inference computation efficient. However, the large\nnumber of expert weights introduces significant GPU memory pressure, especially\nin resource-constrained environments such as single-GPU servers. More\nimportantly, MoE inference consists of two fundamentally different stages: a\nprefill stage where most experts are activated densely, and a decode stage\nwhere only a few experts are triggered sparsely. Treating these stages with a\nuniform scheduling strategy often leads to suboptimal latency and memory usage.\nTo address this, we propose DuoServe-MoE, an inference serving system that\nexplicitly separates prefill and decode stages and applies tailored expert\nscheduling strategies to each. In the prefill stage, DuoServe-MoE uses a\ntwo-stream CUDA pipeline that overlaps expert weight prefetching with the\ncomputation of non-MoE layers, limiting expert residency in GPU memory. In the\ndecode stage, a lightweight layer-level predictor trained offline from\nactivation traces is used to prefetch only the most likely activated experts,\nwithout requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B\nand 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to\n7.54 times while keeping peak memory usage at only 15 percent of the full model\nsize."
                },
                "authors": [
                    {
                        "name": "Yuning Zhang"
                    },
                    {
                        "name": "Grant Pinkert"
                    },
                    {
                        "name": "Nan Yang"
                    },
                    {
                        "name": "Yanli Li"
                    },
                    {
                        "name": "Dong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yuan"
                },
                "author": "Dong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01742v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01742v2",
                "updated": "2025-09-09T00:15:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    9,
                    0,
                    15,
                    5,
                    1,
                    252,
                    0
                ],
                "published": "2025-09-01T19:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    19,
                    49,
                    21,
                    0,
                    244,
                    0
                ],
                "title": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure\n  HBM Accelerators"
                },
                "summary": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Trusted Execution Environments provide a strong foundation for secure\ncloud computing, they remain vulnerable to access pattern leakages. Oblivious\nMaps (OMAPs) mitigate this by fully hiding access patterns but suffer from high\noverhead due to randomized remapping and worst-case padding. We argue these\ncosts are not fundamental. Modern accelerators featuring High-Bandwidth Memory\n(HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that\neavesdropping on HBM is difficult -- even for physical attackers -- as its\nmemory channels are sealed together with processor cores inside the same\nphysical package. Later, Hunt et al. [NSDI'20] show that, with proper\nisolation, HBM can be turned into an unobservable region where both data and\nmemory traces are hidden. This motivates a rethink of OMAP design with\nHBM-backed solutions to finally overcome their traditional performance limits.\nBuilding on these insights, we present BOLT, a Bandwidth Optimized,\nLightning-fast OMAP accelerator that, for the first time, achieves O(1) +\nO(log_2(log_2 (N))) bandwidth overhead. BOLT introduces three key innovations:\n(i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache\nto accelerate oblivious access to large host memory; (ii) a self-hosted\narchitecture that offloads execution and memory control from the host to\nmitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs\nthat maximize resource efficiency. We implement a prototype BOLT on a Xilinx\nU55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in\ninitialization and query time, respectively, over state-of-the-art OMAPs,\nincluding an industry implementation from Facebook."
                },
                "authors": [
                    {
                        "name": "Yitong Guo"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Haobin Hiroki Chen"
                    },
                    {
                        "name": "Yukui Luo"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Chenghong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Chenghong Wang"
                },
                "author": "Chenghong Wang",
                "arxiv_comment": "Accepted by CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01742v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01742v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06949v1",
                "updated": "2025-09-08T17:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T17:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    58,
                    6,
                    0,
                    251,
                    0
                ],
                "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models"
                },
                "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL"
                },
                "authors": [
                    {
                        "name": "Yinjie Wang"
                    },
                    {
                        "name": "Ling Yang"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Ye Tian"
                    },
                    {
                        "name": "Ke Shen"
                    },
                    {
                        "name": "Mengdi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Mengdi Wang"
                },
                "author": "Mengdi Wang",
                "arxiv_comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03377v2",
                "updated": "2025-09-08T17:22:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    22,
                    17,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-03T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    53,
                    45,
                    2,
                    246,
                    0
                ],
                "title": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Effective CXL Memory Bandwidth for LLM Inference via\n  Transparent Near-Data Processing"
                },
                "summary": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference is bottlenecked by the limited bandwidth\nof CXL-based memory used for capacity expansion. We introduce CXL-NDP, a\ntransparent near-data processing architecture that amplifies effective CXL\nbandwidth without requiring changes to the CXL.mem interface or AI models.\nCXL-NDP integrates a precision-scalable bit-plane layout for dynamic\nquantization with transparent lossless compression of weights and KV caches\ndirectly within the CXL device. In end-to-end serving, CXL-NDP improves\nthroughput by 43%, extends the maximum context length by 87%, and reduces the\nKV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms\nits practicality with a modest silicon footprint, lowering the barrier for\nadopting efficient, scalable CXL-based memory in generative AI infrastructure."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v4",
                "updated": "2025-09-08T13:34:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    13,
                    34,
                    54,
                    0,
                    251,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1% average\nscore drop with 7B training tokens and 140 GPU hours. The code for this work is\navailable at https://github.com/AMD-AGI/AMD-Hybrid-Models."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06579v1",
                "updated": "2025-09-08T11:49:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T11:49:51Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    11,
                    49,
                    51,
                    0,
                    251,
                    0
                ],
                "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View\n  Synthesis"
                },
                "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html."
                },
                "authors": [
                    {
                        "name": "Xin Kong"
                    },
                    {
                        "name": "Daniel Watson"
                    },
                    {
                        "name": "Yannick Strmpler"
                    },
                    {
                        "name": "Michael Niemeyer"
                    },
                    {
                        "name": "Federico Tombari"
                    }
                ],
                "author_detail": {
                    "name": "Federico Tombari"
                },
                "author": "Federico Tombari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v1",
                "updated": "2025-09-08T09:54:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v4",
                "updated": "2025-09-08T09:09:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    9,
                    36,
                    0,
                    251,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06444v1",
                "updated": "2025-09-08T08:44:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:44:24Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    44,
                    24,
                    0,
                    251,
                    0
                ],
                "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for\n  Heterogeneous and Privacy-Sensitive Data"
                },
                "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments."
                },
                "authors": [
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Hainan Zhang"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Hong-Wei Zheng"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "arxiv_comment": "9 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v1",
                "updated": "2025-09-08T08:34:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v1",
                "updated": "2025-09-08T00:57:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06047v1",
                "updated": "2025-09-07T13:15:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "published": "2025-09-07T13:15:17Z",
                "published_parsed": [
                    2025,
                    9,
                    7,
                    13,
                    15,
                    17,
                    6,
                    250,
                    0
                ],
                "title": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A facile vector substrate platform via BaTiO3 membrane transfer enables\n  high quality solution processed epitaxial PZT on silicon"
                },
                "summary": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The direct integration of high-performance ferroelectric oxides with silicon\nremains challenging due to lattice mismatch, thermal incompatibility, and the\nneed for high-temperature epitaxial growth. Here, a hybrid integration approach\nis demonstrated in which crystalline BaTiO3 (BTO) membranes are first\ntransferred onto Pt coated Si substrates and subsequently used as vector\nsubstrates (VS) for the growth of epitaxial (001) Pb(Zr0.52Ti0.48)O3 (PZT) thin\nfilms via chemical solution deposition (CSD). A KI and HCl based etchant\nenables rapid and complete dissolution of the SrVO3 sacrificial layer in about\n30 minutes, reducing the release time from days to minutes compared with\nconventional water based approaches to dissolve AVO3 and AMoO3 (A is Ca, Sr,\nBa). The BTO VS imposes dominant (00l) out of plane orientation and in plane\ncube on cube epitaxy in the overlying PZT. Devices exhibit remnant polarization\n10 to 12 micro coulomb/cm2 and coercive field of 100 kV/cm, with stable\nswitching to 10^8 cycles on the VS. From piezoelectric butterfly loops, we\nextract effective d33 of 70 pm/V for PZT on VS, and 54 pm/V for PZT grown on\nconventional Pt Si substrates. This approach demonstrates a scalable and cost\neffective route for integrating functional ferroelectric materials onto silicon\nand offers a promising platform for future CMOS compatible oxide electronics."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Antony Jeyaseelan"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "arxiv_affiliation": "Centre for Nanoscience and Engineering, Indian Institute of Science, Bengaluru, India",
                "author": "Pavan Nukala",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v2",
                "updated": "2025-09-06T05:58:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    6,
                    5,
                    58,
                    51,
                    5,
                    249,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Cache Contention Analysis for WCET Estimation on Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05207v1",
                "updated": "2025-09-05T16:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T16:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    16,
                    10,
                    20,
                    4,
                    248,
                    0
                ],
                "title": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RapidGNN: Energy and Communication-Efficient Distributed Training on\n  Large-Scale Graph Neural Networks"
                },
                "summary": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have become popular across a diverse set of\ntasks in exploring structural relationships between entities. However, due to\nthe highly connected structure of the datasets, distributed training of GNNs on\nlarge-scale graphs poses significant challenges. Traditional sampling-based\napproaches mitigate the computational loads, yet the communication overhead\nremains a challenge. This paper presents RapidGNN, a distributed GNN training\nframework with deterministic sampling-based scheduling to enable efficient\ncache construction and prefetching of remote features. Evaluation on benchmark\ngraph datasets demonstrates RapidGNN's effectiveness across different scales\nand topologies. RapidGNN improves end-to-end training throughput by 2.46x to\n3.00x on average over baseline methods across the benchmark datasets, while\ncutting remote feature fetches by over 9.70x to 15.39x. RapidGNN further\ndemonstrates near-linear scalability with an increasing number of computing\nunits efficiently. Furthermore, it achieves increased energy efficiency over\nthe baseline methods for both CPU and GPU by 44% and 32%, respectively."
                },
                "authors": [
                    {
                        "name": "Arefin Niam"
                    },
                    {
                        "name": "Tevfik Kosar"
                    },
                    {
                        "name": "M S Q Zulkar Nine"
                    }
                ],
                "author_detail": {
                    "name": "M S Q Zulkar Nine"
                },
                "author": "M S Q Zulkar Nine",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2505.10806",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v1",
                "updated": "2025-09-05T14:58:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v2",
                "updated": "2025-09-05T10:39:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    5,
                    10,
                    39,
                    3,
                    4,
                    248,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-Style Channel Controllers for Modern Disaggregated Memory\n  Systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "arxiv_doi": "10.1145/3725783.3764403",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725783.3764403",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.09758v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Camera-ready authors' version for APSys'25",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04377v1",
                "updated": "2025-09-04T16:40:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T16:40:01Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    40,
                    1,
                    3,
                    247,
                    0
                ],
                "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient\n  Large Language Model Inference"
                },
                "summary": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV caching significantly improves the efficiency of Large Language Model\n(LLM) inference by storing attention states from previously processed tokens,\nenabling faster generation of subsequent tokens. However, as sequence length\nincreases, the KV cache quickly becomes a major memory bottleneck. To address\nthis, we propose PagedEviction, a novel fine-grained, structured KV cache\npruning strategy that enhances the memory efficiency of vLLM's PagedAttention.\nUnlike existing approaches that rely on attention-based token importance or\nevict tokens across different vLLM pages, PagedEviction introduces an efficient\nblock-wise eviction algorithm tailored for paged memory layouts. Our method\nintegrates seamlessly with PagedAttention without requiring any modifications\nto its CUDA attention kernels. We evaluate PagedEviction across\nLlama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models\non the LongBench benchmark suite, demonstrating improved memory usage with\nbetter accuracy than baselines on long context tasks."
                },
                "authors": [
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Jie Ye"
                    },
                    {
                        "name": "Xian-He Sun"
                    },
                    {
                        "name": "Anthony Kougkas"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v2",
                "updated": "2025-09-04T15:21:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    15,
                    21,
                    11,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a comprehensive multi-level analysis of the NVIDIA Hopper\nGPU architecture, focusing on its performance characteristics and novel\nfeatures. We benchmark Hopper's memory subsystem, highlighting improvements in\nthe L2 partitioned cache and global memory access compared to Ampere and Ada\nLovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the\nbenefits of FP8 precision and asynchronous wgmma instructions for matrix\noperations. Additionally, we investigate the performance of DPX instructions\nfor dynamic programming, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. Through multi-level evaluation, we discover that the Hopper\narchitecture demonstrates significant acceleration potential in real-world\napplications. For instance, the asynchronous programming model supported by TMA\nachieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double\nthe performance of FP16, and DPX instructions accelerate a computational\nbiology algorithm by at least 4.75x. Our findings provide actionable insights\nfor optimizing compute-intensive workloads, from AI training to bioinformatics,\non Hopper GPUs."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v2",
                "updated": "2025-09-04T13:14:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    14,
                    33,
                    3,
                    247,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04185v1",
                "updated": "2025-09-04T13:02:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T13:02:39Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    13,
                    2,
                    39,
                    3,
                    247,
                    0
                ],
                "title": "Set Block Decoding is a Language Model Inference Accelerator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Set Block Decoding is a Language Model Inference Accelerator"
                },
                "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training."
                },
                "authors": [
                    {
                        "name": "Itai Gat"
                    },
                    {
                        "name": "Heli Ben-Hamu"
                    },
                    {
                        "name": "Marton Havasi"
                    },
                    {
                        "name": "Daniel Haziza"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "David Lopez-Paz"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Yaron Lipman"
                    }
                ],
                "author_detail": {
                    "name": "Yaron Lipman"
                },
                "author": "Yaron Lipman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04180v1",
                "updated": "2025-09-04T12:54:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T12:54:32Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    12,
                    54,
                    32,
                    3,
                    247,
                    0
                ],
                "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision"
                },
                "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}."
                },
                "authors": [
                    {
                        "name": "Safouane El Ghazouali"
                    },
                    {
                        "name": "Umberto Michelucci"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michelucci"
                },
                "author": "Umberto Michelucci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04010v1",
                "updated": "2025-09-04T08:41:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "published": "2025-09-04T08:41:06Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    8,
                    41,
                    6,
                    3,
                    247,
                    0
                ],
                "title": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and\n  Lessons Learned"
                },
                "summary": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PQDSS standardization process requires cryptographic primitives to be\nfree from vulnerabilities, including timing and cache side-channels. Resistance\nto timing leakage is therefore an essential property, and achieving this\ntypically relies on software implementations that follow constant-time\nprinciples. Moreover, ensuring that all implementations are constant-time is\ncrucial for fair performance comparisons, as secure implementations often incur\nadditional overhead. Such analysis also helps identify scheme proposals that\nare inherently difficult to implement in constant time. Because constant-time\nproperties can be broken during compilation, it is often necessary to analyze\nthe compiled binary directly. Since manual binary analysis is extremely\nchallenging, automated analysis becomes highly important. Although several\ntools exist to assist with such analysis, they often have usability limitations\nand are difficult to set up correctly. To support the developers besides the\nNIST committee in verifying candidates, we developed a toolchain that automates\nconfiguration, execution, and result analysis for several widely used\nconstant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify\nconstant-time policy compliance at the binary level, and dudect and RTLF to\ndetect side-channel vulnerabilities through statistical analysis of execution\ntime behavior. We demonstrate its effectiveness and practicability by\nevaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26\nissues in total to the respective developers, and 5 of them have already been\nfixed. We also discuss our different findings, as well as the benefits of\nshortcomings of the different tools."
                },
                "authors": [
                    {
                        "name": "Olivier Adjonyo"
                    },
                    {
                        "name": "Sebastien Bardin"
                    },
                    {
                        "name": "Emanuele Bellini"
                    },
                    {
                        "name": "Gilbert Ndollane Dione"
                    },
                    {
                        "name": "Mahmudul Faisal Al Ameen"
                    },
                    {
                        "name": "Robert Merget"
                    },
                    {
                        "name": "Frederic Recoules"
                    },
                    {
                        "name": "Yanis Sellami"
                    }
                ],
                "author_detail": {
                    "name": "Yanis Sellami"
                },
                "author": "Yanis Sellami",
                "arxiv_comment": "20 pages, 1 figure, to be published and presented at Sixth PQC\n  Standardization Conference by NIST, partially supported by the \"France 2030\"\n  government investment plan managed by the French National Research Agency,\n  under the reference ANR-22-PECY-0005",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v3",
                "updated": "2025-09-04T06:20:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    4,
                    6,
                    20,
                    55,
                    3,
                    247,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "IC-Cache: Efficient Large Language Model Serving via In-context Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IC-Cache: Efficient Large Language Model Serving via In-context Caching"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 70% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge transfer among requests. However, naively caching and reusing past\nresponses leads to a big quality drop. In this paper, we introduce IC-Cache, a\ncaching system that enables live LLM capability augmentation to improve serving\nefficiency: by leveraging historical request-response pairs from larger models\nas in-context examples, IC-Cache empowers small LLMs to imitate and even exceed\nthe compositional abilities (e.g., reasoning) of their larger counterparts,\nenabling selective offloading of requests to reduce cost and latency. Achieving\nthis live augmentation at scale introduces intricate trade-offs between\nresponse quality, latency, and system throughput. For a new request, IC-Cache\nefficiently selects similar, high-utility examples to prepend them to the new\nrequest's input. At scale, it adaptively routes requests across LLMs of varying\ncapabilities, accounting for response quality and serving loads. IC-Cache\nemploys a cost-aware cache replay mechanism that refines example quality\noffline to maximize online cache utility and efficiency. Evaluations on\nmillions of realistic requests demonstrate that IC-Cache improves LLM serving\nthroughput by 1.4-5.9x and reduces latency by 28-71% without hurting response\nquality."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "arxiv_doi": "10.1145/3731569.3764829",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764829",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01228v2",
                "updated": "2025-09-03T20:54:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    20,
                    54,
                    57,
                    2,
                    246,
                    0
                ],
                "published": "2024-10-02T04:12:13Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    4,
                    12,
                    13,
                    2,
                    276,
                    0
                ],
                "title": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline\n  Co-Serving"
                },
                "summary": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving demands low latency and high throughput,\nbut high load variability makes it challenging to achieve high GPU utilization.\nIn this paper, we identify a synergetic but overlooked opportunity to co-serve\nlatency-critical online requests alongside latency-tolerant offline tasks such\nas model benchmarking. While promising, existing serving systems fail to\nco-serve them efficiently, as their coarse-grained resource management at the\nrequest or iteration level cannot harvest millisecond-level GPU idle cycles\nwithout introducing interference that violates online latency objectives.\nConServe is a new LLM co-serving system that achieves high throughput and\nstrong online latency guarantees by managing resources at finer granularities.\nConServe introduces three techniques: (1) a latency-aware token-level scheduler\nthat precisely sizes offline batches and tokens to fit within online latency\nobjectives; (2) sub-iteration, layer-wise preemption that allows offline tasks\nto yield to online load spikes; and (3) incremental KV cache management that\nenables preempting and resuming offline requests at near-zero cost. Evaluations\nwith Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe\ndelivers an average of 2.2$\\times$ higher throughput and reduces online serving\ntail latency by 2.9$\\times$ on average compared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Shu Anzai"
                    },
                    {
                        "name": "Shan Yu"
                    },
                    {
                        "name": "Haoran Ma"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Miryung Kim"
                    },
                    {
                        "name": "Yongji Wu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jiarong Xing"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Harry Xu"
                    }
                ],
                "author_detail": {
                    "name": "Harry Xu"
                },
                "author": "Harry Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03394v1",
                "updated": "2025-09-03T15:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T15:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    15,
                    44,
                    2,
                    246,
                    0
                ],
                "title": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CloudFormer: An Attention-based Performance Prediction for Public Clouds\n  with Unknown Workload"
                },
                "summary": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms are increasingly relied upon to host diverse,\nresource-intensive workloads due to their scalability, flexibility, and\ncost-efficiency. In multi-tenant cloud environments, virtual machines are\nconsolidated on shared physical servers to improve resource utilization. While\nvirtualization guarantees resource partitioning for CPU, memory, and storage,\nit cannot ensure performance isolation. Competition for shared resources such\nas last-level cache, memory bandwidth, and network interfaces often leads to\nsevere performance degradation. Existing management techniques, including VM\nscheduling and resource provisioning, require accurate performance prediction\nto mitigate interference. However, this remains challenging in public clouds\ndue to the black-box nature of VMs and the highly dynamic nature of workloads.\nTo address these limitations, we propose CloudFormer, a dual-branch\nTransformer-based model designed to predict VM performance degradation in\nblack-box environments. CloudFormer jointly models temporal dynamics and\nsystem-level interactions, leveraging 206 system metrics at one-second\nresolution across both static and dynamic scenarios. This design enables the\nmodel to capture transient interference effects and adapt to varying workload\nconditions without scenario-specific tuning. Complementing the methodology, we\nprovide a fine-grained dataset that significantly expands the temporal\nresolution and metric diversity compared to existing benchmarks. Experimental\nresults demonstrate that CloudFormer consistently outperforms state-of-the-art\nbaselines across multiple evaluation metrics, achieving robust generalization\nacross diverse and previously unseen workloads. Notably, CloudFormer attains a\nmean absolute error (MAE) of just 7.8%, representing a substantial improvement\nin predictive accuracy and outperforming existing methods at least by 28%."
                },
                "authors": [
                    {
                        "name": "Amirhossein Shahbazinia"
                    },
                    {
                        "name": "Darong Huang"
                    },
                    {
                        "name": "Luis Costero"
                    },
                    {
                        "name": "David Atienza"
                    }
                ],
                "author_detail": {
                    "name": "David Atienza"
                },
                "author": "David Atienza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00079v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00079v4",
                "updated": "2025-09-03T14:56:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    56,
                    29,
                    2,
                    246,
                    0
                ],
                "published": "2024-06-24T02:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    2,
                    5,
                    32,
                    0,
                    176,
                    0
                ],
                "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving"
                },
                "summary": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mooncake is the serving platform for Kimi, a leading LLM service provided by\nMoonshot AI. It features a KVCache-centric disaggregated architecture that\nseparates the prefill and decoding clusters. It also leverages the\nunderutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a\ndisaggregated cache of KVCache. The core of Mooncake is its KVCache-centric\nscheduler, which balances maximizing overall effective throughput while meeting\nlatency-related Service Level Objectives (SLOs). Unlike traditional studies\nthat assume all requests will be processed, Mooncake faces challenges due to\nhighly overloaded scenarios. To mitigate these, we developed a prediction-based\nearly rejection policy. Experiments show that Mooncake excels in long-context\nscenarios. Compared to the baseline method, Mooncake can achieve up to a 525%\nincrease in throughput in certain simulated scenarios while adhering to SLOs.\nUnder real workloads, Mooncake's innovative architecture enables Kimi to handle\n75% more requests."
                },
                "authors": [
                    {
                        "name": "Ruoyu Qin"
                    },
                    {
                        "name": "Zheming Li"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Weimin Zheng"
                    },
                    {
                        "name": "Xinran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinran Xu"
                },
                "author": "Xinran Xu",
                "arxiv_comment": "23 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00079v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00079v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04416v2",
                "updated": "2025-09-03T14:28:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    14,
                    28,
                    23,
                    2,
                    246,
                    0
                ],
                "published": "2025-07-06T15:08:49Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    15,
                    8,
                    49,
                    6,
                    187,
                    0
                ],
                "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based\n  Sequence Modeling"
                },
                "summary": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have become the cornerstone of modern large-scale language\nmodels, but their reliance on softmax attention poses a computational\nbottleneck at both training and inference. Recurrent models offer high\nefficiency, but compressing the full sequence into a fixed-size and holistic\nrepresentation suffers from memory degradation in long contexts and limits\nfine-grained retrieval. To address this, we propose RAT, an intermediate design\nthat bridges the efficiency of RNNs and capacity of attention. RAT partitions\nthe input into chunks, applies recurrence within each chunk for local\ndependencies, and softmax-based attention across chunks for long-range\ninteractions. This design mitigates memory degradation and enables direct\naccess to distant tokens, while retaining computational efficiency.\nEmpirically, with a chunk size of 16, the RAT block achieves a 7x improvement\nin training speed with 100K token sequences and 9x in generation at the 4K\nposition, while maintaining similar performance compared to standard attention.\nWe demonstrate this by training 1.3B parameter models from scratch and\nperforming large-scale evaluations, including short- and long-context\nbenchmarks, as well as supervised fine-tuning~(SFT). We further propose a\nhybrid architecture that interleaves RAT with local attention. By combining\nefficient long-range modeling with strong local interactions, this hybrid\ndesign not only improves inference speed and reduces cache memory usage, but\nalso consistently enhances performance and shows the overall best results. Code\nis available at https://github.com/CLAIRE-Labo/RAT."
                },
                "authors": [
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Anunay Yadav"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03560v1",
                "updated": "2025-09-03T11:23:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T11:23:35Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    11,
                    23,
                    35,
                    2,
                    246,
                    0
                ],
                "title": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cegar-centric Bounded Reachability Analysis for Compositional Affine\n  Hybrid Systems"
                },
                "summary": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reachability analysis of compositional hybrid systems, where individual\ncomponents are modeled as hybrid automata, poses unique challenges. In addition\nto preserving the compositional semantics while computing system behaviors,\nalgorithms have to cater to the explosion in the number of locations in the\nparallel product automaton. In this paper, we propose a bounded reachability\nanalysis algorithm for compositional hybrid systems with piecewise affine\ndynamics, based on the principle of counterexample guided abstraction\nrefinement (CEGAR). In particular, the algorithm searches for a counterexample\nin the discrete abstraction of the composition model, without explicitly\ncomputing a product automaton. When a counterexample is discovered in the\nabstraction, its validity is verified by a refinement of the state-space guided\nby the abstract counterexample. The state-space refinement is through a\nsymbolic reachability analysis, particularly using a state-of-the-art algorithm\nwith support functions as the continuous state representation. In addition, the\nalgorithm mixes different semantics of composition with the objective of\nimproved efficiency. Step compositional semantics is followed while exploring\nthe abstract (discrete) state-space, while shallow compositional semantics is\nfollowed during state-space refinement with symbolic reachability analysis.\nOptimizations such as caching the results of the symbolic reachability\nanalysis, which can be later reused, have been proposed. We implement this\nalgorithm in the tool SAT-Reach and demonstrate the scalability benefits."
                },
                "authors": [
                    {
                        "name": "Atanu Kundu"
                    },
                    {
                        "name": "Pratyay Sarkar"
                    },
                    {
                        "name": "Rajarshi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Ray"
                },
                "author": "Rajarshi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03136v1",
                "updated": "2025-09-03T08:38:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "published": "2025-09-03T08:38:40Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    8,
                    38,
                    40,
                    2,
                    246,
                    0
                ],
                "title": "Adaptive KV-Cache Compression without Manually Setting Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive KV-Cache Compression without Manually Setting Budget"
                },
                "summary": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) inference relies heavily on KV-caches to\naccelerate autoregressive decoding, but the resulting memory footprint grows\nrapidly with sequence length, posing significant efficiency challenges. Current\nKV-cache compression methods suffer from a Procrustes' bed problem: they force\ndiverse workloads into fixed compression ratios, leading to suboptimal resource\nallocation and inference performance. To this end, we present GVote, an\nadaptive KV-cache compression scheme that eliminates manual budget\nspecification while achieving superior accuracy-efficiency trade-offs. GVote\noperates on the principle that the important keys are the aggregation of keys\nrequired by future queries. The method predicts future query attention demands\nby Monte-Carlo style sampling potential queries and aggregating selected keys\nto determine the optimal cache budget without manual specification.\nExperimental evaluation demonstrates GVote's effectiveness across multiple\nbenchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote\nexhibits 2$\\times$ memory reduction while the accuracy maintains higher or\ncomparable."
                },
                "authors": [
                    {
                        "name": "Chenxia Tang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20353v2",
                "updated": "2025-09-03T06:56:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    3,
                    6,
                    56,
                    21,
                    2,
                    246,
                    0
                ],
                "published": "2025-05-26T05:58:49Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    5,
                    58,
                    49,
                    0,
                    146,
                    0
                ],
                "title": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastCache: Fast Caching for Diffusion Transformer Through Learnable\n  Linear Approximation"
                },
                "summary": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) are powerful generative models but remain\ncomputationally intensive due to their iterative structure and deep transformer\nstacks. To alleviate this inefficiency, we propose FastCache, a\nhidden-state-level caching and compression framework that accelerates DiT\ninference by exploiting redundancy within the model's internal representations.\nFastCache introduces a dual strategy: (1) a spatial-aware token selection\nmechanism that adaptively filters redundant tokens based on hidden state\nsaliency, and (2) a transformer-level cache that reuses latent activations\nacross timesteps when changes are statistically insignificant. These modules\nwork jointly to reduce unnecessary computation while preserving generation\nfidelity through learnable linear approximation. Theoretical analysis shows\nthat FastCache maintains bounded approximation error under a\nhypothesis-testing-based decision rule. Empirical evaluations across multiple\nDiT variants demonstrate substantial reductions in latency and memory usage,\nwith best generation output quality compared to other cache methods, as\nmeasured by FID and t-FID. Code implementation of FastCache is available on\nGitHub at https://github.com/NoakLiu/FastCache-xDiT."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Jiayi Zhang"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Ben Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v2",
                "updated": "2025-09-02T18:10:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    18,
                    10,
                    0,
                    1,
                    245,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Yuchen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Liu"
                },
                "author": "Yuchen Liu",
                "arxiv_comment": "Under Major Revision in IEEE Network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02532v1",
                "updated": "2025-09-02T17:35:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T17:35:42Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    17,
                    35,
                    42,
                    1,
                    245,
                    0
                ],
                "title": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device\n  Networks"
                },
                "summary": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Device-to-device (D2D) communication is one of the most promising techniques\nfor future wireless cellular communication systems. This paper considers coded\ncaching in a partially cooperative wireless D2D network, where only a subset of\nusers transmit during delivery, while all users request files. The\nnon-transmitting users are referred to as selfish users. All existing schemes\nthat do not require knowledge of the identity of selfish users before content\nplacement are limited to the high-memory regime, particularly when the number\nof selfish users is large. We propose a novel coded caching scheme for a\npartially cooperative D2D network that operates in all feasible memory regimes,\nregardless of the number of selfish users. We also derive a lower bound on the\ntransmission load of a partially cooperative D2D coded caching scheme. Using\nthis bound, the proposed scheme is shown to be optimal in the high-memory\nregime."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages and 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v5",
                "updated": "2025-09-02T16:39:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    39,
                    56,
                    1,
                    245,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. However, existing Ethernet-based solutions, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilization due to both increasing traffic demands and\nthe expanding scale of datacenter topologies, which also exacerbate network\nfailures. To address these limitations, we propose REPS, a lightweight\ndecentralized per-packet adaptive load balancing algorithm designed to optimize\nnetwork utilization while ensuring rapid recovery from link failures. REPS\nadapts to network conditions by caching good-performing paths. In case of a\nnetwork failure, REPS re-routes traffic away from it in less than 100\nmicroseconds. REPS is designed to be deployed with next-generation out-of-order\ntransports, such as Ultra Ethernet, and uses less than 25 bytes of\nper-connection state regardless of the topology size. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02480v1",
                "updated": "2025-09-02T16:30:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T16:30:49Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    16,
                    30,
                    49,
                    1,
                    245,
                    0
                ],
                "title": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to\n  Break the GPU Memory Wall"
                },
                "summary": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs larger than the aggregated memory of multiple GPUs is\nincreasingly necessary due to the faster growth of LLM sizes compared to GPU\nmemory. To this end, multi-tier host memory or disk offloading techniques are\nproposed by state of art. Despite advanced asynchronous multi-tier read/write\nstrategies, such offloading strategies result in significant I/O overheads in\nthe critical path of training, resulting in slower iterations. To this end, we\npropose MLP-Offload, a novel multi-level, multi-path offloading engine\nspecifically designed for optimizing LLM training on resource-constrained\nsetups by mitigating I/O bottlenecks. We make several key observations that\ndrive the design of MLP-Offload, such as I/O overheads during the update\ndominate the iteration time; I/O bandwidth of the third-level remote storage\ntier remains unutilized; and, contention due to concurrent offloading amplifies\nI/O bottlenecks. Driven by these insights, we design and implement MLP-Offload\nto offload the optimizer states across multiple tiers in a cache-efficient and\nconcurrency-controlled fashion to mitigate I/O bottlenecks during the backward\nand update phases. Evaluations on models up to 280B parameters shows that\nMLP-Offload achieves 2.5$\\times$ faster iterations compared to the\nstate-of-the-art LLM training runtimes."
                },
                "authors": [
                    {
                        "name": "Avinash Maurya"
                    },
                    {
                        "name": "M. Mustafa Rafique"
                    },
                    {
                        "name": "Franck Cappello"
                    },
                    {
                        "name": "Bogdan Nicolae"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Nicolae"
                },
                "author": "Bogdan Nicolae",
                "arxiv_doi": "10.1145/3712285.3759864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SC'25: The International Conference for High Performance Computing,\n  Networking, Storage and Analysis",
                "arxiv_journal_ref": "SC'25: The International Conference for High Performance\n  Computing, Networking, Storage and Analysis, 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.0; E.2; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02408v1",
                "updated": "2025-09-02T15:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T15:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    15,
                    19,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Cache Management for Mixture-of-Experts LLMs -- extended version",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Management for Mixture-of-Experts LLMs -- extended version"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na variety of tasks. One of the main challenges towards the successful\ndeployment of LLMs is memory management, since they typically involve billions\nof parameters. To this end, architectures based on Mixture-of-Experts have been\nproposed, which aim to reduce the size of the parameters that are activated\nwhen producing a token. This raises the equally critical issue of efficiently\nmanaging the limited cache of the system, in that frequently used experts\nshould be stored in the fast cache rather than in the slower secondary memory.\n  In this work, we introduce and study a new paging problem that models expert\nmanagement optimization. Our formulation captures both the layered architecture\nof LLMs and the requirement that experts are cached efficiently. We first\npresent lower bounds on the competitive ratio of both deterministic and\nrandomized algorithms, which show that under mild assumptions, LRU-like\npolicies have good theoretical competitive performance. We then propose a\nlayer-based extension of LRU that is tailored to the problem at hand.\n  Extensive simulations on both synthetic datasets and actual traces of MoE\nusage show that our algorithm outperforms policies for the classic paging\nproblem, such as the standard LRU."
                },
                "authors": [
                    {
                        "name": "Spyros Angelopoulos"
                    },
                    {
                        "name": "Loris Marchal"
                    },
                    {
                        "name": "Adrien Obrecht"
                    },
                    {
                        "name": "Bertrand Simon"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Simon"
                },
                "author": "Bertrand Simon",
                "arxiv_doi": "10.1007/978-3-031-99872-0_2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-99872-0_2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v2",
                "updated": "2025-09-02T13:09:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    13,
                    9,
                    37,
                    1,
                    245,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing reliance on expensive vector\ndatabase lookups. To scale efficiently, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically skewed MedRAG workload reduces database calls by 78.9% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our work highlights that approximate caching\nis a viable and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Zhang Ji"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    }
                ],
                "author_detail": {
                    "name": "Martijn de Vos"
                },
                "author": "Martijn de Vos",
                "arxiv_doi": "10.1145/3721146.3721941",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721146.3721941",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02232v1",
                "updated": "2025-09-02T11:58:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T11:58:06Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    58,
                    6,
                    1,
                    245,
                    0
                ],
                "title": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Geometry Compression and Communication for 3D Gaussian\n  Splatting Point Clouds"
                },
                "summary": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Storage and transmission challenges in dynamic 3D scene representation based\non the i3DV platform, With increasing scene complexity, the explosive growth of\n3D Gaussian data volume causes excessive storage space occupancy. To address\nthis issue, we propose adopting the AVS PCRM reference software for efficient\ncompression of Gaussian point cloud geometry data. The strategy deeply\nintegrates the advanced encoding capabilities of AVS PCRM into the i3DV\nplatform, forming technical complementarity with the original rate-distortion\noptimization mechanism based on binary hash tables. On one hand, the hash table\nefficiently caches inter-frame Gaussian point transformation relationships,\nwhich allows for high-fidelity transmission within a 40 Mbps bandwidth\nconstraint. On the other hand, AVS PCRM performs precise compression on\ngeometry data. Experimental results demonstrate that the joint framework\nmaintains the advantages of fast rendering and high-quality synthesis in 3D\nGaussian technology while achieving significant 10\\%-25\\% bitrate savings on\nuniversal test sets. It provides a superior rate-distortion tradeoff solution\nfor the storage, transmission, and interaction of 3D volumetric video."
                },
                "authors": [
                    {
                        "name": "Liang Xie"
                    },
                    {
                        "name": "Yanting Li"
                    },
                    {
                        "name": "Luyang Tang"
                    },
                    {
                        "name": "Wei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wei Gao"
                },
                "author": "Wei Gao",
                "arxiv_doi": "10.1145/3680207.3765659",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3680207.3765659",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages,5 figures",
                "arxiv_journal_ref": "ACM MOBICOM 2025",
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v2",
                "updated": "2025-09-02T11:29:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    11,
                    29,
                    34,
                    1,
                    245,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02121v1",
                "updated": "2025-09-02T09:17:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T09:17:40Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    9,
                    17,
                    40,
                    1,
                    245,
                    0
                ],
                "title": "Batch Query Processing and Optimization for Agentic Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Query Processing and Optimization for Agentic Workflows"
                },
                "summary": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) in agentic workflows combine multi-step\nreasoning, tool use, and collaboration across multiple specialized agents.\nExisting LLM serving engines optimize individual calls in isolation, while\nmulti-agent frameworks focus on orchestration without system-level performance\nplanning. As a result, repeated prompts, overlapping contexts, and concurrent\nexecutions create substantial redundancy and poor GPU utilization, especially\nin batch analytics scenarios. We introduce Halo, a system that brings batch\nquery processing and optimization into agentic LLM workflows. Halo represents\neach workflow as a structured query plan DAG and constructs a consolidated\ngraph for batched queries that exposes shared computation. Guided by a cost\nmodel that jointly considers prefill and decode costs, cache reuse, and GPU\nplacement, Halo performs plan-level optimization to minimize redundant\nexecution. Its runtime integrates adaptive batching, KV-cache sharing and\nmigration, along with compute-communication overlap to maximize hardware\nefficiency. Evaluation across six benchmarks shows that Halo achieves up to\n18.6x speedup for batch inference and 4.7x throughput improvement under online\nserving, scaling to workloads of tens of thousands of queries and complex\ngraphs. These gains are achieved without compromising output quality. By\nunifying query optimization with LLM serving, Halo enables efficient agentic\nworkflows in data analytics and decision-making applications."
                },
                "authors": [
                    {
                        "name": "Junyi Shen"
                    },
                    {
                        "name": "Noppanat Wadlom"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02004v1",
                "updated": "2025-09-02T06:40:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "published": "2025-09-02T06:40:45Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    6,
                    40,
                    45,
                    1,
                    245,
                    0
                ],
                "title": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Shuffle Differential Privacy Protocols for Large-Domain\n  Categorical and Key-Value Data"
                },
                "summary": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy\nby introducing a shuffler who randomly shuffles data in a distributed system.\nHowever, most shuffle DP protocols are vulnerable to two attacks: collusion\nattacks by the data collector and users and data poisoning attacks. A recent\nstudy addresses this issue by introducing an augmented shuffle DP protocol,\nwhere users do not add noise and the shuffler performs random sampling and\ndummy data addition. However, it focuses on frequency estimation over\ncategorical data with a small domain and cannot be applied to a large domain\ndue to prohibitively high communication and computational costs.\n  In this paper, we fill this gap by introducing a novel augmented shuffle DP\nprotocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME\nprotocol uses a hash function to filter out unpopular items and then accurately\ncalculates frequencies for popular items. To perform this within one round of\ninteraction between users and the shuffler, our protocol carefully communicates\nwithin a system using multiple encryption. We also apply our FME protocol to\nmore advanced KV (Key-Value) statistics estimation with an additional technique\nto reduce bias. For both categorical and KV data, we prove that our protocol\nprovides computational DP, high robustness to the above two attacks, accuracy,\nand efficiency. We show the effectiveness of our proposals through comparisons\nwith twelve existing protocols."
                },
                "authors": [
                    {
                        "name": "Takao Murakami"
                    },
                    {
                        "name": "Yuichi Sei"
                    },
                    {
                        "name": "Reo Eriguchi"
                    }
                ],
                "author_detail": {
                    "name": "Reo Eriguchi"
                },
                "author": "Reo Eriguchi",
                "arxiv_comment": "Full version of the paper accepted at NDSS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01395v1",
                "updated": "2025-09-01T11:41:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T11:41:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    11,
                    41,
                    10,
                    0,
                    244,
                    0
                ],
                "title": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs cannot spot math errors, even when allowed to peek into the\n  solution"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."
                },
                "authors": [
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15779v2",
                "updated": "2025-09-01T07:26:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    7,
                    26,
                    57,
                    0,
                    244,
                    0
                ],
                "published": "2025-02-17T08:12:34Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    12,
                    34,
                    0,
                    48,
                    0
                ],
                "title": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating\n  Rotation and Learnable Non-uniform Quantizer"
                },
                "summary": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Rotate, Clip, and Partition (RCP), a quantization-aware training\n(QAT) approach that first realizes extreme compression of LLMs with\nW2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP\nintegrates recent rotation techniques with a novel non-uniform weight quantizer\ndesign, by quantitatively analyzing the impact of random rotation on 2-bit\nweight quantization. Our weight quantizer features Learnable Direct\nPartitioning (LDP), which introduces learnable parameters to directly learn\nnon-uniform intervals jointly with LLM weights. We also present a specialized\nGPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP\ncan compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and\n5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging\nmobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and\nMetaMath-7B with no critical problems such as convergence failure and\nrepetition. Code is available at https://github.com/ songsm921/RCP."
                },
                "authors": [
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sumin Song"
                    },
                    {
                        "name": "Woosang Lim"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v3",
                "updated": "2025-09-01T03:51:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    51,
                    9,
                    0,
                    244,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v1",
                "updated": "2025-09-01T03:31:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.06133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.06133v2",
                "updated": "2025-08-31T15:09:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    9,
                    36,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-08T08:54:21Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    8,
                    54,
                    21,
                    4,
                    220,
                    0
                ],
                "title": "LLM Serving Optimization with Variable Prefill and Decode Lengths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Serving Optimization with Variable Prefill and Decode Lengths"
                },
                "summary": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of serving LLM (Large Language Model) requests where\neach request has heterogeneous prefill and decode lengths. In LLM serving, the\nprefill length corresponds to the input prompt length, which determines the\ninitial memory usage in the KV cache. The decode length refers to the number of\noutput tokens generated sequentially, with each additional token increasing the\nKV cache memory usage by one unit. Given a set of n requests, our goal is to\nschedule and process them to minimize the total completion time. We show that\nthis problem is NP-hard due to the interplay of batching, placement\nconstraints, precedence relationships, and linearly increasing memory usage. We\nthen analyze commonly used scheduling strategies in practice, such as\nFirst-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their\ncompetitive ratios scale up sublinearly with the memory limit-a significant\ndrawback in real-world settings where memory demand is large. To address this,\nwe propose a novel algorithm based on a new selection metric that efficiently\nforms batches over time. We prove that this algorithm achieves a constant\ncompetitive ratio. Finally, we develop and evaluate a few algorithm variants\ninspired by this approach, including dynamic programming variants, local search\nmethods, and an LP-based scheduler, demonstrating through comprehensive\nsimulations that they outperform standard baselines while maintaining\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Meixuan Wang"
                    },
                    {
                        "name": "Yinyu Ye"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.06133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.06133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00883v1",
                "updated": "2025-08-31T14:51:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-31T14:51:19Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    14,
                    51,
                    19,
                    6,
                    243,
                    0
                ],
                "title": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Latency-Critical Applications with AI-Powered\n  Semi-Automatic Fine-Grained Parallelization on SMT Processors"
                },
                "summary": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency-critical applications tend to show low utilization of functional\nunits due to frequent cache misses and mispredictions during speculative\nexecution in high-performance superscalar processors. However, due to\nsignificant impact on single-thread performance, Simultaneous Multithreading\n(SMT) technology is rarely used with heavy threads of latency-critical\napplications. In this paper, we explore utilization of SMT technology to\nsupport fine-grained parallelization of latency-critical applications.\nFollowing the advancements in the development of Large Language Models (LLMs),\nwe introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we\nextend AI Coding Agent in Cursor IDE with additional tools connected through\nModel Context Protocol, enabling end-to-end AI Agent for parallelization.\nAdditional connected tools enable LLM-guided hotspot detection, collection of\ndynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance\nsimulation to estimate performance gains. We apply Aira with Relic parallel\nframework for fine-grained task parallelism on SMT cores to parallelize\nlatency-critical benchmarks representing real-world applications used in\nindustry. We show 17% geomean performance gain from parallelization of\nlatency-critical benchmarks using Aira with Relic framework."
                },
                "authors": [
                    {
                        "name": "Denis Los"
                    },
                    {
                        "name": "Igor Petushkov"
                    }
                ],
                "author_detail": {
                    "name": "Igor Petushkov"
                },
                "author": "Igor Petushkov",
                "arxiv_journal_ref": "International Journal of Open Information Technologies, vol. 13,\n  no. 9, pp. 129-134, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00625v1",
                "updated": "2025-08-30T22:47:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T22:47:15Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    22,
                    47,
                    15,
                    5,
                    242,
                    0
                ],
                "title": "NetGent: Agent-Based Automation of Network Application Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetGent: Agent-Based Automation of Network Application Workflows"
                },
                "summary": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present NetGent, an AI-agent framework for automating complex application\nworkflows to generate realistic network traffic datasets. Developing\ngeneralizable ML models for networking requires data collection from network\nenvironments with traffic that results from a diverse set of real-world web\napplications. However, using existing browser automation tools that are\ndiverse, repeatable, realistic, and efficient remains fragile and costly.\nNetGent addresses this challenge by allowing users to specify workflows as\nnatural-language rules that define state-dependent actions. These abstract\nspecifications are compiled into nondeterministic finite automata (NFAs), which\na state synthesis component translates into reusable, executable code. This\ndesign enables deterministic replay, reduces redundant LLM calls through state\ncaching, and adapts quickly when application interfaces change. In experiments,\nNetGent automated more than 50+ workflows spanning video-on-demand streaming,\nlive video streaming, video conferencing, social media, and web scraping,\nproducing realistic traffic traces while remaining robust to UI variability. By\ncombining the flexibility of language-based agents with the reliability of\ncompiled execution, NetGent provides a scalable foundation for generating the\ndiverse, repeatable datasets needed to advance ML in networking."
                },
                "authors": [
                    {
                        "name": "Jaber Daneshamooz"
                    },
                    {
                        "name": "Eugene Vuong"
                    },
                    {
                        "name": "Laasya Koduru"
                    },
                    {
                        "name": "Sanjay Chandrasekaran"
                    },
                    {
                        "name": "Arpit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Gupta"
                },
                "author": "Arpit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00579v1",
                "updated": "2025-08-30T18:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T18:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    18,
                    25,
                    19,
                    5,
                    242,
                    0
                ],
                "title": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for\n  KV Cache"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\npotential in various practical applications. However, long context inference\nposes a significant challenge due to the enormous memory requirements of the\nkey-value (KV) cache, which can scale to multiple gigabytes as sequence length\nand batch size increase. In this paper, we present KVComp, a generic and\nefficient KV cache management framework optimized for long-text generation that\nsynergistically works with both latency-critical and throughput-critical\ninference systems. KVComp employs novel lossy compression techniques\nspecifically designed for KV cache data characteristics, featuring careful\nco-design of compression algorithms and system architecture. Our approach\nmaintains compatibility with the growing nature of KV cache while preserving\nhigh computational efficiency. Experimental results show that KVComp achieves\non average 47\\% and up to 83\\% higher memory reduction rate compared to\nexisting methods with little/no model accuracy degradation. Furthermore, KVComp\nachieves extremely high execution throughput, effectively reducing\ndecompression overhead and, in some cases, even accelerating the matrix-vector\nmultiplication operation and outperform cuBLAS-based attention kernels with\nless data movement."
                },
                "authors": [
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Taolue Yang"
                    },
                    {
                        "name": "Youyuan Liu"
                    },
                    {
                        "name": "Chengming Zhang"
                    },
                    {
                        "name": "Xubin He"
                    },
                    {
                        "name": "Sian Jin"
                    }
                ],
                "author_detail": {
                    "name": "Sian Jin"
                },
                "author": "Sian Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.13777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.13777v2",
                "updated": "2025-08-30T14:49:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    14,
                    49,
                    34,
                    5,
                    242,
                    0
                ],
                "published": "2023-10-20T19:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    20,
                    19,
                    22,
                    58,
                    4,
                    293,
                    0
                ],
                "title": "Discrete and Continuous Caching Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete and Continuous Caching Games"
                },
                "summary": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate a discrete search game called the Multiple Caching Game where\nthe searcher's aim is to find all of a set of $d$ treasures hidden in $n$\nlocations. Allowed queries are sets of locations of size $k$, and the searcher\nwins if in all $d$ queries, at least one treasure is hidden in one of the $k$\npicked locations. P\\'alv\\\"olgyi showed that the value of the game is at most\n$\\frac{k^d}{\\binom{n+d-1}{d}}$, with equality for large enough $n$. We\nconjecture the exact cases of equality. We also investigate variants of the\ngame and show an example where their values are different, answering a question\nof P\\'alv\\\"olgyi.\n  This game is closely related to a continuous variant, Alpern's Caching Game,\nbased on which we define other continous variants of the multiple caching game\nand examine their values."
                },
                "authors": [
                    {
                        "name": "ron Jnosik"
                    },
                    {
                        "name": "Csenge Mikls"
                    },
                    {
                        "name": "Dniel G. Simon"
                    },
                    {
                        "name": "Kristf Zlomy"
                    }
                ],
                "author_detail": {
                    "name": "Kristf Zlomy"
                },
                "author": "Kristf Zlomy",
                "arxiv_doi": "10.1142/S0219198925500057",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1142/S0219198925500057",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.13777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.13777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "International Game Theory Review 27 (3), 2025",
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v3",
                "updated": "2025-08-30T09:35:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    9,
                    35,
                    22,
                    5,
                    242,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffKV: Differentiated Memory Management for Large Language Models with\n  Parallel KV Compaction"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities but face\nsubstantial serving costs due to their high memory demands, with the key-value\n(KV) cache being a primary bottleneck. State-of-the-art KV cache compression\ntechniques, such as quantization and pruning, apply uniform treatment to both\nkeys and values, and discard unimportant tokens entirely, overlooking the\nfine-grained distinctions in the significance of individual KV cache\ncomponents. To address such limitations, we introduce \\textit{DiffKV}, a novel\nframework for efficient KV cache compression that exploits three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. These levels of\ndifferentiation introduce irregular memory usage patterns across different\nrequests and attention heads, posing significant scalability challenges for\nmemory management. To address these challenges, DiffKV proposes an on-GPU\nmemory manager that compacts fragmented free memory list into contiguous\nregions in parallel, effectively translating sparsity in the KV cache into\nperformance gains. We evaluate DiffKV on several mainstream LLMs, including the\nemerging thinking models that generate extended chains of thought. DiffKV is\nable to compress the KV cache by $2.7\\times$ to $5.7\\times$ with near-lossless\naccuracy on complex workloads requiring sophisticated reasoning and\nlong-generation capabilities, and enhances throughput by $1.9\\times$ to\n$5.4\\times$. Source codes of DiffKV are available at\nhttps://github.com/zyqCSL/DiffKV."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "SOSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00419v1",
                "updated": "2025-08-30T08:57:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T08:57:53Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    57,
                    53,
                    5,
                    242,
                    0
                ],
                "title": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging\n  and KV Cache Compression"
                },
                "summary": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce LightVLM, a simple but effective method that can\nbe seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly\naccelerate the inference process in a training-free manner. We divide the\ninference procedure of VLMs into two stages, i.e., encoding and decoding, and\npropose to simultaneously accelerate VLMs in both stages to largely improve\nmodel efficiency. During encoding, we propose pyramid token merging to reduce\ntokens of different LLM layers in a hierarchical manner by finally only keeping\na few dominant tokens to achieve high efficiency. During decoding, aimed at\nreducing the high latency of outputting long sequences, we propose KV Cache\ncompression to remove unnecessary caches to increase the network throughput.\nExperimental results show that LightVLM successfully retains 100% performance\nwhen only preserving 35% image tokens, and maintains around 98% performance\nwhen keeping only 3% image tokens. LightVLM could 2.02$\\times$ the network\nthroughput and reduce the prefilling time by 3.65$\\times$. LightVLM also makes\nlarge VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to\ninfer faster than significantly smaller models (e.g., InternVL2.5 8B),\nhopefully facilitating the real-world deployment. When generating long text\nsequences (e.g., 4096 tokens), LightVLM could reduce the inference time by\n3.21$\\times$, largely outperforming existing methods."
                },
                "authors": [
                    {
                        "name": "Lianyu Hu"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Wei Feng"
                    },
                    {
                        "name": "Liang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wan"
                },
                "author": "Liang Wan",
                "arxiv_comment": "EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00388v1",
                "updated": "2025-08-30T06:56:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "published": "2025-08-30T06:56:28Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    6,
                    56,
                    28,
                    5,
                    242,
                    0
                ],
                "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV\n  Cache Eviction"
                },
                "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."
                },
                "authors": [
                    {
                        "name": "Xuelin Li"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11435v2",
                "updated": "2025-08-29T20:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    20,
                    39,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-15T17:51:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    51,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Containment Queries over Collections of Trimmed NURBS Surfaces\n  via Generalized Winding Numbers"
                },
                "summary": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a containment query that is robust to the watertightness of\nregions bound by trimmed NURBS surfaces, as this property is difficult to\nguarantee for in-the-wild CAD models. Containment is determined through the\ngeneralized winding number (GWN), a mathematical construction that is\nindifferent to the arrangement of surfaces in the shape. Applying contemporary\ntechniques for the 3D GWN to trimmed NURBS surfaces requires some form of\ngeometric discretization, introducing computational inefficiency to the\nalgorithm and even risking containment misclassifications near the surface. In\ncontrast, our proposed method uses a novel reformulation of the relevant\nsurface integral based on Stokes' theorem, which operates on the boundary and\ntrimming curves as provided through rapidly converging adaptive quadrature.\nBatches of queries are further accelerated by memoizing (i.e.\\ caching and\nreusing) quadrature node positions and tangents as they are evaluated. We\ndemonstrate that our GWN method is robust to complex trimming geometry in a CAD\nmodel, and is accurate up to arbitrary precision at arbitrary distances from\nthe surface. The derived containment query is therefore robust to model\nnon-watertightness while respecting all curved features of the input shape."
                },
                "authors": [
                    {
                        "name": "Jacob Spainhour"
                    },
                    {
                        "name": "Kenneth Weiss"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth Weiss"
                },
                "author": "Kenneth Weiss",
                "arxiv_comment": "18 Pages, 16 Figures, 1 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.3.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00202v1",
                "updated": "2025-08-29T19:23:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:23:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    23,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer\n  Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive\n  Inference"
                },
                "summary": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although the Transformer has become the cornerstone of modern AI, its\nautoregressive inference suffers from a linearly growing KV Cache and a\ncomputational complexity of O(N^2 d), severely hindering its ability to process\nultra-long sequences. To overcome this limitation, this paper introduces the\nTConstFormer architecture, building upon our previous work, TLinFormer.\nTConstFormer employs an innovative periodic state update mechanism to achieve a\ntruly constant-size O(1) KV Cache. The computational complexity of this\nmechanism is also O(1) in an amortized sense: it performs purely constant-time\ncomputations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single\nlinear-time global information synchronization only on the $k$-th step.\nTheoretical calculations and experimental results demonstrate that TConstFormer\nexhibits an overwhelming advantage over baseline models in terms of speed,\nmemory efficiency, and overall performance on long-text inference tasks. This\nbreakthrough paves the way for efficient and robust streaming language model\napplications."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00195v1",
                "updated": "2025-08-29T19:12:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T19:12:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    19,
                    12,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Agentic AI with Fast Test-Time Scaling on the Edge"
                },
                "summary": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying agentic AI on edge devices is crucial for privacy and\nresponsiveness, but memory constraints typically relegate these systems to\nsmaller Large Language Models (LLMs) with inferior reasoning capabilities.\nTest-Time Scaling (TTS) can bridge this reasoning gap by dedicating more\ncompute during inference, but existing methods incur prohibitive overhead on\nedge hardware. To overcome this, we introduce FlashTTS, a serving system that\nmakes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces\nthree synergistic optimizations: (i) Speculative Beam Extension to mitigate\nsystem stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model\nMemory Allocation to dynamically balance memory between generation and\nverification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache\nreuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on\na single consumer GPU (24 GB) to match the accuracy and latency of large cloud\nmodels. Our evaluation demonstrates that FlashTTS achieves an average 2.2x\nhigher goodput and reduces latency by 38%-68% compared to a vLLM baseline,\npaving the way for democratized, high-performance agentic AI on edge devices."
                },
                "authors": [
                    {
                        "name": "Hao Mark Chen"
                    },
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Guanxi Lu"
                    },
                    {
                        "name": "Shuang Liang"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "author": "Hongxiang Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v2",
                "updated": "2025-08-29T18:45:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    18,
                    45,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v1",
                "updated": "2025-08-29T02:29:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode\n  Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando Garca-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00105v1",
                "updated": "2025-08-28T00:46:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T00:46:51Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    0,
                    46,
                    51,
                    3,
                    240,
                    0
                ],
                "title": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and\n  High-Quality Language Model Serving"
                },
                "summary": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications often reuse previously processed\ncontext, such as chat history and documents, which introduces significant\nredundant computation. Existing LLM serving systems address such redundant\ncomputation by storing the KV caches of processed context and loading the\ncorresponding KV cache when a new request reuses the context. Further, as these\nLLM applications scale, the total size of KV caches becomes excessively large\nand requires both DRAM and SSD for full storage.\n  However, prior work that stores KV caches in DRAM and SSD suffers from high\nloading delays, as most KV cache hits come from SSD, which is slow to load. To\nincrease the KV cache hit rate on DRAM, we identify lossy KV cache compression\nas a promising approach. We design a lossy compression system that decides the\ncompression algorithm, compression rate and device placement for each KV cache\nentry to maximise DRAM hits and minimise loading delay without significantly\ndegrading generation quality. Compared to various static compression baselines\nacross three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at\nthe same quality and 6--55% quality improvements at the same delay."
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00100v1",
                "updated": "2025-08-27T17:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T17:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    17,
                    45,
                    16,
                    2,
                    239,
                    0
                ],
                "title": "MODE: Mixture of Document Experts for RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MODE: Mixture of Document Experts for RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) often relies on large vector databases\nand cross-encoders tuned for large-scale corpora, which can be excessive for\nsmall, domain-specific collections. We present MODE (Mixture of Document\nExperts), a lightweight alternative that replaces fine-grained nearest-neighbor\nsearch with cluster-and-route retrieval. Documents are embedded, grouped into\nsemantically coherent clusters, and represented by cached centroids. At query\ntime, we route to the top centroid(s) and retrieve context only within those\nclusters, eliminating external vector-database infrastructure and reranking\nwhile keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks,\nMODE matches or exceeds a dense-retrieval baseline in answer quality while\nreducing end-to-end retrieval time. Ablations show that cluster granularity and\nmulti-cluster routing control the recall/precision trade-off, and that tighter\nclusters improve downstream accuracy. MODE offers a practical recipe for small\nand medium corpora where simplicity, speed, and topical focus matter."
                },
                "authors": [
                    {
                        "name": "Rahul Anand"
                    }
                ],
                "author_detail": {
                    "name": "Rahul Anand"
                },
                "author": "Rahul Anand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00052v1",
                "updated": "2025-08-25T02:58:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T02:58:39Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    58,
                    39,
                    0,
                    237,
                    0
                ],
                "title": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightning Fast Caching-based Parallel Denoising Prediction for\n  Accelerating Talking Head Generation"
                },
                "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."
                },
                "authors": [
                    {
                        "name": "Jianzhi Long"
                    },
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rongcheng Tu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.08827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08827v1",
                "updated": "2025-09-10T17:59:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Large Reasoning Models"
                },
                "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuru Wang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "Jiaze Ma"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Zonglin Li"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhenzhao Yuan"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08825v1",
                "updated": "2025-09-10T17:58:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    53,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:58:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation"
                },
                "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant."
                },
                "authors": [
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Aleksandra Urman"
                    },
                    {
                        "name": "Albert Wendsj"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Johannes B. Gruber"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08824v1",
                "updated": "2025-09-10T17:58:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:58:23Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    23,
                    2,
                    253,
                    0
                ],
                "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl\n  Snapshots to Industrial-Grade Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl\n  Snapshots to Industrial-Grade Corpora"
                },
                "summary": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development."
                },
                "authors": [
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    },
                    {
                        "name": "Helio Pedrini"
                    }
                ],
                "author_detail": {
                    "name": "Helio Pedrini"
                },
                "author": "Helio Pedrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08822v1",
                "updated": "2025-09-10T17:53:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    53,
                    39,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:53:39Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    53,
                    39,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of TinyML Applications in Beekeeping for Hive Monitoring and\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of TinyML Applications in Beekeeping for Hive Monitoring and\n  Management"
                },
                "summary": "Honey bee colonies are essential for global food security and ecosystem\nstability, yet they face escalating threats from pests, diseases, and\nenvironmental stressors. Traditional hive inspections are labor-intensive and\ndisruptive, while cloud-based monitoring solutions remain impractical for\nremote or resource-limited apiaries. Recent advances in Internet of Things\n(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring\ndirectly on edge devices, offering scalable and non-invasive alternatives. This\nsurvey synthesizes current innovations at the intersection of TinyML and\napiculture, organized around four key functional areas: monitoring hive\nconditions, recognizing bee behaviors, detecting pests and diseases, and\nforecasting swarming events. We further examine supporting resources, including\npublicly available datasets, lightweight model architectures optimized for\nembedded deployment, and benchmarking strategies tailored to field constraints.\nCritical limitations such as data scarcity, generalization challenges, and\ndeployment barriers in off-grid environments are highlighted, alongside\nemerging opportunities in ultra-efficient inference pipelines, adaptive edge\nlearning, and dataset standardization. By consolidating research and\nengineering practices, this work provides a foundation for scalable, AI-driven,\nand ecologically informed monitoring systems to support sustainable pollinator\nmanagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honey bee colonies are essential for global food security and ecosystem\nstability, yet they face escalating threats from pests, diseases, and\nenvironmental stressors. Traditional hive inspections are labor-intensive and\ndisruptive, while cloud-based monitoring solutions remain impractical for\nremote or resource-limited apiaries. Recent advances in Internet of Things\n(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring\ndirectly on edge devices, offering scalable and non-invasive alternatives. This\nsurvey synthesizes current innovations at the intersection of TinyML and\napiculture, organized around four key functional areas: monitoring hive\nconditions, recognizing bee behaviors, detecting pests and diseases, and\nforecasting swarming events. We further examine supporting resources, including\npublicly available datasets, lightweight model architectures optimized for\nembedded deployment, and benchmarking strategies tailored to field constraints.\nCritical limitations such as data scarcity, generalization challenges, and\ndeployment barriers in off-grid environments are highlighted, alongside\nemerging opportunities in ultra-efficient inference pipelines, adaptive edge\nlearning, and dataset standardization. By consolidating research and\nengineering practices, this work provides a foundation for scalable, AI-driven,\nand ecologically informed monitoring systems to support sustainable pollinator\nmanagement."
                },
                "authors": [
                    {
                        "name": "Willy Sucipto"
                    },
                    {
                        "name": "Jianlong Zhou"
                    },
                    {
                        "name": "Ray Seung Min Kwon"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "arxiv_comment": "30 pages, 8 figures, 3 tables. Survey of TinyML and IoT applications\n  in beekeeping (datasets, benchmarking, deployment). Submitted to ACM\n  Computing Surveys (under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.9; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15474v2",
                "updated": "2025-09-10T17:51:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    51,
                    3,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-21T11:50:56Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    50,
                    56,
                    3,
                    233,
                    0
                ],
                "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective Behaviors and Preferences in LLM: Language of Browsing"
                },
                "summary": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment."
                },
                "authors": [
                    {
                        "name": "Sai Sundaresan"
                    },
                    {
                        "name": "Harshita Chopra"
                    },
                    {
                        "name": "Atanu R. Sinha"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Nagasai Saketh Naidu"
                    },
                    {
                        "name": "Raghav Karan"
                    },
                    {
                        "name": "N Anushka"
                    }
                ],
                "author_detail": {
                    "name": "N Anushka"
                },
                "author": "N Anushka",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08811v1",
                "updated": "2025-09-10T17:44:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    44,
                    4,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:44:04Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    44,
                    4,
                    2,
                    253,
                    0
                ],
                "title": "Teamwork as Linear Interpersonal Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teamwork as Linear Interpersonal Dynamics"
                },
                "summary": "Successful teamwork depends on interpersonal dynamics, the ways in which\nindividuals coordinate, influence, and adapt to one another over time. Existing\nmeasures of interpersonal dynamics, such as CRQA, correlation, Granger\ncausality, and transfer entropy, typically capture only a single dimension:\neither the synchrony/coordination or the direction of influence between\nindividuals. What is missing is a psychologically meaningful representation\nthat unifies these dimensions and varies systematically with behavior. We\npropose the context matrix as one such representation. The context matrix is\nthe transition matrix in a linear dynamical system, with entries specifying how\nmuch each individual's current behavior is attributable to their own versus\nevery other group member's past behaviors. Its values can be distilled into\npsychologically interpretable summary features of synchrony and directional\ninfluence. Evidence for the context matrix as psychologically meaningful is\nprovided in two steps. First, we develop a sequential Bayesian model that\ninfers context matrices from timeseries data and show that it accurately\nrecovers them in noisy simulations. Second, applying the model to human\neyetracking data, we show that summary features of the inferred context\nmatrices capture expected task-based differences in interpersonal dynamics (or\nlack thereof), predict task accuracy in psychologically reasonable ways, and\nshow some correspondence with existing measures (CRQA and Granger causality).\nWe conclude by situating the context matrix within a broader agenda for\nmodeling interpersonal dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Successful teamwork depends on interpersonal dynamics, the ways in which\nindividuals coordinate, influence, and adapt to one another over time. Existing\nmeasures of interpersonal dynamics, such as CRQA, correlation, Granger\ncausality, and transfer entropy, typically capture only a single dimension:\neither the synchrony/coordination or the direction of influence between\nindividuals. What is missing is a psychologically meaningful representation\nthat unifies these dimensions and varies systematically with behavior. We\npropose the context matrix as one such representation. The context matrix is\nthe transition matrix in a linear dynamical system, with entries specifying how\nmuch each individual's current behavior is attributable to their own versus\nevery other group member's past behaviors. Its values can be distilled into\npsychologically interpretable summary features of synchrony and directional\ninfluence. Evidence for the context matrix as psychologically meaningful is\nprovided in two steps. First, we develop a sequential Bayesian model that\ninfers context matrices from timeseries data and show that it accurately\nrecovers them in noisy simulations. Second, applying the model to human\neyetracking data, we show that summary features of the inferred context\nmatrices capture expected task-based differences in interpersonal dynamics (or\nlack thereof), predict task accuracy in psychologically reasonable ways, and\nshow some correspondence with existing measures (CRQA and Granger causality).\nWe conclude by situating the context matrix within a broader agenda for\nmodeling interpersonal dynamics."
                },
                "authors": [
                    {
                        "name": "Andrew Jun Lee"
                    },
                    {
                        "name": "Grace Qiyuan Miao"
                    },
                    {
                        "name": "Rick Dale"
                    },
                    {
                        "name": "Alexia Galati"
                    },
                    {
                        "name": "Hongjing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hongjing Lu"
                },
                "author": "Hongjing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08809v1",
                "updated": "2025-09-10T17:42:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    42,
                    41,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:42:41Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    42,
                    41,
                    2,
                    253,
                    0
                ],
                "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals"
                },
                "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings."
                },
                "authors": [
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Ivor Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Ivor Tsang"
                },
                "author": "Ivor Tsang",
                "arxiv_comment": "11 pages, 10 figures",
                "arxiv_journal_ref": "Published ICLR 2025 Workshop on Scaling Self-Improving Foundation\n  Models without Human Supervision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08808v1",
                "updated": "2025-09-10T17:41:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    41,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:41:08Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    41,
                    8,
                    2,
                    253,
                    0
                ],
                "title": "Handling Open-Vocabulary Constructs in Formalizing Specifications:\n  Retrieval-Augmented Parsing with Expert Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling Open-Vocabulary Constructs in Formalizing Specifications:\n  Retrieval-Augmented Parsing with Expert Knowledge"
                },
                "summary": "We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known\nbeforehand -- in the context of converting natural language (NL) specifications\ninto formal languages (e.g., temporal logic or code). Models fare poorly on\nOVCs due to a lack of necessary knowledge a priori. In such situations, a\ndomain expert can provide correct constructs at inference time based on their\npreferences or domain knowledge. Our goal is to effectively reuse this\ninference-time, expert-provided knowledge for future parses without retraining\nthe model. We present dynamic knowledge-augmented parsing(DKAP), where in\naddition to the input sentence, the model receives (dynamically growing) expert\nknowledge as a key-value lexicon that associates NL phrases with correct OVC\nconstructs. We propose ROLex, a retrieval-augmented parsing approach that uses\nthis lexicon. A retriever and a generator are trained to find and use the\nkey-value store to produce the correct parse. A key challenge lies in curating\ndata for this retrieval-augmented parser. We utilize synthetic data generation\nand the data augmentation techniques on annotated (NL sentence, FL statement)\npairs to train the augmented parser. To improve training effectiveness, we\npropose multiple strategies to teach models to focus on the relevant subset of\nretrieved knowledge. Finally, we introduce a new evaluation paradigm modeled\nafter the DKAP problem and simulate the scenario across three formalization\ntasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a\ndifficult challenge, and ROLex helps improve the performance of baseline models\nby using dynamic expert knowledge effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of Open-Vocabulary Constructs(OVCs) -- ones not known\nbeforehand -- in the context of converting natural language (NL) specifications\ninto formal languages (e.g., temporal logic or code). Models fare poorly on\nOVCs due to a lack of necessary knowledge a priori. In such situations, a\ndomain expert can provide correct constructs at inference time based on their\npreferences or domain knowledge. Our goal is to effectively reuse this\ninference-time, expert-provided knowledge for future parses without retraining\nthe model. We present dynamic knowledge-augmented parsing(DKAP), where in\naddition to the input sentence, the model receives (dynamically growing) expert\nknowledge as a key-value lexicon that associates NL phrases with correct OVC\nconstructs. We propose ROLex, a retrieval-augmented parsing approach that uses\nthis lexicon. A retriever and a generator are trained to find and use the\nkey-value store to produce the correct parse. A key challenge lies in curating\ndata for this retrieval-augmented parser. We utilize synthetic data generation\nand the data augmentation techniques on annotated (NL sentence, FL statement)\npairs to train the augmented parser. To improve training effectiveness, we\npropose multiple strategies to teach models to focus on the relevant subset of\nretrieved knowledge. Finally, we introduce a new evaluation paradigm modeled\nafter the DKAP problem and simulate the scenario across three formalization\ntasks (NL2LTL, NL2Code, and NL2CMD). Our evaluations show that DKAP is a\ndifficult challenge, and ROLex helps improve the performance of baseline models\nby using dynamic expert knowledge effectively."
                },
                "authors": [
                    {
                        "name": "Mohammad Saqib Hasan"
                    },
                    {
                        "name": "Sayontan Ghosh"
                    },
                    {
                        "name": "Dhruv Verma"
                    },
                    {
                        "name": "Geoff Kuenning"
                    },
                    {
                        "name": "Erez Zadok"
                    },
                    {
                        "name": "Scott A. Smolka"
                    },
                    {
                        "name": "Niranjan Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Balasubramanian"
                },
                "author": "Niranjan Balasubramanian",
                "arxiv_comment": "Accepted to COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08803v1",
                "updated": "2025-09-10T17:36:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    36,
                    25,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:36:25Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    36,
                    25,
                    2,
                    253,
                    0
                ],
                "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Truth: The Confidence Paradox in AI Fact-Checking"
                },
                "summary": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking."
                },
                "authors": [
                    {
                        "name": "Ihsan A. Qazi"
                    },
                    {
                        "name": "Zohaib Khan"
                    },
                    {
                        "name": "Abdullah Ghani"
                    },
                    {
                        "name": "Agha A. Raza"
                    },
                    {
                        "name": "Zafar A. Qazi"
                    },
                    {
                        "name": "Wassay Sajjad"
                    },
                    {
                        "name": "Ayesha Ali"
                    },
                    {
                        "name": "Asher Javaid"
                    },
                    {
                        "name": "Muhammad Abdullah Sohail"
                    },
                    {
                        "name": "Abdul H. Azeemi"
                    }
                ],
                "author_detail": {
                    "name": "Abdul H. Azeemi"
                },
                "author": "Abdul H. Azeemi",
                "arxiv_comment": "65 pages, 26 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00074v2",
                "updated": "2025-09-10T17:27:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    27,
                    44,
                    2,
                    253,
                    0
                ],
                "published": "2025-05-29T20:11:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    20,
                    11,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations"
                },
                "summary": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Daniele Barolo"
                    },
                    {
                        "name": "Chiara Valentin"
                    },
                    {
                        "name": "Fariba Karimi"
                    },
                    {
                        "name": "Luis Galrraga"
                    },
                    {
                        "name": "Gonzalo G. Mndez"
                    },
                    {
                        "name": "Lisette Espn-Noboa"
                    }
                ],
                "author_detail": {
                    "name": "Lisette Espn-Noboa"
                },
                "author": "Lisette Espn-Noboa",
                "arxiv_comment": "40 pages: 10 main (incl. 9 figures), 3 references, and 27 appendix.\n  Paper under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4; F.2; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08788v1",
                "updated": "2025-09-10T17:15:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    15,
                    46,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:15:46Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    15,
                    46,
                    2,
                    253,
                    0
                ],
                "title": "Doubly robust average treatment effect estimation for survival data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly robust average treatment effect estimation for survival data"
                },
                "summary": "Considering censored outcomes in survival analysis can lead to quite complex\nresults in the model setting of causal inference. Causal inference has\nattracted a lot of attention over the past few years, but little research has\nbeen done on survival analysis. Even for the only research conducted, the\nmachine learning method was considered assuming a large sample, which is not\nsuitable in that the actual data are high dimensional low sample size (HDLSS)\nmethod. Therefore, penalty is considered for numerous covariates, and the\nrelationship between these covariates and treatment variables is reflected as a\ncovariate balancing property score (CBPS). It also considers censored results.\nTo this end, we will try to solve the above-mentioned problems by using\npenalized empirical likelihood, which considers both estimating equation and\npenalty. The proposed average treatment effect (ATE) estimator possesses the\noracle property, exhibiting key characteristics such as double robustness for\nunbiasedness, sparsity in model selection, and asymptotic normality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considering censored outcomes in survival analysis can lead to quite complex\nresults in the model setting of causal inference. Causal inference has\nattracted a lot of attention over the past few years, but little research has\nbeen done on survival analysis. Even for the only research conducted, the\nmachine learning method was considered assuming a large sample, which is not\nsuitable in that the actual data are high dimensional low sample size (HDLSS)\nmethod. Therefore, penalty is considered for numerous covariates, and the\nrelationship between these covariates and treatment variables is reflected as a\ncovariate balancing property score (CBPS). It also considers censored results.\nTo this end, we will try to solve the above-mentioned problems by using\npenalized empirical likelihood, which considers both estimating equation and\npenalty. The proposed average treatment effect (ATE) estimator possesses the\noracle property, exhibiting key characteristics such as double robustness for\nunbiasedness, sparsity in model selection, and asymptotic normality."
                },
                "authors": [
                    {
                        "name": "Byeonghee Lee"
                    },
                    {
                        "name": "Joonsung Kang"
                    }
                ],
                "author_detail": {
                    "name": "Joonsung Kang"
                },
                "author": "Joonsung Kang",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19958v2",
                "updated": "2025-09-10T17:13:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    13,
                    59,
                    2,
                    253,
                    0
                ],
                "published": "2025-06-24T19:11:42Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    11,
                    42,
                    1,
                    175,
                    0
                ],
                "title": "Introducing RobustiPy: An efficient next generation multiversal library\n  with model selection, averaging, resampling, and explainable artificial\n  intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing RobustiPy: An efficient next generation multiversal library\n  with model selection, averaging, resampling, and explainable artificial\n  intelligence"
                },
                "summary": "Scientific inference is often undermined by the vast but rarely explored\n\"multiverse\" of defensible modelling choices, which can generate results as\nvariable as the phenomena under study. We introduce RobustiPy, an open-source\nPython library that systematizes multiverse analysis and model-uncertainty\nquantification at scale. RobustiPy unifies bootstrap-based inference,\ncombinatorial specification search, model selection and averaging,\njoint-inference routines, and explainable AI methods within a modular,\nreproducible framework. Beyond exhaustive specification curves, it supports\nrigorous out-of-sample validation and quantifies the marginal contribution of\neach covariate. We demonstrate its utility across five simulation designs and\nten empirical case studies spanning economics, sociology, psychology, and\nmedicine, including a re-analysis of widely cited findings with documented\ndiscrepancies. Benchmarking on ~672 million simulated regressions shows that\nRobustiPy delivers state-of-the-art computational efficiency while expanding\ntransparency in empirical research. By standardizing and accelerating\nrobustness analysis, RobustiPy transforms how researchers interrogate\nsensitivity across the analytical multiverse, offering a practical foundation\nfor more reproducible and interpretable computational science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific inference is often undermined by the vast but rarely explored\n\"multiverse\" of defensible modelling choices, which can generate results as\nvariable as the phenomena under study. We introduce RobustiPy, an open-source\nPython library that systematizes multiverse analysis and model-uncertainty\nquantification at scale. RobustiPy unifies bootstrap-based inference,\ncombinatorial specification search, model selection and averaging,\njoint-inference routines, and explainable AI methods within a modular,\nreproducible framework. Beyond exhaustive specification curves, it supports\nrigorous out-of-sample validation and quantifies the marginal contribution of\neach covariate. We demonstrate its utility across five simulation designs and\nten empirical case studies spanning economics, sociology, psychology, and\nmedicine, including a re-analysis of widely cited findings with documented\ndiscrepancies. Benchmarking on ~672 million simulated regressions shows that\nRobustiPy delivers state-of-the-art computational efficiency while expanding\ntransparency in empirical research. By standardizing and accelerating\nrobustness analysis, RobustiPy transforms how researchers interrogate\nsensitivity across the analytical multiverse, offering a practical foundation\nfor more reproducible and interpretable computational science."
                },
                "authors": [
                    {
                        "name": "Daniel Valdenegro Ibarra"
                    },
                    {
                        "name": "Jiani Yan"
                    },
                    {
                        "name": "Duiyi Dai"
                    },
                    {
                        "name": "Charles Rahal"
                    }
                ],
                "author_detail": {
                    "name": "Charles Rahal"
                },
                "author": "Charles Rahal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09151v3",
                "updated": "2025-09-10T16:59:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    59,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-12T08:26:15Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    8,
                    26,
                    15,
                    2,
                    71,
                    0
                ],
                "title": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reangle-A-Video: 4D Video Generation as Video-to-Video Translation"
                },
                "summary": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reangle-A-Video, a unified framework for generating synchronized\nmulti-view videos from a single input video. Unlike mainstream approaches that\ntrain multi-view video diffusion models on large-scale 4D datasets, our method\nreframes the multi-view video generation task as video-to-videos translation,\nleveraging publicly available image and video diffusion priors. In essence,\nReangle-A-Video operates in two stages. (1) Multi-View Motion Learning: An\nimage-to-video diffusion transformer is synchronously fine-tuned in a\nself-supervised manner to distill view-invariant motion from a set of warped\nvideos. (2) Multi-View Consistent Image-to-Images Translation: The first frame\nof the input video is warped and inpainted into various camera perspectives\nunder an inference-time cross-view consistency guidance using DUSt3R,\ngenerating multi-view consistent starting images. Extensive experiments on\nstatic view transport and dynamic camera control show that Reangle-A-Video\nsurpasses existing methods, establishing a new solution for multi-view video\ngeneration. We will publicly release our code and data. Project page:\nhttps://hyeonho99.github.io/reangle-a-video/"
                },
                "authors": [
                    {
                        "name": "Hyeonho Jeong"
                    },
                    {
                        "name": "Suhyeon Lee"
                    },
                    {
                        "name": "Jong Chul Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jong Chul Ye"
                },
                "author": "Jong Chul Ye",
                "arxiv_comment": "ICCV 2025, Project page: https://hyeonho99.github.io/reangle-a-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08757v1",
                "updated": "2025-09-10T16:47:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    47,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:47:00Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    47,
                    0,
                    2,
                    253,
                    0
                ],
                "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot\n  Navigation"
                },
                "summary": "Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub ."
                },
                "authors": [
                    {
                        "name": "Michael J. Munje"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shuijing Liu"
                    },
                    {
                        "name": "Zichao Hu"
                    },
                    {
                        "name": "Yifeng Zhu"
                    },
                    {
                        "name": "Jiaxun Cui"
                    },
                    {
                        "name": "Garrett Warnell"
                    },
                    {
                        "name": "Joydeep Biswas"
                    },
                    {
                        "name": "Peter Stone"
                    }
                ],
                "author_detail": {
                    "name": "Peter Stone"
                },
                "author": "Peter Stone",
                "arxiv_comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://larg.github.io/socialnav-sub",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08755v1",
                "updated": "2025-09-10T16:46:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    46,
                    11,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:46:11Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    46,
                    11,
                    2,
                    253,
                    0
                ],
                "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning"
                },
                "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents."
                },
                "authors": [
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Jixuan Huang"
                    },
                    {
                        "name": "Chenyang Liao"
                    },
                    {
                        "name": "Baodai Huang"
                    },
                    {
                        "name": "Honglin Guo"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Wenxiang Chen"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiecao Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "preprint, 39 pages, 16 figures. Project:\n  https://AgentGym-RL.github.io/. Framework and Code:\n  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02682v2",
                "updated": "2025-09-10T16:45:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    45,
                    42,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-04T14:54:45Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    54,
                    45,
                    1,
                    63,
                    0
                ],
                "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: Boosting LLM Agents with Meta Plan Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, , which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, , which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Bingchan Zhao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08261v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08261v3",
                "updated": "2025-09-10T16:43:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    43,
                    45,
                    2,
                    253,
                    0
                ],
                "published": "2025-01-14T17:15:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    17,
                    15,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "GPU-accelerated LISA parameter estimation with full time domain response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated LISA parameter estimation with full time domain response"
                },
                "summary": "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct the first full Bayesian inference analysis for LISA parameter\nestimation incorporating the effects of subdominant harmonics and\nspin-precession through a full time domain response. The substantial\ncomputational demands of using time domain waveforms for LISA are significantly\nmitigated by implementing a novel Python version of the IMRPhenomT family of\nwaveform models and the LISA response with GPU acceleration. This time domain\nresponse alleviates the theoretical necessity of developing specific transfer\nfunctions to approximate the LISA response in the Fourier domain for each\nspecific type of system and allows for the use of unequal arms configurations\nand realistic LISA orbits. Our analysis includes a series of zero-noise\ninjections for a Massive Black Hole Binary with aligned and precessing spins.\nWe investigate the impact of including subdominant harmonics, compare equal and\nunequal arm configurations, and analyze different Time-Delay-Interferometry\n(TDI) configurations. We utilize full and uniform priors, with a lower\nfrequency cutoff of 0.1mHz, and a signal duration of approximately two months,\nsampled every 5 seconds. The sampler is initialized based on Fisher estimates.\nOur results demonstrate LISA capability to measure the two spin magnitudes and\nthe primary spin tilt angle, alongside sky localization, with percent-level\nprecision, while component masses are determined with sub-percent accuracy."
                },
                "authors": [
                    {
                        "name": "Cecilio Garca-Quirs"
                    },
                    {
                        "name": "Shubhanshu Tiwari"
                    },
                    {
                        "name": "Stanislav Babak"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Babak"
                },
                "author": "Stanislav Babak",
                "arxiv_doi": "10.1103/79kn-53nt",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/79kn-53nt",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.08261v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08261v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 9 figures",
                "arxiv_journal_ref": "Phys. Rev. D 112, 064017 (2025)",
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08753v1",
                "updated": "2025-09-10T16:43:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    43,
                    1,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:43:01Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    43,
                    1,
                    2,
                    253,
                    0
                ],
                "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling"
                },
                "summary": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling"
                },
                "authors": [
                    {
                        "name": "Neil Zeghidour"
                    },
                    {
                        "name": "Eugene Kharitonov"
                    },
                    {
                        "name": "Manu Orsini"
                    },
                    {
                        "name": "Vclav Volhejn"
                    },
                    {
                        "name": "Gabriel de Marmiesse"
                    },
                    {
                        "name": "Edouard Grave"
                    },
                    {
                        "name": "Patrick Prez"
                    },
                    {
                        "name": "Laurent Mazar"
                    },
                    {
                        "name": "Alexandre Dfossez"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Dfossez"
                },
                "author": "Alexandre Dfossez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08752v1",
                "updated": "2025-09-10T16:42:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    42,
                    22,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:42:22Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    42,
                    22,
                    2,
                    253,
                    0
                ],
                "title": "Learning Turbulent Flows with Generative Models: Super-resolution,\n  Forecasting, and Sparse Flow Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Turbulent Flows with Generative Models: Super-resolution,\n  Forecasting, and Sparse Flow Reconstruction"
                },
                "summary": "Neural operators are promising surrogates for dynamical systems but when\ntrained with standard L2 losses they tend to oversmooth fine-scale turbulent\nstructures. Here, we show that combining operator learning with generative\nmodeling overcomes this limitation. We consider three practical turbulent-flow\nchallenges where conventional neural operators fail: spatio-temporal\nsuper-resolution, forecasting, and sparse flow reconstruction. For Schlieren\njet super-resolution, an adversarially trained neural operator (adv-NO) reduces\nthe energy-spectrum error by 15x while preserving sharp gradients at neural\noperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO\ntrained on only 160 timesteps from a single trajectory forecasts accurately for\nfive eddy-turnover times and offers 114x wall-clock speed-up at inference than\nthe baseline diffusion-based forecasters, enabling near-real-time rollouts. For\nreconstructing cylinder wake flows from highly sparse Particle Tracking\nVelocimetry-like inputs, a conditional generative model infers full 3D velocity\nand pressure fields with correct phase alignment and statistics. These advances\nenable accurate reconstruction and forecasting at low compute cost, bringing\nnear-real-time analysis and control within reach in experimental and\ncomputational fluid mechanics. See our project page:\nhttps://vivekoommen.github.io/Gen4Turb/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural operators are promising surrogates for dynamical systems but when\ntrained with standard L2 losses they tend to oversmooth fine-scale turbulent\nstructures. Here, we show that combining operator learning with generative\nmodeling overcomes this limitation. We consider three practical turbulent-flow\nchallenges where conventional neural operators fail: spatio-temporal\nsuper-resolution, forecasting, and sparse flow reconstruction. For Schlieren\njet super-resolution, an adversarially trained neural operator (adv-NO) reduces\nthe energy-spectrum error by 15x while preserving sharp gradients at neural\noperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NO\ntrained on only 160 timesteps from a single trajectory forecasts accurately for\nfive eddy-turnover times and offers 114x wall-clock speed-up at inference than\nthe baseline diffusion-based forecasters, enabling near-real-time rollouts. For\nreconstructing cylinder wake flows from highly sparse Particle Tracking\nVelocimetry-like inputs, a conditional generative model infers full 3D velocity\nand pressure fields with correct phase alignment and statistics. These advances\nenable accurate reconstruction and forecasting at low compute cost, bringing\nnear-real-time analysis and control within reach in experimental and\ncomputational fluid mechanics. See our project page:\nhttps://vivekoommen.github.io/Gen4Turb/"
                },
                "authors": [
                    {
                        "name": "Vivek Oommen"
                    },
                    {
                        "name": "Siavash Khodakarami"
                    },
                    {
                        "name": "Aniruddha Bora"
                    },
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "George Em Karniadakis"
                    }
                ],
                "author_detail": {
                    "name": "George Em Karniadakis"
                },
                "author": "George Em Karniadakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16044v3",
                "updated": "2025-09-10T16:33:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    33,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-21T20:20:31Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    20,
                    20,
                    31,
                    0,
                    202,
                    0
                ],
                "title": "Making REST APIs Agent-Ready: From OpenAPI to MCP Servers for\n  Tool-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making REST APIs Agent-Ready: From OpenAPI to MCP Servers for\n  Tool-Augmented LLMs"
                },
                "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws."
                },
                "authors": [
                    {
                        "name": "Meriem Mastouri"
                    },
                    {
                        "name": "Emna Ksontini"
                    },
                    {
                        "name": "Wael Kessentini"
                    }
                ],
                "author_detail": {
                    "name": "Wael Kessentini"
                },
                "author": "Wael Kessentini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08736v1",
                "updated": "2025-09-10T16:24:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    24,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:24:08Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    24,
                    8,
                    2,
                    253,
                    0
                ],
                "title": "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent\n  System"
                },
                "summary": "The efficiency of Bayesian optimization (BO) in chemistry is often hindered\nby sparse experimental data and complex reaction mechanisms. To overcome these\nlimitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced\nMulti-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization\nprocess is enhanced by LLMs and synergistically employs two strategies:\nknowledge-driven coarse-grained optimization and data-driven fine-grained\noptimization. First, in the knowledge-driven coarse-grained optimization stage,\nLLMs intelligently decompose the vast search space by reasoning over existing\nchemical knowledge to identify promising candidate regions. Subsequently, in\nthe data-driven fine-grained optimization stage, LLMs enhance the BO process\nwithin these candidate regions by generating pseudo-data points, thereby\nimproving data utilization efficiency and accelerating convergence. Benchmark\nevaluations** further confirm that ChemBOMAS significantly enhances\noptimization effectiveness and efficiency compared to various BO algorithms.\nImportantly, the practical utility of ChemBOMAS was validated through wet-lab\nexperiments conducted under pharmaceutical industry protocols, targeting\nconditional optimization for a previously unreported and challenging chemical\nreaction. In the wet experiment, ChemBOMAS achieved an optimal objective value\nof 96%. This was substantially higher than the 15% achieved by domain experts.\nThis real-world success, together with strong performance on benchmark\nevaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Bayesian optimization (BO) in chemistry is often hindered\nby sparse experimental data and complex reaction mechanisms. To overcome these\nlimitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced\nMulti-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization\nprocess is enhanced by LLMs and synergistically employs two strategies:\nknowledge-driven coarse-grained optimization and data-driven fine-grained\noptimization. First, in the knowledge-driven coarse-grained optimization stage,\nLLMs intelligently decompose the vast search space by reasoning over existing\nchemical knowledge to identify promising candidate regions. Subsequently, in\nthe data-driven fine-grained optimization stage, LLMs enhance the BO process\nwithin these candidate regions by generating pseudo-data points, thereby\nimproving data utilization efficiency and accelerating convergence. Benchmark\nevaluations** further confirm that ChemBOMAS significantly enhances\noptimization effectiveness and efficiency compared to various BO algorithms.\nImportantly, the practical utility of ChemBOMAS was validated through wet-lab\nexperiments conducted under pharmaceutical industry protocols, targeting\nconditional optimization for a previously unreported and challenging chemical\nreaction. In the wet experiment, ChemBOMAS achieved an optimal objective value\nof 96%. This was substantially higher than the 15% achieved by domain experts.\nThis real-world success, together with strong performance on benchmark\nevaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Dong Han"
                    },
                    {
                        "name": "Zhehong Ai"
                    },
                    {
                        "name": "Pengxiang Cai"
                    },
                    {
                        "name": "Shuzhou Sun"
                    },
                    {
                        "name": "Shanya Lu"
                    },
                    {
                        "name": "Jianpeng Chen"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Lingli Ge"
                    },
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao XU"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shufei Zhang"
                },
                "author": "Shufei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06037v2",
                "updated": "2025-09-10T16:22:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    22,
                    20,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-09T21:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    21,
                    21,
                    55,
                    6,
                    40,
                    0
                ],
                "title": "Investigating Compositional Reasoning in Time Series Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Compositional Reasoning in Time Series Foundation Models"
                },
                "summary": "Large pre-trained time series foundation models (TSFMs) have demonstrated\npromising zero-shot performance across a wide range of domains. However, a\nquestion remains: Do TSFMs succeed by memorizing patterns in training data, or\ndo they possess the ability to reason about such patterns? While reasoning is a\ntopic of great interest in the study of Large Language Models (LLMs), it is\nundefined and largely unexplored in the context of TSFMs. In this work,\ninspired by language modeling literature, we formally define compositional\nreasoning in forecasting and distinguish it from in-distribution\ngeneralization. We evaluate the reasoning and generalization capabilities of 16\npopular deep learning forecasting models on multiple synthetic and real-world\ndatasets. Additionally, through controlled studies, we systematically examine\nwhich design choices in 7 popular open-source TSFMs contribute to improved\nreasoning capabilities. Our study yields key insights into the impact of TSFM\narchitecture design on compositional reasoning and generalization. We find that\npatch-based Transformers have the best reasoning performance, closely followed\nby residualized MLP-based architectures, which are 97\\% less computationally\ncomplex in terms of FLOPs and 86\\% smaller in terms of the number of trainable\nparameters. Interestingly, in some zero-shot out-of-distribution scenarios,\nthese models can outperform moving average and exponential smoothing\nstatistical baselines trained on in-distribution data. Only a few design\nchoices, such as the tokenization method, had a significant (negative) impact\non Transformer model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained time series foundation models (TSFMs) have demonstrated\npromising zero-shot performance across a wide range of domains. However, a\nquestion remains: Do TSFMs succeed by memorizing patterns in training data, or\ndo they possess the ability to reason about such patterns? While reasoning is a\ntopic of great interest in the study of Large Language Models (LLMs), it is\nundefined and largely unexplored in the context of TSFMs. In this work,\ninspired by language modeling literature, we formally define compositional\nreasoning in forecasting and distinguish it from in-distribution\ngeneralization. We evaluate the reasoning and generalization capabilities of 16\npopular deep learning forecasting models on multiple synthetic and real-world\ndatasets. Additionally, through controlled studies, we systematically examine\nwhich design choices in 7 popular open-source TSFMs contribute to improved\nreasoning capabilities. Our study yields key insights into the impact of TSFM\narchitecture design on compositional reasoning and generalization. We find that\npatch-based Transformers have the best reasoning performance, closely followed\nby residualized MLP-based architectures, which are 97\\% less computationally\ncomplex in terms of FLOPs and 86\\% smaller in terms of the number of trainable\nparameters. Interestingly, in some zero-shot out-of-distribution scenarios,\nthese models can outperform moving average and exponential smoothing\nstatistical baselines trained on in-distribution data. Only a few design\nchoices, such as the tokenization method, had a significant (negative) impact\non Transformer model performance."
                },
                "authors": [
                    {
                        "name": "Willa Potosnak"
                    },
                    {
                        "name": "Cristian Challu"
                    },
                    {
                        "name": "Mononito Goswami"
                    },
                    {
                        "name": "Kin G. Olivares"
                    },
                    {
                        "name": "Micha Wiliski"
                    },
                    {
                        "name": "Nina ukowska"
                    },
                    {
                        "name": "Artur Dubrawski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Dubrawski"
                },
                "author": "Artur Dubrawski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08729v1",
                "updated": "2025-09-10T16:17:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    17,
                    44,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:17:44Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    17,
                    44,
                    2,
                    253,
                    0
                ],
                "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates"
                },
                "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming."
                },
                "authors": [
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Junwoo Ha"
                    },
                    {
                        "name": "Sangyoon Yu"
                    },
                    {
                        "name": "Haon Park"
                    }
                ],
                "author_detail": {
                    "name": "Haon Park"
                },
                "author": "Haon Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08727v1",
                "updated": "2025-09-10T16:17:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    17,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:17:31Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    17,
                    31,
                    2,
                    253,
                    0
                ],
                "title": "Securing Cryptographic Software via Typed Assembly Language (Extended\n  Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Cryptographic Software via Typed Assembly Language (Extended\n  Version)"
                },
                "summary": "Authors of cryptographic software are well aware that their code should not\nleak secrets through its timing behavior, and, until 2018, they believed that\nfollowing industry-standard constant-time coding guidelines was sufficient.\nHowever, the revelation of the Spectre family of speculative execution attacks\ninjected new complexities.\n  To block speculative attacks, prior work has proposed annotating the\nprogram's source code to mark secret data, with hardware using this information\nto decide when to speculate (i.e., when only public values are involved) or not\n(when secrets are in play). While these solutions are able to track secret\ninformation stored on the heap, they suffer from limitations that prevent them\nfrom correctly tracking secrets on the stack, at a cost in performance.\n  This paper introduces SecSep, a transformation framework that rewrites\nassembly programs so that they partition secret and public data on the stack.\nBy moving from the source-code level to assembly rewriting, SecSep is able to\naddress limitations of prior work. The key challenge in performing this\nassembly rewriting stems from the loss of semantic information through the\nlengthy compilation process. The key innovation of our methodology is a new\nvariant of typed assembly language (TAL), Octal, which allows us to address\nthis challenge. Assembly rewriting is driven by compile-time inference within\nOctal. We apply our technique to cryptographic programs and demonstrate that it\nenables secure speculation efficiently, incurring a low average overhead of\n$1.2\\%$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authors of cryptographic software are well aware that their code should not\nleak secrets through its timing behavior, and, until 2018, they believed that\nfollowing industry-standard constant-time coding guidelines was sufficient.\nHowever, the revelation of the Spectre family of speculative execution attacks\ninjected new complexities.\n  To block speculative attacks, prior work has proposed annotating the\nprogram's source code to mark secret data, with hardware using this information\nto decide when to speculate (i.e., when only public values are involved) or not\n(when secrets are in play). While these solutions are able to track secret\ninformation stored on the heap, they suffer from limitations that prevent them\nfrom correctly tracking secrets on the stack, at a cost in performance.\n  This paper introduces SecSep, a transformation framework that rewrites\nassembly programs so that they partition secret and public data on the stack.\nBy moving from the source-code level to assembly rewriting, SecSep is able to\naddress limitations of prior work. The key challenge in performing this\nassembly rewriting stems from the loss of semantic information through the\nlengthy compilation process. The key innovation of our methodology is a new\nvariant of typed assembly language (TAL), Octal, which allows us to address\nthis challenge. Assembly rewriting is driven by compile-time inference within\nOctal. We apply our technique to cryptographic programs and demonstrate that it\nenables secure speculation efficiently, incurring a low average overhead of\n$1.2\\%$."
                },
                "authors": [
                    {
                        "name": "Shixin Song"
                    },
                    {
                        "name": "Tingzhen Dong"
                    },
                    {
                        "name": "Kosi Nwabueze"
                    },
                    {
                        "name": "Julian Zanders"
                    },
                    {
                        "name": "Andres Erbsen"
                    },
                    {
                        "name": "Adam Chlipala"
                    },
                    {
                        "name": "Mengjia Yan"
                    }
                ],
                "author_detail": {
                    "name": "Mengjia Yan"
                },
                "author": "Mengjia Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08724v1",
                "updated": "2025-09-10T16:15:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    15,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:15:23Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    15,
                    23,
                    2,
                    253,
                    0
                ],
                "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across\n  Repositories"
                },
                "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08721v1",
                "updated": "2025-09-10T16:14:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    14,
                    20,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:14:20Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    14,
                    20,
                    2,
                    253,
                    0
                ],
                "title": "Sharing is Caring: Efficient LM Post-Training with Collective RL\n  Experience Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sharing is Caring: Efficient LM Post-Training with Collective RL\n  Experience Sharing"
                },
                "summary": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo."
                },
                "authors": [
                    {
                        "name": "Jeffrey Amico"
                    },
                    {
                        "name": "Gabriel Passamani Andrade"
                    },
                    {
                        "name": "John Donaghy"
                    },
                    {
                        "name": "Ben Fielding"
                    },
                    {
                        "name": "Tristin Forbus"
                    },
                    {
                        "name": "Harry Grieve"
                    },
                    {
                        "name": "Semih Kara"
                    },
                    {
                        "name": "Jari Kolehmainen"
                    },
                    {
                        "name": "Yihua Lou"
                    },
                    {
                        "name": "Christopher Nies"
                    },
                    {
                        "name": "Edward Phillip Flores Nuo"
                    },
                    {
                        "name": "Diogo Ortega"
                    },
                    {
                        "name": "Shikhar Rastogi"
                    },
                    {
                        "name": "Austin Virts"
                    },
                    {
                        "name": "Matthew J. Wright"
                    }
                ],
                "author_detail": {
                    "name": "Matthew J. Wright"
                },
                "author": "Matthew J. Wright",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05570v2",
                "updated": "2025-09-10T16:11:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    11,
                    13,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-06T02:54:13Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    2,
                    54,
                    13,
                    5,
                    249,
                    0
                ],
                "title": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in\n  e-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in\n  e-Commerce"
                },
                "summary": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems."
                },
                "authors": [
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Bowen Liu"
                    },
                    {
                        "name": "Xiaoshuang Zhang"
                    },
                    {
                        "name": "Aritra Mandal"
                    },
                    {
                        "name": "Canran Xu"
                    },
                    {
                        "name": "Zhe Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wu"
                },
                "author": "Zhe Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00419v2",
                "updated": "2025-09-10T16:01:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    1,
                    21,
                    2,
                    253,
                    0
                ],
                "published": "2025-05-31T06:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    48,
                    12,
                    5,
                    151,
                    0
                ],
                "title": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on\n  Distilled Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on\n  Distilled Preferences"
                },
                "summary": "LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo."
                },
                "authors": [
                    {
                        "name": "Mohammad Saqib Hasan"
                    },
                    {
                        "name": "Saikat Chakraborty"
                    },
                    {
                        "name": "Santu Karmaker"
                    },
                    {
                        "name": "Niranjan Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Balasubramanian"
                },
                "author": "Niranjan Balasubramanian",
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16838v2",
                "updated": "2025-09-10T15:49:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    49,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-24T04:49:49Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    4,
                    49,
                    49,
                    0,
                    55,
                    0
                ],
                "title": "REGen: A Reliable Evaluation Framework for Generative Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REGen: A Reliable Evaluation Framework for Generative Event Argument\n  Extraction"
                },
                "summary": "Event argument extraction identifies arguments for predefined event roles in\ntext. Existing work evaluates this task with exact match (EM), where predicted\narguments must align exactly with annotated spans. While suitable for\nspan-based models, this approach falls short for large language models (LLMs),\nwhich often generate diverse yet semantically accurate arguments. EM severely\nunderestimates performance by disregarding valid variations. Furthermore, EM\nevaluation fails to capture implicit arguments (unstated but inferable) and\nscattered arguments (distributed across a document). These limitations\nunderscore the need for an evaluation framework that better captures models'\nactual performance. To bridge this gap, we introduce REGen, a Reliable\nEvaluation framework for Generative event argument extraction. REGen combines\nthe strengths of exact, relaxed, and LLM-based matching to better align with\nhuman judgment. Experiments on six datasets show that REGen reveals an average\nperformance gain of +23.93 F1 over EM, reflecting capabilities overlooked by\nprior evaluation. Human validation further confirms REGen's effectiveness,\nachieving 87.67% alignment with human assessments of argument correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event argument extraction identifies arguments for predefined event roles in\ntext. Existing work evaluates this task with exact match (EM), where predicted\narguments must align exactly with annotated spans. While suitable for\nspan-based models, this approach falls short for large language models (LLMs),\nwhich often generate diverse yet semantically accurate arguments. EM severely\nunderestimates performance by disregarding valid variations. Furthermore, EM\nevaluation fails to capture implicit arguments (unstated but inferable) and\nscattered arguments (distributed across a document). These limitations\nunderscore the need for an evaluation framework that better captures models'\nactual performance. To bridge this gap, we introduce REGen, a Reliable\nEvaluation framework for Generative event argument extraction. REGen combines\nthe strengths of exact, relaxed, and LLM-based matching to better align with\nhuman judgment. Experiments on six datasets show that REGen reveals an average\nperformance gain of +23.93 F1 over EM, reflecting capabilities overlooked by\nprior evaluation. Human validation further confirms REGen's effectiveness,\nachieving 87.67% alignment with human assessments of argument correctness."
                },
                "authors": [
                    {
                        "name": "Omar Sharif"
                    },
                    {
                        "name": "Joseph Gatto"
                    },
                    {
                        "name": "Madhusudan Basak"
                    },
                    {
                        "name": "Sarah M. Preum"
                    }
                ],
                "author_detail": {
                    "name": "Sarah M. Preum"
                },
                "author": "Sarah M. Preum",
                "arxiv_comment": "Accepted at EMNLP-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05080v2",
                "updated": "2025-09-10T15:45:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    45,
                    41,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-05T13:19:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    19,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading"
                },
                "summary": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency."
                },
                "authors": [
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yueheng Jiang"
                    },
                    {
                        "name": "Zhaozhao Ma"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Jacky Keung"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02253v5",
                "updated": "2025-09-11T12:14:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    14,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-03T03:02:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    3,
                    2,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation"
                },
                "summary": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial."
                },
                "authors": [
                    {
                        "name": "Jungkoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jungkoo Kang"
                },
                "author": "Jungkoo Kang",
                "arxiv_comment": "31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v5",
                "updated": "2025-09-10T15:29:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    29,
                    43,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08682v1",
                "updated": "2025-09-10T15:22:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    22,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:22:00Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    22,
                    0,
                    2,
                    253,
                    0
                ],
                "title": "Automatic Failure Attribution and Critical Step Prediction Method for\n  Multi-Agent Systems Based on Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Failure Attribution and Critical Step Prediction Method for\n  Multi-Agent Systems Based on Causal Inference"
                },
                "summary": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Guoqing Ma"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Jingjiang Liu"
                    },
                    {
                        "name": "Yidan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yidan Liang"
                },
                "author": "Yidan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23614v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23614v6",
                "updated": "2025-09-10T15:14:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    14,
                    38,
                    2,
                    253,
                    0
                ],
                "published": "2024-10-31T03:56:15Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    3,
                    56,
                    15,
                    3,
                    305,
                    0
                ],
                "title": "Hypothesis testing with e-values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hypothesis testing with e-values"
                },
                "summary": "This book is written to offer a humble, but unified, treatment of e-values in\nhypothesis testing. It is organized into three parts: Fundamental Concepts,\nCore Ideas, and Advanced Topics. The first part includes four chapters that\nintroduce the basic concepts. The second part includes five chapters of core\nideas such as universal inference, log-optimality, e-processes, operations on\ne-values, and e-values in multiple testing. The third part contains seven\nchapters of advanced topics. The book collates important results from a variety\nof modern papers on e-values and related concepts, and also contains many\nresults not published elsewhere. It offers a coherent and comprehensive picture\non a fast-growing research area, and is ready to use as the basis of a graduate\ncourse in statistics and related fields.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This book is written to offer a humble, but unified, treatment of e-values in\nhypothesis testing. It is organized into three parts: Fundamental Concepts,\nCore Ideas, and Advanced Topics. The first part includes four chapters that\nintroduce the basic concepts. The second part includes five chapters of core\nideas such as universal inference, log-optimality, e-processes, operations on\ne-values, and e-values in multiple testing. The third part contains seven\nchapters of advanced topics. The book collates important results from a variety\nof modern papers on e-values and related concepts, and also contains many\nresults not published elsewhere. It offers a coherent and comprehensive picture\non a fast-growing research area, and is ready to use as the basis of a graduate\ncourse in statistics and related fields."
                },
                "authors": [
                    {
                        "name": "Aaditya Ramdas"
                    },
                    {
                        "name": "Ruodu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ruodu Wang"
                },
                "author": "Ruodu Wang",
                "arxiv_doi": "10.1561/3600000002",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1561/3600000002",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23614v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23614v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: Foundations and Trends in Statistics, Vol. 1: No. 1-2,\n  pp 1-390",
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08657v1",
                "updated": "2025-09-10T14:52:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    52,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:52:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    52,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "Decisive Evidence for the First Overtone Mode in the Ringdown Signal of\n  GW231028",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decisive Evidence for the First Overtone Mode in the Ringdown Signal of\n  GW231028"
                },
                "summary": "The properties of a remnant black hole can be probed by analyzing the\ngravitational waves emitted during its ringdown phase. This signal provides a\ndirect test of general relativity in the strong-field regime. In this study, we\napply a time-domain F-statistic framework to the ringdown of GW231028_153006\nand find decisive evidence for the presence of the first overtone mode in the\nsignal. The detection of the $\\ell|m|n=221$ mode is statistically significant,\nachieving a Bayes factor of $193$ for an analysis beginning at $10\\,M$ after\nthe signal's peak amplitude--a time consistent with the linear perturbation\nregime. The inclusion of both the fundamental and overtone modes in our model\nallows for precise constraints on the remnant's properties. We infer a\nredshifted final mass of $243.0^{+22.7}_{-22.7}\\,M_{\\odot}$ and a final spin of\n$0.80_{-0.11}^{+0.07}$ (at $90\\%$ credibility), derived from a ringdown signal\nwith a network signal-to-noise ratio of approximately $10.5$. A test of the\nno-hair theorem, enabled by this two-mode detection, shows consistency with the\npredictions of general relativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The properties of a remnant black hole can be probed by analyzing the\ngravitational waves emitted during its ringdown phase. This signal provides a\ndirect test of general relativity in the strong-field regime. In this study, we\napply a time-domain F-statistic framework to the ringdown of GW231028_153006\nand find decisive evidence for the presence of the first overtone mode in the\nsignal. The detection of the $\\ell|m|n=221$ mode is statistically significant,\nachieving a Bayes factor of $193$ for an analysis beginning at $10\\,M$ after\nthe signal's peak amplitude--a time consistent with the linear perturbation\nregime. The inclusion of both the fundamental and overtone modes in our model\nallows for precise constraints on the remnant's properties. We infer a\nredshifted final mass of $243.0^{+22.7}_{-22.7}\\,M_{\\odot}$ and a final spin of\n$0.80_{-0.11}^{+0.07}$ (at $90\\%$ credibility), derived from a ringdown signal\nwith a network signal-to-noise ratio of approximately $10.5$. A test of the\nno-hair theorem, enabled by this two-mode detection, shows consistency with the\npredictions of general relativity."
                },
                "authors": [
                    {
                        "name": "Hai-Tian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Tian Wang"
                },
                "author": "Hai-Tian Wang",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08646v1",
                "updated": "2025-09-10T14:41:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    41,
                    7,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:41:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    41,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute\n  Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute\n  Implementations"
                },
                "summary": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents."
                },
                "authors": [
                    {
                        "name": "Ron F. Del Rosario"
                    },
                    {
                        "name": "Klaudia Krawiecka"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16810v2",
                "updated": "2025-09-10T14:40:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    40,
                    37,
                    2,
                    253,
                    0
                ],
                "published": "2025-06-20T07:57:55Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    57,
                    55,
                    4,
                    171,
                    0
                ],
                "title": "Transformers for Stratified Spectropolarimetric Inversion: Proof of\n  Concept",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers for Stratified Spectropolarimetric Inversion: Proof of\n  Concept"
                },
                "summary": "Solar spectropolarimetric inversion -- inferring atmospheric conditions from\nthe Stokes vector -- is a key diagnostic tool for understanding solar\nmagnetism, but traditional inversion methods are computationally expensive and\nsensitive to local minima. Advances in artificial intelligence (AI) offer\nfaster solutions, but are often restricted to shallow models or a few spectral\nlines. We present a proof-of-concept study using a transformer machine learning\n(ML) model for multi-line, full-Stokes inversion, to infer stratified\nparameters from synthetic spectra produced from 3D magnetohydrodynamic\nsimulations. We synthesise a large set of Stokes vectors using forward\nmodelling across 15 spectral lines spanning the deep photosphere towards the\nchromosphere. The model maps full-Stokes input to temperature, magnetic field\nstrength, inclination, azimuth (encoded as $\\sin2\\phi$, $\\cos2\\phi$), and\nline-of-sight velocity as a function of optical depth. The transformer\nincorporates an attention mechanism that allows the model to focus on the most\ninformative regions of the spectrum for each inferred parameter, and uses\npositional embedding to encode wavelength and depth order. We benchmark it\nagainst a multilayer perceptron (MLP), test robustness to noise, and assess\ngeneralisation. The transformer outperforms the MLP, especially in the higher\nlayers and for magnetic parameters, yielding higher correlations and more\nregularised stratifications. The model retains strong performance across a\nrange of noise levels typical for real observations, with magnetic parameter\ninference degrading predictably while temperature and velocity remain stable.\nWe establish transformer architectures as a powerful tool for\nspectropolarimetric inversion. This approach paves the way for analysis of\nlarge datasets from large solar telescopes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar spectropolarimetric inversion -- inferring atmospheric conditions from\nthe Stokes vector -- is a key diagnostic tool for understanding solar\nmagnetism, but traditional inversion methods are computationally expensive and\nsensitive to local minima. Advances in artificial intelligence (AI) offer\nfaster solutions, but are often restricted to shallow models or a few spectral\nlines. We present a proof-of-concept study using a transformer machine learning\n(ML) model for multi-line, full-Stokes inversion, to infer stratified\nparameters from synthetic spectra produced from 3D magnetohydrodynamic\nsimulations. We synthesise a large set of Stokes vectors using forward\nmodelling across 15 spectral lines spanning the deep photosphere towards the\nchromosphere. The model maps full-Stokes input to temperature, magnetic field\nstrength, inclination, azimuth (encoded as $\\sin2\\phi$, $\\cos2\\phi$), and\nline-of-sight velocity as a function of optical depth. The transformer\nincorporates an attention mechanism that allows the model to focus on the most\ninformative regions of the spectrum for each inferred parameter, and uses\npositional embedding to encode wavelength and depth order. We benchmark it\nagainst a multilayer perceptron (MLP), test robustness to noise, and assess\ngeneralisation. The transformer outperforms the MLP, especially in the higher\nlayers and for magnetic parameters, yielding higher correlations and more\nregularised stratifications. The model retains strong performance across a\nrange of noise levels typical for real observations, with magnetic parameter\ninference degrading predictably while temperature and velocity remain stable.\nWe establish transformer architectures as a powerful tool for\nspectropolarimetric inversion. This approach paves the way for analysis of\nlarge datasets from large solar telescopes."
                },
                "authors": [
                    {
                        "name": "Ryan James Campbell"
                    },
                    {
                        "name": "Mihalis Mathioudakis"
                    },
                    {
                        "name": "Carlos Quintero Noda"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Quintero Noda"
                },
                "author": "Carlos Quintero Noda",
                "arxiv_comment": "Revision 1, submitted to the Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08948v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08948v3",
                "updated": "2025-09-10T14:40:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    40,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2024-04-13T09:56:50Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    9,
                    56,
                    50,
                    5,
                    104,
                    0
                ],
                "title": "Large Language Models for Mobile GUI Text Input Generation: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Mobile GUI Text Input Generation: An Empirical\n  Study"
                },
                "summary": "Mobile applications have become an essential part of our daily lives, making\nensuring their quality an important activity. Graphical User Interface (GUI)\ntesting is a quality assurance method that has frequently been used for mobile\napps. Some GUIs require these text inputs to be able to move from one page to\nthe next. Recently, Large Language Models (LLMs) have demonstrated excellent\ntext-generation capabilities. To the best of our knowledge, there has not yet\nbeen any empirical study to evaluate different pre-trained LLMs' effectiveness\nat generating text inputs for mobile GUI testing. This paper reports on a\nlarge-scale empirical study that extensively investigates the effectiveness of\neight state-of-the-art LLMs in Android text-input generation for UI pages. We\ncollected 115 Android apps from Google Play and extracted contextual\ninformation from the UI pages to construct prompts for LLMs. The experimental\nresults show that some LLMs can generate more effective and higher-quality text\ninputs. We conducted an experiment to assess the bug-detection capabilities of\nLLMs by directly generating invalid text inputs. We also invited professional\ntesters to manually evaluate, modify, and re-create the LLM-generated text\ninputs. We integrated the text-input generation process into DroidBot to\naugment its UI-exploration capabilities. Finally, we present several valuable\ninsights regarding the application of LLMs to Android testing, particularly for\nthe generation of text inputs: These insights will benefit the Android testing\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile applications have become an essential part of our daily lives, making\nensuring their quality an important activity. Graphical User Interface (GUI)\ntesting is a quality assurance method that has frequently been used for mobile\napps. Some GUIs require these text inputs to be able to move from one page to\nthe next. Recently, Large Language Models (LLMs) have demonstrated excellent\ntext-generation capabilities. To the best of our knowledge, there has not yet\nbeen any empirical study to evaluate different pre-trained LLMs' effectiveness\nat generating text inputs for mobile GUI testing. This paper reports on a\nlarge-scale empirical study that extensively investigates the effectiveness of\neight state-of-the-art LLMs in Android text-input generation for UI pages. We\ncollected 115 Android apps from Google Play and extracted contextual\ninformation from the UI pages to construct prompts for LLMs. The experimental\nresults show that some LLMs can generate more effective and higher-quality text\ninputs. We conducted an experiment to assess the bug-detection capabilities of\nLLMs by directly generating invalid text inputs. We also invited professional\ntesters to manually evaluate, modify, and re-create the LLM-generated text\ninputs. We integrated the text-input generation process into DroidBot to\naugment its UI-exploration capabilities. Finally, we present several valuable\ninsights regarding the application of LLMs to Android testing, particularly for\nthe generation of text inputs: These insights will benefit the Android testing\ncommunity."
                },
                "authors": [
                    {
                        "name": "Chenhui Cui"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Dave Towey"
                    },
                    {
                        "name": "Rubing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Rubing Huang"
                },
                "author": "Rubing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08948v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08948v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02825v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02825v6",
                "updated": "2025-09-10T14:39:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    39,
                    59,
                    2,
                    253,
                    0
                ],
                "published": "2025-01-06T07:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "title": "Randomly Sampled Language Reasoning Problems Elucidate Limitations of\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomly Sampled Language Reasoning Problems Elucidate Limitations of\n  In-Context Learning"
                },
                "summary": "While LLMs have revolutionized the field of machine learning due to their\nhigh performance on a strikingly wide range of problems, they are also known to\nhallucinate false answers and underperform on less canonical versions of the\nsame tasks. There are several emerging theories of LLM performance, among them\nthat LLMs lack world modeling ability, that they have an undesirable bias\ntowards an autoregressive prior, and that they struggle on more novel problems.\nThe existing literature on LLM input novelty has focused on tasks of relatively\nhigh complexity, studying perturbations of canonical but complex problems. In\nthis paper, we attempt to minimize complexity in order to isolate novelty as a\nfactor in LLM underperformance and investigate the power of\nin-context-learning. To this end, we consider an extremely simple domain: next\ntoken prediction on simple language tasks. The twist is that these language\ntasks are wholly unseen, as they are randomly drawn from a large,\nparsimoniously defined set of languages arising from simple grammar rules. This\nexperimental setup allows us to evaluate ICL independently of models'\nparametric knowledge. We find that LLMs uniformly underperform n-gram models on\nthis task, both when used as next token predictors and in chain-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have revolutionized the field of machine learning due to their\nhigh performance on a strikingly wide range of problems, they are also known to\nhallucinate false answers and underperform on less canonical versions of the\nsame tasks. There are several emerging theories of LLM performance, among them\nthat LLMs lack world modeling ability, that they have an undesirable bias\ntowards an autoregressive prior, and that they struggle on more novel problems.\nThe existing literature on LLM input novelty has focused on tasks of relatively\nhigh complexity, studying perturbations of canonical but complex problems. In\nthis paper, we attempt to minimize complexity in order to isolate novelty as a\nfactor in LLM underperformance and investigate the power of\nin-context-learning. To this end, we consider an extremely simple domain: next\ntoken prediction on simple language tasks. The twist is that these language\ntasks are wholly unseen, as they are randomly drawn from a large,\nparsimoniously defined set of languages arising from simple grammar rules. This\nexperimental setup allows us to evaluate ICL independently of models'\nparametric knowledge. We find that LLMs uniformly underperform n-gram models on\nthis task, both when used as next token predictors and in chain-of-thought."
                },
                "authors": [
                    {
                        "name": "Kavi Gupta"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    }
                ],
                "author_detail": {
                    "name": "Armando Solar-Lezama"
                },
                "author": "Armando Solar-Lezama",
                "arxiv_comment": "10 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02825v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02825v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07218v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07218v2",
                "updated": "2025-09-10T14:38:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    38,
                    5,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-08T20:55:54Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    20,
                    55,
                    54,
                    0,
                    251,
                    0
                ],
                "title": "Electricity Demand and Grid Impacts of AI Data Centers: Challenges and\n  Prospects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electricity Demand and Grid Impacts of AI Data Centers: Challenges and\n  Prospects"
                },
                "summary": "The rapid growth of artificial intelligence (AI) is driving an unprecedented\nincrease in the electricity demand of AI data centers, raising emerging\nchallenges for electric power grids. Understanding the characteristics of AI\ndata center loads and their interactions with the grid is therefore critical\nfor ensuring both reliable power system operation and sustainable AI\ndevelopment. This paper provides a comprehensive review and vision of this\nevolving landscape. Specifically, this paper (i) presents an overview of AI\ndata center infrastructure and its key components, (ii) examines the key\ncharacteristics and patterns of electricity demand across the stages of model\npreparation, training, fine-tuning, and inference, (iii) analyzes the critical\nchallenges that AI data center loads pose to power systems across three\ninterrelated timescales, including long-term planning and interconnection,\nshort-term operation and electricity markets, and real-time dynamics and\nstability, and (iv) discusses potential solutions from the perspectives of the\ngrid, AI data centers, and AI end-users to address these challenges. By\nsynthesizing current knowledge and outlining future directions, this review\naims to guide research and development in support of the joint advancement of\nAI data centers and power systems toward reliable, efficient, and sustainable\noperation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of artificial intelligence (AI) is driving an unprecedented\nincrease in the electricity demand of AI data centers, raising emerging\nchallenges for electric power grids. Understanding the characteristics of AI\ndata center loads and their interactions with the grid is therefore critical\nfor ensuring both reliable power system operation and sustainable AI\ndevelopment. This paper provides a comprehensive review and vision of this\nevolving landscape. Specifically, this paper (i) presents an overview of AI\ndata center infrastructure and its key components, (ii) examines the key\ncharacteristics and patterns of electricity demand across the stages of model\npreparation, training, fine-tuning, and inference, (iii) analyzes the critical\nchallenges that AI data center loads pose to power systems across three\ninterrelated timescales, including long-term planning and interconnection,\nshort-term operation and electricity markets, and real-time dynamics and\nstability, and (iv) discusses potential solutions from the perspectives of the\ngrid, AI data centers, and AI end-users to address these challenges. By\nsynthesizing current knowledge and outlining future directions, this review\naims to guide research and development in support of the joint advancement of\nAI data centers and power systems toward reliable, efficient, and sustainable\noperation."
                },
                "authors": [
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Ana Colacelli"
                    },
                    {
                        "name": "Matt Lee"
                    },
                    {
                        "name": "Le Xie"
                    }
                ],
                "author_detail": {
                    "name": "Le Xie"
                },
                "author": "Le Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07218v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07218v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.02444v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.02444v4",
                "updated": "2025-09-10T14:37:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    37,
                    12,
                    2,
                    253,
                    0
                ],
                "published": "2023-03-04T16:04:17Z",
                "published_parsed": [
                    2023,
                    3,
                    4,
                    16,
                    4,
                    17,
                    5,
                    63,
                    0
                ],
                "title": "Calibrating Transformers via Sparse Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Transformers via Sparse Gaussian Processes"
                },
                "summary": "Transformer models have achieved profound success in prediction tasks in a\nwide range of applications in natural language processing, speech recognition\nand computer vision. Extending Transformer's success to safety-critical domains\nrequires calibrated uncertainty estimation which remains under-explored. To\naddress this, we propose Sparse Gaussian Process attention (SGPA), which\nperforms Bayesian inference directly in the output space of multi-head\nattention blocks (MHAs) in transformer to calibrate its uncertainty. It\nreplaces the scaled dot-product operation with a valid symmetric kernel and\nuses sparse Gaussian processes (SGP) techniques to approximate the posterior\nprocesses of MHA outputs. Empirically, on a suite of prediction tasks on text,\nimages and graphs, SGPA-based Transformers achieve competitive predictive\naccuracy, while noticeably improving both in-distribution calibration and\nout-of-distribution robustness and detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have achieved profound success in prediction tasks in a\nwide range of applications in natural language processing, speech recognition\nand computer vision. Extending Transformer's success to safety-critical domains\nrequires calibrated uncertainty estimation which remains under-explored. To\naddress this, we propose Sparse Gaussian Process attention (SGPA), which\nperforms Bayesian inference directly in the output space of multi-head\nattention blocks (MHAs) in transformer to calibrate its uncertainty. It\nreplaces the scaled dot-product operation with a valid symmetric kernel and\nuses sparse Gaussian processes (SGP) techniques to approximate the posterior\nprocesses of MHA outputs. Empirically, on a suite of prediction tasks on text,\nimages and graphs, SGPA-based Transformers achieve competitive predictive\naccuracy, while noticeably improving both in-distribution calibration and\nout-of-distribution robustness and detection."
                },
                "authors": [
                    {
                        "name": "Wenlong Chen"
                    },
                    {
                        "name": "Yingzhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingzhen Li"
                },
                "author": "Yingzhen Li",
                "arxiv_comment": "Published at The Eleventh International Conference on Learning\n  Representations (ICLR 2023). ECE updated, typo fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.02444v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.02444v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08638v1",
                "updated": "2025-09-10T14:33:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    33,
                    58,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:33:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    33,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models"
                },
                "summary": "Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles."
                },
                "authors": [
                    {
                        "name": "Rebecca Martin"
                    },
                    {
                        "name": "Jay Patrikar"
                    },
                    {
                        "name": "Sebastian Scherer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Scherer"
                },
                "author": "Sebastian Scherer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11042v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11042v2",
                "updated": "2025-09-10T14:32:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    32,
                    3,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-14T19:59:18Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    19,
                    59,
                    18,
                    3,
                    226,
                    0
                ],
                "title": "Sample efficient likelihood-free inference for virus dynamics with\n  different types of experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sample efficient likelihood-free inference for virus dynamics with\n  different types of experiments"
                },
                "summary": "This study applied Bayesian optimization likelihood-free inference(BOLFI) to\nvirus dynamics experimental data and efficiently inferred the model parameters\nwith uncertainty measure. The computational benefit is remarkable compared to\nexisting methodology on the same problem. No likelihood knowledge is needed in\nthe inference. Improvement of the BOLFI algorithm with Gaussian process based\nclassifier for treatment of extreme values are provided. Discrepancy design for\ncombining different forms of data from completely different experiment\nprocesses are suggested and tested with synthetic data, then applied to real\ndata. Reasonable parameter values are estimated for influenza A virus data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study applied Bayesian optimization likelihood-free inference(BOLFI) to\nvirus dynamics experimental data and efficiently inferred the model parameters\nwith uncertainty measure. The computational benefit is remarkable compared to\nexisting methodology on the same problem. No likelihood knowledge is needed in\nthe inference. Improvement of the BOLFI algorithm with Gaussian process based\nclassifier for treatment of extreme values are provided. Discrepancy design for\ncombining different forms of data from completely different experiment\nprocesses are suggested and tested with synthetic data, then applied to real\ndata. Reasonable parameter values are estimated for influenza A virus data."
                },
                "authors": [
                    {
                        "name": "Yingying Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yingying Xu"
                },
                "author": "Yingying Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11042v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11042v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08624v1",
                "updated": "2025-09-10T14:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    19,
                    59,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:19:59Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    19,
                    59,
                    2,
                    253,
                    0
                ],
                "title": "UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image\n  Diagnosis Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image\n  Diagnosis Augmentation"
                },
                "summary": "Significant advancements in AI-driven multimodal medical image diagnosis have\nled to substantial improvements in ophthalmic disease identification in recent\nyears. However, acquiring paired multimodal ophthalmic images remains\nprohibitively expensive. While fundus photography is simple and cost-effective,\nthe limited availability of OCT data and inherent modality imbalance hinder\nfurther progress. Conventional approaches that rely solely on fundus or textual\nfeatures often fail to capture fine-grained spatial information, as each\nimaging modality provides distinct cues about lesion predilection sites. In\nthis study, we propose a novel unpaired multimodal framework \\UOPSL that\nutilizes extensive OCT-derived spatial priors to dynamically identify\npredilection sites, enhancing fundus image-based disease recognition. Our\napproach bridges unpaired fundus and OCTs via extended disease text\ndescriptions. Initially, we employ contrastive learning on a large corpus of\nunpaired OCT and fundus images while simultaneously learning the predilection\nsites matrix in the OCT latent space. Through extensive optimization, this\nmatrix captures lesion localization patterns within the OCT feature space.\nDuring the fine-tuning or inference phase of the downstream classification task\nbased solely on fundus images, where paired OCT data is unavailable, we\neliminate OCT input and utilize the predilection sites matrix to assist in\nfundus image classification learning. Extensive experiments conducted on 9\ndiverse datasets across 28 critical categories demonstrate that our framework\noutperforms existing benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advancements in AI-driven multimodal medical image diagnosis have\nled to substantial improvements in ophthalmic disease identification in recent\nyears. However, acquiring paired multimodal ophthalmic images remains\nprohibitively expensive. While fundus photography is simple and cost-effective,\nthe limited availability of OCT data and inherent modality imbalance hinder\nfurther progress. Conventional approaches that rely solely on fundus or textual\nfeatures often fail to capture fine-grained spatial information, as each\nimaging modality provides distinct cues about lesion predilection sites. In\nthis study, we propose a novel unpaired multimodal framework \\UOPSL that\nutilizes extensive OCT-derived spatial priors to dynamically identify\npredilection sites, enhancing fundus image-based disease recognition. Our\napproach bridges unpaired fundus and OCTs via extended disease text\ndescriptions. Initially, we employ contrastive learning on a large corpus of\nunpaired OCT and fundus images while simultaneously learning the predilection\nsites matrix in the OCT latent space. Through extensive optimization, this\nmatrix captures lesion localization patterns within the OCT feature space.\nDuring the fine-tuning or inference phase of the downstream classification task\nbased solely on fundus images, where paired OCT data is unavailable, we\neliminate OCT input and utilize the predilection sites matrix to assist in\nfundus image classification learning. Extensive experiments conducted on 9\ndiverse datasets across 28 critical categories demonstrate that our framework\noutperforms existing benchmarks."
                },
                "authors": [
                    {
                        "name": "Zhihao Zhao"
                    },
                    {
                        "name": "Yinzheng Zhao"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Xiangtong Yao"
                    },
                    {
                        "name": "Quanmin Liang"
                    },
                    {
                        "name": "Daniel Zapp"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "M. Ali Nasseri"
                    }
                ],
                "author_detail": {
                    "name": "M. Ali Nasseri"
                },
                "author": "M. Ali Nasseri",
                "arxiv_comment": "BIBM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08621v1",
                "updated": "2025-09-10T14:17:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    17,
                    53,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:17:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    17,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "AdsQA: Towards Advertisement Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdsQA: Towards Advertisement Video Understanding"
                },
                "summary": "Large language models (LLMs) have taken a great step towards AGI. Meanwhile,\nan increasing number of domain-specific problems such as math and programming\nboost these general-purpose models to continuously evolve via learning deeper\nexpertise. Now is thus the time further to extend the diversity of specialized\napplications for knowledgeable LLMs, though collecting high quality data with\nunexpected and informative tasks is challenging. In this paper, we propose to\nuse advertisement (ad) videos as a challenging test-bed to probe the ability of\nLLMs in perceiving beyond the objective physical content of common visual\ndomain. Our motivation is to take full advantage of the clue-rich and\ninformation-dense ad videos' traits, e.g., marketing logic, persuasive\nstrategies, and audience engagement. Our contribution is three-fold: (1) To our\nknowledge, this is the first attempt to use ad videos with well-designed tasks\nto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark\nderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing\n5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that\nreflects on questions, and generates answers via reward-driven optimization.\n(3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves\nthe state-of-the-art outperforming strong competitors equipped with long-chain\nreasoning capabilities by a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have taken a great step towards AGI. Meanwhile,\nan increasing number of domain-specific problems such as math and programming\nboost these general-purpose models to continuously evolve via learning deeper\nexpertise. Now is thus the time further to extend the diversity of specialized\napplications for knowledgeable LLMs, though collecting high quality data with\nunexpected and informative tasks is challenging. In this paper, we propose to\nuse advertisement (ad) videos as a challenging test-bed to probe the ability of\nLLMs in perceiving beyond the objective physical content of common visual\ndomain. Our motivation is to take full advantage of the clue-rich and\ninformation-dense ad videos' traits, e.g., marketing logic, persuasive\nstrategies, and audience engagement. Our contribution is three-fold: (1) To our\nknowledge, this is the first attempt to use ad videos with well-designed tasks\nto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark\nderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing\n5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that\nreflects on questions, and generates answers via reward-driven optimization.\n(3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves\nthe state-of-the-art outperforming strong competitors equipped with long-chain\nreasoning capabilities by a clear margin."
                },
                "authors": [
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Jingxuan Li"
                    },
                    {
                        "name": "Sa Yang"
                    },
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jiaheng Ma"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "ICCV-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08614v1",
                "updated": "2025-09-10T14:09:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    9,
                    16,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:09:16Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    9,
                    16,
                    2,
                    253,
                    0
                ],
                "title": "Modular PE-Structured Learning for Cross-Task Wireless Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular PE-Structured Learning for Cross-Task Wireless Communications"
                },
                "summary": "Recent trends in learning wireless policies attempt to develop deep neural\nnetworks (DNNs) for handling multiple tasks with a single model. Existing\napproaches often rely on large models, which are hard to pre-train and\nfine-tune at the wireless edge. In this work, we challenge this paradigm by\nleveraging the structured knowledge of wireless problems -- specifically,\npermutation equivariant (PE) properties. We design three types of PE-aware\nmodules, two of which are Transformer-style sub-layers. These modules can serve\nas building blocks to assemble compact DNNs applicable to the wireless policies\nwith various PE properties. To guide the design, we analyze the hypothesis\nspace associated with each PE property, and show that the PE-structured module\nassembly can boost the learning efficiency. Inspired by the reusability of the\nmodules, we propose PE-MoFormer, a compositional DNN capable of learning a wide\nrange of wireless policies -- including but not limited to precoding,\ncoordinated beamforming, power allocation, and channel estimation -- with\nstrong generalizability, low sample and space complexity. Simulations\ndemonstrate that the proposed modular PE-based framework outperforms relevant\nlarge model in both learning efficiency and inference time, offering a new\ndirection for structured cross-task learning for wireless communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent trends in learning wireless policies attempt to develop deep neural\nnetworks (DNNs) for handling multiple tasks with a single model. Existing\napproaches often rely on large models, which are hard to pre-train and\nfine-tune at the wireless edge. In this work, we challenge this paradigm by\nleveraging the structured knowledge of wireless problems -- specifically,\npermutation equivariant (PE) properties. We design three types of PE-aware\nmodules, two of which are Transformer-style sub-layers. These modules can serve\nas building blocks to assemble compact DNNs applicable to the wireless policies\nwith various PE properties. To guide the design, we analyze the hypothesis\nspace associated with each PE property, and show that the PE-structured module\nassembly can boost the learning efficiency. Inspired by the reusability of the\nmodules, we propose PE-MoFormer, a compositional DNN capable of learning a wide\nrange of wireless policies -- including but not limited to precoding,\ncoordinated beamforming, power allocation, and channel estimation -- with\nstrong generalizability, low sample and space complexity. Simulations\ndemonstrate that the proposed modular PE-based framework outperforms relevant\nlarge model in both learning efficiency and inference time, offering a new\ndirection for structured cross-task learning for wireless communications."
                },
                "authors": [
                    {
                        "name": "Yuxuan Duan"
                    },
                    {
                        "name": "Chenyang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chenyang Yang"
                },
                "author": "Chenyang Yang",
                "arxiv_comment": "14 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02547v2",
                "updated": "2025-09-10T14:05:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    5,
                    14,
                    2,
                    253,
                    0
                ],
                "published": "2025-04-03T12:54:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    54,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Outlier-Robust Multi-Group Gaussian Mixture Modeling with Flexible Group\n  Reassignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier-Robust Multi-Group Gaussian Mixture Modeling with Flexible Group\n  Reassignment"
                },
                "summary": "Do expert-defined or diagnostically-labeled data groups align with clusters\ninferred through statistical modeling? If not, where do discrepancies between\npredefined labels and model-based groupings occur and why? In this work, we\nshow how to address these questions using the multi-group Gaussian mixture\nmodel (MG-GMM). This novel model incorporates prior group information while\nallowing flexibility to reassign observations to alternative groups based on\ndata-driven evidence. We achieve this by modeling the observations of each\ngroup as arising not from a single distribution, but from a Gaussian mixture\ncomprising all group-specific distributions. Moreover, our model offers\nrobustness against cellwise outliers that may obscure or distort the underlying\ngroup structure. We propose a new penalized likelihood approach, called\ncellMG-GMM, to jointly estimate mixture probabilities, location and scale\nparameters of the MG-GMM, and detect outliers through a penalty term on the\nnumber of flagged cellwise outliers in the objective function. We show that our\nestimator has good breakdown properties in presence of cellwise outliers. We\ndevelop a computationally-efficient EM-based algorithm for cellMG-GMM, and\ndemonstrate its strong performance in identifying and diagnosing observations\nat the intersection of multiple groups through simulations and diverse\napplications in meteorology, medicine and oenology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do expert-defined or diagnostically-labeled data groups align with clusters\ninferred through statistical modeling? If not, where do discrepancies between\npredefined labels and model-based groupings occur and why? In this work, we\nshow how to address these questions using the multi-group Gaussian mixture\nmodel (MG-GMM). This novel model incorporates prior group information while\nallowing flexibility to reassign observations to alternative groups based on\ndata-driven evidence. We achieve this by modeling the observations of each\ngroup as arising not from a single distribution, but from a Gaussian mixture\ncomprising all group-specific distributions. Moreover, our model offers\nrobustness against cellwise outliers that may obscure or distort the underlying\ngroup structure. We propose a new penalized likelihood approach, called\ncellMG-GMM, to jointly estimate mixture probabilities, location and scale\nparameters of the MG-GMM, and detect outliers through a penalty term on the\nnumber of flagged cellwise outliers in the objective function. We show that our\nestimator has good breakdown properties in presence of cellwise outliers. We\ndevelop a computationally-efficient EM-based algorithm for cellMG-GMM, and\ndemonstrate its strong performance in identifying and diagnosing observations\nat the intersection of multiple groups through simulations and diverse\napplications in meteorology, medicine and oenology."
                },
                "authors": [
                    {
                        "name": "Patricia Puchhammer"
                    },
                    {
                        "name": "Ines Wilms"
                    },
                    {
                        "name": "Peter Filzmoser"
                    }
                ],
                "author_detail": {
                    "name": "Peter Filzmoser"
                },
                "author": "Peter Filzmoser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12315v2",
                "updated": "2025-09-10T14:03:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    3,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-17T10:17:23Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    10,
                    17,
                    23,
                    6,
                    229,
                    0
                ],
                "title": "Deciphering the global production network from cross-border firm\n  transactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the global production network from cross-border firm\n  transactions"
                },
                "summary": "Critical for policy-making and business operations, the study of global\nsupply chains has been severely hampered by a lack of detailed data. Here we\nharness global firm-level transaction data covering 20m global firms, and 1\nbillion cross-border transactions, to infer key inputs for over 1200 products.\nTransforming this data to a directed network, we find that products are\nclustered into three large groups including textiles, chemicals and food, and\nmachinery and metals. European industrial nations and China dominate critical\nintermediate products in the network such as metals, common components and\ntools, while industrial complexity is correlated with embeddedness in densely\nconnected supply chains. To validate the network, we find structural\nsimilarities with two alternative product networks, one generated via LLM\nqueries and the other derived by NAFTA to track product origins. We further\ndetect linkages between products identified in manually mapped single sector\nsupply chains, including electric vehicle batteries and semi-conductors.\nFinally, metrics derived from network structure capturing both forward and\nbackward linkages are able to predict country-product diversification patterns\nwith high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical for policy-making and business operations, the study of global\nsupply chains has been severely hampered by a lack of detailed data. Here we\nharness global firm-level transaction data covering 20m global firms, and 1\nbillion cross-border transactions, to infer key inputs for over 1200 products.\nTransforming this data to a directed network, we find that products are\nclustered into three large groups including textiles, chemicals and food, and\nmachinery and metals. European industrial nations and China dominate critical\nintermediate products in the network such as metals, common components and\ntools, while industrial complexity is correlated with embeddedness in densely\nconnected supply chains. To validate the network, we find structural\nsimilarities with two alternative product networks, one generated via LLM\nqueries and the other derived by NAFTA to track product origins. We further\ndetect linkages between products identified in manually mapped single sector\nsupply chains, including electric vehicle batteries and semi-conductors.\nFinally, metrics derived from network structure capturing both forward and\nbackward linkages are able to predict country-product diversification patterns\nwith high accuracy."
                },
                "authors": [
                    {
                        "name": "Neave O'Clery"
                    },
                    {
                        "name": "Ben Radcliffe-Brown"
                    },
                    {
                        "name": "Thomas Spencer"
                    },
                    {
                        "name": "Daniel Tarling-Hunter"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Tarling-Hunter"
                },
                "author": "Daniel Tarling-Hunter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03867v2",
                "updated": "2025-09-10T14:02:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    50,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-04T03:58:55Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    3,
                    58,
                    55,
                    3,
                    247,
                    0
                ],
                "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth"
                },
                "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\" - utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a benchmark dataset of over 1,200+ meticulously curated and diverse\nexamples across English, Mandarin, Spanish, French, Japanese, and Korean. Each\nexample underwent careful expert review to verify its Drivelological\ncharacteristics, involving multiple rounds of discussion and adjudication to\naddress disagreements. Using this dataset, we evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss implied rhetorical functions\naltogether. These findings highlight a deep representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\" - utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a benchmark dataset of over 1,200+ meticulously curated and diverse\nexamples across English, Mandarin, Spanish, French, Japanese, and Korean. Each\nexample underwent careful expert review to verify its Drivelological\ncharacteristics, involving multiple rounds of discussion and adjudication to\naddress disagreements. Using this dataset, we evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss implied rhetorical functions\naltogether. These findings highlight a deep representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence."
                },
                "authors": [
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Chia-Yi Hsiao"
                    },
                    {
                        "name": "Zi Yan Chang"
                    },
                    {
                        "name": "Chi-Li Chen"
                    },
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08604v1",
                "updated": "2025-09-10T14:02:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    18,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:02:18Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    18,
                    2,
                    253,
                    0
                ],
                "title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information."
                },
                "authors": [
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Mengmeng Du"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Yihang Fu"
                    },
                    {
                        "name": "Erica Stutz"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Siru Liu"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    },
                    {
                        "name": "Hyunghoon Cho"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Qingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingyu Chen"
                },
                "author": "Qingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08596v1",
                "updated": "2025-09-10T13:50:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    50,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:50:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    50,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question\n  Answering for BioASQ Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question\n  Answering for BioASQ Challenge"
                },
                "summary": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering."
                },
                "authors": [
                    {
                        "name": "Dima Galat"
                    },
                    {
                        "name": "Diego Molla-Aliod"
                    }
                ],
                "author_detail": {
                    "name": "Diego Molla-Aliod"
                },
                "author": "Diego Molla-Aliod",
                "arxiv_comment": "CEUR-WS, CLEF2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08593v1",
                "updated": "2025-09-10T13:46:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    46,
                    40,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:46:40Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    46,
                    40,
                    2,
                    253,
                    0
                ],
                "title": "No-Knowledge Alarms for Misaligned LLMs-as-Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-Knowledge Alarms for Misaligned LLMs-as-Judges"
                },
                "summary": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement."
                },
                "authors": [
                    {
                        "name": "Andrs Corrada-Emmanuel"
                    }
                ],
                "author_detail": {
                    "name": "Andrs Corrada-Emmanuel"
                },
                "author": "Andrs Corrada-Emmanuel",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90C05, 68T27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08591v1",
                "updated": "2025-09-10T13:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    45,
                    58,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:45:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    45,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "Functional Regression with Nonstationarity and Error Contamination:\n  Application to the Economic Impact of Climate Change",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Regression with Nonstationarity and Error Contamination:\n  Application to the Economic Impact of Climate Change"
                },
                "summary": "This paper studies a functional regression model with nonstationary dependent\nand explanatory functional observations, in which the nonstationary stochastic\ntrends of the dependent variable are explained by those of the explanatory\nvariable, and the functional observations may be error-contaminated. We develop\nnovel autocovariance-based estimation and inference methods for this model. The\nmethodology is broadly applicable to economic and statistical functional time\nseries with nonstationary dynamics. To illustrate our methodology and its\nusefulness, we apply it to the evaluation of the global economic impact of\nclimate change, an issue of intrinsic importance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies a functional regression model with nonstationary dependent\nand explanatory functional observations, in which the nonstationary stochastic\ntrends of the dependent variable are explained by those of the explanatory\nvariable, and the functional observations may be error-contaminated. We develop\nnovel autocovariance-based estimation and inference methods for this model. The\nmethodology is broadly applicable to economic and statistical functional time\nseries with nonstationary dynamics. To illustrate our methodology and its\nusefulness, we apply it to the evaluation of the global economic impact of\nclimate change, an issue of intrinsic importance."
                },
                "authors": [
                    {
                        "name": "Kyungsik Nam"
                    },
                    {
                        "name": "Won-Ki Seo"
                    }
                ],
                "author_detail": {
                    "name": "Won-Ki Seo"
                },
                "author": "Won-Ki Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62M10, 62R10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10556v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10556v3",
                "updated": "2025-09-10T13:36:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    36,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2024-11-15T20:07:25Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    20,
                    7,
                    25,
                    4,
                    320,
                    0
                ],
                "title": "Star Log-extended eMulation: a method for efficient computation of the\n  Tolman-Oppenheimer-Volkoff equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Log-extended eMulation: a method for efficient computation of the\n  Tolman-Oppenheimer-Volkoff equations"
                },
                "summary": "We emulate the Tolman-Oppenheimer-Volkoff (TOV) equations, including tidal\ndeformability, for neutron stars using a new method based upon the Dynamic Mode\nDecomposition (DMD). This method, which we call Star Log-extended eMulation\n(SLM), utilizes the underlying logarithmic behavior of the differential\nequations to enable accurate emulation of the nonlinear system. We show\npredictions for well-known equations of state (EOSs) with fixed parameters\nusing the SLM, accurately recreating high-fidelity results while achieving a\ncomputational speed-up of $\\approx 2.4 \\times 10^4$. We test our parametric SLM\nmethod for a two-parameter quarkyonic EOS against high-fidelity RK4 TOV\ncalculations and find a computational speedup of $\\approx 7.0 \\times 10^4$.\nHence, SLM is an efficient emulator for the numerous TOV evaluations required\nby multi-messenger astrophysical frameworks that infer constraints on the EOS.\nThe ability of the SLM algorithm to learn a mapping between parameters of the\nEOS and subsequent neutron star properties also opens up potential extensions\nfor assisting in computationally prohibitive uncertainty quantification (UQ)\nfor any type of EOS. The source code for the methods employed in this work is\nopenly available in a public GitHub repository for community modification and\nuse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We emulate the Tolman-Oppenheimer-Volkoff (TOV) equations, including tidal\ndeformability, for neutron stars using a new method based upon the Dynamic Mode\nDecomposition (DMD). This method, which we call Star Log-extended eMulation\n(SLM), utilizes the underlying logarithmic behavior of the differential\nequations to enable accurate emulation of the nonlinear system. We show\npredictions for well-known equations of state (EOSs) with fixed parameters\nusing the SLM, accurately recreating high-fidelity results while achieving a\ncomputational speed-up of $\\approx 2.4 \\times 10^4$. We test our parametric SLM\nmethod for a two-parameter quarkyonic EOS against high-fidelity RK4 TOV\ncalculations and find a computational speedup of $\\approx 7.0 \\times 10^4$.\nHence, SLM is an efficient emulator for the numerous TOV evaluations required\nby multi-messenger astrophysical frameworks that infer constraints on the EOS.\nThe ability of the SLM algorithm to learn a mapping between parameters of the\nEOS and subsequent neutron star properties also opens up potential extensions\nfor assisting in computationally prohibitive uncertainty quantification (UQ)\nfor any type of EOS. The source code for the methods employed in this work is\nopenly available in a public GitHub repository for community modification and\nuse."
                },
                "authors": [
                    {
                        "name": "Sudhanva Lalit"
                    },
                    {
                        "name": "Alexandra C. Semposki"
                    },
                    {
                        "name": "Joshua M. Maldonado"
                    }
                ],
                "author_detail": {
                    "name": "Joshua M. Maldonado"
                },
                "author": "Joshua M. Maldonado",
                "arxiv_comment": "13 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10556v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10556v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08583v1",
                "updated": "2025-09-10T13:32:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    32,
                    2,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:32:02Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    32,
                    2,
                    2,
                    253,
                    0
                ],
                "title": "EfficientIML: Efficient High-Resolution Image Manipulation Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientIML: Efficient High-Resolution Image Manipulation Localization"
                },
                "summary": "With imaging devices delivering ever-higher resolutions and the emerging\ndiffusion-based forgery methods, current detectors trained only on traditional\ndatasets (with splicing, copy-moving and object removal forgeries) lack\nexposure to this new manipulation type. To address this, we propose a novel\nhigh-resolution SIF dataset of 1200+ diffusion-generated manipulations with\nsemantically extracted masks. However, this also imposes a challenge on\nexisting methods, as they face significant computational resource constraints\ndue to their prohibitive computational complexities. Therefore, we propose a\nnovel EfficientIML model with a lightweight, three-stage EfficientRWKV\nbackbone. EfficientRWKV's hybrid state-space and attention network captures\nglobal context and local details in parallel, while a multi-scale supervision\nstrategy enforces consistency across hierarchical predictions. Extensive\nevaluations on our dataset and standard benchmarks demonstrate that our\napproach outperforms ViT-based and other SOTA lightweight baselines in\nlocalization performance, FLOPs and inference speed, underscoring its\nsuitability for real-time forensic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With imaging devices delivering ever-higher resolutions and the emerging\ndiffusion-based forgery methods, current detectors trained only on traditional\ndatasets (with splicing, copy-moving and object removal forgeries) lack\nexposure to this new manipulation type. To address this, we propose a novel\nhigh-resolution SIF dataset of 1200+ diffusion-generated manipulations with\nsemantically extracted masks. However, this also imposes a challenge on\nexisting methods, as they face significant computational resource constraints\ndue to their prohibitive computational complexities. Therefore, we propose a\nnovel EfficientIML model with a lightweight, three-stage EfficientRWKV\nbackbone. EfficientRWKV's hybrid state-space and attention network captures\nglobal context and local details in parallel, while a multi-scale supervision\nstrategy enforces consistency across hierarchical predictions. Extensive\nevaluations on our dataset and standard benchmarks demonstrate that our\napproach outperforms ViT-based and other SOTA lightweight baselines in\nlocalization performance, FLOPs and inference speed, underscoring its\nsuitability for real-time forensic applications."
                },
                "authors": [
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Haoyang He"
                    },
                    {
                        "name": "Lei Xie"
                    },
                    {
                        "name": "Jiangning Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangning Zhang"
                },
                "author": "Jiangning Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08575v1",
                "updated": "2025-09-10T13:22:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    22,
                    58,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:22:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    22,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "SQLGovernor: An LLM-powered SQL Toolkit for Real World Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQLGovernor: An LLM-powered SQL Toolkit for Real World Application"
                },
                "summary": "SQL queries in real world analytical environments, whether written by humans\nor generated automatically often suffer from syntax errors, inefficiency, or\nsemantic misalignment, especially in complex OLAP scenarios. To address these\nchallenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies\nmultiple functionalities, including syntax correction, query rewriting, query\nmodification, and consistency verification within a structured framework\nenhanced by knowledge management. SQLGovernor introduces a fragment wise\nprocessing strategy to enable fine grained rewriting and localized error\ncorrection, significantly reducing the cognitive load on the LLM. It further\nincorporates a hybrid self learning mechanism guided by expert feedback,\nallowing the system to continuously improve through DBMS output analysis and\nrule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as\nwell as industrial datasets, show that SQLGovernor consistently boosts the\nperformance of base models by up to 10%, while minimizing reliance on manual\nexpertise. Deployed in production environments, SQLGovernor demonstrates strong\npractical utility and effective performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL queries in real world analytical environments, whether written by humans\nor generated automatically often suffer from syntax errors, inefficiency, or\nsemantic misalignment, especially in complex OLAP scenarios. To address these\nchallenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies\nmultiple functionalities, including syntax correction, query rewriting, query\nmodification, and consistency verification within a structured framework\nenhanced by knowledge management. SQLGovernor introduces a fragment wise\nprocessing strategy to enable fine grained rewriting and localized error\ncorrection, significantly reducing the cognitive load on the LLM. It further\nincorporates a hybrid self learning mechanism guided by expert feedback,\nallowing the system to continuously improve through DBMS output analysis and\nrule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as\nwell as industrial datasets, show that SQLGovernor consistently boosts the\nperformance of base models by up to 10%, while minimizing reliance on manual\nexpertise. Deployed in production environments, SQLGovernor demonstrates strong\npractical utility and effective performance."
                },
                "authors": [
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Siqi Shen"
                    },
                    {
                        "name": "Haining Xie"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Bo Qian"
                    },
                    {
                        "name": "Yinjun Wu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Peng Chen"
                },
                "author": "Peng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16523v2",
                "updated": "2025-09-10T13:22:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    22,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-23T10:04:21Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    10,
                    4,
                    21,
                    6,
                    54,
                    0
                ],
                "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation\n  in Machine Reading Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation\n  in Machine Reading Comprehension"
                },
                "summary": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data."
                },
                "authors": [
                    {
                        "name": "Yulong Wu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Riza Batista-Navarro"
                },
                "author": "Riza Batista-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11792v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11792v8",
                "updated": "2025-09-10T13:18:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    18,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2024-01-22T09:44:16Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    9,
                    44,
                    16,
                    0,
                    22,
                    0
                ],
                "title": "Efficient and Generalized end-to-end Autonomous Driving System with\n  Latent Deep Reinforcement Learning and Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Generalized end-to-end Autonomous Driving System with\n  Latent Deep Reinforcement Learning and Demonstrations"
                },
                "summary": "An intelligent driving system should dynamically formulate appropriate\ndriving strategies based on the current environment and vehicle status while\nensuring system security and reliability. However, methods based on\nreinforcement learning and imitation learning often suffer from high sample\ncomplexity, poor generalization, and low safety. To address these challenges,\nthis paper introduces an efficient and generalized end-to-end autonomous\ndriving system (EGADS) for complex and varied scenarios. The RL agent in our\nEGADS combines variational inference with normalizing flows, which are\nindependent of distribution assumptions. This combination allows the agent to\ncapture historical information relevant to driving in latent space effectively,\nthereby significantly reducing sample complexity. Additionally, we enhance\nsafety by formulating robust safety constraints and improve generalization and\nperformance by integrating RL with expert demonstrations. Experimental results\ndemonstrate that, compared to existing methods, EGADS significantly reduces\nsample complexity, greatly improves safety performance, and exhibits strong\ngeneralization capabilities in complex urban scenarios. Particularly, we\ncontributed an expert dataset collected through human expert steering wheel\ncontrol, specifically using the G29 steering wheel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An intelligent driving system should dynamically formulate appropriate\ndriving strategies based on the current environment and vehicle status while\nensuring system security and reliability. However, methods based on\nreinforcement learning and imitation learning often suffer from high sample\ncomplexity, poor generalization, and low safety. To address these challenges,\nthis paper introduces an efficient and generalized end-to-end autonomous\ndriving system (EGADS) for complex and varied scenarios. The RL agent in our\nEGADS combines variational inference with normalizing flows, which are\nindependent of distribution assumptions. This combination allows the agent to\ncapture historical information relevant to driving in latent space effectively,\nthereby significantly reducing sample complexity. Additionally, we enhance\nsafety by formulating robust safety constraints and improve generalization and\nperformance by integrating RL with expert demonstrations. Experimental results\ndemonstrate that, compared to existing methods, EGADS significantly reduces\nsample complexity, greatly improves safety performance, and exhibits strong\ngeneralization capabilities in complex urban scenarios. Particularly, we\ncontributed an expert dataset collected through human expert steering wheel\ncontrol, specifically using the G29 steering wheel."
                },
                "authors": [
                    {
                        "name": "Zuojin Tang"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Yongqiang Li"
                    },
                    {
                        "name": "Jianyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Chen"
                },
                "author": "Jianyu Chen",
                "arxiv_comment": "Accepted by ECML PKDD 2025 (Research Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11792v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11792v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08565v1",
                "updated": "2025-09-10T13:07:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    7,
                    33,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:07:33Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    7,
                    33,
                    2,
                    253,
                    0
                ],
                "title": "Inverse Clausius Thermodynamics in Run-and-Tumble Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse Clausius Thermodynamics in Run-and-Tumble Dynamics"
                },
                "summary": "We consider a one-dimensional run-and-tumble particle (RTP) confined by an\nexternal potential and coupled to a thermal reservoir. Starting from the\ncorresponding Fokker-Planck equation, we derive an explicit expression for the\nlocal entropy flux between the system and the heat bath. We then construct a\nthermodynamic representation of the RTP dynamics, modeling the system as an\noverdamped particle in a medium with a spatially inhomogeneous effective\ntemperature field, determined directly from the entropy flux. This forms the\nbasis of an Inverse Clausius Thermodynamics framework, in which thermodynamic\nquantities are inferred from entropy exchange with the heat bath rather than\npostulated. In addition to an exact expression for the entropy flux, the\nframework introduces a physically motivated approximation for evaluating the\nlocal entropy production rate. The approach is computationally efficient and\nbroadly applicable, and is particularly well suited for RTP models where\npropulsion velocities are redrawn from a continuous distribution at each\ntumbling event rather than restricted to discrete states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a one-dimensional run-and-tumble particle (RTP) confined by an\nexternal potential and coupled to a thermal reservoir. Starting from the\ncorresponding Fokker-Planck equation, we derive an explicit expression for the\nlocal entropy flux between the system and the heat bath. We then construct a\nthermodynamic representation of the RTP dynamics, modeling the system as an\noverdamped particle in a medium with a spatially inhomogeneous effective\ntemperature field, determined directly from the entropy flux. This forms the\nbasis of an Inverse Clausius Thermodynamics framework, in which thermodynamic\nquantities are inferred from entropy exchange with the heat bath rather than\npostulated. In addition to an exact expression for the entropy flux, the\nframework introduces a physically motivated approximation for evaluating the\nlocal entropy production rate. The approach is computationally efficient and\nbroadly applicable, and is particularly well suited for RTP models where\npropulsion velocities are redrawn from a continuous distribution at each\ntumbling event rather than restricted to discrete states."
                },
                "authors": [
                    {
                        "name": "Oded Farago"
                    }
                ],
                "author_detail": {
                    "name": "Oded Farago"
                },
                "author": "Oded Farago",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08541v1",
                "updated": "2025-09-10T12:40:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    40,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:40:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    40,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CM-Align: Consistency-based Multilingual Alignment for Large Language\n  Models"
                },
                "summary": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08538v2",
                "updated": "2025-09-11T11:14:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    14,
                    0,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T12:34:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    34,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models"
                },
                "summary": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos."
                },
                "authors": [
                    {
                        "name": "Garry Yang"
                    },
                    {
                        "name": "Zizhe Chen"
                    },
                    {
                        "name": "Man Hon Wong"
                    },
                    {
                        "name": "Haoyu Lei"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17668v2",
                "updated": "2025-09-10T12:25:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    25,
                    27,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-23T16:31:38Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    31,
                    38,
                    2,
                    204,
                    0
                ],
                "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Should We Meta-Learn Reinforcement Learning Algorithms?"
                },
                "summary": "The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible."
                },
                "authors": [
                    {
                        "name": "Alexander David Goldie"
                    },
                    {
                        "name": "Zilin Wang"
                    },
                    {
                        "name": "Jaron Cohen"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Shimon Whiteson"
                    }
                ],
                "author_detail": {
                    "name": "Shimon Whiteson"
                },
                "author": "Shimon Whiteson",
                "arxiv_comment": "Accepted paper at Reinforcement Learning Conference (RLC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08535v1",
                "updated": "2025-09-10T12:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    25,
                    13,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    25,
                    13,
                    2,
                    253,
                    0
                ],
                "title": "Agents of Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents of Discovery"
                },
                "summary": "The substantial data volumes encountered in modern particle physics and other\ndomains of fundamental physics research allow (and require) the use of\nincreasingly complex data analysis tools and workflows. While the use of\nmachine learning (ML) tools for data analysis has recently proliferated, these\ntools are typically special-purpose algorithms that rely, for example, on\nencoded physics knowledge to reach optimal performance. In this work, we\ninvestigate a new and orthogonal direction: Using recent progress in large\nlanguage models (LLMs) to create a team of agents -- instances of LLMs with\nspecific subtasks -- that jointly solve data analysis-based research problems\nin a way similar to how a human researcher might: by creating code to operate\nstandard tools and libraries (including ML systems) and by building on results\nof previous iterations. If successful, such agent-based systems could be\ndeployed to automate routine analysis components to counteract the increasing\ncomplexity of modern tool chains. To investigate the capabilities of\ncurrent-generation commercial LLMs, we consider the task of anomaly detection\nvia the publicly available and highly-studied LHC Olympics dataset. Several\ncurrent models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated\nand their stability tested. Overall, we observe the capacity of the agent-based\nsystem to solve this data analysis problem. The best agent-created solutions\nmirror the performance of human state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial data volumes encountered in modern particle physics and other\ndomains of fundamental physics research allow (and require) the use of\nincreasingly complex data analysis tools and workflows. While the use of\nmachine learning (ML) tools for data analysis has recently proliferated, these\ntools are typically special-purpose algorithms that rely, for example, on\nencoded physics knowledge to reach optimal performance. In this work, we\ninvestigate a new and orthogonal direction: Using recent progress in large\nlanguage models (LLMs) to create a team of agents -- instances of LLMs with\nspecific subtasks -- that jointly solve data analysis-based research problems\nin a way similar to how a human researcher might: by creating code to operate\nstandard tools and libraries (including ML systems) and by building on results\nof previous iterations. If successful, such agent-based systems could be\ndeployed to automate routine analysis components to counteract the increasing\ncomplexity of modern tool chains. To investigate the capabilities of\ncurrent-generation commercial LLMs, we consider the task of anomaly detection\nvia the publicly available and highly-studied LHC Olympics dataset. Several\ncurrent models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated\nand their stability tested. Overall, we observe the capacity of the agent-based\nsystem to solve this data analysis problem. The best agent-created solutions\nmirror the performance of human state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Sascha Diefenbacher"
                    },
                    {
                        "name": "Anna Hallin"
                    },
                    {
                        "name": "Gregor Kasieczka"
                    },
                    {
                        "name": "Michael Krmer"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Tim Lukas"
                    }
                ],
                "author_detail": {
                    "name": "Tim Lukas"
                },
                "author": "Tim Lukas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.07569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.07569v3",
                "updated": "2025-09-10T12:20:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    20,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2023-06-13T06:44:02Z",
                "published_parsed": [
                    2023,
                    6,
                    13,
                    6,
                    44,
                    2,
                    1,
                    164,
                    0
                ],
                "title": "Ontological Component-based Description of Robot Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontological Component-based Description of Robot Capabilities"
                },
                "summary": "A key aspect of a robot's knowledge base is self-awareness about what it is\ncapable of doing. It allows to define which tasks it can be assigned to and\nwhich it cannot. We will refer to this knowledge as the Capability concept. As\ncapabilities stems from the components the robot owns, they can be linked\ntogether. In this work, we hypothesize that this concept can be inferred from\nthe components rather than merely linked to them. Therefore, we introduce an\nontological means of inferring the agent's capabilities based on the components\nit owns as well as low-level capabilities. This inference allows the agent to\nacknowledge what it is able to do in a responsive way and it is generalizable\nto external entities the agent can carry for example. To initiate an action,\nthe robot needs to link its capabilities with external entities. To do so, it\nneeds to infer affordance relations from its capabilities as well as the\nexternal entity's dispositions. This work is part of a broader effort to\nintegrate social affordances into a Human-Robot collaboration context and is an\nextension of an already existing ontology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key aspect of a robot's knowledge base is self-awareness about what it is\ncapable of doing. It allows to define which tasks it can be assigned to and\nwhich it cannot. We will refer to this knowledge as the Capability concept. As\ncapabilities stems from the components the robot owns, they can be linked\ntogether. In this work, we hypothesize that this concept can be inferred from\nthe components rather than merely linked to them. Therefore, we introduce an\nontological means of inferring the agent's capabilities based on the components\nit owns as well as low-level capabilities. This inference allows the agent to\nacknowledge what it is able to do in a responsive way and it is generalizable\nto external entities the agent can carry for example. To initiate an action,\nthe robot needs to link its capabilities with external entities. To do so, it\nneeds to infer affordance relations from its capabilities as well as the\nexternal entity's dispositions. This work is part of a broader effort to\nintegrate social affordances into a Human-Robot collaboration context and is an\nextension of an already existing ontology."
                },
                "authors": [
                    {
                        "name": "Bastien Dussard"
                    },
                    {
                        "name": "Guillaume Sarthou"
                    },
                    {
                        "name": "Aurlie Clodic"
                    }
                ],
                "author_detail": {
                    "name": "Aurlie Clodic"
                },
                "arxiv_affiliation": "LAAS-IDEA, LAAS-RIS",
                "author": "Aurlie Clodic",
                "arxiv_comment": "International Workshop on Working towards Ontology-based Standards\n  for Robotics and Automation (WOSRA 2023 - 2nd Edition), Jun 2023, Londres,\n  United Kingdom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.07569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.07569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05929v2",
                "updated": "2025-09-10T12:09:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    9,
                    14,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-08T01:40:10Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    1,
                    40,
                    10,
                    4,
                    220,
                    0
                ],
                "title": "Towards Reliable Generative AI-Driven Scaffolding: Reducing\n  Hallucinations and Enhancing Quality in Self-Regulated Learning Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Generative AI-Driven Scaffolding: Reducing\n  Hallucinations and Enhancing Quality in Self-Regulated Learning Support"
                },
                "summary": "Generative Artificial Intelligence (GenAI) holds a potential to advance\nexisting educational technologies with capabilities to automatically generate\npersonalised scaffolds that support students' self-regulated learning (SRL).\nWhile advancements in large language models (LLMs) promise improvements in the\nadaptability and quality of educational technologies for SRL, there remain\nconcerns about the hallucinations in content generated by LLMs, which can\ncompromise both the learning experience and ethical standards. To address these\nchallenges, we proposed GenAI-enabled approaches for evaluating personalised\nSRL scaffolds before they are presented to students, aiming for reducing\nhallucinations and improving the overall quality of LLM-generated personalised\nscaffolds. Specifically, two approaches are investigated. The first approach\ninvolved developing a multi-agent system approach for reliability evaluation to\nassess the extent to which LLM-generated scaffolds accurately target relevant\nSRL processes. The second approach utilised the \"LLM-as-a-Judge\" technique for\nquality evaluation that evaluates LLM-generated scaffolds for their helpfulness\nin supporting students. We constructed evaluation datasets, and compared our\nresults with single-agent LLM systems and machine learning approach baselines.\nOur findings indicate that the reliability evaluation approach is highly\neffective and outperforms the baselines, showing almost perfect alignment with\nhuman experts' evaluations. Moreover, both proposed evaluation approaches can\nbe harnessed to effectively reduce hallucinations. Additionally, we identified\nand discussed bias limitations of the \"LLM-as-a-Judge\" technique in evaluating\nLLM-generated scaffolds. We suggest incorporating these approaches into\nGenAI-powered personalised SRL scaffolding systems to mitigate hallucination\nissues and improve the overall scaffolding quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) holds a potential to advance\nexisting educational technologies with capabilities to automatically generate\npersonalised scaffolds that support students' self-regulated learning (SRL).\nWhile advancements in large language models (LLMs) promise improvements in the\nadaptability and quality of educational technologies for SRL, there remain\nconcerns about the hallucinations in content generated by LLMs, which can\ncompromise both the learning experience and ethical standards. To address these\nchallenges, we proposed GenAI-enabled approaches for evaluating personalised\nSRL scaffolds before they are presented to students, aiming for reducing\nhallucinations and improving the overall quality of LLM-generated personalised\nscaffolds. Specifically, two approaches are investigated. The first approach\ninvolved developing a multi-agent system approach for reliability evaluation to\nassess the extent to which LLM-generated scaffolds accurately target relevant\nSRL processes. The second approach utilised the \"LLM-as-a-Judge\" technique for\nquality evaluation that evaluates LLM-generated scaffolds for their helpfulness\nin supporting students. We constructed evaluation datasets, and compared our\nresults with single-agent LLM systems and machine learning approach baselines.\nOur findings indicate that the reliability evaluation approach is highly\neffective and outperforms the baselines, showing almost perfect alignment with\nhuman experts' evaluations. Moreover, both proposed evaluation approaches can\nbe harnessed to effectively reduce hallucinations. Additionally, we identified\nand discussed bias limitations of the \"LLM-as-a-Judge\" technique in evaluating\nLLM-generated scaffolds. We suggest incorporating these approaches into\nGenAI-powered personalised SRL scaffolding systems to mitigate hallucination\nissues and improve the overall scaffolding quality."
                },
                "authors": [
                    {
                        "name": "Keyang Qian"
                    },
                    {
                        "name": "Shiqi Liu"
                    },
                    {
                        "name": "Tongguang Li"
                    },
                    {
                        "name": "Mladen Rakovi"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Rui Guan"
                    },
                    {
                        "name": "Inge Molenaar"
                    },
                    {
                        "name": "Sadia Nawaz"
                    },
                    {
                        "name": "Zachari Swiecki"
                    },
                    {
                        "name": "Lixiang Yan"
                    },
                    {
                        "name": "Dragan Gaevi"
                    }
                ],
                "author_detail": {
                    "name": "Dragan Gaevi"
                },
                "author": "Dragan Gaevi",
                "arxiv_doi": "10.1016/j.compedu.2025.105448",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compedu.2025.105448",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computers & Education, Volume 240, 2026, 105448",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08524v1",
                "updated": "2025-09-10T12:08:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    8,
                    59,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:08:59Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    8,
                    59,
                    2,
                    253,
                    0
                ],
                "title": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoStub: Genetic Programming-Based Stub Creation for Symbolic Execution"
                },
                "summary": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic execution is a powerful technique for software testing, but suffers\nfrom limitations when encountering external functions, such as native methods\nor third-party libraries. Existing solutions often require additional context,\nexpensive SMT solvers, or manual intervention to approximate these functions\nthrough symbolic stubs. In this work, we propose a novel approach to\nautomatically generate symbolic stubs for external functions during symbolic\nexecution that leverages Genetic Programming. When the symbolic executor\nencounters an external function, AutoStub generates training data by executing\nthe function on randomly generated inputs and collecting the outputs. Genetic\nProgramming then derives expressions that approximate the behavior of the\nfunction, serving as symbolic stubs. These automatically generated stubs allow\nthe symbolic executor to continue the analysis without manual intervention,\nenabling the exploration of program paths that were previously intractable. We\ndemonstrate that AutoStub can automatically approximate external functions with\nover 90% accuracy for 55% of the functions evaluated, and can infer\nlanguage-specific behaviors that reveal edge cases crucial for software\ntesting."
                },
                "authors": [
                    {
                        "name": "Felix Mchtle"
                    },
                    {
                        "name": "Nils Loose"
                    },
                    {
                        "name": "Jan-Niclas Serr"
                    },
                    {
                        "name": "Jonas Sander"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_comment": "2025 HUMIES finalist",
                "arxiv_journal_ref": "18th ACM/IEEE International Workshop on Search-Based and Fuzz\n  Testing, SBFT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08522v1",
                "updated": "2025-09-10T12:00:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    0,
                    21,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:00:21Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    0,
                    21,
                    2,
                    253,
                    0
                ],
                "title": "RoboMatch: A Mobile-Manipulation Teleoperation Platform with\n  Auto-Matching Network Architecture for Long-Horizon Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboMatch: A Mobile-Manipulation Teleoperation Platform with\n  Auto-Matching Network Architecture for Long-Horizon Manipulation"
                },
                "summary": "This paper presents RoboMatch, a novel unified teleoperation platform for\nmobile manipulation with an auto-matching network architecture, designed to\ntackle long-horizon tasks in dynamic environments. Our system enhances\nteleoperation performance, data collection efficiency, task accuracy, and\noperational stability. The core of RoboMatch is a cockpit-style control\ninterface that enables synchronous operation of the mobile base and dual arms,\nsignificantly improving control precision and data collection. Moreover, we\nintroduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which\nleverages Discrete Wavelet Transform (DWT) for multi-scale visual feature\nextraction and integrates high-precision IMUs at the end-effector to enrich\nproprioceptive feedback, substantially boosting fine manipulation performance.\nFurthermore, we propose an Auto-Matching Network (AMN) architecture that\ndecomposes long-horizon tasks into logical sequences and dynamically assigns\nlightweight pre-trained models for distributed inference. Experimental results\ndemonstrate that our approach improves data collection efficiency by over 20%,\nincreases task success rates by 20-30% with PVE-DP, and enhances long-horizon\ninference performance by approximately 40% with AMN, offering a robust solution\nfor complex manipulation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents RoboMatch, a novel unified teleoperation platform for\nmobile manipulation with an auto-matching network architecture, designed to\ntackle long-horizon tasks in dynamic environments. Our system enhances\nteleoperation performance, data collection efficiency, task accuracy, and\noperational stability. The core of RoboMatch is a cockpit-style control\ninterface that enables synchronous operation of the mobile base and dual arms,\nsignificantly improving control precision and data collection. Moreover, we\nintroduce the Proprioceptive-Visual Enhanced Diffusion Policy (PVE-DP), which\nleverages Discrete Wavelet Transform (DWT) for multi-scale visual feature\nextraction and integrates high-precision IMUs at the end-effector to enrich\nproprioceptive feedback, substantially boosting fine manipulation performance.\nFurthermore, we propose an Auto-Matching Network (AMN) architecture that\ndecomposes long-horizon tasks into logical sequences and dynamically assigns\nlightweight pre-trained models for distributed inference. Experimental results\ndemonstrate that our approach improves data collection efficiency by over 20%,\nincreases task success rates by 20-30% with PVE-DP, and enhances long-horizon\ninference performance by approximately 40% with AMN, offering a robust solution\nfor complex manipulation tasks."
                },
                "authors": [
                    {
                        "name": "Hanyu Liu"
                    },
                    {
                        "name": "Yunsheng Ma"
                    },
                    {
                        "name": "Jiaxin Huang"
                    },
                    {
                        "name": "Keqiang Ren"
                    },
                    {
                        "name": "Jiayi Wen"
                    },
                    {
                        "name": "Yilin Zheng"
                    },
                    {
                        "name": "Baishu Wan"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Jiejun Hou"
                    },
                    {
                        "name": "Haoru Luan"
                    },
                    {
                        "name": "Zhihua Wang"
                    },
                    {
                        "name": "Zhigong Song"
                    }
                ],
                "author_detail": {
                    "name": "Zhigong Song"
                },
                "author": "Zhigong Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11171v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11171v4",
                "updated": "2025-09-10T11:56:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    56,
                    52,
                    2,
                    253,
                    0
                ],
                "published": "2025-04-15T13:17:39Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    13,
                    17,
                    39,
                    1,
                    105,
                    0
                ],
                "title": "TerraMind: Large-Scale Generative Multimodality for Earth Observation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TerraMind: Large-Scale Generative Multimodality for Earth Observation"
                },
                "summary": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TerraMind, the first any-to-any generative, multimodal foundation\nmodel for Earth observation (EO). Unlike other multimodal models, TerraMind is\npretrained on dual-scale representations combining both token-level and\npixel-level data across modalities. On a token level, TerraMind encodes\nhigh-level contextual information to learn cross-modal relationships, while on\na pixel level, TerraMind leverages fine-grained representations to capture\ncritical spatial nuances. We pretrained TerraMind on nine geospatial modalities\nof a global, large-scale dataset. In this paper, we demonstrate that (i)\nTerraMind's dual-scale early fusion approach unlocks a range of zero-shot and\nfew-shot applications for Earth observation, (ii) TerraMind introduces\n\"Thinking-in-Modalities\" (TiM) -- the capability of generating additional\nartificial data during finetuning and inference to improve the model output --\nand (iii) TerraMind achieves beyond state-of-the-art performance in\ncommunity-standard benchmarks for EO like PANGAEA. The pretraining dataset, the\nmodel weights, and our code are open-sourced under a permissive license."
                },
                "authors": [
                    {
                        "name": "Johannes Jakubik"
                    },
                    {
                        "name": "Felix Yang"
                    },
                    {
                        "name": "Benedikt Blumenstiel"
                    },
                    {
                        "name": "Erik Scheurer"
                    },
                    {
                        "name": "Rocco Sedona"
                    },
                    {
                        "name": "Stefano Maurogiovanni"
                    },
                    {
                        "name": "Jente Bosmans"
                    },
                    {
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "name": "Valerio Marsocci"
                    },
                    {
                        "name": "Niklas Kopp"
                    },
                    {
                        "name": "Rahul Ramachandran"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Thomas Brunschwiler"
                    },
                    {
                        "name": "Gabriele Cavallaro"
                    },
                    {
                        "name": "Juan Bernabe-Moreno"
                    },
                    {
                        "name": "Nicolas Longp"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Longp"
                },
                "author": "Nicolas Longp",
                "arxiv_comment": "Accepted at ICCV'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11171v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11171v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08519v1",
                "updated": "2025-09-10T11:54:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    54,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:54:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    54,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HuMo: Human-Centric Video Generation via Collaborative Multi-Modal\n  Conditioning"
                },
                "summary": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos\nfrom multimodal inputs, including text, image, and audio. Existing methods\nstruggle to effectively coordinate these heterogeneous modalities due to two\nchallenges: the scarcity of training data with paired triplet conditions and\nthe difficulty of collaborating the sub-tasks of subject preservation and\naudio-visual sync with multimodal inputs. In this work, we present HuMo, a\nunified HCVG framework for collaborative multimodal control. For the first\nchallenge, we construct a high-quality dataset with diverse and paired text,\nreference images, and audio. For the second challenge, we propose a two-stage\nprogressive multimodal training paradigm with task-specific strategies. For the\nsubject preservation task, to maintain the prompt following and visual\ngeneration abilities of the foundation model, we adopt the minimal-invasive\nimage injection strategy. For the audio-visual sync task, besides the commonly\nadopted audio cross-attention layer, we propose a focus-by-predicting strategy\nthat implicitly guides the model to associate audio with facial regions. For\njoint learning of controllabilities across multimodal inputs, building on\npreviously acquired capabilities, we progressively incorporate the audio-visual\nsync task. During inference, for flexible and fine-grained multimodal control,\nwe design a time-adaptive Classifier-Free Guidance strategy that dynamically\nadjusts guidance weights across denoising steps. Extensive experimental results\ndemonstrate that HuMo surpasses specialized state-of-the-art methods in\nsub-tasks, establishing a unified framework for collaborative\nmultimodal-conditioned HCVG. Project Page:\nhttps://phantom-video.github.io/HuMo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Centric Video Generation (HCVG) methods seek to synthesize human videos\nfrom multimodal inputs, including text, image, and audio. Existing methods\nstruggle to effectively coordinate these heterogeneous modalities due to two\nchallenges: the scarcity of training data with paired triplet conditions and\nthe difficulty of collaborating the sub-tasks of subject preservation and\naudio-visual sync with multimodal inputs. In this work, we present HuMo, a\nunified HCVG framework for collaborative multimodal control. For the first\nchallenge, we construct a high-quality dataset with diverse and paired text,\nreference images, and audio. For the second challenge, we propose a two-stage\nprogressive multimodal training paradigm with task-specific strategies. For the\nsubject preservation task, to maintain the prompt following and visual\ngeneration abilities of the foundation model, we adopt the minimal-invasive\nimage injection strategy. For the audio-visual sync task, besides the commonly\nadopted audio cross-attention layer, we propose a focus-by-predicting strategy\nthat implicitly guides the model to associate audio with facial regions. For\njoint learning of controllabilities across multimodal inputs, building on\npreviously acquired capabilities, we progressively incorporate the audio-visual\nsync task. During inference, for flexible and fine-grained multimodal control,\nwe design a time-adaptive Classifier-Free Guidance strategy that dynamically\nadjusts guidance weights across denoising steps. Extensive experimental results\ndemonstrate that HuMo surpasses specialized state-of-the-art methods in\nsub-tasks, establishing a unified framework for collaborative\nmultimodal-conditioned HCVG. Project Page:\nhttps://phantom-video.github.io/HuMo."
                },
                "authors": [
                    {
                        "name": "Liyang Chen"
                    },
                    {
                        "name": "Tianxiang Ma"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Bingchuan Li"
                    },
                    {
                        "name": "Zhuowei Chen"
                    },
                    {
                        "name": "Lijie Liu"
                    },
                    {
                        "name": "Xu He"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16654v3",
                "updated": "2025-09-10T11:47:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    47,
                    41,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-20T05:41:22Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    5,
                    41,
                    22,
                    2,
                    232,
                    0
                ],
                "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning"
                },
                "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL)."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Minghao Zhang"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08516v1",
                "updated": "2025-09-10T11:47:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    47,
                    27,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:47:27Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    47,
                    27,
                    2,
                    253,
                    0
                ],
                "title": "Feedback That Clicks: Introductory Physics Students' Valued Features in\n  AI Feedback Generated From Self-Crafted and Engineered Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback That Clicks: Introductory Physics Students' Valued Features in\n  AI Feedback Generated From Self-Crafted and Engineered Prompts"
                },
                "summary": "Since the advent of GPT-3.5 in 2022, Generative Artificial Intelligence (AI)\nhas shown tremendous potential in STEM education, particularly in providing\nreal-time, customized feedback to students in large-enrollment courses. A\ncrucial skill that mediates effective use of AI is the systematic structuring\nof natural language instructions to AI models, commonly referred to as prompt\nengineering. This study has three objectives: (i) to investigate the\nsophistication of student-generated prompts when seeking feedback from AI on\ntheir arguments, (ii) to examine the features that students value in\nAI-generated feedback, and (iii) to analyze trends in student preferences for\nfeedback generated from self-crafted prompts versus prompts incorporating\nprompt engineering techniques and principles of effective feedback. Results\nindicate that student-generated prompts typically reflect only a subset of\nfoundational prompt engineering techniques. Despite this lack of\nsophistication, such as incomplete descriptions of task context, AI responses\ndemonstrated contextual intuitiveness by accurately inferring context from the\noverall content of the prompt. We also identified 12 distinct features that\nstudents attribute the usefulness of AI-generated feedback, spanning four\nbroader themes: Evaluation, Content, Presentation, and Depth. Finally, results\nshow that students overwhelmingly prefer feedback generated from structured\nprompts, particularly those combining prompt engineering techniques with\nprinciples of effective feedback. Implications of these results such as\nintegrating the principles of effective feedback in design and delivery of\nfeedback through AI systems, and incorporating prompt engineering in\nintroductory physics courses are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the advent of GPT-3.5 in 2022, Generative Artificial Intelligence (AI)\nhas shown tremendous potential in STEM education, particularly in providing\nreal-time, customized feedback to students in large-enrollment courses. A\ncrucial skill that mediates effective use of AI is the systematic structuring\nof natural language instructions to AI models, commonly referred to as prompt\nengineering. This study has three objectives: (i) to investigate the\nsophistication of student-generated prompts when seeking feedback from AI on\ntheir arguments, (ii) to examine the features that students value in\nAI-generated feedback, and (iii) to analyze trends in student preferences for\nfeedback generated from self-crafted prompts versus prompts incorporating\nprompt engineering techniques and principles of effective feedback. Results\nindicate that student-generated prompts typically reflect only a subset of\nfoundational prompt engineering techniques. Despite this lack of\nsophistication, such as incomplete descriptions of task context, AI responses\ndemonstrated contextual intuitiveness by accurately inferring context from the\noverall content of the prompt. We also identified 12 distinct features that\nstudents attribute the usefulness of AI-generated feedback, spanning four\nbroader themes: Evaluation, Content, Presentation, and Depth. Finally, results\nshow that students overwhelmingly prefer feedback generated from structured\nprompts, particularly those combining prompt engineering techniques with\nprinciples of effective feedback. Implications of these results such as\nintegrating the principles of effective feedback in design and delivery of\nfeedback through AI systems, and incorporating prompt engineering in\nintroductory physics courses are discussed."
                },
                "authors": [
                    {
                        "name": "Amogh Sirnoorkar"
                    },
                    {
                        "name": "N. Sanjay Rebello"
                    }
                ],
                "author_detail": {
                    "name": "N. Sanjay Rebello"
                },
                "author": "N. Sanjay Rebello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08515v1",
                "updated": "2025-09-10T11:45:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    45,
                    40,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:45:40Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    45,
                    40,
                    2,
                    253,
                    0
                ],
                "title": "Variational Rank Reduction Autoencoders for Generative",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Rank Reduction Autoencoders for Generative"
                },
                "summary": "Generative thermal design for complex geometries is fundamental in many areas\nof engineering, yet it faces two main challenges: the high computational cost\nof high-fidelity simulations and the limitations of conventional generative\nmodels. Approaches such as autoencoders (AEs) and variational autoencoders\n(VAEs) often produce unstructured latent spaces with discontinuities, which\nrestricts their capacity to explore designs and generate physically consistent\nsolutions.\n  To address these limitations, we propose a hybrid framework that combines\nVariational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks\n(DeepONets). The VRRAE introduces a truncated SVD within the latent space,\nleading to continuous, interpretable, and well-structured representations that\nmitigate posterior collapse and improve geometric reconstruction. The DeepONet\nthen exploits this compact latent encoding in its branch network, together with\nspatial coordinates in the trunk network, to predict temperature gradients\nefficiently and accurately.\n  This hybrid approach not only enhances the quality of generated geometries\nand the accuracy of gradient prediction, but also provides a substantial\nadvantage in inference efficiency compared to traditional numerical solvers.\nOverall, the study underscores the importance of structured latent\nrepresentations for operator learning and highlights the potential of combining\ngenerative models and operator networks in thermal design and broader\nengineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative thermal design for complex geometries is fundamental in many areas\nof engineering, yet it faces two main challenges: the high computational cost\nof high-fidelity simulations and the limitations of conventional generative\nmodels. Approaches such as autoencoders (AEs) and variational autoencoders\n(VAEs) often produce unstructured latent spaces with discontinuities, which\nrestricts their capacity to explore designs and generate physically consistent\nsolutions.\n  To address these limitations, we propose a hybrid framework that combines\nVariational Rank-Reduction Autoencoders (VRRAEs) with Deep Operator Networks\n(DeepONets). The VRRAE introduces a truncated SVD within the latent space,\nleading to continuous, interpretable, and well-structured representations that\nmitigate posterior collapse and improve geometric reconstruction. The DeepONet\nthen exploits this compact latent encoding in its branch network, together with\nspatial coordinates in the trunk network, to predict temperature gradients\nefficiently and accurately.\n  This hybrid approach not only enhances the quality of generated geometries\nand the accuracy of gradient prediction, but also provides a substantial\nadvantage in inference efficiency compared to traditional numerical solvers.\nOverall, the study underscores the importance of structured latent\nrepresentations for operator learning and highlights the potential of combining\ngenerative models and operator networks in thermal design and broader\nengineering applications."
                },
                "authors": [
                    {
                        "name": "Alicia Tierz"
                    },
                    {
                        "name": "Jad Mounayer"
                    },
                    {
                        "name": "Beatriz Moya"
                    },
                    {
                        "name": "Francisco Chinesta"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Chinesta"
                },
                "author": "Francisco Chinesta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08494v1",
                "updated": "2025-09-10T11:10:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    10,
                    10,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:10:10Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    10,
                    10,
                    2,
                    253,
                    0
                ],
                "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants"
                },
                "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets."
                },
                "authors": [
                    {
                        "name": "Benjamin Sturgeon"
                    },
                    {
                        "name": "Daniel Samuelson"
                    },
                    {
                        "name": "Jacob Haimes"
                    },
                    {
                        "name": "Jacy Reese Anthis"
                    }
                ],
                "author_detail": {
                    "name": "Jacy Reese Anthis"
                },
                "author": "Jacy Reese Anthis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08493v1",
                "updated": "2025-09-10T11:08:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    8,
                    52,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:08:52Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    8,
                    52,
                    2,
                    253,
                    0
                ],
                "title": "Send to which account? Evaluation of an LLM-based Scambaiting System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Send to which account? Evaluation of an LLM-based Scambaiting System"
                },
                "summary": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems."
                },
                "authors": [
                    {
                        "name": "Hossein Siadati"
                    },
                    {
                        "name": "Haadi Jafarian"
                    },
                    {
                        "name": "Sima Jafarikhah"
                    }
                ],
                "author_detail": {
                    "name": "Sima Jafarikhah"
                },
                "author": "Sima Jafarikhah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07894v2",
                "updated": "2025-09-10T11:05:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    5,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-09T16:24:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    24,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?"
                },
                "summary": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO."
                },
                "authors": [
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Fujun Han"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Ruilizhen Hu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21749v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21749v2",
                "updated": "2025-09-10T11:03:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    3,
                    43,
                    2,
                    253,
                    0
                ],
                "published": "2024-07-31T17:06:25Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    17,
                    6,
                    25,
                    2,
                    213,
                    0
                ],
                "title": "Ge-based Clinopyroxene series: first principles and experimental local\n  probe study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ge-based Clinopyroxene series: first principles and experimental local\n  probe study"
                },
                "summary": "The structural and electronic properties of the CaMnGe$_2$O$_6$ and\nSrMnGe$_2$O$_6$ clinopyroxene systems have been investigated by means of\nperturbed angular correlation (PAC) measurements, performed at ISOLDE, combined\nwith $ab-initio$ electronic structure calculations within the density\nfunctional theory (DFT) framework. The partial density of states (PDOS) of the\nCaMnGe$_2$O$_6$ and SrMnGe$_2$O$_6$ stable compounds has been determined, and\nit has been observed that the requirement of including an on-site Hubbard-$U$\npotential was necessary in order to describe the highly correlated Mn\n$3d$-states. By considering $U_{eff}$=4 eV, we obtained a band gap width of\n1.82 eV and 1.70 eV, for the CaMnGe$_2$O$_6$ and SrMnGe$_2$O$_6$, respectively.\nCombining electric field gradient (EFG) first principles calculations, using a\nsupercell scheme, with experimental PAC results, we were able to infer that the\nCd probe can replace either the $A$ (Ca, Sr) or the Mn sites in the crystalline\nstructures. We also showed that Cd substitution is expected to lead to a\nreduction in the width of the band gap in these systems, evidencing\nopportunities for potential band-gap engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structural and electronic properties of the CaMnGe$_2$O$_6$ and\nSrMnGe$_2$O$_6$ clinopyroxene systems have been investigated by means of\nperturbed angular correlation (PAC) measurements, performed at ISOLDE, combined\nwith $ab-initio$ electronic structure calculations within the density\nfunctional theory (DFT) framework. The partial density of states (PDOS) of the\nCaMnGe$_2$O$_6$ and SrMnGe$_2$O$_6$ stable compounds has been determined, and\nit has been observed that the requirement of including an on-site Hubbard-$U$\npotential was necessary in order to describe the highly correlated Mn\n$3d$-states. By considering $U_{eff}$=4 eV, we obtained a band gap width of\n1.82 eV and 1.70 eV, for the CaMnGe$_2$O$_6$ and SrMnGe$_2$O$_6$, respectively.\nCombining electric field gradient (EFG) first principles calculations, using a\nsupercell scheme, with experimental PAC results, we were able to infer that the\nCd probe can replace either the $A$ (Ca, Sr) or the Mn sites in the crystalline\nstructures. We also showed that Cd substitution is expected to lead to a\nreduction in the width of the band gap in these systems, evidencing\nopportunities for potential band-gap engineering."
                },
                "authors": [
                    {
                        "name": "Ricardo P. Moreira"
                    },
                    {
                        "name": "E. Lora da Silva"
                    },
                    {
                        "name": "Gonalo N. P. Oliveira"
                    },
                    {
                        "name": "Pedro Rocha-Rodrigues"
                    },
                    {
                        "name": "Alessandro Stroppa"
                    },
                    {
                        "name": "Claire V. Colin"
                    },
                    {
                        "name": "Cline Darie"
                    },
                    {
                        "name": "Joo G. Correia"
                    },
                    {
                        "name": "Lucy V. C. Assali"
                    },
                    {
                        "name": "Helena M. Petrilli"
                    },
                    {
                        "name": "Armandina M. L. Lopes"
                    },
                    {
                        "name": "Joo P. Arajo"
                    }
                ],
                "author_detail": {
                    "name": "Joo P. Arajo"
                },
                "author": "Joo P. Arajo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21749v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21749v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08486v1",
                "updated": "2025-09-10T10:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    51,
                    47,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:51:47Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    51,
                    47,
                    2,
                    253,
                    0
                ],
                "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Too Helpful, Too Harmless, Too Honest or Just Right?"
                },
                "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones."
                },
                "authors": [
                    {
                        "name": "Gautam Siddharth Kashyap"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "EMNLP'25 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08484v1",
                "updated": "2025-09-10T10:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    49,
                    21,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    49,
                    21,
                    2,
                    253,
                    0
                ],
                "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in\n  LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in\n  LLM-Generated Text"
                },
                "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group."
                },
                "authors": [
                    {
                        "name": "Pia Sommerauer"
                    },
                    {
                        "name": "Giulia Rambelli"
                    },
                    {
                        "name": "Tommaso Caselli"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Caselli"
                },
                "author": "Tommaso Caselli",
                "arxiv_comment": "Accepted to EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08480v1",
                "updated": "2025-09-10T10:39:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    39,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:39:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    39,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Acquiescence Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiescence Bias in Large Language Models"
                },
                "summary": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement."
                },
                "authors": [
                    {
                        "name": "Daniel Braun"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Braun"
                },
                "author": "Daniel Braun",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21117v3",
                "updated": "2025-09-10T10:32:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    32,
                    57,
                    2,
                    253,
                    0
                ],
                "published": "2025-04-29T18:56:12Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    56,
                    12,
                    1,
                    119,
                    0
                ],
                "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts"
                },
                "summary": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."
                },
                "authors": [
                    {
                        "name": "Hanhua Hong"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yiqi Liu"
                    },
                    {
                        "name": "Wenge Rong"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "11 pages, accepted by Transactions of the Association for\n  Computational Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08395v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08395v3",
                "updated": "2025-09-10T10:08:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    8,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-12T13:37:03Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance"
                },
                "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs manifest in real user interactions, making it difficult to\naddress the risks from biased LLMs. Therefore, we create IssueBench: a set of\n2.49m realistic English-language prompts to measure issue bias in LLM writing\nassistance, which we construct based on 3.9k templates (e.g. \"write a blog\nabout\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in 10 state-of-the-art LLMs. We also show that biases are very\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs manifest in real user interactions, making it difficult to\naddress the risks from biased LLMs. Therefore, we create IssueBench: a set of\n2.49m realistic English-language prompts to measure issue bias in LLM writing\nassistance, which we construct based on 3.9k templates (e.g. \"write a blog\nabout\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in 10 state-of-the-art LLMs. We also show that biases are very\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."
                },
                "authors": [
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "accepted at TACL (pre-MIT Press publication version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08395v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08395v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08461v2",
                "updated": "2025-09-11T13:03:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    3,
                    4,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T10:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    7,
                    27,
                    2,
                    253,
                    0
                ],
                "title": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics."
                },
                "authors": [
                    {
                        "name": "Dikshant Sagar"
                    },
                    {
                        "name": "Kaiwen Yu"
                    },
                    {
                        "name": "Alejandro Yankelevich"
                    },
                    {
                        "name": "Jianming Bian"
                    },
                    {
                        "name": "Pierre Baldi"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Baldi"
                },
                "author": "Pierre Baldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15108v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15108v3",
                "updated": "2025-09-10T09:57:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    57,
                    3,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-19T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    5,
                    42,
                    2,
                    78,
                    0
                ],
                "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making"
                },
                "summary": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clemence Grislain"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15108v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13107v2",
                "updated": "2025-09-10T09:50:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    50,
                    51,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-18T17:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    14,
                    3,
                    0,
                    230,
                    0
                ],
                "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All for law and law for all: Adaptive RAG Pipeline for Legal Research"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has transformed how we approach text\ngeneration tasks by grounding Large Language Model (LLM) outputs in retrieved\nknowledge. This capability is especially critical in the legal domain. In this\nwork, we introduce a novel end-to-end RAG pipeline that improves upon previous\nbaselines using three targeted enhancements: (i) a context-aware query\ntranslator that disentangles document references from natural-language\nquestions and adapts retrieval depth and response style based on expertise and\nspecificity, (ii) open-source retrieval strategies using SBERT and GTE\nembeddings that achieve substantial performance gains while remaining\ncost-efficient, and (iii) a comprehensive evaluation and generation framework\nthat combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic\nalignment and faithfulness across models and prompt designs. Our results show\nthat carefully designed open-source pipelines can rival proprietary approaches\nin retrieval quality, while a custom legal-grounded prompt consistently\nproduces more faithful and contextually relevant answers than baseline\nprompting. Taken together, these contributions demonstrate the potential of\ntask-aware, component-level tuning to deliver legally grounded, reproducible,\nand cost-effective RAG systems for legal research assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has transformed how we approach text\ngeneration tasks by grounding Large Language Model (LLM) outputs in retrieved\nknowledge. This capability is especially critical in the legal domain. In this\nwork, we introduce a novel end-to-end RAG pipeline that improves upon previous\nbaselines using three targeted enhancements: (i) a context-aware query\ntranslator that disentangles document references from natural-language\nquestions and adapts retrieval depth and response style based on expertise and\nspecificity, (ii) open-source retrieval strategies using SBERT and GTE\nembeddings that achieve substantial performance gains while remaining\ncost-efficient, and (iii) a comprehensive evaluation and generation framework\nthat combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic\nalignment and faithfulness across models and prompt designs. Our results show\nthat carefully designed open-source pipelines can rival proprietary approaches\nin retrieval quality, while a custom legal-grounded prompt consistently\nproduces more faithful and contextually relevant answers than baseline\nprompting. Taken together, these contributions demonstrate the potential of\ntask-aware, component-level tuning to deliver legally grounded, reproducible,\nand cost-effective RAG systems for legal research assistance."
                },
                "authors": [
                    {
                        "name": "Figarri Keisha"
                    },
                    {
                        "name": "Prince Singh"
                    },
                    {
                        "name": "Pallavi"
                    },
                    {
                        "name": "Dion Fernandes"
                    },
                    {
                        "name": "Aravindh Manivannan"
                    },
                    {
                        "name": "Ilham Wicaksono"
                    },
                    {
                        "name": "Faisal Ahmad"
                    },
                    {
                        "name": "Wiem Ben Rim"
                    }
                ],
                "author_detail": {
                    "name": "Wiem Ben Rim"
                },
                "author": "Wiem Ben Rim",
                "arxiv_comment": "submitted to NLLP 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08438v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08438v1",
                "updated": "2025-09-10T09:35:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    35,
                    43,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:35:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    35,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction\n  with a New Dataset and Multi-Order Generative Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction\n  with a New Dataset and Multi-Order Generative Framework"
                },
                "summary": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe."
                },
                "authors": [
                    {
                        "name": "Jinzhong Ning"
                    },
                    {
                        "name": "Paerhati Tulajiang"
                    },
                    {
                        "name": "Yingying Le"
                    },
                    {
                        "name": "Yijia Zhang"
                    },
                    {
                        "name": "Yuanyuan Sun"
                    },
                    {
                        "name": "Hongfei Lin"
                    },
                    {
                        "name": "Haifeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Liu"
                },
                "author": "Haifeng Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08438v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08438v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v3",
                "updated": "2025-09-11T09:37:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    37,
                    46,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21481v2",
                "updated": "2025-09-10T09:19:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    19,
                    54,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-29T10:06:03Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    6,
                    3,
                    4,
                    241,
                    0
                ],
                "title": "Mixed Dark Matter and Galaxy Clustering: The Importance of Relative\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Dark Matter and Galaxy Clustering: The Importance of Relative\n  Perturbations"
                },
                "summary": "We develop a perturbative model to describe large-scale structure in\ncosmologies where dark matter consists of a mixture of cold (CDM) and warm\n(WDM) components. In such mixed dark matter (MDM) scenarios, even a subdominant\nwarm component can introduce distinctive signatures via its free-streaming\neffects, altering the evolution of density and velocity perturbations. We\npresent linear-order solutions for both total and relative perturbations in the\ntwo-fluid system, identifying novel contributions to galaxy bias caused by the\nrelative density and velocity modes between the components. Incorporating these\neffects into the galaxy bias expansion, we compute the linear galaxy power\nspectrum in both real and redshift space. Using Fisher matrix forecasts, we\nassess the sensitivity of upcoming surveys such as DESI and PFS to MDM\nscenarios. Our results demonstrate that neglecting relative perturbations can\nlead to significant biases in inferred constraints on the warm dark matter\nfraction, particularly for lighter WDM masses ($\\lesssim 150~\\mathrm{eV}$ and\n$\\lesssim 80~\\mathrm{eV}$) for PFS and DESI, respectively. This framework\nprovides a consistent and generalizable approach for incorporating\nmulti-component dark matter dynamics into galaxy clustering analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a perturbative model to describe large-scale structure in\ncosmologies where dark matter consists of a mixture of cold (CDM) and warm\n(WDM) components. In such mixed dark matter (MDM) scenarios, even a subdominant\nwarm component can introduce distinctive signatures via its free-streaming\neffects, altering the evolution of density and velocity perturbations. We\npresent linear-order solutions for both total and relative perturbations in the\ntwo-fluid system, identifying novel contributions to galaxy bias caused by the\nrelative density and velocity modes between the components. Incorporating these\neffects into the galaxy bias expansion, we compute the linear galaxy power\nspectrum in both real and redshift space. Using Fisher matrix forecasts, we\nassess the sensitivity of upcoming surveys such as DESI and PFS to MDM\nscenarios. Our results demonstrate that neglecting relative perturbations can\nlead to significant biases in inferred constraints on the warm dark matter\nfraction, particularly for lighter WDM masses ($\\lesssim 150~\\mathrm{eV}$ and\n$\\lesssim 80~\\mathrm{eV}$) for PFS and DESI, respectively. This framework\nprovides a consistent and generalizable approach for incorporating\nmulti-component dark matter dynamics into galaxy clustering analyses."
                },
                "authors": [
                    {
                        "name": "afak elik"
                    },
                    {
                        "name": "Fabian Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Schmidt"
                },
                "author": "Fabian Schmidt",
                "arxiv_comment": "30 pages, 11 figures; comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08422v1",
                "updated": "2025-09-10T09:10:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    10,
                    18,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:10:18Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    10,
                    18,
                    2,
                    253,
                    0
                ],
                "title": "LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations"
                },
                "summary": "Video-based AI systems are increasingly adopted in safety-critical domains\nsuch as autonomous driving and healthcare. However, interpreting their\ndecisions remains challenging due to the inherent spatiotemporal complexity of\nvideo data and the opacity of deep learning models. Existing explanation\ntechniques often suffer from limited temporal coherence, insufficient\nrobustness, and a lack of actionable causal insights. Current counterfactual\nexplanation methods typically do not incorporate guidance from the target\nmodel, reducing semantic fidelity and practical utility. We introduce Latent\nDiffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework\ndesigned to explain the behavior of video-based AI models. Compared to previous\napproaches, LD-ViCE reduces the computational costs of generating explanations\nby operating in latent space using a state-of-the-art diffusion model, while\nproducing realistic and interpretable counterfactuals through an additional\nrefinement step. Our experiments demonstrate the effectiveness of LD-ViCE\nacross three diverse video datasets, including EchoNet-Dynamic (cardiac\nultrasound), FERV39k (facial expression), and Something-Something V2 (action\nrecognition). LD-ViCE outperforms a recent state-of-the-art method, achieving\nan increase in R2 score of up to 68% while reducing inference time by half.\nQualitative analysis confirms that LD-ViCE generates semantically meaningful\nand temporally coherent explanations, offering valuable insights into the\ntarget model behavior. LD-ViCE represents a valuable step toward the\ntrustworthy deployment of AI in safety-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based AI systems are increasingly adopted in safety-critical domains\nsuch as autonomous driving and healthcare. However, interpreting their\ndecisions remains challenging due to the inherent spatiotemporal complexity of\nvideo data and the opacity of deep learning models. Existing explanation\ntechniques often suffer from limited temporal coherence, insufficient\nrobustness, and a lack of actionable causal insights. Current counterfactual\nexplanation methods typically do not incorporate guidance from the target\nmodel, reducing semantic fidelity and practical utility. We introduce Latent\nDiffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework\ndesigned to explain the behavior of video-based AI models. Compared to previous\napproaches, LD-ViCE reduces the computational costs of generating explanations\nby operating in latent space using a state-of-the-art diffusion model, while\nproducing realistic and interpretable counterfactuals through an additional\nrefinement step. Our experiments demonstrate the effectiveness of LD-ViCE\nacross three diverse video datasets, including EchoNet-Dynamic (cardiac\nultrasound), FERV39k (facial expression), and Something-Something V2 (action\nrecognition). LD-ViCE outperforms a recent state-of-the-art method, achieving\nan increase in R2 score of up to 68% while reducing inference time by half.\nQualitative analysis confirms that LD-ViCE generates semantically meaningful\nand temporally coherent explanations, offering valuable insights into the\ntarget model behavior. LD-ViCE represents a valuable step toward the\ntrustworthy deployment of AI in safety-critical domains."
                },
                "authors": [
                    {
                        "name": "Payal Varshney"
                    },
                    {
                        "name": "Adriano Lucieri"
                    },
                    {
                        "name": "Christoph Balada"
                    },
                    {
                        "name": "Sheraz Ahmed"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02424v2",
                "updated": "2025-09-10T09:08:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    8,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-03T08:32:19Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    32,
                    19,
                    3,
                    184,
                    0
                ],
                "title": "CyberRAG: An Agentic RAG cyber attack classification and reporting tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberRAG: An Agentic RAG cyber attack classification and reporting tool"
                },
                "summary": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming analysts with\nlogs requiring rapidly evolving expertise. Conventional machine-learning\ndetectors reduce alert volume but still yield many false positives, while\nstandard Retrieval-Augmented Generation (RAG) pipelines often retrieve\nirrelevant context and fail to justify predictions. We present CyberRAG, a\nmodular agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates: (i) fine-tuned classifiers specialized by attack family; (ii)\ntool adapters for enrichment and alerting; and (iii) an iterative\nretrieval-and-reason loop that queries a domain-specific knowledge base until\nevidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG\nadopts an agentic design that enables dynamic control flow and adaptive\nreasoning. This architecture autonomously refines threat labels and\nnatural-language justifications, reducing false positives and enhancing\ninterpretability. It is also extensible: new attack types can be supported by\nadding classifiers without retraining the core agent. CyberRAG was evaluated on\nSQL Injection, XSS, and SSTI, achieving over 94\\% accuracy per class and a\nfinal classification accuracy of 94.92\\% through semantic orchestration.\nGenerated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation, with robustness preserved against adversarial and unseen\npayloads. These results show that agentic, specialist-oriented RAG can combine\nhigh detection accuracy with trustworthy, SOC-ready prose, offering a flexible\npath toward partially automated cyber-defense workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming analysts with\nlogs requiring rapidly evolving expertise. Conventional machine-learning\ndetectors reduce alert volume but still yield many false positives, while\nstandard Retrieval-Augmented Generation (RAG) pipelines often retrieve\nirrelevant context and fail to justify predictions. We present CyberRAG, a\nmodular agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates: (i) fine-tuned classifiers specialized by attack family; (ii)\ntool adapters for enrichment and alerting; and (iii) an iterative\nretrieval-and-reason loop that queries a domain-specific knowledge base until\nevidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG\nadopts an agentic design that enables dynamic control flow and adaptive\nreasoning. This architecture autonomously refines threat labels and\nnatural-language justifications, reducing false positives and enhancing\ninterpretability. It is also extensible: new attack types can be supported by\nadding classifiers without retraining the core agent. CyberRAG was evaluated on\nSQL Injection, XSS, and SSTI, achieving over 94\\% accuracy per class and a\nfinal classification accuracy of 94.92\\% through semantic orchestration.\nGenerated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation, with robustness preserved against adversarial and unseen\npayloads. These results show that agentic, specialist-oriented RAG can combine\nhigh detection accuracy with trustworthy, SOC-ready prose, offering a flexible\npath toward partially automated cyber-defense workflows."
                },
                "authors": [
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Angelo Furfaro"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Marozzo"
                },
                "author": "Fabrizio Marozzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07473v3",
                "updated": "2025-09-10T09:05:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    5,
                    33,
                    2,
                    253,
                    0
                ],
                "published": "2024-10-09T22:53:48Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    22,
                    53,
                    48,
                    2,
                    283,
                    0
                ],
                "title": "Localizing Factual Inconsistencies in Attributable Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing Factual Inconsistencies in Attributable Text Generation"
                },
                "summary": "There has been an increasing interest in detecting hallucinations in\nmodel-generated texts, both manually and automatically, at varying levels of\ngranularity. However, most existing methods fail to precisely pinpoint the\nerrors. In this work, we introduce QASemConsistency, a new formalism for\nlocalizing factual inconsistencies in attributable text generation, at a\nfine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics,\nwe propose decomposing the generated text into minimal predicate-argument level\npropositions, expressed as simple question-answer (QA) pairs, and assess\nwhether each individual QA pair is supported by a trusted reference text. As\neach QA pair corresponds to a single semantic relation between a predicate and\nan argument, QASemConsistency effectively localizes the unsupported\ninformation. We first demonstrate the effectiveness of the QASemConsistency\nmethodology for human annotation, by collecting crowdsourced annotations of\ngranular consistency errors, while achieving a substantial inter-annotator\nagreement. This benchmark includes more than 3K instances spanning various\ntasks of attributable text generation. We also show that QASemConsistency\nyields factual consistency scores that correlate well with human judgments.\nFinally, we implement several methods for automatically detecting localized\nfactual inconsistencies, with both supervised entailment models and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been an increasing interest in detecting hallucinations in\nmodel-generated texts, both manually and automatically, at varying levels of\ngranularity. However, most existing methods fail to precisely pinpoint the\nerrors. In this work, we introduce QASemConsistency, a new formalism for\nlocalizing factual inconsistencies in attributable text generation, at a\nfine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics,\nwe propose decomposing the generated text into minimal predicate-argument level\npropositions, expressed as simple question-answer (QA) pairs, and assess\nwhether each individual QA pair is supported by a trusted reference text. As\neach QA pair corresponds to a single semantic relation between a predicate and\nan argument, QASemConsistency effectively localizes the unsupported\ninformation. We first demonstrate the effectiveness of the QASemConsistency\nmethodology for human annotation, by collecting crowdsourced annotations of\ngranular consistency errors, while achieving a substantial inter-annotator\nagreement. This benchmark includes more than 3K instances spanning various\ntasks of attributable text generation. We also show that QASemConsistency\nyields factual consistency scores that correlate well with human judgments.\nFinally, we implement several methods for automatically detecting localized\nfactual inconsistencies, with both supervised entailment models and LLMs."
                },
                "authors": [
                    {
                        "name": "Arie Cattan"
                    },
                    {
                        "name": "Paul Roit"
                    },
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "David Wan"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "arxiv_comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00404v2",
                "updated": "2025-09-10T09:04:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    4,
                    12,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-30T08:09:08Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    9,
                    8,
                    5,
                    242,
                    0
                ],
                "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metis: Training Large Language Models with Advanced Low-Bit Quantization"
                },
                "summary": "This work identifies anisotropic parameter distributions as a fundamental\nbarrier to training large language models (LLMs) with low-bit quantization: a\nfew dominant singular values create wide numerical ranges that conflict with\nthe inherent bias of block-wise quantization. This bias disproportionately\npreserves high-magnitude values while discarding smaller ones, causing training\ninstability and low model performance. This work introduces Metis, a training\nframework that combines (i) spectral decomposition with random embedding to\nefficiently disentangle dominant from long-tail components, compressing broad\ndistributions into quantization-friendly narrow ranges; (ii) adaptive learning\nrates in the spectral domain to amplify underrepresented directions and better\ncapture diverse features critical for performance; and (iii) a dual-range\nregularizer that jointly constrains numerical precision and parameter range\ndistribution, ensuring stable, unbiased low-bit training. With Metis, FP8\ntraining surpasses FP32 baselines, and FP4 training achieves accuracy\ncomparable to FP32, paving the way for robust and scalable LLM training under\nadvanced low-bit quantization. The code implementation for Metis is available\nat: https://github.com/sii-research/Metis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies anisotropic parameter distributions as a fundamental\nbarrier to training large language models (LLMs) with low-bit quantization: a\nfew dominant singular values create wide numerical ranges that conflict with\nthe inherent bias of block-wise quantization. This bias disproportionately\npreserves high-magnitude values while discarding smaller ones, causing training\ninstability and low model performance. This work introduces Metis, a training\nframework that combines (i) spectral decomposition with random embedding to\nefficiently disentangle dominant from long-tail components, compressing broad\ndistributions into quantization-friendly narrow ranges; (ii) adaptive learning\nrates in the spectral domain to amplify underrepresented directions and better\ncapture diverse features critical for performance; and (iii) a dual-range\nregularizer that jointly constrains numerical precision and parameter range\ndistribution, ensuring stable, unbiased low-bit training. With Metis, FP8\ntraining surpasses FP32 baselines, and FP4 training achieves accuracy\ncomparable to FP32, paving the way for robust and scalable LLM training under\nadvanced low-bit quantization. The code implementation for Metis is available\nat: https://github.com/sii-research/Metis."
                },
                "authors": [
                    {
                        "name": "Hengjie Cao"
                    },
                    {
                        "name": "Mengyi Chen"
                    },
                    {
                        "name": "Yifeng Yang"
                    },
                    {
                        "name": "Ruijun Huang"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Jixian Zhou"
                    },
                    {
                        "name": "Anrui Chen"
                    },
                    {
                        "name": "Mingzhi Dong"
                    },
                    {
                        "name": "Yujiang Wang"
                    },
                    {
                        "name": "Jinlong Hou"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    },
                    {
                        "name": "Li Shang"
                    }
                ],
                "author_detail": {
                    "name": "Li Shang"
                },
                "author": "Li Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07104v2",
                "updated": "2025-09-10T09:03:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    3,
                    4,
                    2,
                    253,
                    0
                ],
                "published": "2025-06-08T12:18:50Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    12,
                    18,
                    50,
                    6,
                    159,
                    0
                ],
                "title": "How Far Are We from Optimal Reasoning Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We from Optimal Reasoning Efficiency?"
                },
                "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Shu Yan"
                    },
                    {
                        "name": "Qixin Tan"
                    },
                    {
                        "name": "Lu Yang"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08416v1",
                "updated": "2025-09-10T09:00:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    0,
                    32,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:00:32Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    0,
                    32,
                    2,
                    253,
                    0
                ],
                "title": "AutoVeriFix: Automatically Correcting Errors and Enhancing Functional\n  Correctness in LLM-Generated Verilog Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoVeriFix: Automatically Correcting Errors and Enhancing Functional\n  Correctness in LLM-Generated Verilog Code"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating software code for high-level programming languages such as Python\nand C++. However, their application to hardware description languages, such as\nVerilog, is challenging due to the scarcity of high-quality training data.\nCurrent approaches to Verilog code generation using LLMs often focus on\nsyntactic correctness, resulting in code with functional errors. To address\nthese challenges, we present AutoVeriFix, a novel Python-assisted two-stage\nframework designed to enhance the functional correctness of LLM-generated\nVerilog code. In the first stage, LLMs are employed to generate high-level\nPython reference models that define the intended circuit behavior. In the\nsecond stage, these Python models facilitate the creation of automated tests\nthat guide the generation of Verilog RTL implementations. Simulation\ndiscrepancies between the reference model and the Verilog code are iteratively\nused to identify and correct errors, thereby improving the functional accuracy\nand reliability of the LLM-generated Verilog code. Experimental results\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art methods in improving the functional correctness of generated\nVerilog code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating software code for high-level programming languages such as Python\nand C++. However, their application to hardware description languages, such as\nVerilog, is challenging due to the scarcity of high-quality training data.\nCurrent approaches to Verilog code generation using LLMs often focus on\nsyntactic correctness, resulting in code with functional errors. To address\nthese challenges, we present AutoVeriFix, a novel Python-assisted two-stage\nframework designed to enhance the functional correctness of LLM-generated\nVerilog code. In the first stage, LLMs are employed to generate high-level\nPython reference models that define the intended circuit behavior. In the\nsecond stage, these Python models facilitate the creation of automated tests\nthat guide the generation of Verilog RTL implementations. Simulation\ndiscrepancies between the reference model and the Verilog code are iteratively\nused to identify and correct errors, thereby improving the functional accuracy\nand reliability of the LLM-generated Verilog code. Experimental results\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art methods in improving the functional correctness of generated\nVerilog code."
                },
                "authors": [
                    {
                        "name": "Yan Tan"
                    },
                    {
                        "name": "Xiangchen Meng"
                    },
                    {
                        "name": "Zijun Jiang"
                    },
                    {
                        "name": "Yangdi Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Yangdi Lyu"
                },
                "author": "Yangdi Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08407v1",
                "updated": "2025-09-10T08:54:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    54,
                    16,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:54:16Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    54,
                    16,
                    2,
                    253,
                    0
                ],
                "title": "An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight\n  Optimization"
                },
                "summary": "Seed implant brachytherapy (SIBT) is an effective cancer treatment modality;\nhowever, clinical planning often relies on manual adjustment of objective\nfunction weights, leading to inefficiencies and suboptimal results. This study\nproposes an adaptive weight optimization framework for SIBT planning, driven by\nlarge language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated\nwith an automatic planning algorithm in an iterative loop. Starting with fixed\nweights, the LLM evaluates plan quality and recommends new weights in the next\niteration. This process continues until convergence criteria are met, after\nwhich the LLM conducts a comprehensive evaluation to identify the optimal plan.\nA clinical knowledge base, constructed and queried via retrieval-augmented\ngeneration (RAG), enhances the model's domain-specific reasoning. The proposed\nmethod was validated on 23 patient cases, showing that the LLM-assisted\napproach produces plans that are comparable to or exceeding clinically approved\nand fixed-weight plans, in terms of dose homogeneity for the clinical target\nvolume (CTV) and sparing of organs at risk (OARs). The study demonstrates the\npotential use of LLMs in SIBT planning automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed implant brachytherapy (SIBT) is an effective cancer treatment modality;\nhowever, clinical planning often relies on manual adjustment of objective\nfunction weights, leading to inefficiencies and suboptimal results. This study\nproposes an adaptive weight optimization framework for SIBT planning, driven by\nlarge language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated\nwith an automatic planning algorithm in an iterative loop. Starting with fixed\nweights, the LLM evaluates plan quality and recommends new weights in the next\niteration. This process continues until convergence criteria are met, after\nwhich the LLM conducts a comprehensive evaluation to identify the optimal plan.\nA clinical knowledge base, constructed and queried via retrieval-augmented\ngeneration (RAG), enhances the model's domain-specific reasoning. The proposed\nmethod was validated on 23 patient cases, showing that the LLM-assisted\napproach produces plans that are comparable to or exceeding clinically approved\nand fixed-weight plans, in terms of dose homogeneity for the clinical target\nvolume (CTV) and sparing of organs at risk (OARs). The study demonstrates the\npotential use of LLMs in SIBT planning automation."
                },
                "authors": [
                    {
                        "name": "Zhuo Xiao"
                    },
                    {
                        "name": "Qinglong Yao"
                    },
                    {
                        "name": "Jingjing Wang"
                    },
                    {
                        "name": "Fugen Zhou"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Haitao Sun"
                    },
                    {
                        "name": "Zhe Ji"
                    },
                    {
                        "name": "Yuliang Jiang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Qiuwen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwen Wu"
                },
                "arxiv_affiliation": "Department of Radiation Oncology, Duke University Medical Center, Durham, USA",
                "author": "Qiuwen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08401v2",
                "updated": "2025-09-11T05:42:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    5,
                    42,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T08:45:08Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    45,
                    8,
                    2,
                    253,
                    0
                ],
                "title": "Two Sides of the Same Optimization Coin: Model Degradation and\n  Representation Collapse in Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Sides of the Same Optimization Coin: Model Degradation and\n  Representation Collapse in Graph Foundation Models"
                },
                "summary": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios."
                },
                "authors": [
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Daohan Su"
                    },
                    {
                        "name": "Sicheng Liu"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Zhenjun Li"
                    },
                    {
                        "name": "Bing Zhou"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08400v1",
                "updated": "2025-09-10T08:44:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    44,
                    57,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:44:57Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    44,
                    57,
                    2,
                    253,
                    0
                ],
                "title": "Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution"
                },
                "summary": "We introduce ubiquitous intelligence as a paradigm where Large Language\nModels (LLMs) evolve within wireless network-driven ecosystems. Unlike static\nmodel deployments, this approach enables scalable and continuous intelligence\nascension through coordination between networks and LLMs. Wireless networks\nsupport system-orchestrated lifelong learning, while LLMs drive the\nnext-generation network development that is more adaptive and responsive. This\nco-evolution highlights a shift toward self-improving systems, sustaining\ncapability growth across diverse and resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ubiquitous intelligence as a paradigm where Large Language\nModels (LLMs) evolve within wireless network-driven ecosystems. Unlike static\nmodel deployments, this approach enables scalable and continuous intelligence\nascension through coordination between networks and LLMs. Wireless networks\nsupport system-orchestrated lifelong learning, while LLMs drive the\nnext-generation network development that is more adaptive and responsive. This\nco-evolution highlights a shift toward self-improving systems, sustaining\ncapability growth across diverse and resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Xingkun Yin"
                    },
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14161v3",
                "updated": "2025-09-10T08:35:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    35,
                    19,
                    2,
                    253,
                    0
                ],
                "published": "2024-12-18T18:55:40Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    55,
                    40,
                    2,
                    353,
                    0
                ],
                "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks"
                },
                "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com."
                },
                "authors": [
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Yufan Song"
                    },
                    {
                        "name": "Boxuan Li"
                    },
                    {
                        "name": "Yuxuan Tang"
                    },
                    {
                        "name": "Kritanjali Jain"
                    },
                    {
                        "name": "Mengxue Bao"
                    },
                    {
                        "name": "Zora Z. Wang"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zhitong Guo"
                    },
                    {
                        "name": "Murong Cao"
                    },
                    {
                        "name": "Mingyang Yang"
                    },
                    {
                        "name": "Hao Yang Lu"
                    },
                    {
                        "name": "Amaad Martin"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Leander Maben"
                    },
                    {
                        "name": "Raj Mehta"
                    },
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Yiqing Xie"
                    },
                    {
                        "name": "Shuyan Zhou"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.08827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08827v1",
                "updated": "2025-09-10T17:59:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    43,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Reinforcement Learning for Large Reasoning Models"
                },
                "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Pengfei Li"
                    },
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Yuru Wang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Fangfu Liu"
                    },
                    {
                        "name": "Xiang Xu"
                    },
                    {
                        "name": "Jiaze Ma"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Zonglin Li"
                    },
                    {
                        "name": "Huayu Chen"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Yafu Li"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Zhenzhao Yuan"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08825v1",
                "updated": "2025-09-10T17:58:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    53,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:58:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation"
                },
                "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant."
                },
                "authors": [
                    {
                        "name": "Joachim Baumann"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Aleksandra Urman"
                    },
                    {
                        "name": "Albert Wendsj"
                    },
                    {
                        "name": "Flor Miriam Plaza-del-Arco"
                    },
                    {
                        "name": "Johannes B. Gruber"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08824v1",
                "updated": "2025-09-10T17:58:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:58:23Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    58,
                    23,
                    2,
                    253,
                    0
                ],
                "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl\n  Snapshots to Industrial-Grade Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl\n  Snapshots to Industrial-Grade Corpora"
                },
                "summary": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development."
                },
                "authors": [
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    },
                    {
                        "name": "Helio Pedrini"
                    }
                ],
                "author_detail": {
                    "name": "Helio Pedrini"
                },
                "author": "Helio Pedrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08822v1",
                "updated": "2025-09-10T17:53:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    53,
                    39,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:53:39Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    53,
                    39,
                    2,
                    253,
                    0
                ],
                "title": "A Survey of TinyML Applications in Beekeeping for Hive Monitoring and\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of TinyML Applications in Beekeeping for Hive Monitoring and\n  Management"
                },
                "summary": "Honey bee colonies are essential for global food security and ecosystem\nstability, yet they face escalating threats from pests, diseases, and\nenvironmental stressors. Traditional hive inspections are labor-intensive and\ndisruptive, while cloud-based monitoring solutions remain impractical for\nremote or resource-limited apiaries. Recent advances in Internet of Things\n(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring\ndirectly on edge devices, offering scalable and non-invasive alternatives. This\nsurvey synthesizes current innovations at the intersection of TinyML and\napiculture, organized around four key functional areas: monitoring hive\nconditions, recognizing bee behaviors, detecting pests and diseases, and\nforecasting swarming events. We further examine supporting resources, including\npublicly available datasets, lightweight model architectures optimized for\nembedded deployment, and benchmarking strategies tailored to field constraints.\nCritical limitations such as data scarcity, generalization challenges, and\ndeployment barriers in off-grid environments are highlighted, alongside\nemerging opportunities in ultra-efficient inference pipelines, adaptive edge\nlearning, and dataset standardization. By consolidating research and\nengineering practices, this work provides a foundation for scalable, AI-driven,\nand ecologically informed monitoring systems to support sustainable pollinator\nmanagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Honey bee colonies are essential for global food security and ecosystem\nstability, yet they face escalating threats from pests, diseases, and\nenvironmental stressors. Traditional hive inspections are labor-intensive and\ndisruptive, while cloud-based monitoring solutions remain impractical for\nremote or resource-limited apiaries. Recent advances in Internet of Things\n(IoT) and Tiny Machine Learning (TinyML) enable low-power, real-time monitoring\ndirectly on edge devices, offering scalable and non-invasive alternatives. This\nsurvey synthesizes current innovations at the intersection of TinyML and\napiculture, organized around four key functional areas: monitoring hive\nconditions, recognizing bee behaviors, detecting pests and diseases, and\nforecasting swarming events. We further examine supporting resources, including\npublicly available datasets, lightweight model architectures optimized for\nembedded deployment, and benchmarking strategies tailored to field constraints.\nCritical limitations such as data scarcity, generalization challenges, and\ndeployment barriers in off-grid environments are highlighted, alongside\nemerging opportunities in ultra-efficient inference pipelines, adaptive edge\nlearning, and dataset standardization. By consolidating research and\nengineering practices, this work provides a foundation for scalable, AI-driven,\nand ecologically informed monitoring systems to support sustainable pollinator\nmanagement."
                },
                "authors": [
                    {
                        "name": "Willy Sucipto"
                    },
                    {
                        "name": "Jianlong Zhou"
                    },
                    {
                        "name": "Ray Seung Min Kwon"
                    },
                    {
                        "name": "Fang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Fang Chen"
                },
                "author": "Fang Chen",
                "arxiv_comment": "30 pages, 8 figures, 3 tables. Survey of TinyML and IoT applications\n  in beekeeping (datasets, benchmarking, deployment). Submitted to ACM\n  Computing Surveys (under review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.9; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15474v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15474v2",
                "updated": "2025-09-10T17:51:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    51,
                    3,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-21T11:50:56Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    50,
                    56,
                    3,
                    233,
                    0
                ],
                "title": "Subjective Behaviors and Preferences in LLM: Language of Browsing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective Behaviors and Preferences in LLM: Language of Browsing"
                },
                "summary": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model (LLM) offers versatility across domains and tasks,\npurportedly benefiting users with a wide variety of behaviors and preferences.\nWe question this perception about an LLM when users have inherently subjective\nbehaviors and preferences, as seen in their ubiquitous and idiosyncratic\nbrowsing of websites or apps. The sequential behavior logs of pages, thus\ngenerated, form something akin to each user's self-constructed \"language\",\nalbeit without the structure and grammar imbued in natural languages. We ask:\n(i) Can a small LM represent the \"language of browsing\" better than a large LM?\n(ii) Can an LM with a single set of parameters (or, single LM) adequately\ncapture myriad users' heterogeneous, subjective behaviors and preferences?\n(iii) Can a single LM with high average performance, yield low variance in\nperformance to make alignment good at user level? We introduce clusterwise LM\ntraining, HeTLM (Heterogeneity aware Training of Language Model), appropriate\nfor subjective behaviors. We find that (i) a small LM trained using a\npage-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM\nwith heterogeneous cluster specific set of parameters outperforms a single LM\nof the same family, controlling for the number of parameters; and (iii) a\nhigher mean and a lower variance in generation ensues, implying improved\nalignment."
                },
                "authors": [
                    {
                        "name": "Sai Sundaresan"
                    },
                    {
                        "name": "Harshita Chopra"
                    },
                    {
                        "name": "Atanu R. Sinha"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Nagasai Saketh Naidu"
                    },
                    {
                        "name": "Raghav Karan"
                    },
                    {
                        "name": "N Anushka"
                    }
                ],
                "author_detail": {
                    "name": "N Anushka"
                },
                "author": "N Anushka",
                "arxiv_comment": "Accepted at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15474v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15474v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08809v1",
                "updated": "2025-09-10T17:42:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    42,
                    41,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:42:41Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    42,
                    41,
                    2,
                    253,
                    0
                ],
                "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation\n  Through Unsupervised Consistency Signals"
                },
                "summary": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings."
                },
                "authors": [
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Ivor Tsang"
                    }
                ],
                "author_detail": {
                    "name": "Ivor Tsang"
                },
                "author": "Ivor Tsang",
                "arxiv_comment": "11 pages, 10 figures",
                "arxiv_journal_ref": "Published ICLR 2025 Workshop on Scaling Self-Improving Foundation\n  Models without Human Supervision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08807v1",
                "updated": "2025-09-10T17:40:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    40,
                    19,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:40:19Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    40,
                    19,
                    2,
                    253,
                    0
                ],
                "title": "A Pathway to Practical Quantum Advantage in Solving Navier-Stokes\n  Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pathway to Practical Quantum Advantage in Solving Navier-Stokes\n  Equations"
                },
                "summary": "The advent of fault-tolerant quantum computing (FTQC) promises to tackle\nclassically intractable problems. A key milestone is solving the Navier-Stokes\nequations (NSE), which has remained formidable for quantum algorithms due to\ntheir high input-output overhead and nonlinearity. Here, we establish a\nfull-stack framework that charts a practical pathway to a quantum advantage for\nlarge-scale NSE simulation. Our approach integrates a spectral-based\ninput/output algorithm, an explicit and synthesized quantum circuit, and a\nrefined error-correction protocol. The algorithm achieves an end-to-end\nexponential speedup in asymptotic complexity, meeting the lower bound for\ngeneral quantum linear system solvers. Through symmetry-based circuit synthesis\nand optimized error correction, we reduce the required logical and physical\nresources by two orders of magnitude. Our concrete resource analysis\ndemonstrates that solving NSE on a $2^{80}$-grid is feasible with 8.71 million\nphysical qubits (at an error rate of $5 \\times 10^{-4}$) in 42.6 days --\noutperforming a state-of-the-art supercomputer, which would require over a\ncentury. This work bridges the gap between theoretical quantum speedup and the\npractical deployment of high-performance scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of fault-tolerant quantum computing (FTQC) promises to tackle\nclassically intractable problems. A key milestone is solving the Navier-Stokes\nequations (NSE), which has remained formidable for quantum algorithms due to\ntheir high input-output overhead and nonlinearity. Here, we establish a\nfull-stack framework that charts a practical pathway to a quantum advantage for\nlarge-scale NSE simulation. Our approach integrates a spectral-based\ninput/output algorithm, an explicit and synthesized quantum circuit, and a\nrefined error-correction protocol. The algorithm achieves an end-to-end\nexponential speedup in asymptotic complexity, meeting the lower bound for\ngeneral quantum linear system solvers. Through symmetry-based circuit synthesis\nand optimized error correction, we reduce the required logical and physical\nresources by two orders of magnitude. Our concrete resource analysis\ndemonstrates that solving NSE on a $2^{80}$-grid is feasible with 8.71 million\nphysical qubits (at an error rate of $5 \\times 10^{-4}$) in 42.6 days --\noutperforming a state-of-the-art supercomputer, which would require over a\ncentury. This work bridges the gap between theoretical quantum speedup and the\npractical deployment of high-performance scientific computing."
                },
                "authors": [
                    {
                        "name": "Xi-Ning Zhuang"
                    },
                    {
                        "name": "Zhao-Yun Chen"
                    },
                    {
                        "name": "Ming-Yang Tan"
                    },
                    {
                        "name": "Jiaxuan Zhang"
                    },
                    {
                        "name": "Chuang-Chao Ye"
                    },
                    {
                        "name": "Tian-Hao Wei"
                    },
                    {
                        "name": "Teng-Yang Ma"
                    },
                    {
                        "name": "Cheng Xue"
                    },
                    {
                        "name": "Huan-Yu Liu"
                    },
                    {
                        "name": "Qing-Song Li"
                    },
                    {
                        "name": "Tai-Ping Sun"
                    },
                    {
                        "name": "Xiao-Fan Xu"
                    },
                    {
                        "name": "Yun-Jie Wang"
                    },
                    {
                        "name": "Yu-Chun Wu"
                    },
                    {
                        "name": "Guo-Ping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guo-Ping Guo"
                },
                "author": "Guo-Ping Guo",
                "arxiv_comment": "57 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08803v1",
                "updated": "2025-09-10T17:36:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    36,
                    25,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:36:25Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    36,
                    25,
                    2,
                    253,
                    0
                ],
                "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Truth: The Confidence Paradox in AI Fact-Checking"
                },
                "summary": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking."
                },
                "authors": [
                    {
                        "name": "Ihsan A. Qazi"
                    },
                    {
                        "name": "Zohaib Khan"
                    },
                    {
                        "name": "Abdullah Ghani"
                    },
                    {
                        "name": "Agha A. Raza"
                    },
                    {
                        "name": "Zafar A. Qazi"
                    },
                    {
                        "name": "Wassay Sajjad"
                    },
                    {
                        "name": "Ayesha Ali"
                    },
                    {
                        "name": "Asher Javaid"
                    },
                    {
                        "name": "Muhammad Abdullah Sohail"
                    },
                    {
                        "name": "Abdul H. Azeemi"
                    }
                ],
                "author_detail": {
                    "name": "Abdul H. Azeemi"
                },
                "author": "Abdul H. Azeemi",
                "arxiv_comment": "65 pages, 26 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00074v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00074v2",
                "updated": "2025-09-10T17:27:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    27,
                    44,
                    2,
                    253,
                    0
                ],
                "published": "2025-05-29T20:11:11Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    20,
                    11,
                    11,
                    3,
                    149,
                    0
                ],
                "title": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose Name Comes Up? Auditing LLM-Based Scholar Recommendations"
                },
                "summary": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the performance of six open-weight LLMs (llama3-8b,\nllama3.1-8b, gemma2-9b, mixtral-8x7b, llama3-70b, llama3.1-70b) in recommending\nexperts in physics across five tasks: top-k experts by field, influential\nscientists by discipline, epoch, seniority, and scholar counterparts. The\nevaluation examines consistency, factuality, and biases related to gender,\nethnicity, academic popularity, and scholar similarity. Using ground-truth data\nfrom the American Physical Society and OpenAlex, we establish scholarly\nbenchmarks by comparing model outputs to real-world academic records. Our\nanalysis reveals inconsistencies and biases across all models. mixtral-8x7b\nproduces the most stable outputs, while llama3.1-70b shows the highest\nvariability. Many models exhibit duplication, and some, particularly gemma2-9b\nand llama3.1-8b, struggle with formatting errors. LLMs generally recommend real\nscientists, but accuracy drops in field-, epoch-, and seniority-specific\nqueries, consistently favoring senior scholars. Representation biases persist,\nreplicating gender imbalances (reflecting male predominance),\nunder-representing Asian scientists, and over-representing White scholars.\nDespite some diversity in institutional and collaboration networks, models\nfavor highly cited and productive scholars, reinforcing the rich-getricher\neffect while offering limited geographical representation. These findings\nhighlight the need to improve LLMs for more reliable and equitable scholarly\nrecommendations."
                },
                "authors": [
                    {
                        "name": "Daniele Barolo"
                    },
                    {
                        "name": "Chiara Valentin"
                    },
                    {
                        "name": "Fariba Karimi"
                    },
                    {
                        "name": "Luis Galrraga"
                    },
                    {
                        "name": "Gonzalo G. Mndez"
                    },
                    {
                        "name": "Lisette Espn-Noboa"
                    }
                ],
                "author_detail": {
                    "name": "Lisette Espn-Noboa"
                },
                "author": "Lisette Espn-Noboa",
                "arxiv_comment": "40 pages: 10 main (incl. 9 figures), 3 references, and 27 appendix.\n  Paper under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00074v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00074v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4; F.2; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06887v2",
                "updated": "2025-09-10T17:17:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    17,
                    28,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-08T17:08:26Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    17,
                    8,
                    26,
                    0,
                    251,
                    0
                ],
                "title": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSearch: Rethinking Search System with a Unified Generative\n  Architecture"
                },
                "summary": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern search systems play a crucial role in facilitating information\nacquisition. Traditional search engines typically rely on a cascaded\narchitecture, where results are retrieved through recall, pre-ranking, and\nranking stages. The complexity of designing and maintaining multiple modules\nmakes it difficult to achieve holistic performance gains. Recent advances in\ngenerative recommendation have motivated the exploration of unified generative\nsearch as an alternative. However, existing approaches are not genuinely\nend-to-end: they typically train an item encoder to tokenize candidates first\nand then optimize a generator separately, leading to objective inconsistency\nand limited generalization. To address these limitations, we propose UniSearch,\na unified generative search framework for Kuaishou Search. UniSearch replaces\nthe cascaded pipeline with an end-to-end architecture that integrates a Search\nGenerator and a Video Encoder. The Generator produces semantic identifiers of\nrelevant items given a user query, while the Video Encoder learns latent item\nembeddings and provides their tokenized representations. A unified training\nframework jointly optimizes both components, enabling mutual enhancement and\nimproving representation quality and generation accuracy. Furthermore, we\nintroduce Search Preference Optimization (SPO), which leverages a reward model\nand real user feedback to better align generation with user preferences.\nExtensive experiments on industrial-scale datasets, together with online A/B\ntesting in both short-video and live search scenarios, demonstrate the strong\neffectiveness and deployment potential of UniSearch. Notably, its deployment in\nlive search yields the largest single-experiment improvement in recent years of\nour product's history, highlighting its practical value for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Jiahui Chen"
                    },
                    {
                        "name": "Xiaoze Jiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Quanzhi Zhu"
                    },
                    {
                        "name": "Junyao Zhao"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Kang Pan"
                    },
                    {
                        "name": "Ao Xie"
                    },
                    {
                        "name": "Maohua Pei"
                    },
                    {
                        "name": "Zhiheng Qin"
                    },
                    {
                        "name": "Hongjing Zhang"
                    },
                    {
                        "name": "Zhixin Zhai"
                    },
                    {
                        "name": "Xiaobo Guo"
                    },
                    {
                        "name": "Runbin Zhou"
                    },
                    {
                        "name": "Kefeng Wang"
                    },
                    {
                        "name": "Mingyang Geng"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Jingshan Lv"
                    },
                    {
                        "name": "Yupeng Huang"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Han Li"
                    }
                ],
                "author_detail": {
                    "name": "Han Li"
                },
                "author": "Han Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08780v1",
                "updated": "2025-09-10T17:08:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    8,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:08:31Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    8,
                    31,
                    2,
                    253,
                    0
                ],
                "title": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using\n  Mobile-Captured Skin Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using\n  Mobile-Captured Skin Images"
                },
                "summary": "Background: Arsenicosis is a serious public health concern in South and\nSoutheast Asia, primarily caused by long-term consumption of\narsenic-contaminated water. Its early cutaneous manifestations are clinically\nsignificant but often underdiagnosed, particularly in rural areas with limited\naccess to dermatologists. Automated, image-based diagnostic solutions can\nsupport early detection and timely interventions.\n  Methods: In this study, we propose an end-to-end framework for arsenicosis\ndiagnosis using mobile phone-captured skin images. A dataset comprising 20\nclasses and over 11000 images of arsenic-induced and other dermatological\nconditions was curated. Multiple deep learning architectures, including\nconvolutional neural networks (CNNs) and Transformer-based models, were\nbenchmarked for arsenicosis detection. Model interpretability was integrated\nvia LIME and Grad-CAM, while deployment feasibility was demonstrated through a\nweb-based diagnostic tool.\n  Results: Transformer-based models significantly outperformed CNNs, with the\nSwin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM\nvisualizations confirmed that the models attended to lesion-relevant regions,\nincreasing clinical transparency and aiding in error analysis. The framework\nalso demonstrated strong performance on external validation samples, confirming\nits ability to generalize beyond the curated dataset.\n  Conclusion: The proposed framework demonstrates the potential of deep\nlearning for non-invasive, accessible, and explainable diagnosis of arsenicosis\nfrom mobile-acquired images. By enabling reliable image-based screening, it can\nserve as a practical diagnostic aid in rural and resource-limited communities,\nwhere access to dermatologists is scarce, thereby supporting early detection\nand timely intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Arsenicosis is a serious public health concern in South and\nSoutheast Asia, primarily caused by long-term consumption of\narsenic-contaminated water. Its early cutaneous manifestations are clinically\nsignificant but often underdiagnosed, particularly in rural areas with limited\naccess to dermatologists. Automated, image-based diagnostic solutions can\nsupport early detection and timely interventions.\n  Methods: In this study, we propose an end-to-end framework for arsenicosis\ndiagnosis using mobile phone-captured skin images. A dataset comprising 20\nclasses and over 11000 images of arsenic-induced and other dermatological\nconditions was curated. Multiple deep learning architectures, including\nconvolutional neural networks (CNNs) and Transformer-based models, were\nbenchmarked for arsenicosis detection. Model interpretability was integrated\nvia LIME and Grad-CAM, while deployment feasibility was demonstrated through a\nweb-based diagnostic tool.\n  Results: Transformer-based models significantly outperformed CNNs, with the\nSwin Transformer achieving the best results (86\\\\% accuracy). LIME and Grad-CAM\nvisualizations confirmed that the models attended to lesion-relevant regions,\nincreasing clinical transparency and aiding in error analysis. The framework\nalso demonstrated strong performance on external validation samples, confirming\nits ability to generalize beyond the curated dataset.\n  Conclusion: The proposed framework demonstrates the potential of deep\nlearning for non-invasive, accessible, and explainable diagnosis of arsenicosis\nfrom mobile-acquired images. By enabling reliable image-based screening, it can\nserve as a practical diagnostic aid in rural and resource-limited communities,\nwhere access to dermatologists is scarce, thereby supporting early detection\nand timely intervention."
                },
                "authors": [
                    {
                        "name": "Asif Newaz"
                    },
                    {
                        "name": "Asif Ur Rahman Adib"
                    },
                    {
                        "name": "Rajit Sahil"
                    },
                    {
                        "name": "Mashfique Mehzad"
                    }
                ],
                "author_detail": {
                    "name": "Mashfique Mehzad"
                },
                "author": "Mashfique Mehzad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08776v1",
                "updated": "2025-09-10T17:05:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    5,
                    53,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T17:05:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    5,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks\n  with Entropy Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSI Compression Beyond Latents: End-to-End Hybrid Attention-CNN Networks\n  with Entropy Regularization"
                },
                "summary": "Massive MIMO systems rely on accurate Channel State Information (CSI)\nfeedback to enable high-gain beam-forming. However, the feedback overhead\nscales linearly with the number of antennas, presenting a major bottleneck.\nWhile recent deep learning methods have improved CSI compression, most overlook\nthe impact of quantization and entropy coding, limiting their practical\ndeployability. In this work, we propose an end-to-end CSI compression framework\nthat integrates a Spatial Correlation-Guided Attention Mechanism with\nquantization and entropy-aware training. Our model effectively exploits the\nspatial correlation among the antennas, thereby learning compact,\nentropy-optimized latent representations for efficient coding. This reduces the\nrequired feedback bitrates without sacrificing reconstruction accuracy, thereby\nyielding a superior rate-distortion trade-off. Experiments show that our method\nsurpasses existing end-to-end CSI compression schemes, exceeding benchmark\nperformance by an average of 21.5% on indoor datasets and 18.9% on outdoor\ndatasets. The proposed framework results in a practical and efficient CSI\nfeedback scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive MIMO systems rely on accurate Channel State Information (CSI)\nfeedback to enable high-gain beam-forming. However, the feedback overhead\nscales linearly with the number of antennas, presenting a major bottleneck.\nWhile recent deep learning methods have improved CSI compression, most overlook\nthe impact of quantization and entropy coding, limiting their practical\ndeployability. In this work, we propose an end-to-end CSI compression framework\nthat integrates a Spatial Correlation-Guided Attention Mechanism with\nquantization and entropy-aware training. Our model effectively exploits the\nspatial correlation among the antennas, thereby learning compact,\nentropy-optimized latent representations for efficient coding. This reduces the\nrequired feedback bitrates without sacrificing reconstruction accuracy, thereby\nyielding a superior rate-distortion trade-off. Experiments show that our method\nsurpasses existing end-to-end CSI compression schemes, exceeding benchmark\nperformance by an average of 21.5% on indoor datasets and 18.9% on outdoor\ndatasets. The proposed framework results in a practical and efficient CSI\nfeedback scheme."
                },
                "authors": [
                    {
                        "name": "Maryam Ansarifard"
                    },
                    {
                        "name": "Mostafa Rahmani"
                    },
                    {
                        "name": "Mohit K. Sharma"
                    },
                    {
                        "name": "Kishor C. Joshi"
                    },
                    {
                        "name": "George Exarchakos"
                    },
                    {
                        "name": "Alister Burr"
                    }
                ],
                "author_detail": {
                    "name": "Alister Burr"
                },
                "author": "Alister Burr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08755v1",
                "updated": "2025-09-10T16:46:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    46,
                    11,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:46:11Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    46,
                    11,
                    2,
                    253,
                    0
                ],
                "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning"
                },
                "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents."
                },
                "authors": [
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Jixuan Huang"
                    },
                    {
                        "name": "Chenyang Liao"
                    },
                    {
                        "name": "Baodai Huang"
                    },
                    {
                        "name": "Honglin Guo"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Rui Zheng"
                    },
                    {
                        "name": "Junjie Ye"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Wenxiang Chen"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Yiwen Ding"
                    },
                    {
                        "name": "Guanyu Li"
                    },
                    {
                        "name": "Zehui Chen"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Xuesong Yao"
                    },
                    {
                        "name": "Yufei Xu"
                    },
                    {
                        "name": "Jiecao Chen"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "preprint, 39 pages, 16 figures. Project:\n  https://AgentGym-RL.github.io/. Framework and Code:\n  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02682v2",
                "updated": "2025-09-10T16:45:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    45,
                    42,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-04T14:54:45Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    54,
                    45,
                    1,
                    63,
                    0
                ],
                "title": "MPO: Boosting LLM Agents with Meta Plan Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPO: Boosting LLM Agents with Meta Plan Optimization"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, , which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, , which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios."
                },
                "authors": [
                    {
                        "name": "Weimin Xiong"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Bingchan Zhao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16044v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16044v3",
                "updated": "2025-09-10T16:33:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    33,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-21T20:20:31Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    20,
                    20,
                    31,
                    0,
                    202,
                    0
                ],
                "title": "Making REST APIs Agent-Ready: From OpenAPI to MCP Servers for\n  Tool-Augmented LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making REST APIs Agent-Ready: From OpenAPI to MCP Servers for\n  Tool-Augmented LLMs"
                },
                "summary": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are evolving from passive text generators into\nactive agents that invoke external tools. To support this shift, scalable\nprotocols for tool integration are essential. The Model Context Protocol (MCP),\nintroduced by Anthropic in 2024, offers a schema-driven standard for dynamic\ntool discovery and invocation. Yet, building MCP servers remains manual and\nrepetitive, requiring developers to write glue code, handle authentication, and\nconfigure schemas by hand-replicating much of the integration effort MCP aims\nto eliminate.\n  This paper investigates whether MCP server construction can be meaningfully\nautomated. We begin by analyzing adoption trends: among 22,000+ MCP-tagged\nGitHub repositories created within six months of release, fewer than 5% include\nservers, typically small, single-maintainer projects dominated by repetitive\nscaffolding. To address this gap, we present AutoMCP, a compiler that generates\nMCP servers from OpenAPI 2.0/3.0 specifications. AutoMCP parses REST API\ndefinitions and produces complete server implementations, including schema\nregistration and authentication handling.\n  We evaluate AutoMCP on 50 real-world APIs spanning 5,066 endpoints across\nover 10 domains. From a stratified sample of 1,023 tool calls, 76.5% succeeded\nout of the box. Manual failure analysis revealed five recurring issues, all\nattributable to inconsistencies or omissions in the OpenAPI contracts. After\nminor fixes, averaging 19 lines of spec changes per API, AutoMCP achieved 99.9%\nsuccess.\n  Our findings (i) analyze MCP adoption and quantify the cost of manual server\ndevelopment, (ii) demonstrate that OpenAPI specifications, despite quality\nissues, enable near-complete MCP server automation, and (iii) contribute a\ncorpus of 5,066 callable tools along with insights on repairing common\nspecification flaws."
                },
                "authors": [
                    {
                        "name": "Meriem Mastouri"
                    },
                    {
                        "name": "Emna Ksontini"
                    },
                    {
                        "name": "Wael Kessentini"
                    }
                ],
                "author_detail": {
                    "name": "Wael Kessentini"
                },
                "author": "Wael Kessentini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16044v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16044v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08736v1",
                "updated": "2025-09-10T16:24:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    24,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:24:08Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    24,
                    8,
                    2,
                    253,
                    0
                ],
                "title": "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemBOMAS: Accelerated BO in Chemistry with LLM-Enhanced Multi-Agent\n  System"
                },
                "summary": "The efficiency of Bayesian optimization (BO) in chemistry is often hindered\nby sparse experimental data and complex reaction mechanisms. To overcome these\nlimitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced\nMulti-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization\nprocess is enhanced by LLMs and synergistically employs two strategies:\nknowledge-driven coarse-grained optimization and data-driven fine-grained\noptimization. First, in the knowledge-driven coarse-grained optimization stage,\nLLMs intelligently decompose the vast search space by reasoning over existing\nchemical knowledge to identify promising candidate regions. Subsequently, in\nthe data-driven fine-grained optimization stage, LLMs enhance the BO process\nwithin these candidate regions by generating pseudo-data points, thereby\nimproving data utilization efficiency and accelerating convergence. Benchmark\nevaluations** further confirm that ChemBOMAS significantly enhances\noptimization effectiveness and efficiency compared to various BO algorithms.\nImportantly, the practical utility of ChemBOMAS was validated through wet-lab\nexperiments conducted under pharmaceutical industry protocols, targeting\nconditional optimization for a previously unreported and challenging chemical\nreaction. In the wet experiment, ChemBOMAS achieved an optimal objective value\nof 96%. This was substantially higher than the 15% achieved by domain experts.\nThis real-world success, together with strong performance on benchmark\nevaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical\ndiscovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Bayesian optimization (BO) in chemistry is often hindered\nby sparse experimental data and complex reaction mechanisms. To overcome these\nlimitations, we introduce ChemBOMAS, a new framework named LLM-Enhanced\nMulti-Agent System for accelerating BO in chemistry. ChemBOMAS's optimization\nprocess is enhanced by LLMs and synergistically employs two strategies:\nknowledge-driven coarse-grained optimization and data-driven fine-grained\noptimization. First, in the knowledge-driven coarse-grained optimization stage,\nLLMs intelligently decompose the vast search space by reasoning over existing\nchemical knowledge to identify promising candidate regions. Subsequently, in\nthe data-driven fine-grained optimization stage, LLMs enhance the BO process\nwithin these candidate regions by generating pseudo-data points, thereby\nimproving data utilization efficiency and accelerating convergence. Benchmark\nevaluations** further confirm that ChemBOMAS significantly enhances\noptimization effectiveness and efficiency compared to various BO algorithms.\nImportantly, the practical utility of ChemBOMAS was validated through wet-lab\nexperiments conducted under pharmaceutical industry protocols, targeting\nconditional optimization for a previously unreported and challenging chemical\nreaction. In the wet experiment, ChemBOMAS achieved an optimal objective value\nof 96%. This was substantially higher than the 15% achieved by domain experts.\nThis real-world success, together with strong performance on benchmark\nevaluations, highlights ChemBOMAS as a powerful tool to accelerate chemical\ndiscovery."
                },
                "authors": [
                    {
                        "name": "Dong Han"
                    },
                    {
                        "name": "Zhehong Ai"
                    },
                    {
                        "name": "Pengxiang Cai"
                    },
                    {
                        "name": "Shuzhou Sun"
                    },
                    {
                        "name": "Shanya Lu"
                    },
                    {
                        "name": "Jianpeng Chen"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Lingli Ge"
                    },
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Tao XU"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Shufei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shufei Zhang"
                },
                "author": "Shufei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06037v2",
                "updated": "2025-09-10T16:22:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    22,
                    20,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-09T21:21:55Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    21,
                    21,
                    55,
                    6,
                    40,
                    0
                ],
                "title": "Investigating Compositional Reasoning in Time Series Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Compositional Reasoning in Time Series Foundation Models"
                },
                "summary": "Large pre-trained time series foundation models (TSFMs) have demonstrated\npromising zero-shot performance across a wide range of domains. However, a\nquestion remains: Do TSFMs succeed by memorizing patterns in training data, or\ndo they possess the ability to reason about such patterns? While reasoning is a\ntopic of great interest in the study of Large Language Models (LLMs), it is\nundefined and largely unexplored in the context of TSFMs. In this work,\ninspired by language modeling literature, we formally define compositional\nreasoning in forecasting and distinguish it from in-distribution\ngeneralization. We evaluate the reasoning and generalization capabilities of 16\npopular deep learning forecasting models on multiple synthetic and real-world\ndatasets. Additionally, through controlled studies, we systematically examine\nwhich design choices in 7 popular open-source TSFMs contribute to improved\nreasoning capabilities. Our study yields key insights into the impact of TSFM\narchitecture design on compositional reasoning and generalization. We find that\npatch-based Transformers have the best reasoning performance, closely followed\nby residualized MLP-based architectures, which are 97\\% less computationally\ncomplex in terms of FLOPs and 86\\% smaller in terms of the number of trainable\nparameters. Interestingly, in some zero-shot out-of-distribution scenarios,\nthese models can outperform moving average and exponential smoothing\nstatistical baselines trained on in-distribution data. Only a few design\nchoices, such as the tokenization method, had a significant (negative) impact\non Transformer model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained time series foundation models (TSFMs) have demonstrated\npromising zero-shot performance across a wide range of domains. However, a\nquestion remains: Do TSFMs succeed by memorizing patterns in training data, or\ndo they possess the ability to reason about such patterns? While reasoning is a\ntopic of great interest in the study of Large Language Models (LLMs), it is\nundefined and largely unexplored in the context of TSFMs. In this work,\ninspired by language modeling literature, we formally define compositional\nreasoning in forecasting and distinguish it from in-distribution\ngeneralization. We evaluate the reasoning and generalization capabilities of 16\npopular deep learning forecasting models on multiple synthetic and real-world\ndatasets. Additionally, through controlled studies, we systematically examine\nwhich design choices in 7 popular open-source TSFMs contribute to improved\nreasoning capabilities. Our study yields key insights into the impact of TSFM\narchitecture design on compositional reasoning and generalization. We find that\npatch-based Transformers have the best reasoning performance, closely followed\nby residualized MLP-based architectures, which are 97\\% less computationally\ncomplex in terms of FLOPs and 86\\% smaller in terms of the number of trainable\nparameters. Interestingly, in some zero-shot out-of-distribution scenarios,\nthese models can outperform moving average and exponential smoothing\nstatistical baselines trained on in-distribution data. Only a few design\nchoices, such as the tokenization method, had a significant (negative) impact\non Transformer model performance."
                },
                "authors": [
                    {
                        "name": "Willa Potosnak"
                    },
                    {
                        "name": "Cristian Challu"
                    },
                    {
                        "name": "Mononito Goswami"
                    },
                    {
                        "name": "Kin G. Olivares"
                    },
                    {
                        "name": "Micha Wiliski"
                    },
                    {
                        "name": "Nina ukowska"
                    },
                    {
                        "name": "Artur Dubrawski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Dubrawski"
                },
                "author": "Artur Dubrawski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08729v1",
                "updated": "2025-09-10T16:17:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    17,
                    44,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:17:44Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    17,
                    44,
                    2,
                    253,
                    0
                ],
                "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to\n  Single-turn Jailbreak Templates"
                },
                "summary": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming."
                },
                "authors": [
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Junwoo Ha"
                    },
                    {
                        "name": "Sangyoon Yu"
                    },
                    {
                        "name": "Haon Park"
                    }
                ],
                "author_detail": {
                    "name": "Haon Park"
                },
                "author": "Haon Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08724v1",
                "updated": "2025-09-10T16:15:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    15,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:15:23Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    15,
                    23,
                    2,
                    253,
                    0
                ],
                "title": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-Mirror: Scaling Issue-Resolving Datasets by Mirroring Issues Across\n  Repositories"
                },
                "summary": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating large-scale verifiable training datasets for issue-resolving tasks\nis a critical yet notoriously difficult challenge. Existing methods on\nautomating the Gym environment setup process for real-world issues suffer from\nlow success rates and high overhead. Meanwhile, synthesizing new tasks within\nexisting Gym environments leaves the vast pool of authentic, human-reported\nproblems untapped. To maximize the utilization of existing Gym environments and\nalso the rich data of issue-resolving history on GitHub, we introduce\nSWE-Mirror, a pipeline that distills a real-world issue's semantic essence,\nmirrors it into another repository with a configured Gym environment, and\nre-animates it as a verifiable issue-resolving task. SWE-Mirror reuses existing\nGym environments along with the vast pool of issue-resolving history hosted on\nGitHub to construct a large-scale dataset of mirrored authentic and verifiable\ntasks. Applying SWE-Mirror to 40 repositories across 4 languages, we have\ncurated a dataset with 60,671 issue-resolving tasks and demonstrated the value\nof our dataset by training and evaluating coding agents at various scale.\nPost-training experiments show that models trained with the dataset exhibit\nimprovements in issue-resolving capabilities. Furthermore, by extending the\ndataset size to over 12,000 high-quality trajectories, we established a new\nstate-of-the-art (SOTA) among Qwen2.5-Coder-Instruct based LLMs on the\nOpenHands agent framework, which increases the resolve rate on\nSWE-Bench-Verified by +21.8% for the 7B model and +46.0% for the 32B model and\nvalidates the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Junhao Wang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Shulin Xin"
                    },
                    {
                        "name": "Siyao Liu"
                    },
                    {
                        "name": "Yurong Wu"
                    },
                    {
                        "name": "Kai Shen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Shen"
                },
                "author": "Kai Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05570v2",
                "updated": "2025-09-10T16:11:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    11,
                    13,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-06T02:54:13Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    2,
                    54,
                    13,
                    5,
                    249,
                    0
                ],
                "title": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in\n  e-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LESER: Learning to Expand via Search Engine-feedback Reinforcement in\n  e-Commerce"
                },
                "summary": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User queries in e-commerce search are often vague, short, and underspecified,\nmaking it difficult for retrieval systems to match them accurately against\nstructured product catalogs. This challenge is amplified by the one-to-many\nnature of user intent, where a single query can imply diverse and competing\nneeds. Existing methods, including neural query expansion and prompting-based\nLLM approaches, fall short in real-world settings: they struggle to capture\nnuanced user intent, often generate outputs that violate platform constraints,\nand rely on workflows that are difficult to scale in production. We propose\nLearning to Expand via Search Engine-feedback Reinforcement (LESER), a novel\nframework that fine-tunes a context-aware LLM using real-time search engine\nfeedback as supervision. LESER formulates query expansion as a retrieval\noptimization task and leverages Group Relative Policy Optimization to learn\ndirectly from relevance and coverage metrics. LESER is trained to reason over\nsearch results and produce high quality query expansions that align with\nplatform rules and retrieval objectives. We evaluate LESER on large-scale,\nreal-world e-commerce datasets, demonstrating substantial improvements in both\noffline and online settings. Our results show that LESER not only enhances\nsemantic coverage and retrieval relevance but also delivers measurable gains in\nuser engagement, making it a practical and scalable solution for modern search\nsystems."
                },
                "authors": [
                    {
                        "name": "Yipeng Zhang"
                    },
                    {
                        "name": "Bowen Liu"
                    },
                    {
                        "name": "Xiaoshuang Zhang"
                    },
                    {
                        "name": "Aritra Mandal"
                    },
                    {
                        "name": "Canran Xu"
                    },
                    {
                        "name": "Zhe Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Wu"
                },
                "author": "Zhe Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08715v1",
                "updated": "2025-09-10T16:09:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    9,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T16:09:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    9,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated\n  Cross-Modal Fusion"
                },
                "summary": "As multimodal large language models (MLLMs) advance, their large-scale\narchitectures pose challenges for deployment in resource-constrained\nenvironments. In the age of large models, where energy efficiency,\ncomputational scalability and environmental sustainability are paramount, the\ndevelopment of lightweight and high-performance models is critical for\nreal-world applications. As such, we propose a lightweight MLLM framework for\nend-to-end visual question answering. Our proposed approach centres on\nBreezeCLIP, a compact yet powerful vision-language encoder optimised for\nefficient multimodal understanding. With only 1.2 billion parameters overall,\nour model significantly reduces computational cost while achieving performance\ncomparable to standard-size MLLMs. Experiments conducted on multiple datasets\nfurther validate its effectiveness in balancing accuracy and efficiency. The\nmodular and extensible design enables generalisation to broader multimodal\ntasks. The proposed lightweight vision-language framework is denoted as BcQLM\n(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising\npath toward deployable MLLMs under practical hardware constraints. The source\ncode is available at https://github.com/thico0224/BcQLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multimodal large language models (MLLMs) advance, their large-scale\narchitectures pose challenges for deployment in resource-constrained\nenvironments. In the age of large models, where energy efficiency,\ncomputational scalability and environmental sustainability are paramount, the\ndevelopment of lightweight and high-performance models is critical for\nreal-world applications. As such, we propose a lightweight MLLM framework for\nend-to-end visual question answering. Our proposed approach centres on\nBreezeCLIP, a compact yet powerful vision-language encoder optimised for\nefficient multimodal understanding. With only 1.2 billion parameters overall,\nour model significantly reduces computational cost while achieving performance\ncomparable to standard-size MLLMs. Experiments conducted on multiple datasets\nfurther validate its effectiveness in balancing accuracy and efficiency. The\nmodular and extensible design enables generalisation to broader multimodal\ntasks. The proposed lightweight vision-language framework is denoted as BcQLM\n(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising\npath toward deployable MLLMs under practical hardware constraints. The source\ncode is available at https://github.com/thico0224/BcQLM."
                },
                "authors": [
                    {
                        "name": "Sike Xiang"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Amir Atapour-Abarghouei"
                    }
                ],
                "author_detail": {
                    "name": "Amir Atapour-Abarghouei"
                },
                "author": "Amir Atapour-Abarghouei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00419v2",
                "updated": "2025-09-10T16:01:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    16,
                    1,
                    21,
                    2,
                    253,
                    0
                ],
                "published": "2025-05-31T06:48:12Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    48,
                    12,
                    5,
                    151,
                    0
                ],
                "title": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on\n  Distilled Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching an Old LLM Secure Coding: Localized Preference Optimization on\n  Distilled Preferences"
                },
                "summary": "LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM generated code often contains security issues. We address two key\nchallenges in improving secure code generation. First, obtaining high quality\ntraining data covering a broad set of security issues is critical. To address\nthis, we introduce a method for distilling a preference dataset of insecure and\nsecure code pairs from frontier LLMs, along with a security reasoning that\nexplains the issues and the fix. The key idea here is to make use of security\nknowledge sources to devise a systematic prompting strategy that ensures broad\ncoverage. Second, aligning models to secure code requires focusing on localized\nregions of code. Direct preference optimization methods, like SimPO, are not\ndesigned to handle these localized differences and turn out to be ineffective.\nWe address this with a new localized preference optimization algorithm that\nmasks the security related tokens in both the winning (secure) and losing\n(insecure) responses. To prevent loss in code quality, we also add a\nregularizer. Evaluations show that both training on our dataset, DiSCo, and the\nnew preference optimization algorithm, LPO, yield substantial reductions in\ncode insecurity while also improving overall code quality. Code and dataset are\navailable at https://github.com/StonyBrookNLP/disco-lpo."
                },
                "authors": [
                    {
                        "name": "Mohammad Saqib Hasan"
                    },
                    {
                        "name": "Saikat Chakraborty"
                    },
                    {
                        "name": "Santu Karmaker"
                    },
                    {
                        "name": "Niranjan Balasubramanian"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Balasubramanian"
                },
                "author": "Niranjan Balasubramanian",
                "arxiv_comment": "Accepted to ACL 2025 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21369v2",
                "updated": "2025-09-10T15:55:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    55,
                    35,
                    2,
                    253,
                    0
                ],
                "published": "2025-06-26T15:18:00Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    15,
                    18,
                    0,
                    3,
                    177,
                    0
                ],
                "title": "GenFlow: Interactive Modular System for Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenFlow: Interactive Modular System for Image Generation"
                },
                "summary": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative art unlocks boundless creative possibilities, yet its full\npotential remains untapped due to the technical expertise required for advanced\narchitectural concepts and computational workflows. To bridge this gap, we\npresent GenFlow, a novel modular framework that empowers users of all skill\nlevels to generate images with precision and ease. Featuring a node-based\neditor for seamless customization and an intelligent assistant powered by\nnatural language processing, GenFlow transforms the complexity of workflow\ncreation into an intuitive and accessible experience. By automating deployment\nprocesses and minimizing technical barriers, our framework makes cutting-edge\ngenerative art tools available to everyone. A user study demonstrated GenFlow's\nability to optimize workflows, reduce task completion times, and enhance user\nunderstanding through its intuitive interface and adaptive features. These\nresults position GenFlow as a groundbreaking solution that redefines\naccessibility and efficiency in the realm of generative art."
                },
                "authors": [
                    {
                        "name": "Duc-Hung Nguyen"
                    },
                    {
                        "name": "Huu-Phuc Huynh"
                    },
                    {
                        "name": "Minh-Triet Tran"
                    },
                    {
                        "name": "Trung-Nghia Le"
                    }
                ],
                "author_detail": {
                    "name": "Trung-Nghia Le"
                },
                "author": "Trung-Nghia Le",
                "arxiv_comment": "CBMI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16838v2",
                "updated": "2025-09-10T15:49:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    49,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-24T04:49:49Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    4,
                    49,
                    49,
                    0,
                    55,
                    0
                ],
                "title": "REGen: A Reliable Evaluation Framework for Generative Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REGen: A Reliable Evaluation Framework for Generative Event Argument\n  Extraction"
                },
                "summary": "Event argument extraction identifies arguments for predefined event roles in\ntext. Existing work evaluates this task with exact match (EM), where predicted\narguments must align exactly with annotated spans. While suitable for\nspan-based models, this approach falls short for large language models (LLMs),\nwhich often generate diverse yet semantically accurate arguments. EM severely\nunderestimates performance by disregarding valid variations. Furthermore, EM\nevaluation fails to capture implicit arguments (unstated but inferable) and\nscattered arguments (distributed across a document). These limitations\nunderscore the need for an evaluation framework that better captures models'\nactual performance. To bridge this gap, we introduce REGen, a Reliable\nEvaluation framework for Generative event argument extraction. REGen combines\nthe strengths of exact, relaxed, and LLM-based matching to better align with\nhuman judgment. Experiments on six datasets show that REGen reveals an average\nperformance gain of +23.93 F1 over EM, reflecting capabilities overlooked by\nprior evaluation. Human validation further confirms REGen's effectiveness,\nachieving 87.67% alignment with human assessments of argument correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event argument extraction identifies arguments for predefined event roles in\ntext. Existing work evaluates this task with exact match (EM), where predicted\narguments must align exactly with annotated spans. While suitable for\nspan-based models, this approach falls short for large language models (LLMs),\nwhich often generate diverse yet semantically accurate arguments. EM severely\nunderestimates performance by disregarding valid variations. Furthermore, EM\nevaluation fails to capture implicit arguments (unstated but inferable) and\nscattered arguments (distributed across a document). These limitations\nunderscore the need for an evaluation framework that better captures models'\nactual performance. To bridge this gap, we introduce REGen, a Reliable\nEvaluation framework for Generative event argument extraction. REGen combines\nthe strengths of exact, relaxed, and LLM-based matching to better align with\nhuman judgment. Experiments on six datasets show that REGen reveals an average\nperformance gain of +23.93 F1 over EM, reflecting capabilities overlooked by\nprior evaluation. Human validation further confirms REGen's effectiveness,\nachieving 87.67% alignment with human assessments of argument correctness."
                },
                "authors": [
                    {
                        "name": "Omar Sharif"
                    },
                    {
                        "name": "Joseph Gatto"
                    },
                    {
                        "name": "Madhusudan Basak"
                    },
                    {
                        "name": "Sarah M. Preum"
                    }
                ],
                "author_detail": {
                    "name": "Sarah M. Preum"
                },
                "author": "Sarah M. Preum",
                "arxiv_comment": "Accepted at EMNLP-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05080v2",
                "updated": "2025-09-10T15:45:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    45,
                    41,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-05T13:19:51Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    13,
                    19,
                    51,
                    4,
                    248,
                    0
                ],
                "title": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial\n  Trading"
                },
                "summary": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inherent non-stationarity of financial markets and the complexity of\nmulti-modal information pose significant challenges to existing quantitative\ntrading models. Traditional methods relying on fixed structures and unimodal\ndata struggle to adapt to market regime shifts, while large language model\n(LLM)-driven solutions - despite their multi-modal comprehension - suffer from\nstatic strategies and homogeneous expert designs, lacking dynamic adjustment\nand fine-grained decision mechanisms. To address these limitations, we propose\nMM-DREX: a Multimodal-driven, Dynamically-Routed EXpert framework based on\nlarge language models. MM-DREX explicitly decouples market state perception\nfrom strategy execution to enable adaptive sequential decision-making in\nnon-stationary environments. Specifically, it (1) introduces a vision-language\nmodel (VLM)-powered dynamic router that jointly analyzes candlestick chart\npatterns and long-term temporal features to allocate real-time expert weights;\n(2) designs four heterogeneous trading experts (trend, reversal, breakout,\npositioning) generating specialized fine-grained sub-strategies; and (3)\nproposes an SFT-RL hybrid training paradigm to synergistically optimize the\nrouter's market classification capability and experts' risk-adjusted\ndecision-making. Extensive experiments on multi-modal datasets spanning stocks,\nfutures, and cryptocurrencies demonstrate that MM-DREX significantly\noutperforms 15 baselines (including state-of-the-art financial LLMs and deep\nreinforcement learning models) across key metrics: total return, Sharpe ratio,\nand maximum drawdown, validating its robustness and generalization.\nAdditionally, an interpretability module traces routing logic and expert\nbehavior in real time, providing an audit trail for strategy transparency."
                },
                "authors": [
                    {
                        "name": "Yang Chen"
                    },
                    {
                        "name": "Yueheng Jiang"
                    },
                    {
                        "name": "Zhaozhao Ma"
                    },
                    {
                        "name": "Yuchen Cao"
                    },
                    {
                        "name": "Jacky Keung"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Leilei Gan"
                    },
                    {
                        "name": "Yiquan Wu"
                    },
                    {
                        "name": "Fei Wu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wu"
                },
                "author": "Fei Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.TR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08699v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08699v1",
                "updated": "2025-09-10T15:43:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    43,
                    32,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:43:32Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    43,
                    32,
                    2,
                    253,
                    0
                ],
                "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TANGO: Traversability-Aware Navigation with Local Metric Control for\n  Topological Goals"
                },
                "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO."
                },
                "authors": [
                    {
                        "name": "Stefan Podgorski"
                    },
                    {
                        "name": "Sourav Garg"
                    },
                    {
                        "name": "Mehdi Hosseinzadeh"
                    },
                    {
                        "name": "Lachlan Mares"
                    },
                    {
                        "name": "Feras Dayoub"
                    },
                    {
                        "name": "Ian Reid"
                    }
                ],
                "author_detail": {
                    "name": "Ian Reid"
                },
                "author": "Ian Reid",
                "arxiv_doi": "10.1109/ICRA55743.2025.11127998",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICRA55743.2025.11127998",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.08699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08699v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 5 figures, ICRA 2025",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02253v5",
                "updated": "2025-09-11T12:14:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    14,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-07-03T03:02:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    3,
                    2,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation"
                },
                "summary": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust workflow composition is critical for effective agent performance, yet\nprogress in Large Language Model (LLM) planning and reasoning is hindered by a\nscarcity of scalable evaluation data. This work introduces NL2Flow, a fully\nautomated pipeline for generating and evaluating workflow planning problems.\nNL2Flow generates problems parametrically in a structured intermediate\nrepresentation, translating them into both natural language and formal PDDL. I\nevaluate several open-source, instruct-tuned LLMs on a dataset of 2296\nlow-difficulty problems generated by NL2Flow. Results demonstrate that the\nbest-performing model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans (for solvable problems). Regression analysis shows\nthat the influence of problem characteristics on plan generation is contingent\non both model and prompt design. Importantly, translating natural language\nproblems into a structured JSON representation prior to symbolic planning\nsignificantly improved success rates, suggesting a benefit from neuro-symbolic\nintegration. These findings underscore the importance of understanding error\nsources within LLM reasoning as systems scale to more complex tasks. As LLM\nreasoning scales to increasingly complex problems, understanding the shifting\nbottlenecks and sources of error within these systems will be crucial."
                },
                "authors": [
                    {
                        "name": "Jungkoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jungkoo Kang"
                },
                "author": "Jungkoo Kang",
                "arxiv_comment": "31 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13794v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13794v5",
                "updated": "2025-09-10T15:29:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    29,
                    43,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-18T00:50:40Z",
                "published_parsed": [
                    2025,
                    3,
                    18,
                    0,
                    50,
                    40,
                    1,
                    77,
                    0
                ],
                "title": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated\n  Data Generation"
                },
                "summary": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large foundation models trained on large-scale vision-language data can boost\nOpen-Vocabulary Object Detection (OVD) via synthetic training data, yet the\nhand-crafted pipelines often introduce bias and overfit to specific prompts. We\nsidestep this issue by directly fusing hidden states from Large Language Models\n(LLMs) into detectors-an avenue surprisingly under-explored. This paper\npresents a systematic method to enhance visual grounding by utilizing decoder\nlayers of the LLM of an MLLM. We introduce a zero-initialized cross-attention\nadapter to enable efficient knowledge fusion from LLMs to object detectors, a\nnew approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We\nfind that intermediate LLM layers already encode rich spatial semantics;\nadapting only the early layers yields most of the gain. With Swin-T as the\nvision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at\njust 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to\n6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths\nfurther corroborate our design."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyu Zhao"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Can Jin"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13794v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13794v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08682v1",
                "updated": "2025-09-10T15:22:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    22,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:22:00Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    22,
                    0,
                    2,
                    253,
                    0
                ],
                "title": "Automatic Failure Attribution and Critical Step Prediction Method for\n  Multi-Agent Systems Based on Causal Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Failure Attribution and Critical Step Prediction Method for\n  Multi-Agent Systems Based on Causal Inference"
                },
                "summary": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are critical for automating complex tasks, yet\ntheir practical deployment is severely hampered by the challenge of failure\nattribution. Current diagnostic tools, which rely on statistical correlations,\nare fundamentally inadequate; on challenging benchmarks like Who\\&When,\nstate-of-the-art methods achieve less than 15\\% accuracy in locating the\nroot-cause step of a failure. To address this critical gap, we introduce the\nfirst failure attribution framework for MAS grounded in multi-granularity\ncausal inference. Our approach makes two key technical contributions: (1) a\nperformance causal inversion principle, which correctly models performance\ndependencies by reversing the data flow in execution logs, combined with\nShapley values to accurately assign agent-level blame; (2) a novel causal\ndiscovery algorithm, CDC-MAS, that robustly identifies critical failure steps\nby tackling the non-stationary nature of MAS interaction data. The framework's\nattribution results directly fuel an automated optimization loop, generating\ntargeted suggestions whose efficacy is validated via counterfactual\nsimulations. Evaluations on the Who\\&When and TRAIL benchmarks demonstrate a\nsignificant leap in performance. Our method achieves up to 36.2\\% step-level\naccuracy. Crucially, the generated optimizations boost overall task success\nrates by an average of 22.4\\%. This work provides a principled and effective\nsolution for debugging complex agent interactions, paving the way for more\nreliable and interpretable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Guoqing Ma"
                    },
                    {
                        "name": "Jia Zhu"
                    },
                    {
                        "name": "Hanghui Guo"
                    },
                    {
                        "name": "Weijie Shi"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Jingjiang Liu"
                    },
                    {
                        "name": "Yidan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yidan Liang"
                },
                "author": "Yidan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08672v1",
                "updated": "2025-09-10T15:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    9,
                    1,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    9,
                    1,
                    2,
                    253,
                    0
                ],
                "title": "Universal Graph Learning for Power System Reconfigurations: Transfer\n  Across Topology Variations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Graph Learning for Power System Reconfigurations: Transfer\n  Across Topology Variations"
                },
                "summary": "This work addresses a fundamental challenge in applying deep learning to\npower systems: developing neural network models that transfer across\nsignificant system changes, including networks with entirely different\ntopologies and dimensionalities, without requiring training data from unseen\nreconfigurations. Despite extensive research, most ML-based approaches remain\nsystem-specific, limiting real-world deployment. This limitation stems from a\ndual barrier. First, topology changes shift feature distributions and alter\ninput dimensions due to power flow physics. Second, reconfigurations redefine\noutput semantics and dimensionality, requiring models to handle\nconfiguration-specific outputs while maintaining transferable feature\nextraction. To overcome this challenge, we introduce a Universal Graph\nConvolutional Network (UGCN) that achieves transferability to any\nreconfiguration or variation of existing power systems without any prior\nknowledge of new grid topologies or retraining during implementation. Our\napproach applies to both transmission and distribution networks and\ndemonstrates generalization capability to completely unseen system\nreconfigurations, such as network restructuring and major grid expansions.\nExperimental results across power system applications, including false data\ninjection detection and state forecasting, show that UGCN significantly\noutperforms state-of-the-art methods in cross-system zero-shot transferability\nof new reconfigurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses a fundamental challenge in applying deep learning to\npower systems: developing neural network models that transfer across\nsignificant system changes, including networks with entirely different\ntopologies and dimensionalities, without requiring training data from unseen\nreconfigurations. Despite extensive research, most ML-based approaches remain\nsystem-specific, limiting real-world deployment. This limitation stems from a\ndual barrier. First, topology changes shift feature distributions and alter\ninput dimensions due to power flow physics. Second, reconfigurations redefine\noutput semantics and dimensionality, requiring models to handle\nconfiguration-specific outputs while maintaining transferable feature\nextraction. To overcome this challenge, we introduce a Universal Graph\nConvolutional Network (UGCN) that achieves transferability to any\nreconfiguration or variation of existing power systems without any prior\nknowledge of new grid topologies or retraining during implementation. Our\napproach applies to both transmission and distribution networks and\ndemonstrates generalization capability to completely unseen system\nreconfigurations, such as network restructuring and major grid expansions.\nExperimental results across power system applications, including false data\ninjection detection and state forecasting, show that UGCN significantly\noutperforms state-of-the-art methods in cross-system zero-shot transferability\nof new reconfigurations."
                },
                "authors": [
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Anna Scaglione"
                    },
                    {
                        "name": "Sandy Miguel"
                    },
                    {
                        "name": "Daniel Arnold"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Arnold"
                },
                "author": "Daniel Arnold",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08646v1",
                "updated": "2025-09-10T14:41:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    41,
                    7,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:41:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    41,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute\n  Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architecting Resilient LLM Agents: A Guide to Secure Plan-then-Execute\n  Implementations"
                },
                "summary": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Model (LLM) agents become increasingly capable of\nautomating complex, multi-step tasks, the need for robust, secure, and\npredictable architectural patterns is paramount. This paper provides a\ncomprehensive guide to the ``Plan-then-Execute'' (P-t-E) pattern, an agentic\ndesign that separates strategic planning from tactical execution. We explore\nthe foundational principles of P-t-E, detailing its core components - the\nPlanner and the Executor - and its architectural advantages in predictability,\ncost-efficiency, and reasoning quality over reactive patterns like ReAct\n(Reason + Act). A central focus is placed on the security implications of this\ndesign, particularly its inherent resilience to indirect prompt injection\nattacks by establishing control-flow integrity. We argue that while P-t-E\nprovides a strong foundation, a defense-in-depth strategy is necessary, and we\ndetail essential complementary controls such as the Principle of Least\nPrivilege, task-scoped tool access, and sandboxed code execution. To make these\nprinciples actionable, this guide provides detailed implementation blueprints\nand working code references for three leading agentic frameworks: LangChain\n(via LangGraph), CrewAI, and AutoGen. Each framework's approach to implementing\nthe P-t-E pattern is analyzed, highlighting unique features like LangGraph's\nstateful graphs for re-planning, CrewAI's declarative tool scoping for\nsecurity, and AutoGen's built-in Docker sandboxing. Finally, we discuss\nadvanced patterns, including dynamic re-planning loops, parallel execution with\nDirected Acyclic Graphs (DAGs), and the critical role of Human-in-the-Loop\n(HITL) verification, to offer a complete strategic blueprint for architects,\ndevelopers, and security engineers aiming to build production-grade, resilient,\nand trustworthy LLM agents."
                },
                "authors": [
                    {
                        "name": "Ron F. Del Rosario"
                    },
                    {
                        "name": "Klaudia Krawiecka"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schroeder de Witt"
                },
                "author": "Christian Schroeder de Witt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08948v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08948v3",
                "updated": "2025-09-10T14:40:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    40,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2024-04-13T09:56:50Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    9,
                    56,
                    50,
                    5,
                    104,
                    0
                ],
                "title": "Large Language Models for Mobile GUI Text Input Generation: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Mobile GUI Text Input Generation: An Empirical\n  Study"
                },
                "summary": "Mobile applications have become an essential part of our daily lives, making\nensuring their quality an important activity. Graphical User Interface (GUI)\ntesting is a quality assurance method that has frequently been used for mobile\napps. Some GUIs require these text inputs to be able to move from one page to\nthe next. Recently, Large Language Models (LLMs) have demonstrated excellent\ntext-generation capabilities. To the best of our knowledge, there has not yet\nbeen any empirical study to evaluate different pre-trained LLMs' effectiveness\nat generating text inputs for mobile GUI testing. This paper reports on a\nlarge-scale empirical study that extensively investigates the effectiveness of\neight state-of-the-art LLMs in Android text-input generation for UI pages. We\ncollected 115 Android apps from Google Play and extracted contextual\ninformation from the UI pages to construct prompts for LLMs. The experimental\nresults show that some LLMs can generate more effective and higher-quality text\ninputs. We conducted an experiment to assess the bug-detection capabilities of\nLLMs by directly generating invalid text inputs. We also invited professional\ntesters to manually evaluate, modify, and re-create the LLM-generated text\ninputs. We integrated the text-input generation process into DroidBot to\naugment its UI-exploration capabilities. Finally, we present several valuable\ninsights regarding the application of LLMs to Android testing, particularly for\nthe generation of text inputs: These insights will benefit the Android testing\ncommunity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile applications have become an essential part of our daily lives, making\nensuring their quality an important activity. Graphical User Interface (GUI)\ntesting is a quality assurance method that has frequently been used for mobile\napps. Some GUIs require these text inputs to be able to move from one page to\nthe next. Recently, Large Language Models (LLMs) have demonstrated excellent\ntext-generation capabilities. To the best of our knowledge, there has not yet\nbeen any empirical study to evaluate different pre-trained LLMs' effectiveness\nat generating text inputs for mobile GUI testing. This paper reports on a\nlarge-scale empirical study that extensively investigates the effectiveness of\neight state-of-the-art LLMs in Android text-input generation for UI pages. We\ncollected 115 Android apps from Google Play and extracted contextual\ninformation from the UI pages to construct prompts for LLMs. The experimental\nresults show that some LLMs can generate more effective and higher-quality text\ninputs. We conducted an experiment to assess the bug-detection capabilities of\nLLMs by directly generating invalid text inputs. We also invited professional\ntesters to manually evaluate, modify, and re-create the LLM-generated text\ninputs. We integrated the text-input generation process into DroidBot to\naugment its UI-exploration capabilities. Finally, we present several valuable\ninsights regarding the application of LLMs to Android testing, particularly for\nthe generation of text inputs: These insights will benefit the Android testing\ncommunity."
                },
                "authors": [
                    {
                        "name": "Chenhui Cui"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Dave Towey"
                    },
                    {
                        "name": "Rubing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Rubing Huang"
                },
                "author": "Rubing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08948v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08948v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02825v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02825v6",
                "updated": "2025-09-10T14:39:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    39,
                    59,
                    2,
                    253,
                    0
                ],
                "published": "2025-01-06T07:57:51Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    7,
                    57,
                    51,
                    0,
                    6,
                    0
                ],
                "title": "Randomly Sampled Language Reasoning Problems Elucidate Limitations of\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomly Sampled Language Reasoning Problems Elucidate Limitations of\n  In-Context Learning"
                },
                "summary": "While LLMs have revolutionized the field of machine learning due to their\nhigh performance on a strikingly wide range of problems, they are also known to\nhallucinate false answers and underperform on less canonical versions of the\nsame tasks. There are several emerging theories of LLM performance, among them\nthat LLMs lack world modeling ability, that they have an undesirable bias\ntowards an autoregressive prior, and that they struggle on more novel problems.\nThe existing literature on LLM input novelty has focused on tasks of relatively\nhigh complexity, studying perturbations of canonical but complex problems. In\nthis paper, we attempt to minimize complexity in order to isolate novelty as a\nfactor in LLM underperformance and investigate the power of\nin-context-learning. To this end, we consider an extremely simple domain: next\ntoken prediction on simple language tasks. The twist is that these language\ntasks are wholly unseen, as they are randomly drawn from a large,\nparsimoniously defined set of languages arising from simple grammar rules. This\nexperimental setup allows us to evaluate ICL independently of models'\nparametric knowledge. We find that LLMs uniformly underperform n-gram models on\nthis task, both when used as next token predictors and in chain-of-thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs have revolutionized the field of machine learning due to their\nhigh performance on a strikingly wide range of problems, they are also known to\nhallucinate false answers and underperform on less canonical versions of the\nsame tasks. There are several emerging theories of LLM performance, among them\nthat LLMs lack world modeling ability, that they have an undesirable bias\ntowards an autoregressive prior, and that they struggle on more novel problems.\nThe existing literature on LLM input novelty has focused on tasks of relatively\nhigh complexity, studying perturbations of canonical but complex problems. In\nthis paper, we attempt to minimize complexity in order to isolate novelty as a\nfactor in LLM underperformance and investigate the power of\nin-context-learning. To this end, we consider an extremely simple domain: next\ntoken prediction on simple language tasks. The twist is that these language\ntasks are wholly unseen, as they are randomly drawn from a large,\nparsimoniously defined set of languages arising from simple grammar rules. This\nexperimental setup allows us to evaluate ICL independently of models'\nparametric knowledge. We find that LLMs uniformly underperform n-gram models on\nthis task, both when used as next token predictors and in chain-of-thought."
                },
                "authors": [
                    {
                        "name": "Kavi Gupta"
                    },
                    {
                        "name": "Kate Sanders"
                    },
                    {
                        "name": "Armando Solar-Lezama"
                    }
                ],
                "author_detail": {
                    "name": "Armando Solar-Lezama"
                },
                "author": "Armando Solar-Lezama",
                "arxiv_comment": "10 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02825v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02825v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08638v1",
                "updated": "2025-09-10T14:33:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    33,
                    58,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:33:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    33,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoODD: Agentic Audits via Bayesian Red Teaming in Black-Box Models"
                },
                "summary": "Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specialized machine learning models, regardless of architecture and training,\nare susceptible to failures in deployment. With their increasing use in high\nrisk situations, the ability to audit these models by determining their\noperational design domain (ODD) is crucial in ensuring safety and compliance.\nHowever, given the high-dimensional input spaces, this process often requires\nsignificant human resources and domain expertise. To alleviate this, we\nintroduce \\coolname, an LLM-Agent centric framework for automated generation of\nsemantically relevant test cases to search for failure modes in specialized\nblack-box models. By leveraging LLM-Agents as tool orchestrators, we aim to fit\na uncertainty-aware failure distribution model on a learned text-embedding\nmanifold by projecting the high-dimension input space to low-dimension\ntext-embedding latent space. The LLM-Agent is tasked with iteratively building\nthe failure landscape by leveraging tools for generating test-cases to probe\nthe model-under-test (MUT) and recording the response. The agent also guides\nthe search using tools to probe uncertainty estimate on the low dimensional\nmanifold. We demonstrate this process in a simple case using models trained\nwith missing digits on the MNIST dataset and in the real world setting of\nvision-based intruder detection for aerial vehicles."
                },
                "authors": [
                    {
                        "name": "Rebecca Martin"
                    },
                    {
                        "name": "Jay Patrikar"
                    },
                    {
                        "name": "Sebastian Scherer"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Scherer"
                },
                "author": "Sebastian Scherer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08621v1",
                "updated": "2025-09-10T14:17:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    17,
                    53,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:17:53Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    17,
                    53,
                    2,
                    253,
                    0
                ],
                "title": "AdsQA: Towards Advertisement Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdsQA: Towards Advertisement Video Understanding"
                },
                "summary": "Large language models (LLMs) have taken a great step towards AGI. Meanwhile,\nan increasing number of domain-specific problems such as math and programming\nboost these general-purpose models to continuously evolve via learning deeper\nexpertise. Now is thus the time further to extend the diversity of specialized\napplications for knowledgeable LLMs, though collecting high quality data with\nunexpected and informative tasks is challenging. In this paper, we propose to\nuse advertisement (ad) videos as a challenging test-bed to probe the ability of\nLLMs in perceiving beyond the objective physical content of common visual\ndomain. Our motivation is to take full advantage of the clue-rich and\ninformation-dense ad videos' traits, e.g., marketing logic, persuasive\nstrategies, and audience engagement. Our contribution is three-fold: (1) To our\nknowledge, this is the first attempt to use ad videos with well-designed tasks\nto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark\nderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing\n5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that\nreflects on questions, and generates answers via reward-driven optimization.\n(3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves\nthe state-of-the-art outperforming strong competitors equipped with long-chain\nreasoning capabilities by a clear margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have taken a great step towards AGI. Meanwhile,\nan increasing number of domain-specific problems such as math and programming\nboost these general-purpose models to continuously evolve via learning deeper\nexpertise. Now is thus the time further to extend the diversity of specialized\napplications for knowledgeable LLMs, though collecting high quality data with\nunexpected and informative tasks is challenging. In this paper, we propose to\nuse advertisement (ad) videos as a challenging test-bed to probe the ability of\nLLMs in perceiving beyond the objective physical content of common visual\ndomain. Our motivation is to take full advantage of the clue-rich and\ninformation-dense ad videos' traits, e.g., marketing logic, persuasive\nstrategies, and audience engagement. Our contribution is three-fold: (1) To our\nknowledge, this is the first attempt to use ad videos with well-designed tasks\nto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark\nderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing\n5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that\nreflects on questions, and generates answers via reward-driven optimization.\n(3) We benchmark 14 top-tier LLMs on AdsQA, and our \\texttt{ReAd-R}~achieves\nthe state-of-the-art outperforming strong competitors equipped with long-chain\nreasoning capabilities by a clear margin."
                },
                "authors": [
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Kai Tian"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Guoli Jia"
                    },
                    {
                        "name": "Jingxuan Li"
                    },
                    {
                        "name": "Sa Yang"
                    },
                    {
                        "name": "Yihua Shao"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jiaheng Ma"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "ICCV-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12315v2",
                "updated": "2025-09-10T14:03:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    3,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-17T10:17:23Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    10,
                    17,
                    23,
                    6,
                    229,
                    0
                ],
                "title": "Deciphering the global production network from cross-border firm\n  transactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering the global production network from cross-border firm\n  transactions"
                },
                "summary": "Critical for policy-making and business operations, the study of global\nsupply chains has been severely hampered by a lack of detailed data. Here we\nharness global firm-level transaction data covering 20m global firms, and 1\nbillion cross-border transactions, to infer key inputs for over 1200 products.\nTransforming this data to a directed network, we find that products are\nclustered into three large groups including textiles, chemicals and food, and\nmachinery and metals. European industrial nations and China dominate critical\nintermediate products in the network such as metals, common components and\ntools, while industrial complexity is correlated with embeddedness in densely\nconnected supply chains. To validate the network, we find structural\nsimilarities with two alternative product networks, one generated via LLM\nqueries and the other derived by NAFTA to track product origins. We further\ndetect linkages between products identified in manually mapped single sector\nsupply chains, including electric vehicle batteries and semi-conductors.\nFinally, metrics derived from network structure capturing both forward and\nbackward linkages are able to predict country-product diversification patterns\nwith high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical for policy-making and business operations, the study of global\nsupply chains has been severely hampered by a lack of detailed data. Here we\nharness global firm-level transaction data covering 20m global firms, and 1\nbillion cross-border transactions, to infer key inputs for over 1200 products.\nTransforming this data to a directed network, we find that products are\nclustered into three large groups including textiles, chemicals and food, and\nmachinery and metals. European industrial nations and China dominate critical\nintermediate products in the network such as metals, common components and\ntools, while industrial complexity is correlated with embeddedness in densely\nconnected supply chains. To validate the network, we find structural\nsimilarities with two alternative product networks, one generated via LLM\nqueries and the other derived by NAFTA to track product origins. We further\ndetect linkages between products identified in manually mapped single sector\nsupply chains, including electric vehicle batteries and semi-conductors.\nFinally, metrics derived from network structure capturing both forward and\nbackward linkages are able to predict country-product diversification patterns\nwith high accuracy."
                },
                "authors": [
                    {
                        "name": "Neave O'Clery"
                    },
                    {
                        "name": "Ben Radcliffe-Brown"
                    },
                    {
                        "name": "Thomas Spencer"
                    },
                    {
                        "name": "Daniel Tarling-Hunter"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Tarling-Hunter"
                },
                "author": "Daniel Tarling-Hunter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03867v2",
                "updated": "2025-09-10T14:02:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    50,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-04T03:58:55Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    3,
                    58,
                    55,
                    3,
                    247,
                    0
                ],
                "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth"
                },
                "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\" - utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a benchmark dataset of over 1,200+ meticulously curated and diverse\nexamples across English, Mandarin, Spanish, French, Japanese, and Korean. Each\nexample underwent careful expert review to verify its Drivelological\ncharacteristics, involving multiple rounds of discussion and adjudication to\naddress disagreements. Using this dataset, we evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss implied rhetorical functions\naltogether. These findings highlight a deep representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\" - utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a benchmark dataset of over 1,200+ meticulously curated and diverse\nexamples across English, Mandarin, Spanish, French, Japanese, and Korean. Each\nexample underwent careful expert review to verify its Drivelological\ncharacteristics, involving multiple rounds of discussion and adjudication to\naddress disagreements. Using this dataset, we evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss implied rhetorical functions\naltogether. These findings highlight a deep representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence."
                },
                "authors": [
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Chia-Yi Hsiao"
                    },
                    {
                        "name": "Zi Yan Chang"
                    },
                    {
                        "name": "Chi-Li Chen"
                    },
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11260v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11260v4",
                "updated": "2025-09-10T14:02:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2025-01-20T04:00:02Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    4,
                    0,
                    2,
                    0,
                    20,
                    0
                ],
                "title": "A Survey of World Models for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of World Models for Autonomous Driving"
                },
                "summary": "Recent breakthroughs in autonomous driving have been propelled by advances in\nrobust world modeling, fundamentally transforming how vehicles interpret\ndynamic scenes and execute safe decision-making. World models have emerged as a\nlinchpin technology, offering high-fidelity representations of the driving\nenvironment that integrate multi-sensor data, semantic cues, and temporal\ndynamics. This paper systematically reviews recent advances in world models for\nautonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future\nPhysical World, covering Image-, BEV-, OG-, and PC-based generation methods\nthat enhance scene evolution modeling through diffusion models and 4D occupancy\nforecasting; (ii) Behavior Planning for Intelligent Agents, combining\nrule-driven and learning-based paradigms with cost map optimization and\nreinforcement learning for trajectory generation in complex traffic conditions;\n(ii) Interaction between Prediction and Planning, achieving multi-agent\ncollaborative decision-making through latent space diffusion and\nmemory-augmented architectures. The study further analyzes training paradigms,\nincluding self-supervised learning, multimodal pretraining, and generative data\naugmentation, while evaluating world models' performance in scene understanding\nand motion prediction tasks. Future research must address key challenges in\nself-supervised representation learning, multimodal fusion, and advanced\nsimulation to advance the practical deployment of world models in complex urban\nenvironments. Overall, the comprehensive analysis provides a technical roadmap\nfor harnessing the transformative potential of world models in advancing safe\nand reliable autonomous driving solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in autonomous driving have been propelled by advances in\nrobust world modeling, fundamentally transforming how vehicles interpret\ndynamic scenes and execute safe decision-making. World models have emerged as a\nlinchpin technology, offering high-fidelity representations of the driving\nenvironment that integrate multi-sensor data, semantic cues, and temporal\ndynamics. This paper systematically reviews recent advances in world models for\nautonomous driving, proposing a three-tiered taxonomy: (i) Generation of Future\nPhysical World, covering Image-, BEV-, OG-, and PC-based generation methods\nthat enhance scene evolution modeling through diffusion models and 4D occupancy\nforecasting; (ii) Behavior Planning for Intelligent Agents, combining\nrule-driven and learning-based paradigms with cost map optimization and\nreinforcement learning for trajectory generation in complex traffic conditions;\n(ii) Interaction between Prediction and Planning, achieving multi-agent\ncollaborative decision-making through latent space diffusion and\nmemory-augmented architectures. The study further analyzes training paradigms,\nincluding self-supervised learning, multimodal pretraining, and generative data\naugmentation, while evaluating world models' performance in scene understanding\nand motion prediction tasks. Future research must address key challenges in\nself-supervised representation learning, multimodal fusion, and advanced\nsimulation to advance the practical deployment of world models in complex urban\nenvironments. Overall, the comprehensive analysis provides a technical roadmap\nfor harnessing the transformative potential of world models in advancing safe\nand reliable autonomous driving solutions."
                },
                "authors": [
                    {
                        "name": "Tuo Feng"
                    },
                    {
                        "name": "Wenguan Wang"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Ongoing project. Paper list: https://github.com/FengZicai/AwesomeWMAD\n  Benchmark: https://github.com/FengZicai/WMAD-Benchmarks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11260v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11260v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08604v1",
                "updated": "2025-09-10T14:02:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    18,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T14:02:18Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    14,
                    2,
                    18,
                    2,
                    253,
                    0
                ],
                "title": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in Large Language Models in Medicine: Prevalence,\n  Characteristics, and Implications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information."
                },
                "authors": [
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Lingfei Qian"
                    },
                    {
                        "name": "Mengmeng Du"
                    },
                    {
                        "name": "Yu Yin"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Zihao Sun"
                    },
                    {
                        "name": "Yihang Fu"
                    },
                    {
                        "name": "Erica Stutz"
                    },
                    {
                        "name": "Xuguang Ai"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Siru Liu"
                    },
                    {
                        "name": "Yih-Chung Tham"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    },
                    {
                        "name": "Hyunghoon Cho"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Qingyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qingyu Chen"
                },
                "author": "Qingyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08598v1",
                "updated": "2025-09-10T13:57:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    57,
                    21,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:57:21Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    57,
                    21,
                    2,
                    253,
                    0
                ],
                "title": "Low-Complexity CSI Acquisition Exploiting Geographical Diversity in\n  Fluid Antenna System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity CSI Acquisition Exploiting Geographical Diversity in\n  Fluid Antenna System"
                },
                "summary": "The fluid antenna system (FAS) employs reconfigurable antennas for high\nspatial gains in compact spaces, enhancing physical layer flexibility. Channel\nstate information (CSI) acquisition is vital for port selection and FAS\noptimization. Greedy algorithms rely on signal assumptions, and model-free\nmethods face high complexity. A flexible, low-complexity solution is needed for\nmassive connectivity in FAS. Based on expectation maximization-approximate\nmessage passing (EM-AMP) framework, efficient matrix computations and adaptive\nlearning without prior model knowledge naturally suit CSI acquisition for FAS.\nWe propose a EM-AMP variant exploiting FAS geographical priors, improving\nestimation precision, accelerating convergence, and reducing complexity in\nlarge-scale deployment. Simulations validate the efficacy of the proposed\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fluid antenna system (FAS) employs reconfigurable antennas for high\nspatial gains in compact spaces, enhancing physical layer flexibility. Channel\nstate information (CSI) acquisition is vital for port selection and FAS\noptimization. Greedy algorithms rely on signal assumptions, and model-free\nmethods face high complexity. A flexible, low-complexity solution is needed for\nmassive connectivity in FAS. Based on expectation maximization-approximate\nmessage passing (EM-AMP) framework, efficient matrix computations and adaptive\nlearning without prior model knowledge naturally suit CSI acquisition for FAS.\nWe propose a EM-AMP variant exploiting FAS geographical priors, improving\nestimation precision, accelerating convergence, and reducing complexity in\nlarge-scale deployment. Simulations validate the efficacy of the proposed\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Zhentian Zhang"
                    },
                    {
                        "name": "David Morales-Jimenez"
                    },
                    {
                        "name": "Jian Dang"
                    },
                    {
                        "name": "Zaichen Zhang"
                    },
                    {
                        "name": "Christos Masouros"
                    },
                    {
                        "name": "Hao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Jiang"
                },
                "author": "Hao Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08596v1",
                "updated": "2025-09-10T13:50:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    50,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:50:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    50,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question\n  Answering for BioASQ Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question\n  Answering for BioASQ Challenge"
                },
                "summary": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering."
                },
                "authors": [
                    {
                        "name": "Dima Galat"
                    },
                    {
                        "name": "Diego Molla-Aliod"
                    }
                ],
                "author_detail": {
                    "name": "Diego Molla-Aliod"
                },
                "author": "Diego Molla-Aliod",
                "arxiv_comment": "CEUR-WS, CLEF2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08593v1",
                "updated": "2025-09-10T13:46:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    46,
                    40,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:46:40Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    46,
                    40,
                    2,
                    253,
                    0
                ],
                "title": "No-Knowledge Alarms for Misaligned LLMs-as-Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No-Knowledge Alarms for Misaligned LLMs-as-Judges"
                },
                "summary": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If we use LLMs as judges to evaluate the complex decisions of other LLMs, who\nor what monitors the judges? Infinite monitoring chains are inevitable whenever\nwe do not know the ground truth of the decisions by experts and we do not want\nto trust them. One way to ameliorate our evaluation uncertainty is to exploit\nthe use of logical consistency between disagreeing experts. By observing how\nLLM judges agree and disagree while grading other LLMs, we can compute the only\npossible evaluations of their grading ability. For example, if two LLM judges\ndisagree on which tasks a third one completed correctly, they cannot both be\n100\\% correct in their judgments. This logic can be formalized as a Linear\nProgramming problem in the space of integer response counts for any finite\ntest. We use it here to develop no-knowledge alarms for misaligned LLM judges.\nThe alarms can detect, with no false positives, that at least one member or\nmore of an ensemble of judges are violating a user specified grading ability\nrequirement."
                },
                "authors": [
                    {
                        "name": "Andrs Corrada-Emmanuel"
                    }
                ],
                "author_detail": {
                    "name": "Andrs Corrada-Emmanuel"
                },
                "author": "Andrs Corrada-Emmanuel",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90C05, 68T27",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08578v1",
                "updated": "2025-09-10T13:27:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    27,
                    40,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:27:40Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    27,
                    40,
                    2,
                    253,
                    0
                ],
                "title": "MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust\n  Optimization"
                },
                "summary": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. To address this, we present MAESTRO, a Multi-modal\nAdaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves\nrobustness by adaptively fusing multi-modal inputs-including surveillance, web\nsearch trends, and meteorological data-and leveraging a comprehensive\nspectro-temporal architecture. The model first decomposes time series into\nseasonal and trend components. These are then processed through a hybrid\nfeature enhancement pipeline combining Transformer-based encoders, a Mamba\nstate-space model for long-range dependencies, multi-scale temporal\nconvolutions, and a frequency-domain analysis module. A cross-channel attention\nmechanism further integrates information across the different data modalities.\nFinally, a temporal projection head performs sequence-to-sequence forecasting,\nwith an optional estimator to quantify prediction uncertainty. Evaluated on\nover 11 years of Hong Kong influenza data (excluding the COVID-19 period),\nMAESTRO shows strong competitive performance, demonstrating a superior model\nfit and relative accuracy, achieving a state-of-the-art R-square of 0.956.\nExtensive ablations confirm the significant contributions of both multi-modal\nfusion and the spectro-temporal components. Our modular and reproducible\npipeline is made publicly available to facilitate deployment and extension to\nother regions and pathogens.Our publicly available pipeline presents a\npowerful, unified framework, demonstrating the critical synergy of advanced\nspectro-temporal modeling and multi-modal data fusion for robust\nepidemiological forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely and robust influenza incidence forecasting is critical for public\nhealth decision-making. To address this, we present MAESTRO, a Multi-modal\nAdaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achieves\nrobustness by adaptively fusing multi-modal inputs-including surveillance, web\nsearch trends, and meteorological data-and leveraging a comprehensive\nspectro-temporal architecture. The model first decomposes time series into\nseasonal and trend components. These are then processed through a hybrid\nfeature enhancement pipeline combining Transformer-based encoders, a Mamba\nstate-space model for long-range dependencies, multi-scale temporal\nconvolutions, and a frequency-domain analysis module. A cross-channel attention\nmechanism further integrates information across the different data modalities.\nFinally, a temporal projection head performs sequence-to-sequence forecasting,\nwith an optional estimator to quantify prediction uncertainty. Evaluated on\nover 11 years of Hong Kong influenza data (excluding the COVID-19 period),\nMAESTRO shows strong competitive performance, demonstrating a superior model\nfit and relative accuracy, achieving a state-of-the-art R-square of 0.956.\nExtensive ablations confirm the significant contributions of both multi-modal\nfusion and the spectro-temporal components. Our modular and reproducible\npipeline is made publicly available to facilitate deployment and extension to\nother regions and pathogens.Our publicly available pipeline presents a\npowerful, unified framework, demonstrating the critical synergy of advanced\nspectro-temporal modeling and multi-modal data fusion for robust\nepidemiological forecasting."
                },
                "authors": [
                    {
                        "name": "Hong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Liu"
                },
                "author": "Hong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08575v1",
                "updated": "2025-09-10T13:22:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    22,
                    58,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T13:22:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    22,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "SQLGovernor: An LLM-powered SQL Toolkit for Real World Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQLGovernor: An LLM-powered SQL Toolkit for Real World Application"
                },
                "summary": "SQL queries in real world analytical environments, whether written by humans\nor generated automatically often suffer from syntax errors, inefficiency, or\nsemantic misalignment, especially in complex OLAP scenarios. To address these\nchallenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies\nmultiple functionalities, including syntax correction, query rewriting, query\nmodification, and consistency verification within a structured framework\nenhanced by knowledge management. SQLGovernor introduces a fragment wise\nprocessing strategy to enable fine grained rewriting and localized error\ncorrection, significantly reducing the cognitive load on the LLM. It further\nincorporates a hybrid self learning mechanism guided by expert feedback,\nallowing the system to continuously improve through DBMS output analysis and\nrule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as\nwell as industrial datasets, show that SQLGovernor consistently boosts the\nperformance of base models by up to 10%, while minimizing reliance on manual\nexpertise. Deployed in production environments, SQLGovernor demonstrates strong\npractical utility and effective performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL queries in real world analytical environments, whether written by humans\nor generated automatically often suffer from syntax errors, inefficiency, or\nsemantic misalignment, especially in complex OLAP scenarios. To address these\nchallenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies\nmultiple functionalities, including syntax correction, query rewriting, query\nmodification, and consistency verification within a structured framework\nenhanced by knowledge management. SQLGovernor introduces a fragment wise\nprocessing strategy to enable fine grained rewriting and localized error\ncorrection, significantly reducing the cognitive load on the LLM. It further\nincorporates a hybrid self learning mechanism guided by expert feedback,\nallowing the system to continuously improve through DBMS output analysis and\nrule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as\nwell as industrial datasets, show that SQLGovernor consistently boosts the\nperformance of base models by up to 10%, while minimizing reliance on manual\nexpertise. Deployed in production environments, SQLGovernor demonstrates strong\npractical utility and effective performance."
                },
                "authors": [
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Siqi Shen"
                    },
                    {
                        "name": "Haining Xie"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yu Shen"
                    },
                    {
                        "name": "Danqing Huang"
                    },
                    {
                        "name": "Bo Qian"
                    },
                    {
                        "name": "Yinjun Wu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Peng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Peng Chen"
                },
                "author": "Peng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16523v2",
                "updated": "2025-09-10T13:22:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    13,
                    22,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-23T10:04:21Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    10,
                    4,
                    21,
                    6,
                    54,
                    0
                ],
                "title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation\n  in Machine Reading Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation\n  in Machine Reading Comprehension"
                },
                "summary": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As neural language models achieve human-comparable performance on Machine\nReading Comprehension (MRC) and see widespread adoption, ensuring their\nrobustness in real-world scenarios has become increasingly important. Current\nrobustness evaluation research, though, primarily develops synthetic\nperturbation methods, leaving unclear how well they reflect real life\nscenarios. Considering this, we present a framework to automatically examine\nMRC models on naturally occurring textual perturbations, by replacing paragraph\nin MRC benchmarks with their counterparts based on available Wikipedia edit\nhistory. Such perturbation type is natural as its design does not stem from an\narteficial generative process, inherently distinct from the previously\ninvestigated synthetic approaches. In a large-scale study encompassing SQUAD\ndatasets and various model architectures we observe that natural perturbations\nresult in performance degradation in pre-trained encoder language models. More\nworryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs)\ninherit these errors. Further experiments demonstrate that our findings\ngeneralise to natural perturbations found in other more challenging MRC\nbenchmarks. In an effort to mitigate these errors, we show that it is possible\nto improve the robustness to natural perturbations by training on naturally or\nsynthetically perturbed examples, though a noticeable gap still remains\ncompared to performance on unperturbed data."
                },
                "authors": [
                    {
                        "name": "Yulong Wu"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Riza Batista-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "Riza Batista-Navarro"
                },
                "author": "Riza Batista-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08541v1",
                "updated": "2025-09-10T12:40:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    40,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:40:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    40,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CM-Align: Consistency-based Multilingual Alignment for Large Language\n  Models"
                },
                "summary": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data."
                },
                "authors": [
                    {
                        "name": "Xue Zhang"
                    },
                    {
                        "name": "Yunlong Liang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Songming Zhang"
                    },
                    {
                        "name": "Yufeng Chen"
                    },
                    {
                        "name": "Jinan Xu"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08538v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08538v2",
                "updated": "2025-09-11T11:14:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    11,
                    14,
                    0,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T12:34:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    34,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MESH -- Understanding Videos Like Human: Measuring Hallucinations in\n  Large Video Models"
                },
                "summary": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Video Models (LVMs) build on the semantic capabilities of Large\nLanguage Models (LLMs) and vision modules by integrating temporal information\nto better understand dynamic video content. Despite their progress, LVMs are\nprone to hallucinations-producing inaccurate or irrelevant descriptions.\nCurrent benchmarks for video hallucination depend heavily on manual\ncategorization of video content, neglecting the perception-based processes\nthrough which humans naturally interpret videos. We introduce MESH, a benchmark\ndesigned to evaluate hallucinations in LVMs systematically. MESH uses a\nQuestion-Answering framework with binary and multi-choice formats incorporating\ntarget and trap instances. It follows a bottom-up approach, evaluating basic\nobjects, coarse-to-fine subject features, and subject-action pairs, aligning\nwith human video understanding. We demonstrate that MESH offers an effective\nand comprehensive approach for identifying hallucinations in videos. Our\nevaluations show that while LVMs excel at recognizing basic objects and\nfeatures, their susceptibility to hallucinations increases markedly when\nhandling fine details or aligning multiple actions involving various subjects\nin longer videos."
                },
                "authors": [
                    {
                        "name": "Garry Yang"
                    },
                    {
                        "name": "Zizhe Chen"
                    },
                    {
                        "name": "Man Hon Wong"
                    },
                    {
                        "name": "Haoyu Lei"
                    },
                    {
                        "name": "Yongqiang Chen"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Kaiwen Zhou"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08538v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08538v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17668v2",
                "updated": "2025-09-10T12:25:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    25,
                    27,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-23T16:31:38Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    31,
                    38,
                    2,
                    204,
                    0
                ],
                "title": "How Should We Meta-Learn Reinforcement Learning Algorithms?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Should We Meta-Learn Reinforcement Learning Algorithms?"
                },
                "summary": "The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of meta-learning algorithms from data, instead of relying on\nmanual design, is growing in popularity as a paradigm for improving the\nperformance of machine learning systems. Meta-learning shows particular promise\nfor reinforcement learning (RL), where algorithms are often adapted from\nsupervised or unsupervised learning despite their suboptimality for RL.\nHowever, until now there has been a severe lack of comparison between different\nmeta-learning algorithms, such as using evolution to optimise over black-box\nfunctions or LLMs to propose code. In this paper, we carry out this empirical\ncomparison of the different approaches when applied to a range of meta-learned\nalgorithms which target different parts of the RL pipeline. In addition to\nmeta-train and meta-test performance, we also investigate factors including the\ninterpretability, sample cost and train time for each meta-learning algorithm.\nBased on these findings, we propose several guidelines for meta-learning new RL\nalgorithms which will help ensure that future learned algorithms are as\nperformant as possible."
                },
                "authors": [
                    {
                        "name": "Alexander David Goldie"
                    },
                    {
                        "name": "Zilin Wang"
                    },
                    {
                        "name": "Jaron Cohen"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Shimon Whiteson"
                    }
                ],
                "author_detail": {
                    "name": "Shimon Whiteson"
                },
                "author": "Shimon Whiteson",
                "arxiv_comment": "Accepted paper at Reinforcement Learning Conference (RLC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08535v1",
                "updated": "2025-09-10T12:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    25,
                    13,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    25,
                    13,
                    2,
                    253,
                    0
                ],
                "title": "Agents of Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents of Discovery"
                },
                "summary": "The substantial data volumes encountered in modern particle physics and other\ndomains of fundamental physics research allow (and require) the use of\nincreasingly complex data analysis tools and workflows. While the use of\nmachine learning (ML) tools for data analysis has recently proliferated, these\ntools are typically special-purpose algorithms that rely, for example, on\nencoded physics knowledge to reach optimal performance. In this work, we\ninvestigate a new and orthogonal direction: Using recent progress in large\nlanguage models (LLMs) to create a team of agents -- instances of LLMs with\nspecific subtasks -- that jointly solve data analysis-based research problems\nin a way similar to how a human researcher might: by creating code to operate\nstandard tools and libraries (including ML systems) and by building on results\nof previous iterations. If successful, such agent-based systems could be\ndeployed to automate routine analysis components to counteract the increasing\ncomplexity of modern tool chains. To investigate the capabilities of\ncurrent-generation commercial LLMs, we consider the task of anomaly detection\nvia the publicly available and highly-studied LHC Olympics dataset. Several\ncurrent models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated\nand their stability tested. Overall, we observe the capacity of the agent-based\nsystem to solve this data analysis problem. The best agent-created solutions\nmirror the performance of human state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The substantial data volumes encountered in modern particle physics and other\ndomains of fundamental physics research allow (and require) the use of\nincreasingly complex data analysis tools and workflows. While the use of\nmachine learning (ML) tools for data analysis has recently proliferated, these\ntools are typically special-purpose algorithms that rely, for example, on\nencoded physics knowledge to reach optimal performance. In this work, we\ninvestigate a new and orthogonal direction: Using recent progress in large\nlanguage models (LLMs) to create a team of agents -- instances of LLMs with\nspecific subtasks -- that jointly solve data analysis-based research problems\nin a way similar to how a human researcher might: by creating code to operate\nstandard tools and libraries (including ML systems) and by building on results\nof previous iterations. If successful, such agent-based systems could be\ndeployed to automate routine analysis components to counteract the increasing\ncomplexity of modern tool chains. To investigate the capabilities of\ncurrent-generation commercial LLMs, we consider the task of anomaly detection\nvia the publicly available and highly-studied LHC Olympics dataset. Several\ncurrent models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated\nand their stability tested. Overall, we observe the capacity of the agent-based\nsystem to solve this data analysis problem. The best agent-created solutions\nmirror the performance of human state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Sascha Diefenbacher"
                    },
                    {
                        "name": "Anna Hallin"
                    },
                    {
                        "name": "Gregor Kasieczka"
                    },
                    {
                        "name": "Michael Krmer"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Tim Lukas"
                    }
                ],
                "author_detail": {
                    "name": "Tim Lukas"
                },
                "author": "Tim Lukas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08530v1",
                "updated": "2025-09-10T12:18:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    18,
                    52,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:18:52Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    18,
                    52,
                    2,
                    253,
                    0
                ],
                "title": "Data Skeleton Learning: Scalable Active Clustering with Sparse Graph\n  Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Skeleton Learning: Scalable Active Clustering with Sparse Graph\n  Structures"
                },
                "summary": "In this work, we focus on the efficiency and scalability of pairwise\nconstraint-based active clustering, crucial for processing large-scale data in\napplications such as data mining, knowledge annotation, and AI model\npre-training. Our goals are threefold: (1) to reduce computational costs for\niterative clustering updates; (2) to enhance the impact of user-provided\nconstraints to minimize annotation requirements for precise clustering; and (3)\nto cut down memory usage in practical deployments. To achieve these aims, we\npropose a graph-based active clustering algorithm that utilizes two sparse\ngraphs: one for representing relationships between data (our proposed data\nskeleton) and another for updating this data skeleton. These two graphs work in\nconcert, enabling the refinement of connected subgraphs within the data\nskeleton to create nested clusters. Our empirical analysis confirms that the\nproposed algorithm consistently facilitates more accurate clustering with\ndramatically less input of user-provided constraints, and outperforms its\ncounterparts in terms of computational performance and scalability, while\nmaintaining robustness across various distance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we focus on the efficiency and scalability of pairwise\nconstraint-based active clustering, crucial for processing large-scale data in\napplications such as data mining, knowledge annotation, and AI model\npre-training. Our goals are threefold: (1) to reduce computational costs for\niterative clustering updates; (2) to enhance the impact of user-provided\nconstraints to minimize annotation requirements for precise clustering; and (3)\nto cut down memory usage in practical deployments. To achieve these aims, we\npropose a graph-based active clustering algorithm that utilizes two sparse\ngraphs: one for representing relationships between data (our proposed data\nskeleton) and another for updating this data skeleton. These two graphs work in\nconcert, enabling the refinement of connected subgraphs within the data\nskeleton to create nested clusters. Our empirical analysis confirms that the\nproposed algorithm consistently facilitates more accurate clustering with\ndramatically less input of user-provided constraints, and outperforms its\ncounterparts in terms of computational performance and scalability, while\nmaintaining robustness across various distance metrics."
                },
                "authors": [
                    {
                        "name": "Wen-Bo Xie"
                    },
                    {
                        "name": "Xun Fu"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Yan-Li Lee"
                    },
                    {
                        "name": "Tao Deng"
                    },
                    {
                        "name": "Tian Zou"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Jaideep Srivastavad"
                    }
                ],
                "author_detail": {
                    "name": "Jaideep Srivastavad"
                },
                "author": "Jaideep Srivastavad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05929v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05929v2",
                "updated": "2025-09-10T12:09:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    9,
                    14,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-08T01:40:10Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    1,
                    40,
                    10,
                    4,
                    220,
                    0
                ],
                "title": "Towards Reliable Generative AI-Driven Scaffolding: Reducing\n  Hallucinations and Enhancing Quality in Self-Regulated Learning Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Reliable Generative AI-Driven Scaffolding: Reducing\n  Hallucinations and Enhancing Quality in Self-Regulated Learning Support"
                },
                "summary": "Generative Artificial Intelligence (GenAI) holds a potential to advance\nexisting educational technologies with capabilities to automatically generate\npersonalised scaffolds that support students' self-regulated learning (SRL).\nWhile advancements in large language models (LLMs) promise improvements in the\nadaptability and quality of educational technologies for SRL, there remain\nconcerns about the hallucinations in content generated by LLMs, which can\ncompromise both the learning experience and ethical standards. To address these\nchallenges, we proposed GenAI-enabled approaches for evaluating personalised\nSRL scaffolds before they are presented to students, aiming for reducing\nhallucinations and improving the overall quality of LLM-generated personalised\nscaffolds. Specifically, two approaches are investigated. The first approach\ninvolved developing a multi-agent system approach for reliability evaluation to\nassess the extent to which LLM-generated scaffolds accurately target relevant\nSRL processes. The second approach utilised the \"LLM-as-a-Judge\" technique for\nquality evaluation that evaluates LLM-generated scaffolds for their helpfulness\nin supporting students. We constructed evaluation datasets, and compared our\nresults with single-agent LLM systems and machine learning approach baselines.\nOur findings indicate that the reliability evaluation approach is highly\neffective and outperforms the baselines, showing almost perfect alignment with\nhuman experts' evaluations. Moreover, both proposed evaluation approaches can\nbe harnessed to effectively reduce hallucinations. Additionally, we identified\nand discussed bias limitations of the \"LLM-as-a-Judge\" technique in evaluating\nLLM-generated scaffolds. We suggest incorporating these approaches into\nGenAI-powered personalised SRL scaffolding systems to mitigate hallucination\nissues and improve the overall scaffolding quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) holds a potential to advance\nexisting educational technologies with capabilities to automatically generate\npersonalised scaffolds that support students' self-regulated learning (SRL).\nWhile advancements in large language models (LLMs) promise improvements in the\nadaptability and quality of educational technologies for SRL, there remain\nconcerns about the hallucinations in content generated by LLMs, which can\ncompromise both the learning experience and ethical standards. To address these\nchallenges, we proposed GenAI-enabled approaches for evaluating personalised\nSRL scaffolds before they are presented to students, aiming for reducing\nhallucinations and improving the overall quality of LLM-generated personalised\nscaffolds. Specifically, two approaches are investigated. The first approach\ninvolved developing a multi-agent system approach for reliability evaluation to\nassess the extent to which LLM-generated scaffolds accurately target relevant\nSRL processes. The second approach utilised the \"LLM-as-a-Judge\" technique for\nquality evaluation that evaluates LLM-generated scaffolds for their helpfulness\nin supporting students. We constructed evaluation datasets, and compared our\nresults with single-agent LLM systems and machine learning approach baselines.\nOur findings indicate that the reliability evaluation approach is highly\neffective and outperforms the baselines, showing almost perfect alignment with\nhuman experts' evaluations. Moreover, both proposed evaluation approaches can\nbe harnessed to effectively reduce hallucinations. Additionally, we identified\nand discussed bias limitations of the \"LLM-as-a-Judge\" technique in evaluating\nLLM-generated scaffolds. We suggest incorporating these approaches into\nGenAI-powered personalised SRL scaffolding systems to mitigate hallucination\nissues and improve the overall scaffolding quality."
                },
                "authors": [
                    {
                        "name": "Keyang Qian"
                    },
                    {
                        "name": "Shiqi Liu"
                    },
                    {
                        "name": "Tongguang Li"
                    },
                    {
                        "name": "Mladen Rakovi"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Rui Guan"
                    },
                    {
                        "name": "Inge Molenaar"
                    },
                    {
                        "name": "Sadia Nawaz"
                    },
                    {
                        "name": "Zachari Swiecki"
                    },
                    {
                        "name": "Lixiang Yan"
                    },
                    {
                        "name": "Dragan Gaevi"
                    }
                ],
                "author_detail": {
                    "name": "Dragan Gaevi"
                },
                "author": "Dragan Gaevi",
                "arxiv_doi": "10.1016/j.compedu.2025.105448",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compedu.2025.105448",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05929v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05929v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computers & Education, Volume 240, 2026, 105448",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16654v3",
                "updated": "2025-09-10T11:47:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    47,
                    41,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-20T05:41:22Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    5,
                    41,
                    22,
                    2,
                    232,
                    0
                ],
                "title": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and\n  LLM Spatial Reasoning"
                },
                "summary": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) requires an agent to interpret natural\nlanguage instructions and navigate complex environments. Current approaches\noften adopt a \"black-box\" paradigm, where a single Large Language Model (LLM)\nmakes end-to-end decisions. However, it is plagued by critical vulnerabilities,\nincluding poor spatial reasoning, weak cross-modal grounding, and memory\noverload in long-horizon tasks. To systematically address these issues, we\npropose Memory Spatial Navigation(MSNav), a framework that fuses three modules\ninto a synergistic architecture, which transforms fragile inference into a\nrobust, integrated intelligence. MSNav integrates three modules: Memory Module,\na dynamic map memory module that tackles memory overload through selective node\npruning, enhancing long-range exploration; Spatial Module, a module for spatial\nreasoning and object relationship inference that improves endpoint recognition;\nand Decision Module, a module using LLM-based path planning to execute robust\nactions. Powering Spatial Module, we also introduce an Instruction-Object-Space\n(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),\nwhich outperforms leading commercial LLMs in object list extraction, achieving\nhigher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the\nRoom-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art\nperformance with significant improvements in Success Rate (SR) and Success\nweighted by Path Length (SPL)."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Minghao Zhang"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06591v3",
                "updated": "2025-09-10T11:32:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    32,
                    27,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-08T12:02:38Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    12,
                    2,
                    38,
                    0,
                    251,
                    0
                ],
                "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT\n  Denoising"
                },
                "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications."
                },
                "authors": [
                    {
                        "name": "Yichao Liu"
                    },
                    {
                        "name": "Hengzhi Xue"
                    },
                    {
                        "name": "YueYang Teng"
                    }
                ],
                "author_detail": {
                    "name": "YueYang Teng"
                },
                "author": "YueYang Teng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08494v1",
                "updated": "2025-09-10T11:10:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    10,
                    10,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:10:10Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    10,
                    10,
                    2,
                    253,
                    0
                ],
                "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants"
                },
                "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets."
                },
                "authors": [
                    {
                        "name": "Benjamin Sturgeon"
                    },
                    {
                        "name": "Daniel Samuelson"
                    },
                    {
                        "name": "Jacob Haimes"
                    },
                    {
                        "name": "Jacy Reese Anthis"
                    }
                ],
                "author_detail": {
                    "name": "Jacy Reese Anthis"
                },
                "author": "Jacy Reese Anthis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08493v1",
                "updated": "2025-09-10T11:08:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    8,
                    52,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T11:08:52Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    8,
                    52,
                    2,
                    253,
                    0
                ],
                "title": "Send to which account? Evaluation of an LLM-based Scambaiting System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Send to which account? Evaluation of an LLM-based Scambaiting System"
                },
                "summary": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scammers are increasingly harnessing generative AI(GenAI) technologies to\nproduce convincing phishing content at scale, amplifying financial fraud and\nundermining public trust. While conventional defenses, such as detection\nalgorithms, user training, and reactive takedown efforts remain important, they\noften fall short in dismantling the infrastructure scammers depend on,\nincluding mule bank accounts and cryptocurrency wallets. To bridge this gap, a\nproactive and emerging strategy involves using conversational honeypots to\nengage scammers and extract actionable threat intelligence. This paper presents\nthe first large-scale, real-world evaluation of a scambaiting system powered by\nlarge language models (LLMs). Over a five-month deployment, the system\ninitiated over 2,600 engagements with actual scammers, resulting in a dataset\nof more than 18,700 messages. It achieved an Information Disclosure Rate (IDR)\nof approximately 32%, successfully extracting sensitive financial information\nsuch as mule accounts. Additionally, the system maintained a Human Acceptance\nRate (HAR) of around 70%, indicating strong alignment between LLM-generated\nresponses and human operator preferences. Alongside these successes, our\nanalysis reveals key operational challenges. In particular, the system\nstruggled with engagement takeoff: only 48.7% of scammers responded to the\ninitial seed message sent by defenders. These findings highlight the need for\nfurther refinement and provide actionable insights for advancing the design of\nautomated scambaiting systems."
                },
                "authors": [
                    {
                        "name": "Hossein Siadati"
                    },
                    {
                        "name": "Haadi Jafarian"
                    },
                    {
                        "name": "Sima Jafarikhah"
                    }
                ],
                "author_detail": {
                    "name": "Sima Jafarikhah"
                },
                "author": "Sima Jafarikhah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07894v2",
                "updated": "2025-09-10T11:05:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    11,
                    5,
                    31,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-09T16:24:51Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    16,
                    24,
                    51,
                    1,
                    252,
                    0
                ],
                "title": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiPhO: How Far Are (M)LLMs from Humans in the Latest High School Physics\n  Olympiad Benchmark?"
                },
                "summary": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the physical capabilities of (M)LLMs have garnered increasing\nattention. However, existing benchmarks for physics suffer from two major gaps:\nthey neither provide systematic and up-to-date coverage of real-world physics\ncompetitions such as physics Olympiads, nor enable direct performance\ncomparison with humans. To bridge these gaps, we present HiPhO, the first\nbenchmark dedicated to high school physics Olympiads with human-aligned\nevaluation. Specifically, HiPhO highlights three key innovations. (1)\nComprehensive Data: It compiles 13 latest Olympiad exams from 2024-2025,\nspanning both international and regional competitions, and covering mixed\nmodalities that encompass problems spanning text-only to diagram-based. (2)\nProfessional Evaluation: We adopt official marking schemes to perform\nfine-grained grading at both the answer and step level, fully aligned with\nhuman examiners to ensure high-quality and domain-specific evaluation. (3)\nComparison with Human Contestants: We assign gold, silver, and bronze medals to\nmodels based on official medal thresholds, thereby enabling direct comparison\nbetween (M)LLMs and human contestants. Our large-scale evaluation of 30\nstate-of-the-art (M)LLMs shows that: across 13 exams, open-source MLLMs mostly\nremain at or below the bronze level; open-source LLMs show promising progress\nwith occasional golds; closed-source reasoning MLLMs can achieve 6 to 12 gold\nmedals; and most models still have a significant gap from full marks. These\nresults highlight a substantial performance gap between open-source models and\ntop students, the strong physical reasoning capabilities of closed-source\nreasoning models, and the fact that there is still significant room for\nimprovement. HiPhO, as a rigorous, human-aligned, and Olympiad-focused\nbenchmark for advancing multimodal physical reasoning, is open-source and\navailable at https://github.com/SciYu/HiPhO."
                },
                "authors": [
                    {
                        "name": "Fangchen Yu"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Jiacheng Chen"
                    },
                    {
                        "name": "Fujun Han"
                    },
                    {
                        "name": "Yulun Wu"
                    },
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Ruilizhen Hu"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Yu Cheng"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yun Luo"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08488v1",
                "updated": "2025-09-10T10:56:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    56,
                    28,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:56:28Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    56,
                    28,
                    2,
                    253,
                    0
                ],
                "title": "Design and Development of a Scalable and Energy-Efficient Localization\n  Framework Leveraging LoRa Ranging-Capable Transceivers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Development of a Scalable and Energy-Efficient Localization\n  Framework Leveraging LoRa Ranging-Capable Transceivers"
                },
                "summary": "Precise and energy-efficient localization is a critical requirement in many\nInternet of Things (IoT) applications, particularly in large-scale deployments\nsuch as asset tagging, agriculture, and smart cities, where long battery life\nand cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver\npresents a promising solution for IoT localization. It combines low cost, low\npower, and precise ranging capability over distances of up to 1 km. However,\nthe ranging process requires two devices to be simultaneously active, one\ninitiating the ranging request and the other responding to it, which can lead\nto significant energy expenditure if not properly managed. Despite the\ntransceiver's excellent performance, no existing system-level framework\neffectively manages sleep-wake coordination and role assignment needed for\nenergy-efficient operation. This paper presents a coordination framework that\nsignificantly reduces power consumption while maintaining the inherent precise\nranging capability of the chip. The framework schedules short, synchronized\nwake-up windows between the initiator and the responder, allowing devices to\nremain in deep sleep for most of their duty cycle. This scheduling strategy\nminimizes reliance on precise continuous timing and mitigates drift in low-cost\noscillators. To validate the framework, we designed and developed custom nodes\nthat are compliant with the framework's protocol. Experimental results show\nthat the proposed approach allows a node to stay in ultra-low power mode and\nwake periodically to check for instructions. The node can remain in standby\nmode for up to nine months on a single coin cell battery and can perform\nranging operations on demand in near real-time, all while maintaining a\nlocalization accuracy within five meters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise and energy-efficient localization is a critical requirement in many\nInternet of Things (IoT) applications, particularly in large-scale deployments\nsuch as asset tagging, agriculture, and smart cities, where long battery life\nand cost-effectiveness are crucial. The Semtech SX1280 LoRa transceiver\npresents a promising solution for IoT localization. It combines low cost, low\npower, and precise ranging capability over distances of up to 1 km. However,\nthe ranging process requires two devices to be simultaneously active, one\ninitiating the ranging request and the other responding to it, which can lead\nto significant energy expenditure if not properly managed. Despite the\ntransceiver's excellent performance, no existing system-level framework\neffectively manages sleep-wake coordination and role assignment needed for\nenergy-efficient operation. This paper presents a coordination framework that\nsignificantly reduces power consumption while maintaining the inherent precise\nranging capability of the chip. The framework schedules short, synchronized\nwake-up windows between the initiator and the responder, allowing devices to\nremain in deep sleep for most of their duty cycle. This scheduling strategy\nminimizes reliance on precise continuous timing and mitigates drift in low-cost\noscillators. To validate the framework, we designed and developed custom nodes\nthat are compliant with the framework's protocol. Experimental results show\nthat the proposed approach allows a node to stay in ultra-low power mode and\nwake periodically to check for instructions. The node can remain in standby\nmode for up to nine months on a single coin cell battery and can perform\nranging operations on demand in near real-time, all while maintaining a\nlocalization accuracy within five meters."
                },
                "authors": [
                    {
                        "name": "Hasan Albinsaid"
                    },
                    {
                        "name": "Bodhibrata Mukhopadhyay"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08486v1",
                "updated": "2025-09-10T10:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    51,
                    47,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:51:47Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    51,
                    47,
                    2,
                    253,
                    0
                ],
                "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Too Helpful, Too Harmless, Too Honest or Just Right?"
                },
                "summary": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones."
                },
                "authors": [
                    {
                        "name": "Gautam Siddharth Kashyap"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "EMNLP'25 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08484v1",
                "updated": "2025-09-10T10:49:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    49,
                    21,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:49:21Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    49,
                    21,
                    2,
                    253,
                    0
                ],
                "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in\n  LLM-Generated Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in\n  LLM-Generated Text"
                },
                "summary": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group."
                },
                "authors": [
                    {
                        "name": "Pia Sommerauer"
                    },
                    {
                        "name": "Giulia Rambelli"
                    },
                    {
                        "name": "Tommaso Caselli"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Caselli"
                },
                "author": "Tommaso Caselli",
                "arxiv_comment": "Accepted to EMNLP Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08480v1",
                "updated": "2025-09-10T10:39:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    39,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:39:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    39,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Acquiescence Bias in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiescence Bias in Large Language Models"
                },
                "summary": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement."
                },
                "authors": [
                    {
                        "name": "Daniel Braun"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Braun"
                },
                "author": "Daniel Braun",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21117v3",
                "updated": "2025-09-10T10:32:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    32,
                    57,
                    2,
                    253,
                    0
                ],
                "published": "2025-04-29T18:56:12Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    18,
                    56,
                    12,
                    1,
                    119,
                    0
                ],
                "title": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG\n  Evaluation Prompts"
                },
                "summary": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating natural language generation systems is challenging due to the\ndiversity of valid outputs. While human evaluation is the gold standard, it\nsuffers from inconsistencies, lack of standardisation, and demographic biases,\nlimiting reproducibility. LLM-based evaluators offer a scalable alternative but\nare highly sensitive to prompt design, where small variations can lead to\nsignificant discrepancies. In this work, we propose an inversion learning\nmethod that learns effective reverse mappings from model outputs back to their\ninput instructions, enabling the automatic generation of highly effective,\nmodel-specific evaluation prompts. Our method requires only a single evaluation\nsample and eliminates the need for time-consuming manual prompt engineering,\nthereby improving both efficiency and robustness. Our work contributes toward a\nnew direction for more robust and efficient LLM-based evaluation."
                },
                "authors": [
                    {
                        "name": "Hanhua Hong"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Yiqi Liu"
                    },
                    {
                        "name": "Wenge Rong"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "11 pages, accepted by Transactions of the Association for\n  Computational Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08395v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08395v3",
                "updated": "2025-09-10T10:08:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    8,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-12T13:37:03Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    37,
                    3,
                    2,
                    43,
                    0
                ],
                "title": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in\n  LLM Writing Assistance"
                },
                "summary": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs manifest in real user interactions, making it difficult to\naddress the risks from biased LLMs. Therefore, we create IssueBench: a set of\n2.49m realistic English-language prompts to measure issue bias in LLM writing\nassistance, which we construct based on 3.9k templates (e.g. \"write a blog\nabout\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in 10 state-of-the-art LLMs. We also show that biases are very\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are helping millions of users write texts about\ndiverse issues, and in doing so expose users to different ideas and\nperspectives. This creates concerns about issue bias, where an LLM tends to\npresent just one perspective on a given issue, which in turn may influence how\nusers think about this issue. So far, it has not been possible to measure which\nissue biases LLMs manifest in real user interactions, making it difficult to\naddress the risks from biased LLMs. Therefore, we create IssueBench: a set of\n2.49m realistic English-language prompts to measure issue bias in LLM writing\nassistance, which we construct based on 3.9k templates (e.g. \"write a blog\nabout\") and 212 political issues (e.g. \"AI regulation\") from real user\ninteractions. Using IssueBench, we show that issue biases are common and\npersistent in 10 state-of-the-art LLMs. We also show that biases are very\nsimilar across models, and that all models align more with US Democrat than\nRepublican voter opinion on a subset of issues. IssueBench can easily be\nadapted to include other issues, templates, or tasks. By enabling robust and\nrealistic measurement, we hope that IssueBench can bring a new quality of\nevidence to ongoing discussions about LLM biases and how to address them."
                },
                "authors": [
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Musashi Hinck"
                    },
                    {
                        "name": "Valentin Hofmann"
                    },
                    {
                        "name": "Kobi Hackenburg"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Dirk Hovy"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Hovy"
                },
                "author": "Dirk Hovy",
                "arxiv_comment": "accepted at TACL (pre-MIT Press publication version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08395v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08395v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08461v2",
                "updated": "2025-09-11T13:03:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    13,
                    3,
                    4,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T10:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    7,
                    27,
                    2,
                    253,
                    0
                ],
                "title": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Vision-Language Models for Neutrino Event Classification in\n  High-Energy Physics"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated their\nremarkable capacity to process and reason over structured and unstructured data\nmodalities beyond natural language. In this work, we explore the applications\nof Vision Language Models (VLMs), specifically a fine-tuned variant of LLaMa\n3.2, to the task of identifying neutrino interactions in pixelated detector\ndata from high-energy physics (HEP) experiments. We benchmark this model\nagainst a state-of-the-art convolutional neural network (CNN) architecture,\nsimilar to those used in the NOvA and DUNE experiments, which have achieved\nhigh efficiency and purity in classifying electron and muon neutrino events.\nOur evaluation considers both the classification performance and\ninterpretability of the model predictions. We find that VLMs can outperform\nCNNs, while also providing greater flexibility in integrating auxiliary textual\nor semantic information and offering more interpretable, reasoning-based\npredictions. This work highlights the potential of VLMs as a general-purpose\nbackbone for physics event classification, due to their high performance,\ninterpretability, and generalizability, which opens new avenues for integrating\nmultimodal reasoning in experimental neutrino physics."
                },
                "authors": [
                    {
                        "name": "Dikshant Sagar"
                    },
                    {
                        "name": "Kaiwen Yu"
                    },
                    {
                        "name": "Alejandro Yankelevich"
                    },
                    {
                        "name": "Jianming Bian"
                    },
                    {
                        "name": "Pierre Baldi"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Baldi"
                },
                "author": "Pierre Baldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08460v1",
                "updated": "2025-09-10T10:05:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    5,
                    0,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:05:00Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    5,
                    0,
                    2,
                    253,
                    0
                ],
                "title": "Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Stage Safe Herding Framework for Adversarial Attacker in Dynamic\n  Environment"
                },
                "summary": "Recent advances in robotics have enabled the widespread deployment of\nautonomous robotic systems in complex operational environments, presenting both\nunprecedented opportunities and significant security problems. Traditional\nshepherding approaches based on fixed formations are often ineffective or risky\nin urban and obstacle-rich scenarios, especially when facing adversarial agents\nwith unknown and adaptive behaviors. This paper addresses this challenge as an\nextended herding problem, where defensive robotic systems must safely guide\nadversarial agents with unknown strategies away from protected areas and into\npredetermined safe regions, while maintaining collision-free navigation in\ndynamic environments. We propose a hierarchical hybrid framework based on\nreach-avoid game theory and local motion planning, incorporating a virtual\ncontainment boundary and event-triggered pursuit mechanisms to enable scalable\nand robust multi-agent coordination. Simulation results demonstrate that the\nproposed approach achieves safe and efficient guidance of adversarial agents to\ndesignated regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in robotics have enabled the widespread deployment of\nautonomous robotic systems in complex operational environments, presenting both\nunprecedented opportunities and significant security problems. Traditional\nshepherding approaches based on fixed formations are often ineffective or risky\nin urban and obstacle-rich scenarios, especially when facing adversarial agents\nwith unknown and adaptive behaviors. This paper addresses this challenge as an\nextended herding problem, where defensive robotic systems must safely guide\nadversarial agents with unknown strategies away from protected areas and into\npredetermined safe regions, while maintaining collision-free navigation in\ndynamic environments. We propose a hierarchical hybrid framework based on\nreach-avoid game theory and local motion planning, incorporating a virtual\ncontainment boundary and event-triggered pursuit mechanisms to enable scalable\nand robust multi-agent coordination. Simulation results demonstrate that the\nproposed approach achieves safe and efficient guidance of adversarial agents to\ndesignated regions."
                },
                "authors": [
                    {
                        "name": "Wenqing Wang"
                    },
                    {
                        "name": "Ye Zhang"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyu Wang"
                },
                "author": "Jingyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08459v1",
                "updated": "2025-09-10T10:01:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    1,
                    35,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T10:01:35Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    10,
                    1,
                    35,
                    2,
                    253,
                    0
                ],
                "title": "Printegrated Circuits: Personal Fabrication of 3D Printed Devices with\n  Embedded PCBs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Printegrated Circuits: Personal Fabrication of 3D Printed Devices with\n  Embedded PCBs"
                },
                "summary": "Consumer-level multi-material 3D printing with conductive thermoplastics\nenables fabrication of interactive elements for bespoke tangible devices.\nHowever, large feature sizes, high resistance materials, and limitations of\nprintable control circuitry mean that deployable devices cannot be printed\nwithout post-print assembly steps. To address these challenges, we present\nPrintegrated Circuits, a technique that uses traditional electronics as\nmaterial to 3D print self-contained interactive objects. Embedded PCBs are\nplaced into recesses during a pause in the print, and through a process we term\n\\textit{Prinjection}, conductive filament is injected into their plated-through\nholes. This automatically creates reliable electrical and mechanical contact,\neliminating the need for manual wiring or bespoke connectors. We describe the\ncustom machine code generation that supports our approach, and characterise its\nelectrical and mechanical properties. With our 6 demonstrations, we highlight\nhow the Printegrated Circuits process fits into existing design and prototyping\nworkflows as well as informs future research agendas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consumer-level multi-material 3D printing with conductive thermoplastics\nenables fabrication of interactive elements for bespoke tangible devices.\nHowever, large feature sizes, high resistance materials, and limitations of\nprintable control circuitry mean that deployable devices cannot be printed\nwithout post-print assembly steps. To address these challenges, we present\nPrintegrated Circuits, a technique that uses traditional electronics as\nmaterial to 3D print self-contained interactive objects. Embedded PCBs are\nplaced into recesses during a pause in the print, and through a process we term\n\\textit{Prinjection}, conductive filament is injected into their plated-through\nholes. This automatically creates reliable electrical and mechanical contact,\neliminating the need for manual wiring or bespoke connectors. We describe the\ncustom machine code generation that supports our approach, and characterise its\nelectrical and mechanical properties. With our 6 demonstrations, we highlight\nhow the Printegrated Circuits process fits into existing design and prototyping\nworkflows as well as informs future research agendas."
                },
                "authors": [
                    {
                        "name": "Oliver Child"
                    },
                    {
                        "name": "Ollie Hanton"
                    },
                    {
                        "name": "Jack Dawson"
                    },
                    {
                        "name": "Steve Hodges"
                    },
                    {
                        "name": "Mike Fraser"
                    }
                ],
                "author_detail": {
                    "name": "Mike Fraser"
                },
                "author": "Mike Fraser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15108v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15108v3",
                "updated": "2025-09-10T09:57:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    57,
                    3,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-19T11:05:42Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    11,
                    5,
                    42,
                    2,
                    78,
                    0
                ],
                "title": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VIPER: Visual Perception and Explainable Reasoning for Sequential\n  Decision-Making"
                },
                "summary": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) excel at reasoning on text and\nVision-Language Models (VLMs) are highly effective for visual perception,\napplying those models for visual instruction-based planning remains a widely\nopen problem. In this paper, we introduce VIPER, a novel framework for\nmultimodal instruction-based planning that integrates VLM-based perception with\nLLM-based reasoning. Our approach uses a modular pipeline where a frozen VLM\ngenerates textual descriptions of image observations, which are then processed\nby an LLM policy to predict actions based on the task goal. We fine-tune the\nreasoning module using behavioral cloning and reinforcement learning, improving\nour agent's decision-making capabilities. Experiments on the ALFWorld benchmark\nshow that VIPER significantly outperforms state-of-the-art visual\ninstruction-based planners while narrowing the gap with purely text-based\noracles. By leveraging text as an intermediate representation, VIPER also\nenhances explainability, paving the way for a fine-grained analysis of\nperception and reasoning components."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clemence Grislain"
                    },
                    {
                        "name": "Mohamed Chetouani"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15108v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15108v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13107v2",
                "updated": "2025-09-10T09:50:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    50,
                    51,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-18T17:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    14,
                    3,
                    0,
                    230,
                    0
                ],
                "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All for law and law for all: Adaptive RAG Pipeline for Legal Research"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has transformed how we approach text\ngeneration tasks by grounding Large Language Model (LLM) outputs in retrieved\nknowledge. This capability is especially critical in the legal domain. In this\nwork, we introduce a novel end-to-end RAG pipeline that improves upon previous\nbaselines using three targeted enhancements: (i) a context-aware query\ntranslator that disentangles document references from natural-language\nquestions and adapts retrieval depth and response style based on expertise and\nspecificity, (ii) open-source retrieval strategies using SBERT and GTE\nembeddings that achieve substantial performance gains while remaining\ncost-efficient, and (iii) a comprehensive evaluation and generation framework\nthat combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic\nalignment and faithfulness across models and prompt designs. Our results show\nthat carefully designed open-source pipelines can rival proprietary approaches\nin retrieval quality, while a custom legal-grounded prompt consistently\nproduces more faithful and contextually relevant answers than baseline\nprompting. Taken together, these contributions demonstrate the potential of\ntask-aware, component-level tuning to deliver legally grounded, reproducible,\nand cost-effective RAG systems for legal research assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has transformed how we approach text\ngeneration tasks by grounding Large Language Model (LLM) outputs in retrieved\nknowledge. This capability is especially critical in the legal domain. In this\nwork, we introduce a novel end-to-end RAG pipeline that improves upon previous\nbaselines using three targeted enhancements: (i) a context-aware query\ntranslator that disentangles document references from natural-language\nquestions and adapts retrieval depth and response style based on expertise and\nspecificity, (ii) open-source retrieval strategies using SBERT and GTE\nembeddings that achieve substantial performance gains while remaining\ncost-efficient, and (iii) a comprehensive evaluation and generation framework\nthat combines RAGAS, BERTScore-F1, and ROUGE-Recall to assess semantic\nalignment and faithfulness across models and prompt designs. Our results show\nthat carefully designed open-source pipelines can rival proprietary approaches\nin retrieval quality, while a custom legal-grounded prompt consistently\nproduces more faithful and contextually relevant answers than baseline\nprompting. Taken together, these contributions demonstrate the potential of\ntask-aware, component-level tuning to deliver legally grounded, reproducible,\nand cost-effective RAG systems for legal research assistance."
                },
                "authors": [
                    {
                        "name": "Figarri Keisha"
                    },
                    {
                        "name": "Prince Singh"
                    },
                    {
                        "name": "Pallavi"
                    },
                    {
                        "name": "Dion Fernandes"
                    },
                    {
                        "name": "Aravindh Manivannan"
                    },
                    {
                        "name": "Ilham Wicaksono"
                    },
                    {
                        "name": "Faisal Ahmad"
                    },
                    {
                        "name": "Wiem Ben Rim"
                    }
                ],
                "author_detail": {
                    "name": "Wiem Ben Rim"
                },
                "author": "Wiem Ben Rim",
                "arxiv_comment": "submitted to NLLP 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2; H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06806v3",
                "updated": "2025-09-11T09:37:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    9,
                    37,
                    46,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-08T15:38:31Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    15,
                    38,
                    31,
                    0,
                    251,
                    0
                ],
                "title": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MachineLearningLM: Scaling Many-shot In-context Learning via Continued\n  Pretraining"
                },
                "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU."
                },
                "authors": [
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Pengkun Zhang"
                    },
                    {
                        "name": "Mingzhe Lu"
                    },
                    {
                        "name": "Yanzhen Shen"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08435v1",
                "updated": "2025-09-10T09:31:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    31,
                    17,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:31:17Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    31,
                    17,
                    2,
                    253,
                    0
                ],
                "title": "PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot\n  Diffusion Planner Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot\n  Diffusion Planner Flow Matching"
                },
                "summary": "Diffusion models offer powerful generative capabilities for robot trajectory\nplanning, yet their practical deployment on robots is hindered by a critical\nbottleneck: a reliance on imitation learning from expert demonstrations. This\nparadigm is often impractical for specialized robots where data is scarce and\ncreates an inefficient, theoretically suboptimal training pipeline. To overcome\nthis, we introduce PegasusFlow, a hierarchical rolling-denoising framework that\nenables direct and parallel sampling of trajectory score gradients from\nenvironmental interaction, completely bypassing the need for expert data. Our\ncore innovation is a novel sampling algorithm, Weighted Basis Function\nOptimization (WBFO), which leverages spline basis representations to achieve\nsuperior sample efficiency and faster convergence compared to traditional\nmethods like MPPI. The framework is embedded within a scalable, asynchronous\nparallel simulation architecture that supports massively parallel rollouts for\nefficient data collection. Extensive experiments on trajectory optimization and\nrobotic navigation tasks demonstrate that our approach, particularly\nAction-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,\nsignificantly outperforms baselines. In a challenging barrier-crossing task,\nour method achieved a 100% success rate and was 18% faster than the next-best\nmethod, validating its effectiveness for complex terrain locomotion planning.\nhttps://masteryip.github.io/pegasusflow.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models offer powerful generative capabilities for robot trajectory\nplanning, yet their practical deployment on robots is hindered by a critical\nbottleneck: a reliance on imitation learning from expert demonstrations. This\nparadigm is often impractical for specialized robots where data is scarce and\ncreates an inefficient, theoretically suboptimal training pipeline. To overcome\nthis, we introduce PegasusFlow, a hierarchical rolling-denoising framework that\nenables direct and parallel sampling of trajectory score gradients from\nenvironmental interaction, completely bypassing the need for expert data. Our\ncore innovation is a novel sampling algorithm, Weighted Basis Function\nOptimization (WBFO), which leverages spline basis representations to achieve\nsuperior sample efficiency and faster convergence compared to traditional\nmethods like MPPI. The framework is embedded within a scalable, asynchronous\nparallel simulation architecture that supports massively parallel rollouts for\nefficient data collection. Extensive experiments on trajectory optimization and\nrobotic navigation tasks demonstrate that our approach, particularly\nAction-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,\nsignificantly outperforms baselines. In a challenging barrier-crossing task,\nour method achieved a 100% success rate and was 18% faster than the next-best\nmethod, validating its effectiveness for complex terrain locomotion planning.\nhttps://masteryip.github.io/pegasusflow.github.io/"
                },
                "authors": [
                    {
                        "name": "Lei Ye"
                    },
                    {
                        "name": "Haibo Gao"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Zhelin Zhang"
                    },
                    {
                        "name": "Junqi Shan"
                    },
                    {
                        "name": "Ao Zhang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Ruyi Zhou"
                    },
                    {
                        "name": "Zongquan Deng"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "arxiv_comment": "8 pages, 7 figures, conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08432v1",
                "updated": "2025-09-10T09:24:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    24,
                    37,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:24:37Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    24,
                    37,
                    2,
                    253,
                    0
                ],
                "title": "Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain\n  Location",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluid-Antenna-aided AAV Secure Communications in Eavesdropper Uncertain\n  Location"
                },
                "summary": "For autonomous aerial vehicle (AAV) secure communications, traditional\ndesigns based on fixed position antenna (FPA) lack sufficient spatial degrees\nof freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable\nto eavesdropping. To overcome this problem, this paper proposes a framework\nthat effectively incorporates the fluid antenna (FA) and the artificial noise\n(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple\neavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN\nprecoders, and FA positions. In particular, the worst-case MSR is considered by\ntaking the channel uncertainties due to the uncertainty about eavesdropping\nlocations into account. To tackle the highly coupled optimization variables and\nthe channel uncertainties in the formulated problem, an efficient and robust\nalgorithm is proposed. Particularly, the uncertain regions of eavesdroppers,\nwhose shapes can be arbitrary, are disposed by constructing convex hull. In\naddition, two movement modes of FAs are considered, namely, free movement mode\nand zonal movement mode, for which different optimization techniques are\napplied, respectively. Numerical results show that, the proposed FA schemes\nboost security by exploiting additional spatial DoF rather than transmit power,\nwhile AN provides remarkable gains under high transmit power. Furthermore, the\nsynergy between FA and AN results in a secure advantage that exceeds the sum of\ntheir individual contributions, achieving a balance between security and\nreliability under limited resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For autonomous aerial vehicle (AAV) secure communications, traditional\ndesigns based on fixed position antenna (FPA) lack sufficient spatial degrees\nof freedom (DoF), which leaves the line-of-sight-dominated AAV links vulnerable\nto eavesdropping. To overcome this problem, this paper proposes a framework\nthat effectively incorporates the fluid antenna (FA) and the artificial noise\n(AN) techniques. Specifically, the minimum secrecy rate (MSR) among multiple\neavesdroppers is maximized by jointly optimizing AAV deployment, signal and AN\nprecoders, and FA positions. In particular, the worst-case MSR is considered by\ntaking the channel uncertainties due to the uncertainty about eavesdropping\nlocations into account. To tackle the highly coupled optimization variables and\nthe channel uncertainties in the formulated problem, an efficient and robust\nalgorithm is proposed. Particularly, the uncertain regions of eavesdroppers,\nwhose shapes can be arbitrary, are disposed by constructing convex hull. In\naddition, two movement modes of FAs are considered, namely, free movement mode\nand zonal movement mode, for which different optimization techniques are\napplied, respectively. Numerical results show that, the proposed FA schemes\nboost security by exploiting additional spatial DoF rather than transmit power,\nwhile AN provides remarkable gains under high transmit power. Furthermore, the\nsynergy between FA and AN results in a secure advantage that exceeds the sum of\ntheir individual contributions, achieving a balance between security and\nreliability under limited resources."
                },
                "authors": [
                    {
                        "name": "Yingjie Wu"
                    },
                    {
                        "name": "Junshan Luo"
                    },
                    {
                        "name": "Weiyu Chen"
                    },
                    {
                        "name": "Shilian Wang"
                    },
                    {
                        "name": "Fanggang Wang"
                    },
                    {
                        "name": "Haiyang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Haiyang Ding"
                },
                "author": "Haiyang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08422v1",
                "updated": "2025-09-10T09:10:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    10,
                    18,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:10:18Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    10,
                    18,
                    2,
                    253,
                    0
                ],
                "title": "LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations"
                },
                "summary": "Video-based AI systems are increasingly adopted in safety-critical domains\nsuch as autonomous driving and healthcare. However, interpreting their\ndecisions remains challenging due to the inherent spatiotemporal complexity of\nvideo data and the opacity of deep learning models. Existing explanation\ntechniques often suffer from limited temporal coherence, insufficient\nrobustness, and a lack of actionable causal insights. Current counterfactual\nexplanation methods typically do not incorporate guidance from the target\nmodel, reducing semantic fidelity and practical utility. We introduce Latent\nDiffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework\ndesigned to explain the behavior of video-based AI models. Compared to previous\napproaches, LD-ViCE reduces the computational costs of generating explanations\nby operating in latent space using a state-of-the-art diffusion model, while\nproducing realistic and interpretable counterfactuals through an additional\nrefinement step. Our experiments demonstrate the effectiveness of LD-ViCE\nacross three diverse video datasets, including EchoNet-Dynamic (cardiac\nultrasound), FERV39k (facial expression), and Something-Something V2 (action\nrecognition). LD-ViCE outperforms a recent state-of-the-art method, achieving\nan increase in R2 score of up to 68% while reducing inference time by half.\nQualitative analysis confirms that LD-ViCE generates semantically meaningful\nand temporally coherent explanations, offering valuable insights into the\ntarget model behavior. LD-ViCE represents a valuable step toward the\ntrustworthy deployment of AI in safety-critical domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based AI systems are increasingly adopted in safety-critical domains\nsuch as autonomous driving and healthcare. However, interpreting their\ndecisions remains challenging due to the inherent spatiotemporal complexity of\nvideo data and the opacity of deep learning models. Existing explanation\ntechniques often suffer from limited temporal coherence, insufficient\nrobustness, and a lack of actionable causal insights. Current counterfactual\nexplanation methods typically do not incorporate guidance from the target\nmodel, reducing semantic fidelity and practical utility. We introduce Latent\nDiffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework\ndesigned to explain the behavior of video-based AI models. Compared to previous\napproaches, LD-ViCE reduces the computational costs of generating explanations\nby operating in latent space using a state-of-the-art diffusion model, while\nproducing realistic and interpretable counterfactuals through an additional\nrefinement step. Our experiments demonstrate the effectiveness of LD-ViCE\nacross three diverse video datasets, including EchoNet-Dynamic (cardiac\nultrasound), FERV39k (facial expression), and Something-Something V2 (action\nrecognition). LD-ViCE outperforms a recent state-of-the-art method, achieving\nan increase in R2 score of up to 68% while reducing inference time by half.\nQualitative analysis confirms that LD-ViCE generates semantically meaningful\nand temporally coherent explanations, offering valuable insights into the\ntarget model behavior. LD-ViCE represents a valuable step toward the\ntrustworthy deployment of AI in safety-critical domains."
                },
                "authors": [
                    {
                        "name": "Payal Varshney"
                    },
                    {
                        "name": "Adriano Lucieri"
                    },
                    {
                        "name": "Christoph Balada"
                    },
                    {
                        "name": "Sheraz Ahmed"
                    },
                    {
                        "name": "Andreas Dengel"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Dengel"
                },
                "author": "Andreas Dengel",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02424v2",
                "updated": "2025-09-10T09:08:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    8,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-03T08:32:19Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    8,
                    32,
                    19,
                    3,
                    184,
                    0
                ],
                "title": "CyberRAG: An Agentic RAG cyber attack classification and reporting tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CyberRAG: An Agentic RAG cyber attack classification and reporting tool"
                },
                "summary": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming analysts with\nlogs requiring rapidly evolving expertise. Conventional machine-learning\ndetectors reduce alert volume but still yield many false positives, while\nstandard Retrieval-Augmented Generation (RAG) pipelines often retrieve\nirrelevant context and fail to justify predictions. We present CyberRAG, a\nmodular agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates: (i) fine-tuned classifiers specialized by attack family; (ii)\ntool adapters for enrichment and alerting; and (iii) an iterative\nretrieval-and-reason loop that queries a domain-specific knowledge base until\nevidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG\nadopts an agentic design that enables dynamic control flow and adaptive\nreasoning. This architecture autonomously refines threat labels and\nnatural-language justifications, reducing false positives and enhancing\ninterpretability. It is also extensible: new attack types can be supported by\nadding classifiers without retraining the core agent. CyberRAG was evaluated on\nSQL Injection, XSS, and SSTI, achieving over 94\\% accuracy per class and a\nfinal classification accuracy of 94.92\\% through semantic orchestration.\nGenerated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation, with robustness preserved against adversarial and unseen\npayloads. These results show that agentic, specialist-oriented RAG can combine\nhigh detection accuracy with trustworthy, SOC-ready prose, offering a flexible\npath toward partially automated cyber-defense workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can\ngenerate hundreds of thousands of alerts per hour, overwhelming analysts with\nlogs requiring rapidly evolving expertise. Conventional machine-learning\ndetectors reduce alert volume but still yield many false positives, while\nstandard Retrieval-Augmented Generation (RAG) pipelines often retrieve\nirrelevant context and fail to justify predictions. We present CyberRAG, a\nmodular agent-based RAG framework that delivers real-time classification,\nexplanation, and structured reporting for cyber-attacks. A central LLM agent\norchestrates: (i) fine-tuned classifiers specialized by attack family; (ii)\ntool adapters for enrichment and alerting; and (iii) an iterative\nretrieval-and-reason loop that queries a domain-specific knowledge base until\nevidence is relevant and self-consistent. Unlike traditional RAG, CyberRAG\nadopts an agentic design that enables dynamic control flow and adaptive\nreasoning. This architecture autonomously refines threat labels and\nnatural-language justifications, reducing false positives and enhancing\ninterpretability. It is also extensible: new attack types can be supported by\nadding classifiers without retraining the core agent. CyberRAG was evaluated on\nSQL Injection, XSS, and SSTI, achieving over 94\\% accuracy per class and a\nfinal classification accuracy of 94.92\\% through semantic orchestration.\nGenerated explanations reached 0.94 in BERTScore and 4.9/5 in GPT-4-based\nexpert evaluation, with robustness preserved against adversarial and unseen\npayloads. These results show that agentic, specialist-oriented RAG can combine\nhigh detection accuracy with trustworthy, SOC-ready prose, offering a flexible\npath toward partially automated cyber-defense workflows."
                },
                "authors": [
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Cristian Cosentino"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Angelo Furfaro"
                    },
                    {
                        "name": "Fabrizio Marozzo"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Marozzo"
                },
                "author": "Fabrizio Marozzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07473v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07473v3",
                "updated": "2025-09-10T09:05:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    5,
                    33,
                    2,
                    253,
                    0
                ],
                "published": "2024-10-09T22:53:48Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    22,
                    53,
                    48,
                    2,
                    283,
                    0
                ],
                "title": "Localizing Factual Inconsistencies in Attributable Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localizing Factual Inconsistencies in Attributable Text Generation"
                },
                "summary": "There has been an increasing interest in detecting hallucinations in\nmodel-generated texts, both manually and automatically, at varying levels of\ngranularity. However, most existing methods fail to precisely pinpoint the\nerrors. In this work, we introduce QASemConsistency, a new formalism for\nlocalizing factual inconsistencies in attributable text generation, at a\nfine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics,\nwe propose decomposing the generated text into minimal predicate-argument level\npropositions, expressed as simple question-answer (QA) pairs, and assess\nwhether each individual QA pair is supported by a trusted reference text. As\neach QA pair corresponds to a single semantic relation between a predicate and\nan argument, QASemConsistency effectively localizes the unsupported\ninformation. We first demonstrate the effectiveness of the QASemConsistency\nmethodology for human annotation, by collecting crowdsourced annotations of\ngranular consistency errors, while achieving a substantial inter-annotator\nagreement. This benchmark includes more than 3K instances spanning various\ntasks of attributable text generation. We also show that QASemConsistency\nyields factual consistency scores that correlate well with human judgments.\nFinally, we implement several methods for automatically detecting localized\nfactual inconsistencies, with both supervised entailment models and LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There has been an increasing interest in detecting hallucinations in\nmodel-generated texts, both manually and automatically, at varying levels of\ngranularity. However, most existing methods fail to precisely pinpoint the\nerrors. In this work, we introduce QASemConsistency, a new formalism for\nlocalizing factual inconsistencies in attributable text generation, at a\nfine-grained level. Drawing inspiration from Neo-Davidsonian formal semantics,\nwe propose decomposing the generated text into minimal predicate-argument level\npropositions, expressed as simple question-answer (QA) pairs, and assess\nwhether each individual QA pair is supported by a trusted reference text. As\neach QA pair corresponds to a single semantic relation between a predicate and\nan argument, QASemConsistency effectively localizes the unsupported\ninformation. We first demonstrate the effectiveness of the QASemConsistency\nmethodology for human annotation, by collecting crowdsourced annotations of\ngranular consistency errors, while achieving a substantial inter-annotator\nagreement. This benchmark includes more than 3K instances spanning various\ntasks of attributable text generation. We also show that QASemConsistency\nyields factual consistency scores that correlate well with human judgments.\nFinally, we implement several methods for automatically detecting localized\nfactual inconsistencies, with both supervised entailment models and LLMs."
                },
                "authors": [
                    {
                        "name": "Arie Cattan"
                    },
                    {
                        "name": "Paul Roit"
                    },
                    {
                        "name": "Shiyue Zhang"
                    },
                    {
                        "name": "David Wan"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Mohit Bansal"
                    },
                    {
                        "name": "Ido Dagan"
                    }
                ],
                "author_detail": {
                    "name": "Ido Dagan"
                },
                "author": "Ido Dagan",
                "arxiv_comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07473v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07473v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00404v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00404v2",
                "updated": "2025-09-10T09:04:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    4,
                    12,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-30T08:09:08Z",
                "published_parsed": [
                    2025,
                    8,
                    30,
                    8,
                    9,
                    8,
                    5,
                    242,
                    0
                ],
                "title": "Metis: Training Large Language Models with Advanced Low-Bit Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metis: Training Large Language Models with Advanced Low-Bit Quantization"
                },
                "summary": "This work identifies anisotropic parameter distributions as a fundamental\nbarrier to training large language models (LLMs) with low-bit quantization: a\nfew dominant singular values create wide numerical ranges that conflict with\nthe inherent bias of block-wise quantization. This bias disproportionately\npreserves high-magnitude values while discarding smaller ones, causing training\ninstability and low model performance. This work introduces Metis, a training\nframework that combines (i) spectral decomposition with random embedding to\nefficiently disentangle dominant from long-tail components, compressing broad\ndistributions into quantization-friendly narrow ranges; (ii) adaptive learning\nrates in the spectral domain to amplify underrepresented directions and better\ncapture diverse features critical for performance; and (iii) a dual-range\nregularizer that jointly constrains numerical precision and parameter range\ndistribution, ensuring stable, unbiased low-bit training. With Metis, FP8\ntraining surpasses FP32 baselines, and FP4 training achieves accuracy\ncomparable to FP32, paving the way for robust and scalable LLM training under\nadvanced low-bit quantization. The code implementation for Metis is available\nat: https://github.com/sii-research/Metis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work identifies anisotropic parameter distributions as a fundamental\nbarrier to training large language models (LLMs) with low-bit quantization: a\nfew dominant singular values create wide numerical ranges that conflict with\nthe inherent bias of block-wise quantization. This bias disproportionately\npreserves high-magnitude values while discarding smaller ones, causing training\ninstability and low model performance. This work introduces Metis, a training\nframework that combines (i) spectral decomposition with random embedding to\nefficiently disentangle dominant from long-tail components, compressing broad\ndistributions into quantization-friendly narrow ranges; (ii) adaptive learning\nrates in the spectral domain to amplify underrepresented directions and better\ncapture diverse features critical for performance; and (iii) a dual-range\nregularizer that jointly constrains numerical precision and parameter range\ndistribution, ensuring stable, unbiased low-bit training. With Metis, FP8\ntraining surpasses FP32 baselines, and FP4 training achieves accuracy\ncomparable to FP32, paving the way for robust and scalable LLM training under\nadvanced low-bit quantization. The code implementation for Metis is available\nat: https://github.com/sii-research/Metis."
                },
                "authors": [
                    {
                        "name": "Hengjie Cao"
                    },
                    {
                        "name": "Mengyi Chen"
                    },
                    {
                        "name": "Yifeng Yang"
                    },
                    {
                        "name": "Ruijun Huang"
                    },
                    {
                        "name": "Fang Dong"
                    },
                    {
                        "name": "Jixian Zhou"
                    },
                    {
                        "name": "Anrui Chen"
                    },
                    {
                        "name": "Mingzhi Dong"
                    },
                    {
                        "name": "Yujiang Wang"
                    },
                    {
                        "name": "Jinlong Hou"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Ning Gu"
                    },
                    {
                        "name": "Li Shang"
                    }
                ],
                "author_detail": {
                    "name": "Li Shang"
                },
                "author": "Li Shang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.00404v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00404v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07104v2",
                "updated": "2025-09-10T09:03:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    3,
                    4,
                    2,
                    253,
                    0
                ],
                "published": "2025-06-08T12:18:50Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    12,
                    18,
                    50,
                    6,
                    159,
                    0
                ],
                "title": "How Far Are We from Optimal Reasoning Efficiency?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Far Are We from Optimal Reasoning Efficiency?"
                },
                "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gao"
                    },
                    {
                        "name": "Shu Yan"
                    },
                    {
                        "name": "Qixin Tan"
                    },
                    {
                        "name": "Lu Yang"
                    },
                    {
                        "name": "Shusheng Xu"
                    },
                    {
                        "name": "Wei Fu"
                    },
                    {
                        "name": "Zhiyu Mei"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08416v1",
                "updated": "2025-09-10T09:00:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    0,
                    32,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T09:00:32Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    9,
                    0,
                    32,
                    2,
                    253,
                    0
                ],
                "title": "AutoVeriFix: Automatically Correcting Errors and Enhancing Functional\n  Correctness in LLM-Generated Verilog Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoVeriFix: Automatically Correcting Errors and Enhancing Functional\n  Correctness in LLM-Generated Verilog Code"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating software code for high-level programming languages such as Python\nand C++. However, their application to hardware description languages, such as\nVerilog, is challenging due to the scarcity of high-quality training data.\nCurrent approaches to Verilog code generation using LLMs often focus on\nsyntactic correctness, resulting in code with functional errors. To address\nthese challenges, we present AutoVeriFix, a novel Python-assisted two-stage\nframework designed to enhance the functional correctness of LLM-generated\nVerilog code. In the first stage, LLMs are employed to generate high-level\nPython reference models that define the intended circuit behavior. In the\nsecond stage, these Python models facilitate the creation of automated tests\nthat guide the generation of Verilog RTL implementations. Simulation\ndiscrepancies between the reference model and the Verilog code are iteratively\nused to identify and correct errors, thereby improving the functional accuracy\nand reliability of the LLM-generated Verilog code. Experimental results\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art methods in improving the functional correctness of generated\nVerilog code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\ngenerating software code for high-level programming languages such as Python\nand C++. However, their application to hardware description languages, such as\nVerilog, is challenging due to the scarcity of high-quality training data.\nCurrent approaches to Verilog code generation using LLMs often focus on\nsyntactic correctness, resulting in code with functional errors. To address\nthese challenges, we present AutoVeriFix, a novel Python-assisted two-stage\nframework designed to enhance the functional correctness of LLM-generated\nVerilog code. In the first stage, LLMs are employed to generate high-level\nPython reference models that define the intended circuit behavior. In the\nsecond stage, these Python models facilitate the creation of automated tests\nthat guide the generation of Verilog RTL implementations. Simulation\ndiscrepancies between the reference model and the Verilog code are iteratively\nused to identify and correct errors, thereby improving the functional accuracy\nand reliability of the LLM-generated Verilog code. Experimental results\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art methods in improving the functional correctness of generated\nVerilog code."
                },
                "authors": [
                    {
                        "name": "Yan Tan"
                    },
                    {
                        "name": "Xiangchen Meng"
                    },
                    {
                        "name": "Zijun Jiang"
                    },
                    {
                        "name": "Yangdi Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Yangdi Lyu"
                },
                "author": "Yangdi Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08407v1",
                "updated": "2025-09-10T08:54:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    54,
                    16,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:54:16Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    54,
                    16,
                    2,
                    253,
                    0
                ],
                "title": "An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Iterative LLM Framework for SIBT utilizing RAG-based Adaptive Weight\n  Optimization"
                },
                "summary": "Seed implant brachytherapy (SIBT) is an effective cancer treatment modality;\nhowever, clinical planning often relies on manual adjustment of objective\nfunction weights, leading to inefficiencies and suboptimal results. This study\nproposes an adaptive weight optimization framework for SIBT planning, driven by\nlarge language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated\nwith an automatic planning algorithm in an iterative loop. Starting with fixed\nweights, the LLM evaluates plan quality and recommends new weights in the next\niteration. This process continues until convergence criteria are met, after\nwhich the LLM conducts a comprehensive evaluation to identify the optimal plan.\nA clinical knowledge base, constructed and queried via retrieval-augmented\ngeneration (RAG), enhances the model's domain-specific reasoning. The proposed\nmethod was validated on 23 patient cases, showing that the LLM-assisted\napproach produces plans that are comparable to or exceeding clinically approved\nand fixed-weight plans, in terms of dose homogeneity for the clinical target\nvolume (CTV) and sparing of organs at risk (OARs). The study demonstrates the\npotential use of LLMs in SIBT planning automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seed implant brachytherapy (SIBT) is an effective cancer treatment modality;\nhowever, clinical planning often relies on manual adjustment of objective\nfunction weights, leading to inefficiencies and suboptimal results. This study\nproposes an adaptive weight optimization framework for SIBT planning, driven by\nlarge language models (LLMs). A locally deployed DeepSeek-R1 LLM is integrated\nwith an automatic planning algorithm in an iterative loop. Starting with fixed\nweights, the LLM evaluates plan quality and recommends new weights in the next\niteration. This process continues until convergence criteria are met, after\nwhich the LLM conducts a comprehensive evaluation to identify the optimal plan.\nA clinical knowledge base, constructed and queried via retrieval-augmented\ngeneration (RAG), enhances the model's domain-specific reasoning. The proposed\nmethod was validated on 23 patient cases, showing that the LLM-assisted\napproach produces plans that are comparable to or exceeding clinically approved\nand fixed-weight plans, in terms of dose homogeneity for the clinical target\nvolume (CTV) and sparing of organs at risk (OARs). The study demonstrates the\npotential use of LLMs in SIBT planning automation."
                },
                "authors": [
                    {
                        "name": "Zhuo Xiao"
                    },
                    {
                        "name": "Qinglong Yao"
                    },
                    {
                        "name": "Jingjing Wang"
                    },
                    {
                        "name": "Fugen Zhou"
                    },
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Haitao Sun"
                    },
                    {
                        "name": "Zhe Ji"
                    },
                    {
                        "name": "Yuliang Jiang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Qiuwen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qiuwen Wu"
                },
                "arxiv_affiliation": "Department of Radiation Oncology, Duke University Medical Center, Durham, USA",
                "author": "Qiuwen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08401v2",
                "updated": "2025-09-11T05:42:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    5,
                    42,
                    48,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-10T08:45:08Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    45,
                    8,
                    2,
                    253,
                    0
                ],
                "title": "Two Sides of the Same Optimization Coin: Model Degradation and\n  Representation Collapse in Graph Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Sides of the Same Optimization Coin: Model Degradation and\n  Representation Collapse in Graph Foundation Models"
                },
                "summary": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph foundation models, inspired by the success of LLMs, are designed to\nlearn the optimal embedding from multi-domain TAGs for the downstream\ncross-task generalization capability. During our investigation, graph VQ-MAE\nstands out among the increasingly diverse landscape of GFM architectures. This\nis attributed to its ability to jointly encode topology and textual attributes\nfrom multiple domains into discrete embedding spaces with clear semantic\nboundaries. Despite its potential, domain generalization conflicts cause\nimperceptible pitfalls. In this paper, we instantiate two of them, and they are\njust like two sides of the same GFM optimization coin - Side 1 Model\nDegradation: The encoder and codebook fail to capture the diversity of inputs;\nSide 2 Representation Collapse: The hidden embedding and codebook vector fail\nto preserve semantic separability due to constraints from narrow representation\nsubspaces. These two pitfalls (sides) collectively impair the decoder and\ngenerate the low-quality reconstructed supervision, causing the GFM\noptimization dilemma during pre-training (coin). Through empirical\ninvestigation, we attribute the above challenges to Information Bottleneck and\nRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -\n(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semantic\nfusion strategy and a mixture-of-codebooks with domain-aware routing to improve\ninformation capacity. (2) Regularization Tinker for Optimization Coin, which\nutilizes two additional regularizations to further improve gradient supervision\nin our proposed Information Tinker. Notably, as a flexible architecture, MoT\nadheres to the scaling laws of GFM, offering a controllable model scale.\nCompared to SOTA baselines, experiments on 22 datasets across 6 domains\ndemonstrate that MoT achieves significant improvements in supervised, few-shot,\nand zero-shot scenarios."
                },
                "authors": [
                    {
                        "name": "Xunkai Li"
                    },
                    {
                        "name": "Daohan Su"
                    },
                    {
                        "name": "Sicheng Liu"
                    },
                    {
                        "name": "Ru Zhang"
                    },
                    {
                        "name": "Zhenjun Li"
                    },
                    {
                        "name": "Bing Zhou"
                    },
                    {
                        "name": "Rong-Hua Li"
                    },
                    {
                        "name": "Guoren Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoren Wang"
                },
                "author": "Guoren Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08400v1",
                "updated": "2025-09-10T08:44:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    44,
                    57,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:44:57Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    44,
                    57,
                    2,
                    253,
                    0
                ],
                "title": "Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ubiquitous Intelligence Via Wireless Network-Driven LLMs Evolution"
                },
                "summary": "We introduce ubiquitous intelligence as a paradigm where Large Language\nModels (LLMs) evolve within wireless network-driven ecosystems. Unlike static\nmodel deployments, this approach enables scalable and continuous intelligence\nascension through coordination between networks and LLMs. Wireless networks\nsupport system-orchestrated lifelong learning, while LLMs drive the\nnext-generation network development that is more adaptive and responsive. This\nco-evolution highlights a shift toward self-improving systems, sustaining\ncapability growth across diverse and resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ubiquitous intelligence as a paradigm where Large Language\nModels (LLMs) evolve within wireless network-driven ecosystems. Unlike static\nmodel deployments, this approach enables scalable and continuous intelligence\nascension through coordination between networks and LLMs. Wireless networks\nsupport system-orchestrated lifelong learning, while LLMs drive the\nnext-generation network development that is more adaptive and responsive. This\nco-evolution highlights a shift toward self-improving systems, sustaining\ncapability growth across diverse and resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Xingkun Yin"
                    },
                    {
                        "name": "Feiran You"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14161v3",
                "updated": "2025-09-10T08:35:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    35,
                    19,
                    2,
                    253,
                    0
                ],
                "published": "2024-12-18T18:55:40Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    55,
                    40,
                    2,
                    353,
                    0
                ],
                "title": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TheAgentCompany: Benchmarking LLM Agents on Consequential Real World\n  Tasks"
                },
                "summary": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We interact with computers on an everyday basis, be it in everyday life or\nwork, and many aspects of work can be done entirely with access to a computer\nand the Internet. At the same time, thanks to improvements in large language\nmodels (LLMs), there has also been a rapid development in AI agents that\ninteract with and affect change in their surrounding environments. But how\nperformant are AI agents at accelerating or even autonomously performing\nwork-related tasks? The answer to this question has important implications both\nfor industry looking to adopt AI into their workflows and for economic policy\nto understand the effects that adoption of AI may have on the labor market. To\nmeasure the progress of these LLM agents' performance on performing real-world\nprofessional tasks, in this paper we introduce TheAgentCompany, an extensible\nbenchmark for evaluating AI agents that interact with the world in similar ways\nto those of a digital worker: by browsing the Web, writing code, running\nprograms, and communicating with other coworkers. We build a self-contained\nenvironment with internal web sites and data that mimics a small software\ncompany environment, and create a variety of tasks that may be performed by\nworkers in such a company. We test baseline agents powered by both closed\nAPI-based and open-weights language models (LMs), and find that the most\ncompetitive agent can complete 30% of tasks autonomously. This paints a nuanced\npicture on task automation with LM agents--in a setting simulating a real\nworkplace, a good portion of simpler tasks could be solved autonomously, but\nmore difficult long-horizon tasks are still beyond the reach of current\nsystems. We release code, data, environment, and experiments on\nhttps://the-agent-company.com."
                },
                "authors": [
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Yufan Song"
                    },
                    {
                        "name": "Boxuan Li"
                    },
                    {
                        "name": "Yuxuan Tang"
                    },
                    {
                        "name": "Kritanjali Jain"
                    },
                    {
                        "name": "Mengxue Bao"
                    },
                    {
                        "name": "Zora Z. Wang"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Zhitong Guo"
                    },
                    {
                        "name": "Murong Cao"
                    },
                    {
                        "name": "Mingyang Yang"
                    },
                    {
                        "name": "Hao Yang Lu"
                    },
                    {
                        "name": "Amaad Martin"
                    },
                    {
                        "name": "Zhe Su"
                    },
                    {
                        "name": "Leander Maben"
                    },
                    {
                        "name": "Raj Mehta"
                    },
                    {
                        "name": "Wayne Chi"
                    },
                    {
                        "name": "Lawrence Jang"
                    },
                    {
                        "name": "Yiqing Xie"
                    },
                    {
                        "name": "Shuyan Zhou"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08385v1",
                "updated": "2025-09-10T08:23:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    23,
                    58,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:23:58Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    23,
                    58,
                    2,
                    253,
                    0
                ],
                "title": "LLM-Guided Anstze Design for Quantum Circuit Born Machines in\n  Financial Generative Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Anstze Design for Quantum Circuit Born Machines in\n  Financial Generative Modeling"
                },
                "summary": "Quantum generative modeling using quantum circuit Born machines (QCBMs) shows\npromising potential for practical quantum advantage. However, discovering\nans\\\"atze that are both expressive and hardware-efficient remains a key\nchallenge, particularly on noisy intermediate-scale quantum (NISQ) devices. In\nthis work, we introduce a prompt-based framework that leverages large language\nmodels (LLMs) to generate hardware-aware QCBM architectures. Prompts are\nconditioned on qubit connectivity, gate error rates, and hardware topology,\nwhile iterative feedback, including Kullback-Leibler (KL) divergence, circuit\ndepth, and validity, is used to refine the circuits. We evaluate our method on\na financial modeling task involving daily changes in Japanese government bond\n(JGB) interest rates. Our results show that the LLM-generated ans\\\"atze are\nsignificantly shallower and achieve superior generative performance compared to\nthe standard baseline when executed on real IBM quantum hardware using 12\nqubits. These findings demonstrate the practical utility of LLM-driven quantum\narchitecture search and highlight a promising path toward robust, deployable\ngenerative models for near-term quantum devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum generative modeling using quantum circuit Born machines (QCBMs) shows\npromising potential for practical quantum advantage. However, discovering\nans\\\"atze that are both expressive and hardware-efficient remains a key\nchallenge, particularly on noisy intermediate-scale quantum (NISQ) devices. In\nthis work, we introduce a prompt-based framework that leverages large language\nmodels (LLMs) to generate hardware-aware QCBM architectures. Prompts are\nconditioned on qubit connectivity, gate error rates, and hardware topology,\nwhile iterative feedback, including Kullback-Leibler (KL) divergence, circuit\ndepth, and validity, is used to refine the circuits. We evaluate our method on\na financial modeling task involving daily changes in Japanese government bond\n(JGB) interest rates. Our results show that the LLM-generated ans\\\"atze are\nsignificantly shallower and achieve superior generative performance compared to\nthe standard baseline when executed on real IBM quantum hardware using 12\nqubits. These findings demonstrate the practical utility of LLM-driven quantum\narchitecture search and highlight a promising path toward robust, deployable\ngenerative models for near-term quantum devices."
                },
                "authors": [
                    {
                        "name": "Yaswitha Gujju"
                    },
                    {
                        "name": "Romain Harang"
                    },
                    {
                        "name": "Tetsuo Shibuya"
                    }
                ],
                "author_detail": {
                    "name": "Tetsuo Shibuya"
                },
                "author": "Tetsuo Shibuya",
                "arxiv_comment": "Work presented at the 3rd International Workshop on Quantum Machine\n  Learning: From Research to Practice (QML@QCE'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08383v1",
                "updated": "2025-09-10T08:23:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    23,
                    14,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:23:14Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    23,
                    14,
                    2,
                    253,
                    0
                ],
                "title": "Efficient Decoding Methods for Language Models on Encrypted Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Decoding Methods for Language Models on Encrypted Data"
                },
                "summary": "Large language models (LLMs) power modern AI applications, but processing\nsensitive data on untrusted servers raises privacy concerns. Homomorphic\nencryption (HE) enables computation on encrypted data for secure inference.\nHowever, neural text generation requires decoding methods like argmax and\nsampling, which are non-polynomial and thus computationally expensive under\nencryption, creating a significant performance bottleneck. We introduce cutmax,\nan HE-friendly argmax algorithm that reduces ciphertext operations compared to\nprior methods, enabling practical greedy decoding under encryption. We also\npropose the first HE-compatible nucleus (top-p) sampling method, leveraging\ncutmax for efficient stochastic decoding with provable privacy guarantees. Both\ntechniques are polynomial, supporting efficient inference in privacy-preserving\nsettings. Moreover, their differentiability facilitates gradient-based\nsequence-level optimization as a polynomial alternative to straight-through\nestimators. We further provide strong theoretical guarantees for cutmax,\nproving it converges globally to a unique two-level fixed point, independent of\nthe input values beyond the identity of the maximizer, which explains its rapid\nconvergence in just a few iterations. Evaluations on realistic LLM outputs show\nlatency reductions of 24x-35x over baselines, advancing secure text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) power modern AI applications, but processing\nsensitive data on untrusted servers raises privacy concerns. Homomorphic\nencryption (HE) enables computation on encrypted data for secure inference.\nHowever, neural text generation requires decoding methods like argmax and\nsampling, which are non-polynomial and thus computationally expensive under\nencryption, creating a significant performance bottleneck. We introduce cutmax,\nan HE-friendly argmax algorithm that reduces ciphertext operations compared to\nprior methods, enabling practical greedy decoding under encryption. We also\npropose the first HE-compatible nucleus (top-p) sampling method, leveraging\ncutmax for efficient stochastic decoding with provable privacy guarantees. Both\ntechniques are polynomial, supporting efficient inference in privacy-preserving\nsettings. Moreover, their differentiability facilitates gradient-based\nsequence-level optimization as a polynomial alternative to straight-through\nestimators. We further provide strong theoretical guarantees for cutmax,\nproving it converges globally to a unique two-level fixed point, independent of\nthe input values beyond the identity of the maximizer, which explains its rapid\nconvergence in just a few iterations. Evaluations on realistic LLM outputs show\nlatency reductions of 24x-35x over baselines, advancing secure text generation."
                },
                "authors": [
                    {
                        "name": "Matan Avitan"
                    },
                    {
                        "name": "Moran Baruch"
                    },
                    {
                        "name": "Nir Drucker"
                    },
                    {
                        "name": "Itamar Zimerman"
                    },
                    {
                        "name": "Yoav Goldberg"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Goldberg"
                },
                "author": "Yoav Goldberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08381v1",
                "updated": "2025-09-10T08:19:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    19,
                    7,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:19:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    19,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information\n  Extraction with a Billion-Parameter Instruction-Tuned Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Resource Fine-Tuning for Multi-Task Structured Information\n  Extraction with a Billion-Parameter Instruction-Tuned Model"
                },
                "summary": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Yu Cheng Chih"
                    },
                    {
                        "name": "Yong Hao Hou"
                    }
                ],
                "author_detail": {
                    "name": "Yong Hao Hou"
                },
                "author": "Yong Hao Hou",
                "arxiv_comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08380v1",
                "updated": "2025-09-10T08:16:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    16,
                    4,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:16:04Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    16,
                    4,
                    2,
                    253,
                    0
                ],
                "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML\n  Compliance Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML\n  Compliance Narratives"
                },
                "summary": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a\nhigh-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.\nWhile large language models (LLMs) offer promising fluency, they suffer from\nfactual hallucination, limited crime typology alignment, and poor\nexplainability -- posing unacceptable risks in compliance-critical domains.\nThis paper introduces Co-Investigator AI, an agentic framework optimized to\nproduce Suspicious Activity Reports (SARs) significantly faster and with\ngreater accuracy than traditional methods. Drawing inspiration from recent\nadvances in autonomous agent architectures, such as the AI Co-Scientist, our\napproach integrates specialized agents for planning, crime type detection,\nexternal intelligence gathering, and compliance validation. The system features\ndynamic memory management, an AI-Privacy Guard layer for sensitive data\nhandling, and a real-time validation agent employing the Agent-as-a-Judge\nparadigm to ensure continuous narrative quality assurance. Human investigators\nremain firmly in the loop, empowered to review and refine drafts in a\ncollaborative workflow that blends AI efficiency with domain expertise. We\ndemonstrate the versatility of Co-Investigator AI across a range of complex\nfinancial crime scenarios, highlighting its ability to streamline SAR drafting,\nalign narratives with regulatory expectations, and enable compliance teams to\nfocus on higher-order analytical work. This approach marks the beginning of a\nnew era in compliance reporting -- bringing the transformative benefits of AI\nagents to the core of regulatory processes and paving the way for scalable,\nreliable, and transparent SAR generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a\nhigh-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows.\nWhile large language models (LLMs) offer promising fluency, they suffer from\nfactual hallucination, limited crime typology alignment, and poor\nexplainability -- posing unacceptable risks in compliance-critical domains.\nThis paper introduces Co-Investigator AI, an agentic framework optimized to\nproduce Suspicious Activity Reports (SARs) significantly faster and with\ngreater accuracy than traditional methods. Drawing inspiration from recent\nadvances in autonomous agent architectures, such as the AI Co-Scientist, our\napproach integrates specialized agents for planning, crime type detection,\nexternal intelligence gathering, and compliance validation. The system features\ndynamic memory management, an AI-Privacy Guard layer for sensitive data\nhandling, and a real-time validation agent employing the Agent-as-a-Judge\nparadigm to ensure continuous narrative quality assurance. Human investigators\nremain firmly in the loop, empowered to review and refine drafts in a\ncollaborative workflow that blends AI efficiency with domain expertise. We\ndemonstrate the versatility of Co-Investigator AI across a range of complex\nfinancial crime scenarios, highlighting its ability to streamline SAR drafting,\nalign narratives with regulatory expectations, and enable compliance teams to\nfocus on higher-order analytical work. This approach marks the beginning of a\nnew era in compliance reporting -- bringing the transformative benefits of AI\nagents to the core of regulatory processes and paving the way for scalable,\nreliable, and transparent SAR generation."
                },
                "authors": [
                    {
                        "name": "Prathamesh Vasudeo Naik"
                    },
                    {
                        "name": "Naresh Kumar Dintakurthi"
                    },
                    {
                        "name": "Zhanghao Hu"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Robby Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Robby Qiu"
                },
                "author": "Robby Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08378v1",
                "updated": "2025-09-10T08:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    15,
                    44,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:15:44Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    15,
                    44,
                    2,
                    253,
                    0
                ],
                "title": "A Planning Strategy for Building a Heterogeneous Smart EM Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Planning Strategy for Building a Heterogeneous Smart EM Environment"
                },
                "summary": "This paper presents a planning strategy for the deployment of smart\nelectromagnetic entities (SEEs) to enhance the wireless coverage and the\nQuality-of-Service (QoS) in large urban areas. The integration of different\ntechnological solutions such as integrated access-and-backhaul nodes (IABs),\nsmart repeaters (SRs), and electromagnetic skins (EMSs) is here addressed to\nenable an effective and efficient implementation of the concept of Smart\nElectromagnetic Environment (SEME). By combining the features of such\nheterogeneous SEEs and optimizing their number, positions, orientations, and\nconfiguration, the electromagnetic (EM) coverage in a set of\nRegions-of-Interest (RoIs) of outdoor scenarios is recovered and/or enhanced\nsubject to installation costs and energy consumption requirements. Numerical\nvalidations from real-world scenarios are reported to assess the effectiveness\nof the proposed planning scheme as well as to show the potentialities of an\nheterogeneous deployment of SEMEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a planning strategy for the deployment of smart\nelectromagnetic entities (SEEs) to enhance the wireless coverage and the\nQuality-of-Service (QoS) in large urban areas. The integration of different\ntechnological solutions such as integrated access-and-backhaul nodes (IABs),\nsmart repeaters (SRs), and electromagnetic skins (EMSs) is here addressed to\nenable an effective and efficient implementation of the concept of Smart\nElectromagnetic Environment (SEME). By combining the features of such\nheterogeneous SEEs and optimizing their number, positions, orientations, and\nconfiguration, the electromagnetic (EM) coverage in a set of\nRegions-of-Interest (RoIs) of outdoor scenarios is recovered and/or enhanced\nsubject to installation costs and energy consumption requirements. Numerical\nvalidations from real-world scenarios are reported to assess the effectiveness\nof the proposed planning scheme as well as to show the potentialities of an\nheterogeneous deployment of SEMEs."
                },
                "authors": [
                    {
                        "name": "Arianna Benoni"
                    },
                    {
                        "name": "Marco Salucci"
                    },
                    {
                        "name": "Baozhu Li"
                    },
                    {
                        "name": "Andrea Massa"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Massa"
                },
                "author": "Andrea Massa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02390v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02390v2",
                "updated": "2025-09-10T08:09:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    9,
                    2,
                    2,
                    253,
                    0
                ],
                "published": "2025-02-04T15:10:33Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    10,
                    33,
                    1,
                    35,
                    0
                ],
                "title": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large\n  Language Models Reasoning"
                },
                "summary": "Research on LLM technologies is rapidly emerging, with most of them employ a\n'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. We validate CoAT's\neffectiveness across a variety of generative and reasoning tasks. Quantitative\nexperiments show that CoAT achieves over 10% performance improvement on\nopen-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15%\ngain on our proprietary CRB dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on LLM technologies is rapidly emerging, with most of them employ a\n'fast thinking' approach to inference. Most LLMs generate the final result\nbased solely on a single query and LLM's reasoning capabilities. However, with\nthe advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing\nattention because its process is closer to the human thought process. Inspired\nby the human ability to constantly associate and replenish knowledge during\nthinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework,\nwhich introduces an innovative synergy between the Monte Carlo Tree Search\n(MCTS) algorithm and a dynamic mechanism for integrating new key information,\ntermed 'associative memory'. By combining the structured exploration\ncapabilities of MCTS with the adaptive learning capacity of associative memory,\nCoAT significantly expands the LLM search space, enabling our framework to\nexplore diverse reasoning pathways and dynamically update its knowledge base in\nreal-time. This allows the framework to not only revisit and refine earlier\ninferences but also adaptively incorporate evolving information, ensuring that\nthe final output is both accurate and comprehensive. We validate CoAT's\neffectiveness across a variety of generative and reasoning tasks. Quantitative\nexperiments show that CoAT achieves over 10% performance improvement on\nopen-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15%\ngain on our proprietary CRB dataset."
                },
                "authors": [
                    {
                        "name": "Jianfeng Pan"
                    },
                    {
                        "name": "Senyou Deng"
                    },
                    {
                        "name": "Shaomang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shaomang Huang"
                },
                "author": "Shaomang Huang",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02390v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02390v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08364v1",
                "updated": "2025-09-10T08:02:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    2,
                    7,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T08:02:07Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    8,
                    2,
                    7,
                    2,
                    253,
                    0
                ],
                "title": "Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate\n  Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Overcoming DNSSEC Islands of Security: A TLS and IP-Based Certificate\n  Solution"
                },
                "summary": "The Domain Name System (DNS) serves as the backbone of the Internet,\nprimarily translating domain names to IP addresses. Over time, various\nenhancements have been introduced to strengthen the integrity of DNS. Among\nthese, DNSSEC stands out as a leading cryptographic solution. It protects\nagainst attacks (such as DNS spoofing) by establishing a chain of trust\nthroughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is\ncompromised when there is a break in this chain, resulting in \"Islands of\nSecurity\", where domains can authenticate locally but not across hierarchical\nlevels, leading to a loss of trust and validation between them. Leading\napproaches to addressing these issues were centralized, with a single authority\nmaintaining some kind of bulletin board. This approach requires significantly\nmore infrastructure and places excessive trust in the entity responsible for\nmanaging it properly. In this paper, we propose a decentralized approach to\naddressing gaps in DNSSEC's chain of trust, commonly referred to as \"Islands of\nSecurity\". We leverage TLS and IP-based certificates to enable end-to-end\nauthentication between hierarchical levels, eliminating the need for uniform\nDNSSEC deployment across every level of the DNS hierarchy. This approach\nenhances the overall integrity of DNSSEC, while reducing dependence on\nregistrars for maintaining signature records to verify the child nameserver's\nauthenticity. By offering a more flexible and efficient solution, our method\nstrengthens DNS security and streamlines deployment across diverse\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Domain Name System (DNS) serves as the backbone of the Internet,\nprimarily translating domain names to IP addresses. Over time, various\nenhancements have been introduced to strengthen the integrity of DNS. Among\nthese, DNSSEC stands out as a leading cryptographic solution. It protects\nagainst attacks (such as DNS spoofing) by establishing a chain of trust\nthroughout the DNS nameserver hierarchy. However, DNSSEC's effectiveness is\ncompromised when there is a break in this chain, resulting in \"Islands of\nSecurity\", where domains can authenticate locally but not across hierarchical\nlevels, leading to a loss of trust and validation between them. Leading\napproaches to addressing these issues were centralized, with a single authority\nmaintaining some kind of bulletin board. This approach requires significantly\nmore infrastructure and places excessive trust in the entity responsible for\nmanaging it properly. In this paper, we propose a decentralized approach to\naddressing gaps in DNSSEC's chain of trust, commonly referred to as \"Islands of\nSecurity\". We leverage TLS and IP-based certificates to enable end-to-end\nauthentication between hierarchical levels, eliminating the need for uniform\nDNSSEC deployment across every level of the DNS hierarchy. This approach\nenhances the overall integrity of DNSSEC, while reducing dependence on\nregistrars for maintaining signature records to verify the child nameserver's\nauthenticity. By offering a more flexible and efficient solution, our method\nstrengthens DNS security and streamlines deployment across diverse\nenvironments."
                },
                "authors": [
                    {
                        "name": "Aduma Rishith"
                    },
                    {
                        "name": "Aditya Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Vivek Balachandran"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Balachandran"
                },
                "author": "Vivek Balachandran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08358v1",
                "updated": "2025-09-10T07:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    48,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    48,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "<think> So let's replace this phrase with insult... </think> Lessons\n  learned from generation of toxic texts with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "<think> So let's replace this phrase with insult... </think> Lessons\n  learned from generation of toxic texts with LLMs"
                },
                "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems."
                },
                "authors": [
                    {
                        "name": "Sergey Pletenev"
                    },
                    {
                        "name": "Daniil Moskovskiy"
                    },
                    {
                        "name": "Alexander Panchenko"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Panchenko"
                },
                "author": "Alexander Panchenko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08353v1",
                "updated": "2025-09-10T07:42:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    42,
                    51,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:42:51Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    42,
                    51,
                    2,
                    253,
                    0
                ],
                "title": "An Adaptive Scoring Framework for Attention Assessment in NDD Children\n  via Serious Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Scoring Framework for Attention Assessment in NDD Children\n  via Serious Games"
                },
                "summary": "This paper introduces an innovative adaptive scoring framework for children\nwith Neurodevelopmental Disorders (NDD) that is attributed to the integration\nof multiple metrics, such as spatial attention patterns, temporal engagement,\nand game performance data, to create a comprehensive assessment of learning\nthat goes beyond traditional game scoring. The framework employs a progressive\ndifficulty adaptation method, which focuses on specific stimuli for each level\nand adjusts weights dynamically to accommodate increasing cognitive load and\nlearning complexity. Additionally, it includes capabilities for temporal\nanalysis, such as detecting engagement periods, providing rewards for sustained\nattention, and implementing an adaptive multiplier framework based on\nperformance levels. To avoid over-rewarding high performers while maximizing\nimprovement potential for students who are struggling, the designed framework\nfeatures an adaptive temporal impact framework that adjusts performance scales\naccordingly. We also established a multi-metric validation framework using Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation, and\nSpearman correlation, along with defined quality thresholds for assessing\ndeployment readiness in educational settings. This research bridges the gap\nbetween technical eye-tracking metrics and educational insights by explicitly\nmapping attention patterns to learning behaviors, enabling actionable\npedagogical interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an innovative adaptive scoring framework for children\nwith Neurodevelopmental Disorders (NDD) that is attributed to the integration\nof multiple metrics, such as spatial attention patterns, temporal engagement,\nand game performance data, to create a comprehensive assessment of learning\nthat goes beyond traditional game scoring. The framework employs a progressive\ndifficulty adaptation method, which focuses on specific stimuli for each level\nand adjusts weights dynamically to accommodate increasing cognitive load and\nlearning complexity. Additionally, it includes capabilities for temporal\nanalysis, such as detecting engagement periods, providing rewards for sustained\nattention, and implementing an adaptive multiplier framework based on\nperformance levels. To avoid over-rewarding high performers while maximizing\nimprovement potential for students who are struggling, the designed framework\nfeatures an adaptive temporal impact framework that adjusts performance scales\naccordingly. We also established a multi-metric validation framework using Mean\nAbsolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation, and\nSpearman correlation, along with defined quality thresholds for assessing\ndeployment readiness in educational settings. This research bridges the gap\nbetween technical eye-tracking metrics and educational insights by explicitly\nmapping attention patterns to learning behaviors, enabling actionable\npedagogical interventions."
                },
                "authors": [
                    {
                        "name": "Abdul Rehman"
                    },
                    {
                        "name": "Ilona Heldal"
                    },
                    {
                        "name": "Cristina Costescu"
                    },
                    {
                        "name": "Carmen David"
                    },
                    {
                        "name": "Jerry Chun-Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jerry Chun-Wei Lin"
                },
                "author": "Jerry Chun-Wei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08344v1",
                "updated": "2025-09-10T07:31:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    31,
                    23,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:31:23Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    31,
                    23,
                    2,
                    253,
                    0
                ],
                "title": "Few-shot Personalization via In-Context Learning for Speech Emotion\n  Recognition based on Speech-Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot Personalization via In-Context Learning for Speech Emotion\n  Recognition based on Speech-Language Model"
                },
                "summary": "This paper proposes a personalization method for speech emotion recognition\n(SER) through in-context learning (ICL). Since the expression of emotions\nvaries from person to person, speaker-specific adaptation is crucial for\nimproving the SER performance. Conventional SER methods have been personalized\nusing emotional utterances of a target speaker, but it is often difficult to\nprepare utterances corresponding to all emotion labels in advance. Our idea to\novercome this difficulty is to obtain speaker characteristics by conditioning a\nfew emotional utterances of the target speaker in ICL-based inference. ICL is a\nmethod to perform unseen tasks by conditioning a few input-output examples\nthrough inference in large language models (LLMs). We meta-train a\nspeech-language model extended from the LLM to learn how to perform\npersonalized SER via ICL. Experimental results using our newly collected SER\ndataset demonstrate that the proposed method outperforms conventional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a personalization method for speech emotion recognition\n(SER) through in-context learning (ICL). Since the expression of emotions\nvaries from person to person, speaker-specific adaptation is crucial for\nimproving the SER performance. Conventional SER methods have been personalized\nusing emotional utterances of a target speaker, but it is often difficult to\nprepare utterances corresponding to all emotion labels in advance. Our idea to\novercome this difficulty is to obtain speaker characteristics by conditioning a\nfew emotional utterances of the target speaker in ICL-based inference. ICL is a\nmethod to perform unseen tasks by conditioning a few input-output examples\nthrough inference in large language models (LLMs). We meta-train a\nspeech-language model extended from the LLM to learn how to perform\npersonalized SER via ICL. Experimental results using our newly collected SER\ndataset demonstrate that the proposed method outperforms conventional methods."
                },
                "authors": [
                    {
                        "name": "Mana Ihori"
                    },
                    {
                        "name": "Taiga Yamane"
                    },
                    {
                        "name": "Naotaka Kawata"
                    },
                    {
                        "name": "Naoki Makishima"
                    },
                    {
                        "name": "Tomohiro Tanaka"
                    },
                    {
                        "name": "Satoshi Suzuki"
                    },
                    {
                        "name": "Shota Orihashi"
                    },
                    {
                        "name": "Ryo Masumura"
                    }
                ],
                "author_detail": {
                    "name": "Ryo Masumura"
                },
                "author": "Ryo Masumura",
                "arxiv_comment": "Accepted by ASRU 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18106v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18106v2",
                "updated": "2025-09-10T07:24:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    24,
                    52,
                    2,
                    253,
                    0
                ],
                "published": "2025-08-25T15:11:11Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    11,
                    11,
                    0,
                    237,
                    0
                ],
                "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code"
                },
                "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI programming\nscenarios, making them inadequate for assessing the practical security risks\nassociated with AI-generated code in production environments. To address this\ngap, we introduce A.S.E (AI Code Generation Security Evaluation), a\nrepository-level evaluation benchmark designed to closely mirror real-world AI\nprogramming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMorever, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation, assisting developers in selecting the most appropriate\nmodels for practical tasks, while laying the foundation for refining LLMs to\ngenerate secure and efficient code in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks often lack relevance to real-world AI programming\nscenarios, making them inadequate for assessing the practical security risks\nassociated with AI-generated code in production environments. To address this\ngap, we introduce A.S.E (AI Code Generation Security Evaluation), a\nrepository-level evaluation benchmark designed to closely mirror real-world AI\nprogramming tasks, offering a comprehensive and reliable framework for\nassessing the security of AI-generated code. Our evaluation of leading LLMs on\nA.S.E reveals several key findings. In particular, current LLMs still struggle\nwith secure coding. The complexity in repository-level scenarios presents\nchallenges for LLMs that typically perform well on snippet-level tasks.\nMorever, a larger reasoning budget does not necessarily lead to better code\ngeneration. These observations offer valuable insights into the current state\nof AI code generation, assisting developers in selecting the most appropriate\nmodels for practical tasks, while laying the foundation for refining LLMs to\ngenerate secure and efficient code in real-world applications."
                },
                "authors": [
                    {
                        "name": "Keke Lian"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Libo Chen"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Ziming Zhao"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Haotong Duan"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Shuang Liao"
                    },
                    {
                        "name": "Mingda Guo"
                    },
                    {
                        "name": "Jiazheng Quan"
                    },
                    {
                        "name": "Yilu Zhong"
                    },
                    {
                        "name": "Chenhao He"
                    },
                    {
                        "name": "Zichuan Chen"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Haoling Li"
                    },
                    {
                        "name": "Zhaoxuan Li"
                    },
                    {
                        "name": "Jiongchi Yu"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Dong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Zhang"
                },
                "author": "Dong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18106v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18106v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08329v1",
                "updated": "2025-09-10T07:08:04Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    8,
                    4,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:08:04Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    8,
                    4,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Reinforcement Learning Algorithms Convergence using\n  Pre-trained Large Language Models as Tutors With Advice Reusing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Reinforcement Learning Algorithms Convergence using\n  Pre-trained Large Language Models as Tutors With Advice Reusing"
                },
                "summary": "Reinforcement Learning (RL) algorithms often require long training to become\nuseful, especially in complex environments with sparse rewards. While\ntechniques like reward shaping and curriculum learning exist to accelerate\ntraining, these are often extremely specific and require the developer's\nprofessionalism and dedicated expertise in the problem's domain. Tackling this\nchallenge, in this study, we explore the effectiveness of pre-trained Large\nLanguage Models (LLMs) as tutors in a student-teacher architecture with RL\nalgorithms, hypothesizing that LLM-generated guidance allows for faster\nconvergence. In particular, we explore the effectiveness of reusing the LLM's\nadvice on the RL's convergence dynamics. Through an extensive empirical\nexamination, which included 54 configurations, varying the RL algorithm (DQN,\nPPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,\nSnake, Connect Four), our results demonstrate that LLM tutoring significantly\naccelerates RL convergence while maintaining comparable optimal performance.\nFurthermore, the advice reuse mechanism shows a further improvement in training\nduration but also results in less stable convergence dynamics. Our findings\nsuggest that LLM tutoring generally improves convergence, and its effectiveness\nis sensitive to the specific task, RL algorithm, and LLM model combination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning (RL) algorithms often require long training to become\nuseful, especially in complex environments with sparse rewards. While\ntechniques like reward shaping and curriculum learning exist to accelerate\ntraining, these are often extremely specific and require the developer's\nprofessionalism and dedicated expertise in the problem's domain. Tackling this\nchallenge, in this study, we explore the effectiveness of pre-trained Large\nLanguage Models (LLMs) as tutors in a student-teacher architecture with RL\nalgorithms, hypothesizing that LLM-generated guidance allows for faster\nconvergence. In particular, we explore the effectiveness of reusing the LLM's\nadvice on the RL's convergence dynamics. Through an extensive empirical\nexamination, which included 54 configurations, varying the RL algorithm (DQN,\nPPO, A2C), LLM tutor (Llama, Vicuna, DeepSeek), and environment (Blackjack,\nSnake, Connect Four), our results demonstrate that LLM tutoring significantly\naccelerates RL convergence while maintaining comparable optimal performance.\nFurthermore, the advice reuse mechanism shows a further improvement in training\nduration but also results in less stable convergence dynamics. Our findings\nsuggest that LLM tutoring generally improves convergence, and its effectiveness\nis sensitive to the specific task, RL algorithm, and LLM model combination."
                },
                "authors": [
                    {
                        "name": "Lukas Toral"
                    },
                    {
                        "name": "Teddy Lazebnik"
                    }
                ],
                "author_detail": {
                    "name": "Teddy Lazebnik"
                },
                "author": "Teddy Lazebnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15337v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15337v3",
                "updated": "2025-09-10T07:03:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    3,
                    3,
                    2,
                    253,
                    0
                ],
                "published": "2025-05-21T10:08:39Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    8,
                    39,
                    2,
                    141,
                    0
                ],
                "title": "Your Language Model Can Secretly Write Like Humans: Contrastive\n  Paraphrase Attacks on LLM-Generated Text Detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Language Model Can Secretly Write Like Humans: Contrastive\n  Paraphrase Attacks on LLM-Generated Text Detectors"
                },
                "summary": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The misuse of large language models (LLMs), such as academic plagiarism, has\ndriven the development of detectors to identify LLM-generated texts. To bypass\nthese detectors, paraphrase attacks have emerged to purposely rewrite these\ntexts to evade detection. Despite the success, existing methods require\nsubstantial data and computational budgets to train a specialized paraphraser,\nand their attack efficacy greatly reduces when faced with advanced detection\nalgorithms. To address this, we propose \\textbf{Co}ntrastive\n\\textbf{P}araphrase \\textbf{A}ttack (CoPA), a training-free method that\neffectively deceives text detectors using off-the-shelf LLMs. The first step is\nto carefully craft instructions that encourage LLMs to produce more human-like\ntexts. Nonetheless, we observe that the inherent statistical biases of LLMs can\nstill result in some generated texts carrying certain machine-like attributes\nthat can be captured by detectors. To overcome this, CoPA constructs an\nauxiliary machine-like word distribution as a contrast to the human-like\ndistribution generated by the LLM. By subtracting the machine-like patterns\nfrom the human-like distribution during the decoding process, CoPA is able to\nproduce sentences that are less discernible by text detectors. Our theoretical\nanalysis suggests the superiority of the proposed attack. Extensive experiments\nvalidate the effectiveness of CoPA in fooling text detectors across various\nscenarios."
                },
                "authors": [
                    {
                        "name": "Hao Fang"
                    },
                    {
                        "name": "Jiawei Kong"
                    },
                    {
                        "name": "Tianqu Zhuang"
                    },
                    {
                        "name": "Yixiang Qiu"
                    },
                    {
                        "name": "Kuofeng Gao"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Yaowei Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by EMNLP-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15337v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15337v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22943v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22943v3",
                "updated": "2025-09-10T06:59:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    59,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-03-29T02:28:32Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    2,
                    28,
                    32,
                    5,
                    88,
                    0
                ],
                "title": "Event Camera Meets Resource-Aware Mobile Computing: Abstraction,\n  Algorithm, Acceleration, Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event Camera Meets Resource-Aware Mobile Computing: Abstraction,\n  Algorithm, Acceleration, Application"
                },
                "summary": "With the increasing complexity of mobile device applications, these devices\nare evolving toward high agility. This shift imposes new demands on mobile\nsensing, particularly in achieving high-accuracy and low-latency. Event-based\nvision has emerged as a disruptive paradigm, offering high temporal resolution\nand low latency, making it well-suited for high-accuracy and low-latency\nsensing tasks on high-agility platforms. However, the presence of substantial\nnoisy events, lack of stable, persistent semantic information, and large data\nvolume pose challenges for event-based data processing on resource-constrained\nmobile devices. This paper surveys the literature from 2014 to 2025 and\npresents a comprehensive overview of event-based mobile sensing, encompassing\nits fundamental principles, event \\textit{abstraction} methods,\n\\textit{algorithm} advancements, and both hardware and software\n\\textit{acceleration} strategies. We discuss key \\textit{applications} of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow, and 3D reconstruction, while highlighting challenges associated with\nevent data processing, sensor fusion, and real-time deployment. Furthermore, we\noutline future research directions, such as improving the event camera with\nadvanced optics, leveraging neuromorphic computing for efficient processing,\nand integrating bio-inspired algorithms. To support ongoing research, we\nprovide an open-source \\textit{Online Sheet} with recent developments. We hope\nthis survey serves as a reference, facilitating the adoption of event-based\nvision across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing complexity of mobile device applications, these devices\nare evolving toward high agility. This shift imposes new demands on mobile\nsensing, particularly in achieving high-accuracy and low-latency. Event-based\nvision has emerged as a disruptive paradigm, offering high temporal resolution\nand low latency, making it well-suited for high-accuracy and low-latency\nsensing tasks on high-agility platforms. However, the presence of substantial\nnoisy events, lack of stable, persistent semantic information, and large data\nvolume pose challenges for event-based data processing on resource-constrained\nmobile devices. This paper surveys the literature from 2014 to 2025 and\npresents a comprehensive overview of event-based mobile sensing, encompassing\nits fundamental principles, event \\textit{abstraction} methods,\n\\textit{algorithm} advancements, and both hardware and software\n\\textit{acceleration} strategies. We discuss key \\textit{applications} of event\ncameras in mobile sensing, including visual odometry, object tracking, optical\nflow, and 3D reconstruction, while highlighting challenges associated with\nevent data processing, sensor fusion, and real-time deployment. Furthermore, we\noutline future research directions, such as improving the event camera with\nadvanced optics, leveraging neuromorphic computing for efficient processing,\nand integrating bio-inspired algorithms. To support ongoing research, we\nprovide an open-source \\textit{Online Sheet} with recent developments. We hope\nthis survey serves as a reference, facilitating the adoption of event-based\nvision across diverse applications."
                },
                "authors": [
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Ruishan Guo"
                    },
                    {
                        "name": "Pengtao Ma"
                    },
                    {
                        "name": "Ciyu Ruan"
                    },
                    {
                        "name": "Xinyu Luo"
                    },
                    {
                        "name": "Wenhua Ding"
                    },
                    {
                        "name": "Tianyang Zhong"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Yunhao Liu"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22943v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22943v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08318v1",
                "updated": "2025-09-10T06:47:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    47,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:47:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    47,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "Boosted Training of Lightweight Early Exits for Optimizing CNN Image\n  Classification Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosted Training of Lightweight Early Exits for Optimizing CNN Image\n  Classification Inference"
                },
                "summary": "Real-time image classification on resource-constrained platforms demands\ninference methods that balance accuracy with strict latency and power budgets.\nEarly-exit strategies address this need by attaching auxiliary classifiers to\nintermediate layers of convolutional neural networks (CNNs), allowing \"easy\"\nsamples to terminate inference early. However, conventional training of early\nexits introduces a covariance shift: downstream branches are trained on full\ndatasets, while at inference they process only the harder, non-exited samples.\nThis mismatch limits efficiency--accuracy trade-offs in practice. We introduce\nthe Boosted Training Scheme for Early Exits (BTS-EE), a sequential training\napproach that aligns branch training with inference-time data distributions.\nEach branch is trained and calibrated before the next, ensuring robustness\nunder selective inference conditions. To further support embedded deployment,\nwe propose a lightweight branch architecture based on 1D convolutions and a\nClass Precision Margin (CPM) calibration method that enables per-class\nthreshold tuning for reliable exit decisions. Experiments on the CINIC-10\ndataset with a ResNet18 backbone demonstrate that BTS-EE consistently\noutperforms non-boosted training across 64 configurations, achieving up to 45\npercent reduction in computation with only 2 percent accuracy degradation.\nThese results expand the design space for deploying CNNs in real-time image\nprocessing systems, offering practical efficiency gains for applications such\nas industrial inspection, embedded vision, and UAV-based monitoring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time image classification on resource-constrained platforms demands\ninference methods that balance accuracy with strict latency and power budgets.\nEarly-exit strategies address this need by attaching auxiliary classifiers to\nintermediate layers of convolutional neural networks (CNNs), allowing \"easy\"\nsamples to terminate inference early. However, conventional training of early\nexits introduces a covariance shift: downstream branches are trained on full\ndatasets, while at inference they process only the harder, non-exited samples.\nThis mismatch limits efficiency--accuracy trade-offs in practice. We introduce\nthe Boosted Training Scheme for Early Exits (BTS-EE), a sequential training\napproach that aligns branch training with inference-time data distributions.\nEach branch is trained and calibrated before the next, ensuring robustness\nunder selective inference conditions. To further support embedded deployment,\nwe propose a lightweight branch architecture based on 1D convolutions and a\nClass Precision Margin (CPM) calibration method that enables per-class\nthreshold tuning for reliable exit decisions. Experiments on the CINIC-10\ndataset with a ResNet18 backbone demonstrate that BTS-EE consistently\noutperforms non-boosted training across 64 configurations, achieving up to 45\npercent reduction in computation with only 2 percent accuracy degradation.\nThese results expand the design space for deploying CNNs in real-time image\nprocessing systems, offering practical efficiency gains for applications such\nas industrial inspection, embedded vision, and UAV-based monitoring."
                },
                "authors": [
                    {
                        "name": "Yehudit Aperstein"
                    },
                    {
                        "name": "Alexander Apartsin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Apartsin"
                },
                "author": "Alexander Apartsin",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08315v1",
                "updated": "2025-09-10T06:32:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T06:32:49Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    32,
                    49,
                    2,
                    253,
                    0
                ],
                "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvolKV: Evolutionary KV Cache Compression for LLM Inference"
                },
                "summary": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation."
                },
                "authors": [
                    {
                        "name": "Bohan Yu"
                    },
                    {
                        "name": "Yekun Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yekun Chai"
                },
                "author": "Yekun Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04373v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04373v3",
                "updated": "2025-09-10T06:08:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    6,
                    8,
                    26,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-04T16:32:18Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    16,
                    32,
                    18,
                    3,
                    247,
                    0
                ],
                "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature\n  of LLM Gender Biases"
                },
                "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that prompts that more clearly align with (gender bias)\nevaluation framing elicit distinct gender output distributions compared to less\nevaluation-framed prompts. Discrete-choice metrics further tend to amplify bias\nrelative to probabilistic measures. These findings do not only highlight the\nbrittleness of LLM gender bias evaluations but open a new puzzle for the NLP\nbenchmarking and development community: To what extent can well-controlled\ntesting designs trigger LLM \"testing mode\" performance, and what does this mean\nfor the ecological validity of future benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that prompts that more clearly align with (gender bias)\nevaluation framing elicit distinct gender output distributions compared to less\nevaluation-framed prompts. Discrete-choice metrics further tend to amplify bias\nrelative to probabilistic measures. These findings do not only highlight the\nbrittleness of LLM gender bias evaluations but open a new puzzle for the NLP\nbenchmarking and development community: To what extent can well-controlled\ntesting designs trigger LLM \"testing mode\" performance, and what does this mean\nfor the ecological validity of future benchmarks."
                },
                "authors": [
                    {
                        "name": "Bufan Gao"
                    },
                    {
                        "name": "Elisa Kreiss"
                    }
                ],
                "author_detail": {
                    "name": "Elisa Kreiss"
                },
                "author": "Elisa Kreiss",
                "arxiv_comment": "To be published at EMNLP 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04373v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04373v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]