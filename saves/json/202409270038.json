[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorović"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Lekić"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Lekić"
                },
                "author": "Aleksandra Lekić",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v2",
                "updated": "2024-09-25T06:46:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    46,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clément Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v1",
                "updated": "2024-09-22T08:30:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS 2024 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "François Treussart"
                    }
                ],
                "author_detail": {
                    "name": "François Treussart"
                },
                "author": "François Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tönnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Lukáš Kývala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "André Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v1",
                "updated": "2024-09-11T11:40:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v1",
                "updated": "2024-09-05T17:56:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03568v1",
                "updated": "2024-09-05T14:22:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-05T14:22:02Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    14,
                    22,
                    2,
                    3,
                    249,
                    0
                ],
                "title": "Enabling Practical and Privacy-Preserving Image Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Practical and Privacy-Preserving Image Processing"
                },
                "summary": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Homomorphic Encryption (FHE) enables computations on encrypted data,\npreserving confidentiality without the need for decryption. However, FHE is\noften hindered by significant performance overhead, particularly for\nhigh-precision and complex data like images. Due to serious efficiency issues,\ntraditional FHE methods often encrypt images by monolithic data blocks (such as\npixel rows), instead of pixels. However, this strategy compromises the\nadvantages of homomorphic operations and disables pixel-level image processing.\nIn this study, we address these challenges by proposing and implementing a\npixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS\nscheme. To enhance computational efficiency, we introduce three novel caching\nmechanisms to pre-encrypt radix values or frequently occurring pixel values,\nsubstantially reducing redundant encryption operations. Extensive experiments\ndemonstrate that our approach achieves up to a 19-fold improvement in\nencryption speed compared to the original CKKS, while maintaining high image\nquality. Additionally, real-world image applications such as mean filtering,\nbrightness enhancement, image matching and watermarking are tested based on\nFHE, showcasing up to a 91.53% speed improvement. We also proved that our\nmethod is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure,\nproviding strong encryption security. These results underscore the practicality\nand efficiency of iCHEETAH, marking a significant advancement in\nprivacy-preserving image processing at scale."
                },
                "authors": [
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Shubing Yang"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Jun Dai"
                    },
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.0; K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v2",
                "updated": "2024-09-05T01:12:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    12,
                    4,
                    3,
                    249,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELCC: Coherent Caching over Compute-Limited Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in data centers. It is important to cache data in the compute\nnodes and maintain cache coherence across multiple compute nodes to save on\nround-trip communication cost between the disaggregated memory and the compute\nnodes. However, the limited computing power on the disaggregated memory servers\nmakes it challenging to maintain cache coherence among multiple compute-side\ncaches over disaggregated shared memory. This paper introduces SELCC; a\nShared-Exclusive Latch Cache Coherence protocol that maintains cache coherence\nwithout imposing any computational burden on the remote memory side. SELCC\nbuilds on a one-sided shared-exclusive latch protocol by introducing lazy latch\nrelease and invalidation messages among the compute nodes so that it can\nguarantee both data access atomicity and cache coherence. SELCC minimizes\ncommunication round-trips by embedding the current cache copy holder IDs into\nRDMA latch words and prioritizes local concurrency control over global\nconcurrency control. We instantiate the SELCC protocol onto compute-sided\ncache, forming an abstraction layer over disaggregated memory. This abstraction\nlayer provides main-memory-like APIs to upper-level applications, and thus\nenabling existing data structures and algorithms to function over disaggregated\nmemory with minimal code change. To demonstrate the usability of SELCC, we\nimplement a B-tree and three transaction concurrency control algorithms over\nSELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves\nbetter performance compared to RPC-based cache-coherence protocols.\nAdditionally, YCSB and TPC-C benchmarks indicate that applications over SELCC\ncan achieve comparable or superior performance against competitors over\ndisaggregated memory."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v3",
                "updated": "2024-09-05T01:06:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    1,
                    6,
                    40,
                    3,
                    249,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v6",
                "updated": "2024-09-04T10:04:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    10,
                    4,
                    52,
                    2,
                    248,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02480v1",
                "updated": "2024-09-04T07:13:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "published": "2024-09-04T07:13:01Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    7,
                    13,
                    1,
                    2,
                    248,
                    0
                ],
                "title": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A brown dwarf orbiting around the planetary-nebula central binary KV Vel"
                },
                "summary": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary\ncontaining a very hot subdwarf primary (77000 K) and a cool low-mass secondary\nstar (3400 K) that is located at the center of the planetary nebula DS 1. The\nchanges in the orbital period of the close binary were analyzed based on 262\nnew times of light maximum together with those compiled from the literature. It\nis discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic\nperiod variation with a period of 29.55 years. The explanation by the\nsolar-type magnetic activity cycles of the cool component is ruled out because\nthe required energies are much larger than the total radiant energy of this\ncomponent in a whole cycle. Therefore, the cyclic variation was plausibly\nexplained as the light-travel time effect via the presence of a tertiary\ncomponent, which is supported by the periodic changes of the O-C curve and the\nrather symmetric and stable light curves obtained by TESS. The mass of the\ntertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third\nbody is coplanar with the central binary (i.e., i' = 62.5{\\deg}), the mass of\nthe tertiary component is computed as M_3 ~ 0.068 M\\sun, and thus it would be\nbelow the stable hydrogen-burning limit and is a brown dwarf. The orbital\nseparation is shorter than 9.35 astronomical units (AU). KV Vel together with\nits surrounding planetary nebula and the brown-dwarf companion may be formed\nthrough the common-envelope evolution after the primary filled its Roche lobe\nduring the early asymptotic giant branch stage."
                },
                "authors": [
                    {
                        "name": "S. -B. Qian"
                    },
                    {
                        "name": "L. -Y. Zhu"
                    },
                    {
                        "name": "F. -X. Li"
                    },
                    {
                        "name": "L. -J. Li"
                    },
                    {
                        "name": "Z. -T. Han"
                    },
                    {
                        "name": "J. -J. He"
                    },
                    {
                        "name": "L. Zang"
                    },
                    {
                        "name": "L. -F. Chang"
                    },
                    {
                        "name": "Q. -B. Sun"
                    },
                    {
                        "name": "M. -Y. Li"
                    },
                    {
                        "name": "H. -T. Zhang"
                    },
                    {
                        "name": "F. -Z. Yan"
                    }
                ],
                "author_detail": {
                    "name": "F. -Z. Yan"
                },
                "author": "F. -Z. Yan",
                "arxiv_doi": "10.3847/1538-4357/ad631a",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad631a",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v1",
                "updated": "2024-09-03T15:35:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Contemporary Model Compression on Large Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary Model Compression on Large Language Models Inference"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby achieving state-of-the-art results across a variety of tasks. However, the\ncomputational demands of LLM inference, including high memory consumption and\nslow processing speeds, pose significant challenges for real-world\napplications, particularly on resource-constrained devices. Efficient inference\nis crucial for scaling the deployment of LLMs to a broader range of platforms,\nincluding mobile and edge devices.\n  This survey explores contemporary techniques in model compression that\naddress these challenges by reducing the size and computational requirements of\nLLMs while maintaining their performance. We focus on model-level compression\nmethods, including quantization, knowledge distillation, and pruning, as well\nas system-level optimizations like KV cache efficient design. Each of these\nmethodologies offers a unique approach to optimizing LLMs, from reducing\nnumerical precision to transferring knowledge between models and structurally\nsimplifying neural networks. Additionally, we discuss emerging trends in\nsystem-level design that further enhance the efficiency of LLM inference. This\nsurvey aims to provide a comprehensive overview of current advancements in\nmodel compression and their potential to make LLMs more accessible and\npractical for diverse applications."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Liu"
                },
                "author": "Dong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01890v1",
                "updated": "2024-09-03T13:29:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "published": "2024-09-03T13:29:13Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    29,
                    13,
                    1,
                    247,
                    0
                ],
                "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training\n  with Corrector Networks"
                },
                "summary": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In dense retrieval, deep encoders provide embeddings for both inputs and\ntargets, and the softmax function is used to parameterize a distribution over a\nlarge number of candidate targets (e.g., textual passages for information\nretrieval). Significant challenges arise in training such encoders in the\nincreasingly prevalent scenario of (1) a large number of targets, (2) a\ncomputationally expensive target encoder model, (3) cached target embeddings\nthat are out-of-date due to ongoing training of target encoder parameters. This\npaper presents a simple and highly scalable response to these challenges by\ntraining a small parametric corrector network that adjusts stale cached target\nembeddings, enabling an accurate softmax approximation and thereby sampling of\nup-to-date high scoring \"hard negatives.\" We theoretically investigate the\ngeneralization properties of our proposed target corrector, relating the\ncomplexity of the network, staleness of cached representations, and the amount\nof training data. We present experimental results on large benchmark dense\nretrieval datasets as well as on QA with retrieval augmented language models.\nOur approach matches state-of-the-art results even when no target embedding\nupdates are made during training beyond an initial cache from the unsupervised\npre-trained model, providing a 4-80x reduction in re-embedding computational\ncost."
                },
                "authors": [
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Will Grathwohl"
                    },
                    {
                        "name": "Michael Boratko"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Andrew McCallum"
                    },
                    {
                        "name": "Manzil Zaheer"
                    }
                ],
                "author_detail": {
                    "name": "Manzil Zaheer"
                },
                "author": "Manzil Zaheer",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02137v1",
                "updated": "2024-09-02T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    15,
                    7,
                    5,
                    0,
                    246,
                    0
                ],
                "title": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Augmentation in Reinforcement Learning for Testing Distributed\n  Systems"
                },
                "summary": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bugs in popular distributed protocol implementations have been the source of\nmany downtimes in popular internet services. We describe a randomized testing\napproach for distributed protocol implementations based on reinforcement\nlearning. Since the natural reward structure is very sparse, the key to\nsuccessful exploration in reinforcement learning is reward augmentation. We\nshow two different techniques that build on one another. First, we provide a\ndecaying exploration bonus based on the discovery of new states -- the reward\ndecays as the same state is visited multiple times. The exploration bonus\ncaptures the intuition from coverage-guided fuzzing of prioritizing new\ncoverage points; in contrast to other schemes, we show that taking the maximum\nof the bonus and the Q-value leads to more effective exploration. Second, we\nprovide waypoints to the algorithm as a sequence of predicates that capture\ninteresting semantic scenarios. Waypoints exploit designer insight about the\nprotocol and guide the exploration to ``interesting'' parts of the state space.\nOur reward structure ensures that new episodes can reliably get to deep\ninteresting states even without execution caching. We have implemented our\nalgorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and\nRSL) shows that our algorithm can significantly outperform baseline approaches\nin terms of coverage and bug finding."
                },
                "authors": [
                    {
                        "name": "Andrea Borgarelli"
                    },
                    {
                        "name": "Constantin Enea"
                    },
                    {
                        "name": "Rupak Majumdar"
                    },
                    {
                        "name": "Srinidhi Nagendra"
                    }
                ],
                "author_detail": {
                    "name": "Srinidhi Nagendra"
                },
                "author": "Srinidhi Nagendra",
                "arxiv_doi": "10.1145/3689779",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689779",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.02137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01066v1",
                "updated": "2024-09-02T08:41:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T08:41:45Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    41,
                    45,
                    0,
                    246,
                    0
                ],
                "title": "Learning in Hybrid Active Inference Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning in Hybrid Active Inference Models"
                },
                "summary": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An open problem in artificial intelligence is how systems can flexibly learn\ndiscrete abstractions that are useful for solving inherently continuous\nproblems. Previous work in computational neuroscience has considered this\nfunctional integration of discrete and continuous variables during\ndecision-making under the formalism of active inference (Parr, Friston & de\nVries, 2017; Parr & Friston, 2018). However, their focus is on the expressive\nphysical implementation of categorical decisions and the hierarchical mixed\ngenerative model is assumed to be known. As a consequence, it is unclear how\nthis framework might be extended to learning. We therefore present a novel\nhierarchical hybrid active inference agent in which a high-level discrete\nactive inference planner sits above a low-level continuous active inference\ncontroller. We make use of recent work in recurrent switching linear dynamical\nsystems (rSLDS) which implement end-to-end learning of meaningful discrete\nrepresentations via the piecewise linear decomposition of complex continuous\ndynamics (Linderman et al., 2016). The representations learned by the rSLDS\ninform the structure of the hybrid decision-making agent and allow us to (1)\nspecify temporally-abstracted sub-goals in a method reminiscent of the options\nframework, (2) lift the exploration into discrete space allowing us to exploit\ninformation-theoretic exploration bonuses and (3) `cache' the approximate\nsolutions to low-level problems in the discrete planner. We apply our model to\nthe sparse Continuous Mountain Car task, demonstrating fast system\nidentification via enhanced exploration and successful planning through the\ndelineation of abstract sub-goals."
                },
                "authors": [
                    {
                        "name": "Poppy Collis"
                    },
                    {
                        "name": "Ryan Singh"
                    },
                    {
                        "name": "Paul F Kinghorn"
                    },
                    {
                        "name": "Christopher L Buckley"
                    }
                ],
                "author_detail": {
                    "name": "Christopher L Buckley"
                },
                "author": "Christopher L Buckley",
                "arxiv_comment": "11 pages (+ appendix). Accepted to the International Workshop on\n  Active Inference 2024. arXiv admin note: substantial text overlap with\n  arXiv:2408.10970",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00905v1",
                "updated": "2024-09-02T02:36:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T02:36:22Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    2,
                    36,
                    22,
                    0,
                    246,
                    0
                ],
                "title": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Throughput Optimization in Cache-aided Networks: An Opportunistic\n  Probing and Scheduling Approach"
                },
                "summary": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenges of throughput optimization in wireless\ncache-aided cooperative networks. We propose an opportunistic cooperative\nprobing and scheduling strategy for efficient content delivery. The strategy\ninvolves the base station probing the relaying channels and cache states of\nmultiple cooperative nodes, thereby enabling opportunistic user scheduling for\ncontent delivery. Leveraging the theory of Sequentially Planned Decision (SPD)\noptimization, we dynamically formulate decisions on cooperative probing and\nstopping time. Our proposed Reward Expected Thresholds (RET)-based strategy\noptimizes opportunistic probing and scheduling. This approach significantly\nenhances system throughput by exploiting gains from local caching, cooperative\ntransmission and time diversity. Simulations confirm the effectiveness and\npracticality of the proposed Media Access Control (MAC) strategy."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "arxiv_comment": "2024 IEEE GLOBECOM, Cape Town, South Africa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00876v1",
                "updated": "2024-09-02T00:05:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "published": "2024-09-02T00:05:20Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    0,
                    5,
                    20,
                    0,
                    246,
                    0
                ],
                "title": "Rapid GPU-Based Pangenome Graph Layout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid GPU-Based Pangenome Graph Layout"
                },
                "summary": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Pangenomics is an emerging field that studies genetic variation\nusing a graph structure encompassing multiple genomes. Visualizing pangenome\ngraphs is vital for understanding genome diversity. Yet, handling large graphs\ncan be challenging due to the high computational demands of the graph layout\nprocess.\n  In this work, we conduct a thorough performance characterization of a\nstate-of-the-art pangenome graph layout algorithm, revealing significant\ndata-level parallelism, which makes GPUs a promising option for compute\nacceleration. However, irregular data access and the algorithm's memory-bound\nnature present significant hurdles. To overcome these challenges, we develop a\nsolution implementing three key optimizations: a cache-friendly data layout,\ncoalesced random states, and warp merging. Additionally, we propose a\nquantitative metric for scalable evaluation of pangenome layout quality.\n  Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution\nachieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline\nwithout layout quality loss, reducing execution time from hours to minutes."
                },
                "authors": [
                    {
                        "name": "Jiajie Li"
                    },
                    {
                        "name": "Jan-Niklas Schmelzle"
                    },
                    {
                        "name": "Yixiao Du"
                    },
                    {
                        "name": "Simon Heumos"
                    },
                    {
                        "name": "Andrea Guarracino"
                    },
                    {
                        "name": "Giulia Guidi"
                    },
                    {
                        "name": "Pjotr Prins"
                    },
                    {
                        "name": "Erik Garrison"
                    },
                    {
                        "name": "Zhiru Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiru Zhang"
                },
                "author": "Zhiru Zhang",
                "arxiv_comment": "SC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10539v1",
                "updated": "2024-08-31T15:45:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T15:45:44Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    15,
                    45,
                    44,
                    5,
                    244,
                    0
                ],
                "title": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D\n  Stacks for Heterogeneous System Integration & AI Systems"
                },
                "summary": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D integration offers key advantages in improving system performance and\nefficiency for the End-of-Scaling era. It enables the incorporation of\nheterogeneous system components and disparate technologies, eliminates off-chip\ncommunication constraints, reduces on-chip latency and total power dissipation.\nMoreover, AIs demand for increased computational power, larger GPU cache\ncapacity, energy efficiency and low power custom AI hardware integration all\nserve as drivers for 3D integration. Although 3D advantages such as enhanced\ninterconnectivity and increased performance have been demonstrated through\nnumerous technology sites, heterogeneous 3D system design raises numerous\nunanswered questions. Among the primary challenges are the temperature and\nlifetime reliability issues caused by the complex interaction patterns among\nsystem components. Such interactions are harder to model with current modeling\ntools and require detailed hardware characterization. This study presents the\nlatest drivers for 3D integration and the resulting need for hardware emulation\nframeworks. It then presents a design to profile power, temperature, noise,\ninter-layer bandwidth and lifetime reliability characterization that can\nemulate a wide range of stacking alternatives. This framework allows for\ncontrolling activity levels at the macro-level, along with customized sensor\ninfrastructure to characterize heat propagation, inter-layer noise, power\ndelivery, reliability and inter-connectivity as well as the interactions among\ncritical design objectives."
                },
                "authors": [
                    {
                        "name": "Eren Kurshan"
                    },
                    {
                        "name": "Paul Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Paul Franzon"
                },
                "author": "Paul Franzon",
                "arxiv_journal_ref": "IEEE 3D IC Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00364v1",
                "updated": "2024-08-31T06:33:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T06:33:50Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    6,
                    33,
                    50,
                    5,
                    244,
                    0
                ],
                "title": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,\n  Communication and Computing Systems"
                },
                "summary": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate an intelligent reflecting surface (IRS)\nassisted full-duplex (FD) integrated sensing, communication and computing\nsystem. Specifically, an FD base station (BS) provides service for uplink and\ndownlink transmission, and a local cache is connected to the BS through a\nbackhaul link to store data. Meanwhile, active sensing elements are deployed on\nthe IRS to receive target echo signals. On this basis, in order to evaluate the\noverall performance of the system under consideration, we propose a system\nutility maximization problem while ensuring the sensing quality, expressed as\nthe difference between the sum of communication throughput, total computation\nbits (offloading bits and local computation bits) and the total backhaul cost\nfor content delivery. This makes the problem difficult to solve due to the\nhighly non-convex coupling of the optimization variables. To effectively solve\nthis problem, we first design the most effective caching strategy. Then, we\ndevelop an algorithm based on weighted minimum mean square error, alternative\ndirection method of multipliers, majorization-minimization framework,\nsemi-definite relaxation techniques, and several complex transformations to\njointly solve the optimization variables. Finally, simulation results are\nprovided to verify the utility performance of the proposed algorithm and\ndemonstrate the advantages of the proposed scheme compared with the baseline\nscheme."
                },
                "authors": [
                    {
                        "name": "Wanming Hao"
                    },
                    {
                        "name": "Xue Wu"
                    },
                    {
                        "name": "Xingwang Li"
                    },
                    {
                        "name": "Gangcan Sun"
                    },
                    {
                        "name": "Qingqing Wu"
                    },
                    {
                        "name": "Liang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Yang"
                },
                "author": "Liang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00344v1",
                "updated": "2024-08-31T04:20:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "published": "2024-08-31T04:20:58Z",
                "published_parsed": [
                    2024,
                    8,
                    31,
                    4,
                    20,
                    58,
                    5,
                    244,
                    0
                ],
                "title": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": ">3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction\n  Termination Extension and Sub-1V Turn-on"
                },
                "summary": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction\ndiodes (HJDs) with a 2-step space-modulated junction termination extension.\nDistinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown\nvoltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a\nforward current density (IF) of 1 A-cm-2. The measured devices exhibit\nexcellent turn-on characteristics achieving 100 A-cm-2 current density at a\nforward bias of 1.5V along with a low differential specific on-resistance\n(Ron,sp) of 4.4 m{\\Omega}-cm2. The SM-JTE was realized using concentric NiO\nrings with varying widths and spacing that approximates a gradual reduction in\nJTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and\nis among the best reported for devices with a sub-1V turn-on. The fabricated\ndevices also displayed minimal change in forward I-V characteristics post\nreverse bias stress of 3 kV applied during breakdown voltage testing."
                },
                "authors": [
                    {
                        "name": "Advait Gilankar"
                    },
                    {
                        "name": "Abishek Katta"
                    },
                    {
                        "name": "Nabasindhu Das"
                    },
                    {
                        "name": "Nidhin Kurian Kalarickal"
                    }
                ],
                "author_detail": {
                    "name": "Nidhin Kurian Kalarickal"
                },
                "author": "Nidhin Kurian Kalarickal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00184v1",
                "updated": "2024-08-30T18:04:53Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T18:04:53Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    18,
                    4,
                    53,
                    4,
                    243,
                    0
                ],
                "title": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume\n  Visualization through Functional Approximation"
                },
                "summary": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional approximation as a high-order continuous representation provides a\nmore accurate value and gradient query compared to the traditional discrete\nvolume representation. Volume visualization directly rendered from functional\napproximation generates high-quality rendering results without high-order\nartifacts caused by trilinear interpolations. However, querying an encoded\nfunctional approximation is computationally expensive, especially when the\ninput dataset is large, making functional approximation impractical for\ninteractive visualization. In this paper, we proposed a novel functional\napproximation multi-resolution representation, Adaptive-FAM, which is\nlightweight and fast to query. We also design a GPU-accelerated out-of-core\nmulti-resolution volume visualization framework that directly utilizes the\nAdaptive-FAM representation to generate high-quality rendering with interactive\nresponsiveness. Our method can not only dramatically decrease the caching time,\none of the main contributors to input latency, but also effectively improve the\ncache hit rate through prefetching. Our approach significantly outperforms the\ntraditional function approximation method in terms of input latency while\nmaintaining comparable rendering quality."
                },
                "authors": [
                    {
                        "name": "Jianxin Sun"
                    },
                    {
                        "name": "David Lenz"
                    },
                    {
                        "name": "Hongfeng Yu"
                    },
                    {
                        "name": "Tom Peterka"
                    }
                ],
                "author_detail": {
                    "name": "Tom Peterka"
                },
                "author": "Tom Peterka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17178v1",
                "updated": "2024-08-30T10:26:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T10:26:50Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    10,
                    26,
                    50,
                    4,
                    243,
                    0
                ],
                "title": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling the High-Voltage Grid Using Open Data for Europe and Beyond"
                },
                "summary": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the background, methodology and validation for\nconstructing a representation of the European high-voltage grid, including and\nabove 200 kV, based on public data provided by OpenStreetMap. The\nmodel-independent grid dataset is published under the Open Data Commons Open\nDatabase (ODbL 1.0) licence and can be used for large-scale electricity as well\nas energy system modelling. The dataset and workflow are provided as part of\nPyPSA-Eur -- an open-source, sector-coupled optimisation model of the European\nenergy system. By integrating with the codebase for initiatives such as\nPyPSA-Earth, the value of open and maintainable high-voltage grid data extends\nto the global context. By accessing the latest data through the the Overpass\nturbo API, the dataset can be easily reconstructed and updated within minutes.\nTo assess the data quality, this paper further compares the dataset with\nofficial statistics and representative model runs using PyPSA-Eur based on\ndifferent electricity grid representations."
                },
                "authors": [
                    {
                        "name": "Bobby Xiong"
                    },
                    {
                        "name": "Davide Fioriti"
                    },
                    {
                        "name": "Fabian Neumann"
                    },
                    {
                        "name": "Iegor Riepin"
                    },
                    {
                        "name": "Tom Brown"
                    }
                ],
                "author_detail": {
                    "name": "Tom Brown"
                },
                "author": "Tom Brown",
                "arxiv_comment": "20 pages, 15 figures, 8 tables. For associated prebuilt electricity\n  network, see https://doi.org/10.5281/zenodo.13358976",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16967v1",
                "updated": "2024-08-30T02:01:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "published": "2024-08-30T02:01:56Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    2,
                    1,
                    56,
                    4,
                    243,
                    0
                ],
                "title": "MemLong: Memory-Augmented Retrieval for Long Text Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemLong: Memory-Augmented Retrieval for Long Text Modeling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have yielded remarkable\nsuccess across diverse fields. However, handling long contexts remains a\nsignificant challenge for LLMs due to the quadratic time and space complexity\nof attention mechanisms and the growing memory consumption of the key-value\ncache during generation. This work introduces MemLong: Memory-Augmented\nRetrieval for Long Text Generation, a method designed to enhance the\ncapabilities of long-context language modeling by utilizing an external\nretriever for historical information retrieval. MemLong combines a\nnon-differentiable ``ret-mem'' module with a partially trainable decoder-only\nlanguage model and introduces a fine-grained, controllable retrieval attention\nmechanism that leverages semantic-level relevant chunks. Comprehensive\nevaluations on multiple long-context language modeling benchmarks demonstrate\nthat MemLong consistently outperforms other state-of-the-art LLMs. More\nimportantly, MemLong can extend the context length on a single 3090 GPU from 4k\nup to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong"
                },
                "authors": [
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Zecheng Tang"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.07975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.07975v2",
                "updated": "2024-08-29T17:43:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    43,
                    26,
                    3,
                    242,
                    0
                ],
                "published": "2023-09-14T18:18:10Z",
                "published_parsed": [
                    2023,
                    9,
                    14,
                    18,
                    18,
                    10,
                    3,
                    257,
                    0
                ],
                "title": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load"
                },
                "summary": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio\nheads (eRRHs) are connected to a macro base station (MBS) through fronthaul\nlinks. Deploying a massive number of eRRHs is not always feasible due to site\nconstraints and the cost of fronthaul links. This paper introduces an\ninnovative concept of using smart helpers (SHs) in F-RANs. These SHs do not\nrequire fronthaul links and listen to the nearby eRRHs' communications. Then,\nthey smartly select and cache popular content. This capability enables SHs to\nserve users with frequent on-demand service requests potentially. As such,\nnetwork operators have the flexibility to easily deploy SHs in various\nscenarios, such as dense urban areas and temporary public events, to expand\ntheir F-RANs and improve the quality of service (QoS). To study the performance\nof the proposed SH-aided F-RAN, we formulate an optimization problem of\nminimizing the average transmission delay that jointly optimizes cache\nresources and user scheduling. To tackle the formulated problem, we develop an\ninnovative multi-stage algorithm that uses a reinforcement learning (RL)\nframework. Various performance measures, e.g., the average transmission delay,\nfronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated\nnumerically and compared with those of traditional F-RANs."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed S. Al-Abiad"
                    },
                    {
                        "name": "Md Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.07975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.07975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16730v1",
                "updated": "2024-08-29T17:21:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T17:21:58Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    17,
                    21,
                    58,
                    3,
                    242,
                    0
                ],
                "title": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths\n  Vision Computation"
                },
                "summary": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is\nthat while increasing the number of vision tokens generally enhances visual\nunderstanding, it also significantly raises memory and computational costs,\nespecially in long-term, dense video frame streaming scenarios. Although\nlearnable approaches like Q-Former and Perceiver Resampler have been developed\nto reduce the vision token burden, they overlook the context causally modeled\nby LLMs (i.e., key-value cache), potentially leading to missed visual cues when\naddressing user queries. In this paper, we introduce a novel approach to reduce\nvision compute by leveraging redundant vision tokens \"skipping layers\" rather\nthan decreasing the number of vision tokens. Our method, VideoLLM-MoD, is\ninspired by mixture-of-depths LLMs and addresses the challenge of numerous\nvision tokens in long-term or streaming video. Specifically, for each\ntransformer layer, we learn to skip the computation for a high proportion\n(e.g., 80\\%) of vision tokens, passing them directly to the next layer. This\napproach significantly enhances model efficiency, achieving approximately\n\\textasciitilde42\\% time and \\textasciitilde30\\% memory savings for the entire\ntraining. Moreover, our method reduces the computation in the context and avoid\ndecreasing the vision tokens, thus preserving or even improving performance\ncompared to the vanilla model. We conduct extensive experiments to demonstrate\nthe effectiveness of VideoLLM-MoD, showing its state-of-the-art results on\nmultiple benchmarks, including narration, forecasting, and summarization tasks\nin COIN, Ego4D, and Ego-Exo4D datasets."
                },
                "authors": [
                    {
                        "name": "Shiwei Wu"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Tong Xu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v3",
                "updated": "2024-08-29T16:48:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    16,
                    48,
                    58,
                    3,
                    242,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v1",
                "updated": "2024-08-29T02:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through\n  Targeted Instruction Hardening"
                },
                "summary": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several software mitigations have been proposed to defend against Spectre\nvulnerabilities. However, these countermeasures often suffer from high\nperformance overhead, largely due to unnecessary protections. We propose\nLightSLH, designed to mitigate this overhead by hardening instructions only\nwhen they are under threat from Spectre vulnerabilities. LightSLH leverages\nprogram analysis techniques based on abstract interpretation to identify all\ninstructions that could potentially lead to Spectre vulnerabilities and\nprovides provable protection. To enhance analysis efficiency and precision,\nLightSLH employs novel taint and value domains. The taint domain enables\nbit-level taint tracking, while the value domain allows LightSLH to analyze\ncomplex program structures such as pointers and structures. Furthermore,\nLightSLH uses a two-stage abstract interpretation approach to circumvent\npotential analysis paralysis issues.\n  We demonstrate the security guarantees of LightSLH and evaluate its\nperformance on cryptographic algorithm implementations from OpenSSL. LightSLH\nsignificantly reduces the overhead associated with speculative-load-hardening\ntechniques. Our results show that LightSLH introduces no protection and thus no\noverhead on 4 out of the 7 studied algorithms, which contrasts with existing\ncountermeasures that introduce additional overhead due to unnecessary\nhardening. Additionally, LightSLH performs, for the first time, a rigorous\nanalysis of the security guarantees of RSA against Spectre v1, highlighting\nthat the memory access patterns generated by the scatter-gather algorithm\ndepend on secrets, even for observers at the cache line granularity,\nnecessitating protection for such accesses."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08286v1",
                "updated": "2024-08-28T17:28:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "published": "2024-08-28T17:28:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    17,
                    28,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of ISA Extension on Energy Consumption of I-Cache in\n  Extensible Processors"
                },
                "summary": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is widely known, the computational speed and power consumption are two\ncritical parameters in microprocessor design. A solution for these issues is\nthe application specific instruction set processor (ASIP) methodology, which\ncan improve speed and reduce power consumption of the general purpose processor\n(GPP) technique. In ASIP, changing the instruction set architecture (ISA) of\nthe processor will lead to alter the number and the mean time of accesses to\nthe cache memory. This issue has a direct impact on the processor energy\nconsumption. In this work, we study the impacts of extended ISA on the energy\nconsumption of the extended ISA processor. Also, we demonstrate the extended\nISA let the designer to reduce the cache size in order to minimize the energy\nconsumption while meeting performance constraint."
                },
                "authors": [
                    {
                        "name": "Noushin Behboudi"
                    },
                    {
                        "name": "Mehdi Kamal"
                    },
                    {
                        "name": "Ali Afzali-Kusha"
                    }
                ],
                "author_detail": {
                    "name": "Ali Afzali-Kusha"
                },
                "author": "Ali Afzali-Kusha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06942v3",
                "updated": "2024-08-28T08:41:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    28,
                    8,
                    41,
                    45,
                    2,
                    241,
                    0
                ],
                "published": "2023-06-12T08:24:14Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    8,
                    24,
                    14,
                    0,
                    163,
                    0
                ],
                "title": "RIP Linked List",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIP Linked List"
                },
                "summary": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linked lists have long served as a valuable teaching tool in programming.\nHowever, the question arises: Are they truly practical for everyday program\nuse? In most cases, it appears that array-based data structures offer distinct\nadvantages, particularly in terms of memory efficiency and,more importantly,\nexecution speed. While it's relatively straightforward to calculate the\ncomplexity of operations, gauging actual execution efficiency remains a\nchallenge. This paper addresses this question by introducing a new benchmark.\nOur study compares various linked list implementations with several array-based\nalternatives. We also demonstrate the ease of incorporating memory caching for\nlinked lists, enhancing their performance. Additionally, we introduce a new\narray-based data structure designed to excel in a wide range of operations."
                },
                "authors": [
                    {
                        "name": "Benoît Sonntag"
                    },
                    {
                        "name": "Dominique Colnet"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Colnet"
                },
                "arxiv_affiliation": "LORIA",
                "author": "Dominique Colnet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v2",
                "updated": "2024-08-27T22:06:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    22,
                    6,
                    20,
                    1,
                    240,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v3",
                "updated": "2024-08-27T17:30:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    30,
                    41,
                    1,
                    240,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches either provide serializable\ntransactional guarantees at the level of individual functions, or separate\napplication logic from the state and use inefficient transactional protocols.\nThese design choices increase the execution latency, limiting the adoption of\nSFaaS systems.\n  In this paper, we present Styx, a novel SFaaS runtime that executes\nserializable transactions across functions with exactly-once guarantees. Styx\nextends a deterministic transactional protocol to support an arbitrary call\ngraph of stateful functions. It introduces a transaction-execution\nacknowledgment scheme that allows tracking a transactional workflow's SFaaS\ncalls, guaranteeing atomicity and exactly-once processing. Finally, Styx\nfeatures a function-execution caching mechanism and early transactional commit\nreplies for optimized performance. Experiments with the YCSB-T, TPC-C, and\nDeathstar benchmarks show that Styx outperforms state-of-the-art approaches by\nachieving at least one order of magnitude higher throughput while exhibiting\nnear-linear scalability and low latency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14906v1",
                "updated": "2024-08-27T09:34:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T09:34:38Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    34,
                    38,
                    1,
                    240,
                    0
                ],
                "title": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing in the Margins: Better Inference Pattern for Long Context\n  Retrieval"
                },
                "summary": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Writing in the Margins (WiM), a new inference\npattern for Large Language Models designed to optimize the handling of long\ninput sequences in retrieval-oriented tasks. This approach leverages the\nchunked prefill of the key-value cache to perform segment-wise inference, which\nenables efficient processing of extensive contexts along with the generation\nand classification of intermediate information (\"margins\") that guide the model\ntowards specific tasks. This method increases computational overhead marginally\nwhile significantly enhancing the performance of off-the-shelf models without\nthe need for fine-tuning. Specifically, we observe that WiM provides an average\nenhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG)\nand more than a 30.0% increase in the F1-score for aggregation tasks (CWE).\nAdditionally, we show how the proposed pattern fits into an interactive\nretrieval design that provides end-users with ongoing updates about the\nprogress of context processing, and pinpoints the integration of relevant\ninformation into the final response. We release our implementation of WiM using\nHugging Face Transformers library at\nhttps://github.com/writer/writing-in-the-margins."
                },
                "authors": [
                    {
                        "name": "Melisa Russak"
                    },
                    {
                        "name": "Umar Jamil"
                    },
                    {
                        "name": "Christopher Bryant"
                    },
                    {
                        "name": "Kiran Kamble"
                    },
                    {
                        "name": "Axel Magnuson"
                    },
                    {
                        "name": "Mateusz Russak"
                    },
                    {
                        "name": "Waseem AlShikh"
                    }
                ],
                "author_detail": {
                    "name": "Waseem AlShikh"
                },
                "author": "Waseem AlShikh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14735v1",
                "updated": "2024-08-27T02:03:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "published": "2024-08-27T02:03:36Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    3,
                    36,
                    1,
                    240,
                    0
                ],
                "title": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework\n  with Correlated Differential Privacy"
                },
                "summary": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online video streaming has evolved into an integral component of the\ncontemporary Internet landscape. Yet, the disclosure of user requests presents\nformidable privacy challenges. As users stream their preferred online videos,\ntheir requests are automatically seized by video content providers, potentially\nleaking users' privacy.\n  Unfortunately, current protection methods are not well-suited to preserving\nuser request privacy from content providers while maintaining high-quality\nonline video services. To tackle this challenge, we introduce a novel\nPrivacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge\ndevices to pre-fetch and cache videos, ensuring the privacy of users' requests\nwhile optimizing the efficiency of edge caching. More specifically, we design\nPPVF with three core components: (1) \\textit{Online privacy budget scheduler},\nwhich employs a theoretically guaranteed online algorithm to select\nnon-requested videos as candidates with assigned privacy budgets. Alternative\nvideos are chosen by an online algorithm that is theoretically guaranteed to\nconsider both video utilities and available privacy budgets. (2) \\textit{Noisy\nvideo request generator}, which generates redundant video requests (in addition\nto original ones) utilizing correlated differential privacy to obfuscate\nrequest privacy. (3) \\textit{Online video utility predictor}, which leverages\nfederated learning to collaboratively evaluate video utility in an online\nfashion, aiding in video selection in (1) and noise generation in (2). Finally,\nwe conduct extensive experiments using real-world video request traces from\nTencent Video. The results demonstrate that PPVF effectively safeguards user\nrequest privacy while upholding high video caching performance."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10774v2",
                "updated": "2024-08-26T21:01:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    21,
                    1,
                    2,
                    0,
                    239,
                    0
                ],
                "published": "2024-06-16T01:33:02Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    1,
                    33,
                    2,
                    6,
                    168,
                    0
                ],
                "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
                },
                "summary": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for long-context large language models (LLMs) increases, models\nwith context windows of up to 128K or 1M tokens are becoming increasingly\nprevalent. However, long-context LLM inference is challenging since the\ninference speed decreases significantly as the sequence length grows. This\nslowdown is primarily caused by loading a large KV cache during self-attention.\nPrevious works have shown that a small portion of critical tokens will dominate\nthe attention outcomes. However, we observe the criticality of a token highly\ndepends on the query. To this end, we propose Quest, a query-aware KV cache\nselection algorithm. Quest keeps track of the minimal and maximal Key values in\nKV cache pages and estimates the criticality of a given page using Query\nvectors. By only loading the Top-K critical KV cache pages for attention, Quest\nsignificantly speeds up self-attention without sacrificing accuracy. We show\nthat Quest can achieve up to 2.23x self-attention speedup, which reduces\ninference latency by 7.03x while performing well on tasks with long\ndependencies with negligible accuracy loss. Code is available at\nhttp://github.com/mit-han-lab/Quest ."
                },
                "authors": [
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Yilong Zhao"
                    },
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14434v1",
                "updated": "2024-08-26T17:21:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-26T17:21:19Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    17,
                    21,
                    19,
                    0,
                    239,
                    0
                ],
                "title": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Employing Artificial Intelligence to Steer Exascale Workflows with\n  Colmena"
                },
                "summary": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational workflows are a common class of application on supercomputers,\nyet the loosely coupled and heterogeneous nature of workflows often fails to\ntake full advantage of their capabilities. We created Colmena to leverage the\nmassive parallelism of a supercomputer by using Artificial Intelligence (AI) to\nlearn from and adapt a workflow as it executes. Colmena allows scientists to\ndefine how their application should respond to events (e.g., task completion)\nas a series of cooperative agents. In this paper, we describe the design of\nColmena, the challenges we overcame while deploying applications on exascale\nsystems, and the science workflows we have enhanced through interweaving AI.\nThe scaling challenges we discuss include developing steering strategies that\nmaximize node utilization, introducing data fabrics that reduce communication\noverhead of data-intensive tasks, and implementing workflow tasks that cache\ncostly operations between invocations. These innovations coupled with a variety\nof application patterns accessible through our agent-based steering model have\nenabled science advances in chemistry, biophysics, and materials science using\ndifferent types of AI. Our vision is that Colmena will spur creative solutions\nthat harness AI across many domains of scientific computing."
                },
                "authors": [
                    {
                        "name": "Logan Ward"
                    },
                    {
                        "name": "J. Gregory Pauloski"
                    },
                    {
                        "name": "Valerie Hayot-Sasson"
                    },
                    {
                        "name": "Yadu Babuji"
                    },
                    {
                        "name": "Alexander Brace"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Kyle Chard"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Ian Foster"
                    }
                ],
                "author_detail": {
                    "name": "Ian Foster"
                },
                "author": "Ian Foster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06876v2",
                "updated": "2024-08-26T11:29:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    11,
                    29,
                    7,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-13T13:14:54Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    14,
                    54,
                    1,
                    226,
                    0
                ],
                "title": "Decision-Focused Learning to Predict Action Costs for Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Focused Learning to Predict Action Costs for Planning"
                },
                "summary": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input\nfeatures (e.g., weather forecasts) and use the predicted action costs in\nautomated planning afterward. Decision-Focused Learning (DFL) has been\nsuccessful in learning to predict the parameters of combinatorial optimization\nproblems in a way that optimizes solution quality rather than prediction\nquality. This approach yields better results than treating prediction and\noptimization as separate tasks. In this paper, we investigate for the first\ntime the challenges of implementing DFL for automated planning in order to\nlearn to predict the action costs. There are two main challenges to overcome:\n(1) planning systems are called during gradient descent learning, to solve\nplanning problems with negative action costs, which are not supported in\nplanning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit\nthe scalability of the method. We experiment with different methods\napproximating the optimal plan as well as an easy-to-implement caching\nmechanism to speed up the learning process. As the first work that addresses\nDFL for automated planning, we demonstrate that the proposed gradient\ncomputation consistently yields significantly better plans than predictions\naimed at minimizing prediction error; and that caching can temper the\ncomputation requirements."
                },
                "authors": [
                    {
                        "name": "Jayanta Mandi"
                    },
                    {
                        "name": "Marco Foschini"
                    },
                    {
                        "name": "Daniel Holler"
                    },
                    {
                        "name": "Sylvie Thiebaux"
                    },
                    {
                        "name": "Jorg Hoffmann"
                    },
                    {
                        "name": "Tias Guns"
                    }
                ],
                "author_detail": {
                    "name": "Tias Guns"
                },
                "author": "Tias Guns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16343v2",
                "updated": "2024-08-26T07:26:27Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    7,
                    26,
                    27,
                    0,
                    239,
                    0
                ],
                "published": "2024-02-26T06:55:36Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    6,
                    55,
                    36,
                    0,
                    57,
                    0
                ],
                "title": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems"
                },
                "summary": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid main memory systems combine both performance and capacity advantages\nfrom heterogeneous memory technologies. With larger capacities, higher\nassociativities, and finer granularities, hybrid memory systems currently\nexhibit significant metadata storage and lookup overheads for flexibly\nremapping data blocks between the two memory tiers. To alleviate the\ninefficiencies of existing designs, we propose Trimma, the combination of a\nmulti-level metadata structure and an efficient metadata cache design. Trimma\nuses a multi-level metadata table to only track truly necessary address remap\nentries. The saved memory space is effectively utilized as extra DRAM cache\ncapacity to improve performance. Trimma also uses separate formats to store the\nentries with non-identity and identity address mappings. This improves the\noverall remap cache hit rate, further boosting the performance. Trimma is\ntransparent to software and compatible with various types of hybrid memory\nsystems. When evaluated on a representative hybrid memory system with HBM3 and\nDDR5, Trimma achieves up to 1.68$\\times$ and on average 1.33$\\times$ speedup\nbenefits, compared to state-of-the-art hybrid memory designs. These results\nshow that Trimma effectively addresses metadata management overheads,\nespecially for future scalable large-scale hybrid memory architectures."
                },
                "authors": [
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "Accepted by PACT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08795v2",
                "updated": "2024-08-26T04:32:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    26,
                    4,
                    32,
                    56,
                    0,
                    239,
                    0
                ],
                "published": "2024-08-16T15:11:12Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    15,
                    11,
                    12,
                    4,
                    229,
                    0
                ],
                "title": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollingCache: Using Runtime Behavior to Defend Against Cache Side\n  Channel Attacks"
                },
                "summary": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared caches are vulnerable to side channel attacks through contention in\ncache sets. Besides being a simple source of information leak, these side\nchannels form useful gadgets for more sophisticated attacks that compromise the\nsecurity of shared systems.\n  The fundamental design aspect that contention attacks exploit is the\ndeterministic nature of the set of addresses contending for a cache set. In\nthis paper, we present RollingCache, a cache design that defends against\ncontention attacks by dynamically changing the set of addresses contending for\ncache sets. Unlike prior defenses, RollingCache does not rely on address\nencryption/decryption, data relocation, or cache partitioning. We use one level\nof indirection to implement dynamic mapping controlled by the whole-cache\nruntime behavior. Our solution does not depend on having defined security\ndomains, and can defend against an attacker running on the same or another\ncore.\n  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our\nsecurity evaluation shows that our dynamic mapping removes the deterministic\nability to identify the source of contention. The performance evaluation shows\nan impact of 1.67\\% over a mix of workloads, with a corresponding"
                },
                "authors": [
                    {
                        "name": "Divya Ojha"
                    },
                    {
                        "name": "Sandhya Dwarkadas"
                    }
                ],
                "author_detail": {
                    "name": "Sandhya Dwarkadas"
                },
                "author": "Sandhya Dwarkadas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.17146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17146v1",
                "updated": "2024-09-25T17:59:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:59:51Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    51,
                    2,
                    269,
                    0
                ],
                "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art\n  Multimodal Models"
                },
                "summary": "Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org."
                },
                "authors": [
                    {
                        "name": "Matt Deitke"
                    },
                    {
                        "name": "Christopher Clark"
                    },
                    {
                        "name": "Sangho Lee"
                    },
                    {
                        "name": "Rohun Tripathi"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Mohammadreza Salehi"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Kyle Lo"
                    },
                    {
                        "name": "Luca Soldaini"
                    },
                    {
                        "name": "Jiasen Lu"
                    },
                    {
                        "name": "Taira Anderson"
                    },
                    {
                        "name": "Erin Bransom"
                    },
                    {
                        "name": "Kiana Ehsani"
                    },
                    {
                        "name": "Huong Ngo"
                    },
                    {
                        "name": "YenSung Chen"
                    },
                    {
                        "name": "Ajay Patel"
                    },
                    {
                        "name": "Mark Yatskar"
                    },
                    {
                        "name": "Chris Callison-Burch"
                    },
                    {
                        "name": "Andrew Head"
                    },
                    {
                        "name": "Rose Hendrix"
                    },
                    {
                        "name": "Favyen Bastani"
                    },
                    {
                        "name": "Eli VanderBilt"
                    },
                    {
                        "name": "Nathan Lambert"
                    },
                    {
                        "name": "Yvonne Chou"
                    },
                    {
                        "name": "Arnavi Chheda"
                    },
                    {
                        "name": "Jenna Sparks"
                    },
                    {
                        "name": "Sam Skjonsberg"
                    },
                    {
                        "name": "Michael Schmitz"
                    },
                    {
                        "name": "Aaron Sarnat"
                    },
                    {
                        "name": "Byron Bischoff"
                    },
                    {
                        "name": "Pete Walsh"
                    },
                    {
                        "name": "Chris Newell"
                    },
                    {
                        "name": "Piper Wolters"
                    },
                    {
                        "name": "Tanmay Gupta"
                    },
                    {
                        "name": "Kuo-Hao Zeng"
                    },
                    {
                        "name": "Jon Borchardt"
                    },
                    {
                        "name": "Dirk Groeneveld"
                    },
                    {
                        "name": "Jen Dumas"
                    },
                    {
                        "name": "Crystal Nam"
                    },
                    {
                        "name": "Sophie Lebrecht"
                    },
                    {
                        "name": "Caitlin Wittlif"
                    },
                    {
                        "name": "Carissa Schoenick"
                    },
                    {
                        "name": "Oscar Michel"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Luca Weihs"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Hannaneh Hajishirzi"
                    },
                    {
                        "name": "Ross Girshick"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    }
                ],
                "author_detail": {
                    "name": "Aniruddha Kembhavi"
                },
                "author": "Aniruddha Kembhavi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07726v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07726v2",
                "updated": "2024-09-25T17:59:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    18,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-11T21:09:45Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    21,
                    9,
                    45,
                    1,
                    163,
                    0
                ],
                "title": "A Concise Mathematical Description of Active Inference in Discrete Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Concise Mathematical Description of Active Inference in Discrete Time"
                },
                "summary": "In this paper we present a concise mathematical description of active\ninference in discrete time. The main part of the paper serves as a basic\nintroduction to the topic, including a detailed example illustrating the theory\non action selection. In the appendix the more subtle mathematical details are\ndiscussed. This part is aimed at readers who have already studied the active\ninference literature but struggle to make sense of the mathematical details and\nderivations. Throughout the whole manuscript, special attention has been paid\nto adopting notation that is both precise and in line with standard\nmathematical texts. All equations and derivations are linked to specific\nequation numbers in other popular text on the topic. Furthermore, Python code\nis provided that implements the action selection mechanism described in this\npaper and is compatible with pymdp environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we present a concise mathematical description of active\ninference in discrete time. The main part of the paper serves as a basic\nintroduction to the topic, including a detailed example illustrating the theory\non action selection. In the appendix the more subtle mathematical details are\ndiscussed. This part is aimed at readers who have already studied the active\ninference literature but struggle to make sense of the mathematical details and\nderivations. Throughout the whole manuscript, special attention has been paid\nto adopting notation that is both precise and in line with standard\nmathematical texts. All equations and derivations are linked to specific\nequation numbers in other popular text on the topic. Furthermore, Python code\nis provided that implements the action selection mechanism described in this\npaper and is compatible with pymdp environments."
                },
                "authors": [
                    {
                        "name": "Jesse van Oostrum"
                    },
                    {
                        "name": "Carlotta Langer"
                    },
                    {
                        "name": "Nihat Ay"
                    }
                ],
                "author_detail": {
                    "name": "Nihat Ay"
                },
                "author": "Nihat Ay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07726v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07726v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17143v1",
                "updated": "2024-09-25T17:59:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    13,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:59:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "Attention Prompting on Image for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Prompting on Image for Large Vision-Language Models"
                },
                "summary": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Website, see https://yu-rp.github.io/api-prompting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17141v1",
                "updated": "2024-09-25T17:58:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:58:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression"
                },
                "summary": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem."
                },
                "authors": [
                    {
                        "name": "Fazal Mittu"
                    },
                    {
                        "name": "Yihuan Bu"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Ashok Devireddy"
                    },
                    {
                        "name": "Alp Eren Ozdarendeli"
                    },
                    {
                        "name": "Anant Singh"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17140v1",
                "updated": "2024-09-25T17:58:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    8,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:58:08Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    8,
                    2,
                    269,
                    0
                ],
                "title": "Turn Every Application into an Agent: Towards Efficient\n  Human-Agent-Computer Interaction with API-First LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turn Every Application into an Agent: Towards Efficient\n  Human-Agent-Computer Interaction with API-First LLM-Based Agents"
                },
                "summary": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Office Word\ndemonstrate that AXIS reduces task completion time by 65%-70% and cognitive\nworkload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.\nOur work contributes to a new human-agent-computer interaction (HACI) framework\nand a fresh UI design principle for application providers in the era of LLMs.\nIt also explores the possibility of turning every applications into agents,\npaving the way towards an agent-centric operating system (Agent OS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Office Word\ndemonstrate that AXIS reduces task completion time by 65%-70% and cognitive\nworkload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.\nOur work contributes to a new human-agent-computer interaction (HACI) framework\nand a fresh UI design principle for application providers in the era of LLMs.\nIt also explores the possibility of turning every applications into agents,\npaving the way towards an agent-centric operating system (Agent OS)."
                },
                "authors": [
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17129v1",
                "updated": "2024-09-25T17:48:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    48,
                    3,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:48:03Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    48,
                    3,
                    2,
                    269,
                    0
                ],
                "title": "Bayesian Bivariate Conway-Maxwell-Poisson Regression Model for\n  Correlated Count Data in Sports",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Bivariate Conway-Maxwell-Poisson Regression Model for\n  Correlated Count Data in Sports"
                },
                "summary": "Count data play a crucial role in sports analytics, providing valuable\ninsights into various aspects of the game. Models that accurately capture the\ncharacteristics of count data are essential for making reliable inferences. In\nthis paper, we propose the use of the Conway-Maxwell-Poisson (CMP) model for\nanalyzing count data in sports. The CMP model offers flexibility in modeling\ndata with different levels of dispersion. Here we consider a bivariate CMP\nmodel that models the potential correlation between home and away scores by\nincorporating a random effect specification. We illustrate the advantages of\nthe CMP model through simulations. We then analyze data from baseball and\nsoccer games before, during, and after the COVID-19 pandemic. The performance\nof our proposed CMP model matches or outperforms standard Poisson and Negative\nBinomial models, providing a good fit and an accurate estimation of the\nobserved effects in count data with any level of dispersion. The results\nhighlight the robustness and flexibility of the CMP model in analyzing count\ndata in sports, making it a suitable default choice for modeling a diverse\nrange of count data types in sports, where the data dispersion may vary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Count data play a crucial role in sports analytics, providing valuable\ninsights into various aspects of the game. Models that accurately capture the\ncharacteristics of count data are essential for making reliable inferences. In\nthis paper, we propose the use of the Conway-Maxwell-Poisson (CMP) model for\nanalyzing count data in sports. The CMP model offers flexibility in modeling\ndata with different levels of dispersion. Here we consider a bivariate CMP\nmodel that models the potential correlation between home and away scores by\nincorporating a random effect specification. We illustrate the advantages of\nthe CMP model through simulations. We then analyze data from baseball and\nsoccer games before, during, and after the COVID-19 pandemic. The performance\nof our proposed CMP model matches or outperforms standard Poisson and Negative\nBinomial models, providing a good fit and an accurate estimation of the\nobserved effects in count data with any level of dispersion. The results\nhighlight the robustness and flexibility of the CMP model in analyzing count\ndata in sports, making it a suitable default choice for modeling a diverse\nrange of count data types in sports, where the data dispersion may vary."
                },
                "authors": [
                    {
                        "name": "Mauro Florez"
                    },
                    {
                        "name": "Michele Guindani"
                    },
                    {
                        "name": "Marina Vannucci"
                    }
                ],
                "author_detail": {
                    "name": "Marina Vannucci"
                },
                "author": "Marina Vannucci",
                "arxiv_comment": "Journal of Quantitative Analysis in Sports (in press)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17115v1",
                "updated": "2024-09-25T17:28:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    28,
                    13,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    28,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale"
                },
                "summary": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX"
                },
                "authors": [
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "45 pages, 13 figures, 34 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v1",
                "updated": "2024-09-25T17:27:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17093v1",
                "updated": "2024-09-25T17:03:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    3,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:03:49Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    3,
                    49,
                    2,
                    269,
                    0
                ],
                "title": "BitQ: Tailoring Block Floating Point Precision for Improved DNN\n  Efficiency on Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitQ: Tailoring Block Floating Point Precision for Improved DNN\n  Efficiency on Resource-Constrained Devices"
                },
                "summary": "Deep neural networks (DNNs) are powerful for cognitive tasks such as image\nclassification, object detection, and scene segmentation. One drawback however\nis the significant high computational complexity and memory consumption, which\nmakes them unfeasible to run real-time on embedded platforms because of the\nlimited hardware resources. Block floating point (BFP) quantization is one of\nthe representative compression approaches for reducing the memory and\ncomputational burden owing to their capability to effectively capture the broad\ndata distribution of DNN models. Unfortunately, prior works on BFP-based\nquantization empirically choose the block size and the precision that preserve\naccuracy. In this paper, we develop a BFP-based bitwidth-aware analytical\nmodeling framework (called ``BitQ'') for the best BFP implementation of DNN\ninference on embedded platforms. We formulate and resolve an optimization\nproblem to identify the optimal BFP block size and bitwidth distribution by the\ntrade-off of both accuracy and performance loss. Experimental results show that\ncompared with an equal bitwidth setting, the BFP DNNs with optimized bitwidth\nallocation provide efficient computation, preserving accuracy on famous\nbenchmarks. The source code and data are available at\nhttps://github.com/Cheliosoops/BitQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) are powerful for cognitive tasks such as image\nclassification, object detection, and scene segmentation. One drawback however\nis the significant high computational complexity and memory consumption, which\nmakes them unfeasible to run real-time on embedded platforms because of the\nlimited hardware resources. Block floating point (BFP) quantization is one of\nthe representative compression approaches for reducing the memory and\ncomputational burden owing to their capability to effectively capture the broad\ndata distribution of DNN models. Unfortunately, prior works on BFP-based\nquantization empirically choose the block size and the precision that preserve\naccuracy. In this paper, we develop a BFP-based bitwidth-aware analytical\nmodeling framework (called ``BitQ'') for the best BFP implementation of DNN\ninference on embedded platforms. We formulate and resolve an optimization\nproblem to identify the optimal BFP block size and bitwidth distribution by the\ntrade-off of both accuracy and performance loss. Experimental results show that\ncompared with an equal bitwidth setting, the BFP DNNs with optimized bitwidth\nallocation provide efficient computation, preserving accuracy on famous\nbenchmarks. The source code and data are available at\nhttps://github.com/Cheliosoops/BitQ."
                },
                "authors": [
                    {
                        "name": "Yongqi Xu"
                    },
                    {
                        "name": "Yujian Lee"
                    },
                    {
                        "name": "Gao Yi"
                    },
                    {
                        "name": "Bosheng Liu"
                    },
                    {
                        "name": "Yucong Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Jigang Wu"
                    },
                    {
                        "name": "Xiaoming Chen"
                    },
                    {
                        "name": "Yinhe Han"
                    }
                ],
                "author_detail": {
                    "name": "Yinhe Han"
                },
                "author": "Yinhe Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17092v1",
                "updated": "2024-09-25T16:58:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:58:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "Accumulator-Aware Post-Training Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accumulator-Aware Post-Training Quantization"
                },
                "summary": "Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods."
                },
                "authors": [
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Fabian Grob"
                    },
                    {
                        "name": "Giuseppe Franco"
                    },
                    {
                        "name": "Jinjie Zhang"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17012v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17012v3",
                "updated": "2024-09-25T16:57:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    57,
                    20,
                    2,
                    269,
                    0
                ],
                "published": "2023-09-29T06:53:10Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    6,
                    53,
                    10,
                    4,
                    272,
                    0
                ],
                "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
                },
                "summary": "Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler."
                },
                "authors": [
                    {
                        "name": "Ryan Koo"
                    },
                    {
                        "name": "Minhwa Lee"
                    },
                    {
                        "name": "Vipul Raheja"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "Publishsed at ACL 2024. 29 pages, 9 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17012v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17012v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17085v1",
                "updated": "2024-09-25T16:49:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    49,
                    25,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:49:25Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    49,
                    25,
                    2,
                    269,
                    0
                ],
                "title": "Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth\n  Estimation"
                },
                "summary": "State-of-the-art computer vision tasks, like monocular depth estimation\n(MDE), rely heavily on large, modern Transformer-based architectures. However,\ntheir application in safety-critical domains demands reliable predictive\nperformance and uncertainty quantification. While Bayesian neural networks\nprovide a conceptually simple approach to serve those requirements, they suffer\nfrom the high dimensionality of the parameter space. Parameter-efficient\nfine-tuning (PEFT) methods, in particular low-rank adaptations (LoRA), have\nemerged as a popular strategy for adapting large-scale models to down-stream\ntasks by performing parameter inference on lower-dimensional subspaces. In this\nwork, we investigate the suitability of PEFT methods for subspace Bayesian\ninference in large-scale Transformer-based vision models. We show that, indeed,\ncombining BitFit, DiffFit, LoRA, and CoLoRA, a novel LoRA-inspired PEFT method,\nwith Bayesian inference enables more robust and reliable predictive performance\nin MDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art computer vision tasks, like monocular depth estimation\n(MDE), rely heavily on large, modern Transformer-based architectures. However,\ntheir application in safety-critical domains demands reliable predictive\nperformance and uncertainty quantification. While Bayesian neural networks\nprovide a conceptually simple approach to serve those requirements, they suffer\nfrom the high dimensionality of the parameter space. Parameter-efficient\nfine-tuning (PEFT) methods, in particular low-rank adaptations (LoRA), have\nemerged as a popular strategy for adapting large-scale models to down-stream\ntasks by performing parameter inference on lower-dimensional subspaces. In this\nwork, we investigate the suitability of PEFT methods for subspace Bayesian\ninference in large-scale Transformer-based vision models. We show that, indeed,\ncombining BitFit, DiffFit, LoRA, and CoLoRA, a novel LoRA-inspired PEFT method,\nwith Bayesian inference enables more robust and reliable predictive performance\nin MDE."
                },
                "authors": [
                    {
                        "name": "Richard D. Paul"
                    },
                    {
                        "name": "Alessio Quercia"
                    },
                    {
                        "name": "Vincent Fortuin"
                    },
                    {
                        "name": "Katharina Nöh"
                    },
                    {
                        "name": "Hanno Scharr"
                    }
                ],
                "author_detail": {
                    "name": "Hanno Scharr"
                },
                "author": "Hanno Scharr",
                "arxiv_comment": "Presented at UnCV Workshop at ECCV'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17081v1",
                "updated": "2024-09-25T16:45:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    45,
                    26,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:45:26Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    45,
                    26,
                    2,
                    269,
                    0
                ],
                "title": "The effect of image quality on galaxy merger identification with deep\n  learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effect of image quality on galaxy merger identification with deep\n  learning"
                },
                "summary": "Studies have shown that the morphologies of galaxies are substantially\ntransformed following coalescence after a merger, but post-mergers are\nnotoriously difficult to identify, especially in imaging that is shallow or\nlow-resolution. We train convolutional neural networks (CNNs) to identify\nsimulated post-merger galaxies in a range of image qualities, modelled after\nfive real surveys: the Sloan Digital Sky Survey (SDSS), the Dark Energy Camera\nLegacy Survey (DECaLS), the Canada-France Imaging Survey (CFIS), the Hyper\nSuprime-Cam Subaru Strategic Program (HSC-SSP), and the Legacy Survey of Space\nand Time (LSST). Holding constant all variables other than imaging quality, we\npresent the performance of the CNNs on reserved test set data for each image\nquality. The success of CNNs on a given dataset is found to be sensitive to\nboth imaging depth and resolution. We find that post-merger recovery generally\nincreases with depth, but that limiting 5 sigma point-source depths in excess\nof ~25 mag, similar to what is achieved in CFIS, are only marginally\nbeneficial. Finally, we present the results of a cross-survey inference\nexperiment, and find that CNNs trained on a given image quality can sometimes\nbe applied to different imaging data to good effect. The work presented here\ntherefore represents a useful reference for the application of CNNs for merger\nsearches in both current and future imaging surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Studies have shown that the morphologies of galaxies are substantially\ntransformed following coalescence after a merger, but post-mergers are\nnotoriously difficult to identify, especially in imaging that is shallow or\nlow-resolution. We train convolutional neural networks (CNNs) to identify\nsimulated post-merger galaxies in a range of image qualities, modelled after\nfive real surveys: the Sloan Digital Sky Survey (SDSS), the Dark Energy Camera\nLegacy Survey (DECaLS), the Canada-France Imaging Survey (CFIS), the Hyper\nSuprime-Cam Subaru Strategic Program (HSC-SSP), and the Legacy Survey of Space\nand Time (LSST). Holding constant all variables other than imaging quality, we\npresent the performance of the CNNs on reserved test set data for each image\nquality. The success of CNNs on a given dataset is found to be sensitive to\nboth imaging depth and resolution. We find that post-merger recovery generally\nincreases with depth, but that limiting 5 sigma point-source depths in excess\nof ~25 mag, similar to what is achieved in CFIS, are only marginally\nbeneficial. Finally, we present the results of a cross-survey inference\nexperiment, and find that CNNs trained on a given image quality can sometimes\nbe applied to different imaging data to good effect. The work presented here\ntherefore represents a useful reference for the application of CNNs for merger\nsearches in both current and future imaging surveys."
                },
                "authors": [
                    {
                        "name": "Robert W. Bickley"
                    },
                    {
                        "name": "Scott Wilkinson"
                    },
                    {
                        "name": "Leonardo Ferreira"
                    },
                    {
                        "name": "Sara L. Ellison"
                    },
                    {
                        "name": "Connor Bottrell"
                    },
                    {
                        "name": "Debarpita Jyoti"
                    }
                ],
                "author_detail": {
                    "name": "Debarpita Jyoti"
                },
                "author": "Debarpita Jyoti",
                "arxiv_comment": "19 pages, 17 figures. Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07028v2",
                "updated": "2024-09-25T16:41:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    41,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-11T05:55:51Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    5,
                    55,
                    51,
                    2,
                    255,
                    0
                ],
                "title": "Adaptive Error-Bounded Hierarchical Matrices for Efficient Neural\n  Network Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Error-Bounded Hierarchical Matrices for Efficient Neural\n  Network Compression"
                },
                "summary": "This paper introduces a dynamic, error-bounded hierarchical matrix (H-matrix)\ncompression method tailored for Physics-Informed Neural Networks (PINNs). The\nproposed approach reduces the computational complexity and memory demands of\nlarge-scale physics-based models while preserving the essential properties of\nthe Neural Tangent Kernel (NTK). By adaptively refining hierarchical matrix\napproximations based on local error estimates, our method ensures efficient\ntraining and robust model performance. Empirical results demonstrate that this\ntechnique outperforms traditional compression methods, such as Singular Value\nDecomposition (SVD), pruning, and quantization, by maintaining high accuracy\nand improving generalization capabilities. Additionally, the dynamic H-matrix\nmethod enhances inference speed, making it suitable for real-time applications.\nThis approach offers a scalable and efficient solution for deploying PINNs in\ncomplex scientific and engineering domains, bridging the gap between\ncomputational feasibility and real-world applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a dynamic, error-bounded hierarchical matrix (H-matrix)\ncompression method tailored for Physics-Informed Neural Networks (PINNs). The\nproposed approach reduces the computational complexity and memory demands of\nlarge-scale physics-based models while preserving the essential properties of\nthe Neural Tangent Kernel (NTK). By adaptively refining hierarchical matrix\napproximations based on local error estimates, our method ensures efficient\ntraining and robust model performance. Empirical results demonstrate that this\ntechnique outperforms traditional compression methods, such as Singular Value\nDecomposition (SVD), pruning, and quantization, by maintaining high accuracy\nand improving generalization capabilities. Additionally, the dynamic H-matrix\nmethod enhances inference speed, making it suitable for real-time applications.\nThis approach offers a scalable and efficient solution for deploying PINNs in\ncomplex scientific and engineering domains, bridging the gap between\ncomputational feasibility and real-world applicability."
                },
                "authors": [
                    {
                        "name": "John Mango"
                    },
                    {
                        "name": "Ronald Katende"
                    }
                ],
                "author_detail": {
                    "name": "Ronald Katende"
                },
                "author": "Ronald Katende",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17073v1",
                "updated": "2024-09-25T16:32:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    32,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:32:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    32,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition"
                },
                "summary": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nprecisely should be attributed, with an emphasis on identifying the information\nunits within an answer that necessitate grounding? In this paper, we propose\nand investigate a novel approach to the factual decomposition of generated\nanswers for attribution, employing template-based in-context learning. To\naccomplish this, we utilize the question and integrate negative sampling during\nfew-shot in-context learning for decomposition. This approach enhances the\nsemantic understanding of both abstractive and extractive answers. We examine\nthe impact of answer decomposition by providing a thorough examination of\nvarious attribution approaches, ranging from retrieval-based techniques to\nLLM-based attributors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nprecisely should be attributed, with an emphasis on identifying the information\nunits within an answer that necessitate grounding? In this paper, we propose\nand investigate a novel approach to the factual decomposition of generated\nanswers for attribution, employing template-based in-context learning. To\naccomplish this, we utilize the question and integrate negative sampling during\nfew-shot in-context learning for decomposition. This approach enhances the\nsemantic understanding of both abstractive and extractive answers. We examine\nthe impact of answer decomposition by providing a thorough examination of\nvarious attribution approaches, ranging from retrieval-based techniques to\nLLM-based attributors."
                },
                "authors": [
                    {
                        "name": "Pritika Ramu"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Balaji Vasan Srinivavsan"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Vasan Srinivavsan"
                },
                "author": "Balaji Vasan Srinivavsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17066v1",
                "updated": "2024-09-25T16:25:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    25,
                    45,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:25:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    25,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large\n  Language Models"
                },
                "summary": "Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA."
                },
                "authors": [
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Jicheng Wen"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Shengyu Ye"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17058v1",
                "updated": "2024-09-25T16:15:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    15,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:15:21Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    15,
                    21,
                    2,
                    269,
                    0
                ],
                "title": "Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors"
                },
                "summary": "Diffusion-based image super-resolution (SR) methods have achieved remarkable\nsuccess by leveraging large pre-trained text-to-image diffusion models as\npriors. However, these methods still face two challenges: the requirement for\ndozens of sampling steps to achieve satisfactory results, which limits\nefficiency in real scenarios, and the neglect of degradation models, which are\ncritical auxiliary information in solving the SR problem. In this work, we\nintroduced a novel one-step SR model, which significantly addresses the\nefficiency issue of diffusion-based SR methods. Unlike existing fine-tuning\nstrategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module\nspecifically for SR, which corrects the model parameters based on the\npre-estimated degradation information from low-resolution images. This module\nnot only facilitates a powerful data-dependent or degradation-dependent SR\nmodel but also preserves the generative prior of the pre-trained diffusion\nmodel as much as possible. Furthermore, we tailor a novel training pipeline by\nintroducing an online negative sample generation strategy. Combined with the\nclassifier-free guidance strategy during inference, it largely improves the\nperceptual quality of the super-resolution results. Extensive experiments have\ndemonstrated the superior efficiency and effectiveness of the proposed model\ncompared to recent state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image super-resolution (SR) methods have achieved remarkable\nsuccess by leveraging large pre-trained text-to-image diffusion models as\npriors. However, these methods still face two challenges: the requirement for\ndozens of sampling steps to achieve satisfactory results, which limits\nefficiency in real scenarios, and the neglect of degradation models, which are\ncritical auxiliary information in solving the SR problem. In this work, we\nintroduced a novel one-step SR model, which significantly addresses the\nefficiency issue of diffusion-based SR methods. Unlike existing fine-tuning\nstrategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module\nspecifically for SR, which corrects the model parameters based on the\npre-estimated degradation information from low-resolution images. This module\nnot only facilitates a powerful data-dependent or degradation-dependent SR\nmodel but also preserves the generative prior of the pre-trained diffusion\nmodel as much as possible. Furthermore, we tailor a novel training pipeline by\nintroducing an online negative sample generation strategy. Combined with the\nclassifier-free guidance strategy during inference, it largely improves the\nperceptual quality of the super-resolution results. Extensive experiments have\ndemonstrated the superior efficiency and effectiveness of the proposed model\ncompared to recent state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Aiping Zhang"
                    },
                    {
                        "name": "Zongsheng Yue"
                    },
                    {
                        "name": "Renjing Pei"
                    },
                    {
                        "name": "Wenqi Ren"
                    },
                    {
                        "name": "Xiaochun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaochun Cao"
                },
                "author": "Xiaochun Cao",
                "arxiv_comment": "The code is available at https://github.com/ArcticHare105/S3Diff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17054v1",
                "updated": "2024-09-25T16:13:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    13,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:13:42Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    13,
                    42,
                    2,
                    269,
                    0
                ],
                "title": "Using LLM for Real-Time Transcription and Summarization of\n  Doctor-Patient Interactions into ePuskesmas in Indonesia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLM for Real-Time Transcription and Summarization of\n  Doctor-Patient Interactions into ePuskesmas in Indonesia"
                },
                "summary": "One of the key issues contributing to inefficiency in Puskesmas is the\ntime-consuming nature of doctor-patient interactions. Doctors need to conduct\nthorough consultations, which include diagnosing the patient's condition,\nproviding treatment advice, and transcribing detailed notes into medical\nrecords. In regions with diverse linguistic backgrounds, doctors often have to\nask clarifying questions, further prolonging the process. While diagnosing is\nessential, transcription and summarization can often be automated using AI to\nimprove time efficiency and help doctors enhance care quality and enable early\ndiagnosis and intervention. This paper proposes a solution using a localized\nlarge language model (LLM) to transcribe, translate, and summarize\ndoctor-patient conversations. We utilize the Whisper model for transcription\nand GPT-3 to summarize them into the ePuskemas medical records format. This\nsystem is implemented as an add-on to an existing web browser extension,\nallowing doctors to fill out patient forms while talking. By leveraging this\nsolution for real-time transcription, translation, and summarization, doctors\ncan improve the turnaround time for patient care while enhancing the quality of\nrecords, which become more detailed and insightful for future visits. This\ninnovation addresses challenges like overcrowded facilities and the\nadministrative burden on healthcare providers in Indonesia. We believe this\nsolution will help doctors save time, provide better care, and produce more\naccurate medical records, representing a significant step toward modernizing\nhealthcare and ensuring patients receive timely, high-quality care, even in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key issues contributing to inefficiency in Puskesmas is the\ntime-consuming nature of doctor-patient interactions. Doctors need to conduct\nthorough consultations, which include diagnosing the patient's condition,\nproviding treatment advice, and transcribing detailed notes into medical\nrecords. In regions with diverse linguistic backgrounds, doctors often have to\nask clarifying questions, further prolonging the process. While diagnosing is\nessential, transcription and summarization can often be automated using AI to\nimprove time efficiency and help doctors enhance care quality and enable early\ndiagnosis and intervention. This paper proposes a solution using a localized\nlarge language model (LLM) to transcribe, translate, and summarize\ndoctor-patient conversations. We utilize the Whisper model for transcription\nand GPT-3 to summarize them into the ePuskemas medical records format. This\nsystem is implemented as an add-on to an existing web browser extension,\nallowing doctors to fill out patient forms while talking. By leveraging this\nsolution for real-time transcription, translation, and summarization, doctors\ncan improve the turnaround time for patient care while enhancing the quality of\nrecords, which become more detailed and insightful for future visits. This\ninnovation addresses challenges like overcrowded facilities and the\nadministrative burden on healthcare providers in Indonesia. We believe this\nsolution will help doctors save time, provide better care, and produce more\naccurate medical records, representing a significant step toward modernizing\nhealthcare and ensuring patients receive timely, high-quality care, even in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Azmul Asmar Irfan"
                    },
                    {
                        "name": "Nur Ahmad Khatim"
                    },
                    {
                        "name": "Mansur M. Arief"
                    }
                ],
                "author_detail": {
                    "name": "Mansur M. Arief"
                },
                "author": "Mansur M. Arief",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17044v1",
                "updated": "2024-09-25T15:54:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not"
                },
                "summary": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM."
                },
                "authors": [
                    {
                        "name": "Francesco Verdini"
                    },
                    {
                        "name": "Pierfrancesco Melucci"
                    },
                    {
                        "name": "Stefano Perna"
                    },
                    {
                        "name": "Francesco Cariaggi"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Szymon Mazurek"
                    },
                    {
                        "name": "Marek Kasztelnik"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    },
                    {
                        "name": "Sébastien Bratières"
                    },
                    {
                        "name": "Paolo Merialdo"
                    },
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17043v1",
                "updated": "2024-09-25T15:51:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    51,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:51:27Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    51,
                    27,
                    2,
                    269,
                    0
                ],
                "title": "Gaussian Processes for Observational Dose-Response Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Processes for Observational Dose-Response Inference"
                },
                "summary": "We adapt Gaussian processes for estimating the average dose-response function\nin observational settings, introducing a powerful complement to treatment\neffect estimation for understanding heterogeneous effects. We incorporate\nsamples from a Gaussian process posterior for the propensity score into a\nGaussian process response model using Girard's approach to integrating over\nuncertainty in training data. We show Girard's method admits a\npositive-definite kernel, and provide theoretical justification by identifying\nit with an inner product of kernel mean embeddings. We demonstrate double\nrobustness of our approach under a misspecified response function or propensity\nscore. We characterize and mitigate regularization-induced confounding in\nGaussian process response models. We show improvement over other methods for\naverage dose-response function estimation in terms of coverage of the\ndose-response function and estimation bias, with less sensitivity to\nmisspecification across experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We adapt Gaussian processes for estimating the average dose-response function\nin observational settings, introducing a powerful complement to treatment\neffect estimation for understanding heterogeneous effects. We incorporate\nsamples from a Gaussian process posterior for the propensity score into a\nGaussian process response model using Girard's approach to integrating over\nuncertainty in training data. We show Girard's method admits a\npositive-definite kernel, and provide theoretical justification by identifying\nit with an inner product of kernel mean embeddings. We demonstrate double\nrobustness of our approach under a misspecified response function or propensity\nscore. We characterize and mitigate regularization-induced confounding in\nGaussian process response models. We show improvement over other methods for\naverage dose-response function estimation in terms of coverage of the\ndose-response function and estimation bias, with less sensitivity to\nmisspecification across experiments."
                },
                "authors": [
                    {
                        "name": "Jake R. Dailey"
                    }
                ],
                "author_detail": {
                    "name": "Jake R. Dailey"
                },
                "author": "Jake R. Dailey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v3",
                "updated": "2024-09-25T15:50:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    50,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13084v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13084v2",
                "updated": "2024-09-25T15:34:19Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    34,
                    19,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-19T20:49:39Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    20,
                    49,
                    39,
                    3,
                    263,
                    0
                ],
                "title": "Real-time estimation of overt attention from dynamic features of the\n  face using deep-learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time estimation of overt attention from dynamic features of the\n  face using deep-learning"
                },
                "summary": "Students often drift in and out of focus during class. Effective teachers\nrecognize this and re-engage them when necessary. With the shift to remote\nlearning, teachers have lost the visual feedback needed to adapt to varying\nstudent engagement. We propose using readily available front-facing video to\ninfer attention levels based on movements of the eyes, head, and face. We train\na deep learning model to predict a measure of attention based on overt eye\nmovements. Specifically, we measure Inter-Subject Correlation of eye movements\nin ten-second intervals while students watch the same educational videos. In 3\ndifferent experiments (N=83) we show that the trained model predicts this\nobjective metric of attention on unseen data with $R^2$=0.38, and on unseen\nsubjects with $R^2$=0.26-0.30. The deep network relies mostly on a student's\neye movements, but to some extent also on movements of the brows, cheeks, and\nhead. In contrast to Inter-Subject Correlation of the eyes, the model can\nestimate attentional engagement from individual students' movements without\nneeding reference data from an attentive group. This enables a much broader set\nof online applications. The solution is lightweight and can operate on the\nclient side, which mitigates some of the privacy concerns associated with\nonline attention monitoring. GitHub implementation is available at\nhttps://github.com/asortubay/timeISC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Students often drift in and out of focus during class. Effective teachers\nrecognize this and re-engage them when necessary. With the shift to remote\nlearning, teachers have lost the visual feedback needed to adapt to varying\nstudent engagement. We propose using readily available front-facing video to\ninfer attention levels based on movements of the eyes, head, and face. We train\na deep learning model to predict a measure of attention based on overt eye\nmovements. Specifically, we measure Inter-Subject Correlation of eye movements\nin ten-second intervals while students watch the same educational videos. In 3\ndifferent experiments (N=83) we show that the trained model predicts this\nobjective metric of attention on unseen data with $R^2$=0.38, and on unseen\nsubjects with $R^2$=0.26-0.30. The deep network relies mostly on a student's\neye movements, but to some extent also on movements of the brows, cheeks, and\nhead. In contrast to Inter-Subject Correlation of the eyes, the model can\nestimate attentional engagement from individual students' movements without\nneeding reference data from an attentive group. This enables a much broader set\nof online applications. The solution is lightweight and can operate on the\nclient side, which mitigates some of the privacy concerns associated with\nonline attention monitoring. GitHub implementation is available at\nhttps://github.com/asortubay/timeISC"
                },
                "authors": [
                    {
                        "name": "Aimar Silvan Ortubay"
                    },
                    {
                        "name": "Lucas C. Parra"
                    },
                    {
                        "name": "Jens Madsen"
                    }
                ],
                "author_detail": {
                    "name": "Jens Madsen"
                },
                "author": "Jens Madsen",
                "arxiv_comment": "10 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13084v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13084v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17020v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17020v1",
                "updated": "2024-09-25T15:23:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:23:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "PTQ4RIS: Post-Training Quantization for Referring Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PTQ4RIS: Post-Training Quantization for Referring Image Segmentation"
                },
                "summary": "Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code will be available at {https://github.com/gugu511yy/PTQ4RIS}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring Image Segmentation (RIS), aims to segment the object referred by a\ngiven sentence in an image by understanding both visual and linguistic\ninformation. However, existing RIS methods tend to explore top-performance\nmodels, disregarding considerations for practical applications on\nresources-limited edge devices. This oversight poses a significant challenge\nfor on-device RIS inference. To this end, we propose an effective and efficient\npost-training quantization framework termed PTQ4RIS. Specifically, we first\nconduct an in-depth analysis of the root causes of performance degradation in\nRIS model quantization and propose dual-region quantization (DRQ) and\nreorder-based outlier-retained quantization (RORQ) to address the quantization\ndifficulties in visual and text encoders. Extensive experiments on three\nbenchmarks with different bits settings (from 8 to 4 bits) demonstrates its\nsuperior performance. Importantly, we are the first PTQ method specifically\ndesigned for the RIS task, highlighting the feasibility of PTQ in RIS\napplications. Code will be available at {https://github.com/gugu511yy/PTQ4RIS}."
                },
                "authors": [
                    {
                        "name": "Xiaoyan Jiang"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Kaiying Zhu"
                    },
                    {
                        "name": "Xihe Qiu"
                    },
                    {
                        "name": "Shibo Zhao"
                    },
                    {
                        "name": "Sifan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Sifan Zhou"
                },
                "author": "Sifan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17020v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17020v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17016v1",
                "updated": "2024-09-25T15:19:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    19,
                    4,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:19:04Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    19,
                    4,
                    2,
                    269,
                    0
                ],
                "title": "CNN Mixture-of-Depths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNN Mixture-of-Depths"
                },
                "summary": "We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks\n(CNNs), a novel approach that enhances the computational efficiency of CNNs by\nselectively processing channels based on their relevance to the current\nprediction. This method optimizes computational resources by dynamically\nselecting key channels in feature maps for focused processing within the\nconvolutional blocks (Conv-Blocks), while skipping less relevant channels.\nUnlike conditional computation methods that require dynamic computation graphs,\nCNN MoD uses a static computation graph with fixed tensor sizes which improve\nhardware efficiency. It speeds up the training and inference processes without\nthe need for customized CUDA kernels, unique loss functions, or finetuning. CNN\nMoD either matches the performance of traditional CNNs with reduced inference\ntimes, GMACs, and parameters, or exceeds their performance while maintaining\nsimilar inference times, GMACs, and parameters. For example, on ImageNet,\nResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a\n6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same\nperformance as ResNet50 with a 25% speedup on CPU and 15% on GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks\n(CNNs), a novel approach that enhances the computational efficiency of CNNs by\nselectively processing channels based on their relevance to the current\nprediction. This method optimizes computational resources by dynamically\nselecting key channels in feature maps for focused processing within the\nconvolutional blocks (Conv-Blocks), while skipping less relevant channels.\nUnlike conditional computation methods that require dynamic computation graphs,\nCNN MoD uses a static computation graph with fixed tensor sizes which improve\nhardware efficiency. It speeds up the training and inference processes without\nthe need for customized CUDA kernels, unique loss functions, or finetuning. CNN\nMoD either matches the performance of traditional CNNs with reduced inference\ntimes, GMACs, and parameters, or exceeds their performance while maintaining\nsimilar inference times, GMACs, and parameters. For example, on ImageNet,\nResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a\n6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same\nperformance as ResNet50 with a 25% speedup on CPU and 15% on GPU."
                },
                "authors": [
                    {
                        "name": "Rinor Cakaj"
                    },
                    {
                        "name": "Jens Mehnert"
                    },
                    {
                        "name": "Bin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yang"
                },
                "author": "Bin Yang",
                "arxiv_comment": "Conference Paper of the Asian Conference on Computer Vision (ACCV)\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v3",
                "updated": "2024-09-25T15:17:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    17,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: https://sgcode.codes/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: https://sgcode.codes/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_doi": "10.1145/3658644.3691367",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3691367",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17011v1",
                "updated": "2024-09-25T15:15:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    15,
                    57,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:15:57Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    15,
                    57,
                    2,
                    269,
                    0
                ],
                "title": "LLM-CARD: Towards a Description and Landscape of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CARD: Towards a Description and Landscape of Large Language Models"
                },
                "summary": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As an increasing number of papers are presented, researchers and\ndevelopers face the challenge of information overload. Thus, it is particularly\nimportant to develop a system that can automatically extract and organise key\ninformation about LLMs from academic papers (\\textbf{LLM model card}). This\nwork is to develop such a pioneer system by using Named Entity Recognition\n(\\textbf{NER}) and Relation Extraction (\\textbf{RE}) methods that automatically\nextract key information about large language models from the papers, helping\nresearchers to efficiently access information about LLMs. These features\ninclude model \\textit{licence}, model \\textit{name}, and model\n\\textit{application}. With these features, we can form a model card for each\npaper. \\textbf{Data-contribution} wise, 106 academic papers were processed by\ndefining three dictionaries - LLMs name, licence, and application. 11,051\nsentences were extracted through dictionary lookup, and the dataset was\nconstructed through manual review of the final selection of 129 sentences that\nhave a link between the name and the licence, and 106 sentences that have a\nlink between the model name and the application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As an increasing number of papers are presented, researchers and\ndevelopers face the challenge of information overload. Thus, it is particularly\nimportant to develop a system that can automatically extract and organise key\ninformation about LLMs from academic papers (\\textbf{LLM model card}). This\nwork is to develop such a pioneer system by using Named Entity Recognition\n(\\textbf{NER}) and Relation Extraction (\\textbf{RE}) methods that automatically\nextract key information about large language models from the papers, helping\nresearchers to efficiently access information about LLMs. These features\ninclude model \\textit{licence}, model \\textit{name}, and model\n\\textit{application}. With these features, we can form a model card for each\npaper. \\textbf{Data-contribution} wise, 106 academic papers were processed by\ndefining three dictionaries - LLMs name, licence, and application. 11,051\nsentences were extracted through dictionary lookup, and the dataset was\nconstructed through manual review of the final selection of 129 sentences that\nhave a link between the name and the licence, and 106 sentences that have a\nlink between the model name and the application."
                },
                "authors": [
                    {
                        "name": "Shengwei Tian"
                    },
                    {
                        "name": "Lifeng Han"
                    },
                    {
                        "name": "Erick Mendez Guzman"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "ongoing work, 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17004v1",
                "updated": "2024-09-25T15:07:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    7,
                    47,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:07:47Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    7,
                    47,
                    2,
                    269,
                    0
                ],
                "title": "Semantically-Driven Disambiguation for Human-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantically-Driven Disambiguation for Human-Robot Interaction"
                },
                "summary": "Ambiguities are common in human-robot interaction, especially when a robot\nfollows user instructions in a large collocated space. For instance, when the\nuser asks the robot to find an object in a home environment, the object might\nbe in several places depending on its varying semantic properties (e.g., a bowl\ncan be in the kitchen cabinet or on the dining room table, depending on whether\nit is clean/dirty, full/empty and the other objects around it). Previous works\non object semantics have predicted such relationships using one shot-inferences\nwhich are likely to fail for ambiguous or partially understood instructions.\nThis paper focuses on this gap and suggests a semantically-driven\ndisambiguation approach by utilizing follow-up clarifications to handle such\nuncertainties. To achieve this, we first obtain semantic knowledge embeddings,\nand then these embeddings are used to generate clarifying questions by\nfollowing an iterative process. The evaluation of our method shows that our\napproach is model agnostic, i.e., applicable to different semantic embedding\nmodels, and follow-up clarifications improve the performance regardless of the\nembedding model. Additionally, our ablation studies show the significance of\ninformative clarifications and iterative predictions to enhance system\naccuracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguities are common in human-robot interaction, especially when a robot\nfollows user instructions in a large collocated space. For instance, when the\nuser asks the robot to find an object in a home environment, the object might\nbe in several places depending on its varying semantic properties (e.g., a bowl\ncan be in the kitchen cabinet or on the dining room table, depending on whether\nit is clean/dirty, full/empty and the other objects around it). Previous works\non object semantics have predicted such relationships using one shot-inferences\nwhich are likely to fail for ambiguous or partially understood instructions.\nThis paper focuses on this gap and suggests a semantically-driven\ndisambiguation approach by utilizing follow-up clarifications to handle such\nuncertainties. To achieve this, we first obtain semantic knowledge embeddings,\nand then these embeddings are used to generate clarifying questions by\nfollowing an iterative process. The evaluation of our method shows that our\napproach is model agnostic, i.e., applicable to different semantic embedding\nmodels, and follow-up clarifications improve the performance regardless of the\nembedding model. Additionally, our ablation studies show the significance of\ninformative clarifications and iterative predictions to enhance system\naccuracies."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Weiyu Liu"
                    },
                    {
                        "name": "Iolanda Leite"
                    },
                    {
                        "name": "Sonia Chernova"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Chernova"
                },
                "author": "Sonia Chernova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16998v1",
                "updated": "2024-09-25T15:03:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    3,
                    22,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:03:22Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    3,
                    22,
                    2,
                    269,
                    0
                ],
                "title": "PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PitRSDNet: Predicting Intra-operative Remaining Surgery Duration in\n  Endoscopic Pituitary Surgery"
                },
                "summary": "Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate intra-operative Remaining Surgery Duration (RSD) predictions allow\nfor anaesthetists to more accurately decide when to administer anaesthetic\nagents and drugs, as well as to notify hospital staff to send in the next\npatient. Therefore RSD plays an important role in improving patient care and\nminimising surgical theatre costs via efficient scheduling. In endoscopic\npituitary surgery, it is uniquely challenging due to variable workflow\nsequences with a selection of optional steps contributing to high variability\nin surgery duration. This paper presents PitRSDNet for predicting RSD during\npituitary surgery, a spatio-temporal neural network model that learns from\nhistorical data focusing on workflow sequences. PitRSDNet integrates workflow\nknowledge into RSD prediction in two forms: 1) multi-task learning for\nconcurrently predicting step and RSD; and 2) incorporating prior steps as\ncontext in temporal learning and inference. PitRSDNet is trained and evaluated\non a new endoscopic pituitary surgery dataset with 88 videos to show\ncompetitive performance improvements over previous statistical and machine\nlearning methods. The findings also highlight how PitRSDNet improve RSD\nprecision on outlier cases utilising the knowledge of prior steps."
                },
                "authors": [
                    {
                        "name": "Anjana Wijekoon"
                    },
                    {
                        "name": "Adrito Das"
                    },
                    {
                        "name": "Roxana R. Herrera"
                    },
                    {
                        "name": "Danyal Z. Khan"
                    },
                    {
                        "name": "John Hanrahan"
                    },
                    {
                        "name": "Eleanor Carter"
                    },
                    {
                        "name": "Valpuri Luoma"
                    },
                    {
                        "name": "Danail Stoyanov"
                    },
                    {
                        "name": "Hani J. Marcus"
                    },
                    {
                        "name": "Sophia Bano"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Bano"
                },
                "author": "Sophia Bano",
                "arxiv_comment": "Accepted to the Augmented Environments for Computer-Assisted\n  Interventions (AE-CAI) Workshop at the Medical Image Computing and\n  Computer-Assisted Interventions (MICCAI) Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16997v2",
                "updated": "2024-09-26T06:13:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    13,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T15:02:25Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    2,
                    25,
                    2,
                    269,
                    0
                ],
                "title": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization"
                },
                "summary": "As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format."
                },
                "authors": [
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Zhiying Wu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Peizhuang Cong"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03589v3",
                "updated": "2024-09-25T14:59:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    59,
                    24,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-05T19:14:21Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    19,
                    14,
                    21,
                    2,
                    157,
                    0
                ],
                "title": "Ranking Manipulation for Conversational Search Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Manipulation for Conversational Search Engines"
                },
                "summary": "Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity$.$ai. Given the strong\nfinancial incentive for website owners to boost their search ranking, we argue\nthat our problem formulation is of critical importance for future robustness\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity$.$ai. Given the strong\nfinancial incentive for website owners to boost their search ranking, we argue\nthat our problem formulation is of critical importance for future robustness\nwork."
                },
                "authors": [
                    {
                        "name": "Samuel Pfrommer"
                    },
                    {
                        "name": "Yatong Bai"
                    },
                    {
                        "name": "Tanmay Gautam"
                    },
                    {
                        "name": "Somayeh Sojoudi"
                    }
                ],
                "author_detail": {
                    "name": "Somayeh Sojoudi"
                },
                "author": "Somayeh Sojoudi",
                "arxiv_comment": "2024 Conference on Empirical Methods in Natural Language Processing\n  (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16984v1",
                "updated": "2024-09-25T14:45:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    45,
                    52,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:45:52Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    45,
                    52,
                    2,
                    269,
                    0
                ],
                "title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs"
                },
                "summary": "Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs."
                },
                "authors": [
                    {
                        "name": "P Aditya Sreekar"
                    },
                    {
                        "name": "Sahil Verma"
                    },
                    {
                        "name": "Suransh Chopra"
                    },
                    {
                        "name": "Sarik Ghazarian"
                    },
                    {
                        "name": "Abhishek Persad"
                    },
                    {
                        "name": "Narayanan Sadagopan"
                    }
                ],
                "author_detail": {
                    "name": "Narayanan Sadagopan"
                },
                "author": "Narayanan Sadagopan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16974v1",
                "updated": "2024-09-25T14:36:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    36,
                    30,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:36:30Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    36,
                    30,
                    2,
                    269,
                    0
                ],
                "title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions"
                },
                "summary": "There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations."
                },
                "authors": [
                    {
                        "name": "Zeyneb N. Kaya"
                    },
                    {
                        "name": "Souvick Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Souvick Ghosh"
                },
                "author": "Souvick Ghosh",
                "arxiv_comment": "28 pages, 5 figures, preprint submitted to journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16973v1",
                "updated": "2024-09-25T14:35:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    35,
                    6,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:35:06Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    35,
                    6,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization"
                },
                "summary": "Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device."
                },
                "authors": [
                    {
                        "name": "Rafael Mendoza"
                    },
                    {
                        "name": "Isabella Cruz"
                    },
                    {
                        "name": "Richard Liu"
                    },
                    {
                        "name": "Aarav Deshmukh"
                    },
                    {
                        "name": "David Williams"
                    },
                    {
                        "name": "Jesscia Peng"
                    },
                    {
                        "name": "Rohan Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Iyer"
                },
                "author": "Rohan Iyer",
                "arxiv_comment": "First ASLS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16953v1",
                "updated": "2024-09-25T14:08:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    8,
                    37,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:08:37Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    8,
                    37,
                    2,
                    269,
                    0
                ],
                "title": "Path-adaptive Spatio-Temporal State Space Model for Event-based\n  Recognition with Arbitrary Duration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path-adaptive Spatio-Temporal State Space Model for Event-based\n  Recognition with Arbitrary Duration"
                },
                "summary": "Event cameras are bio-inspired sensors that capture the intensity changes\nasynchronously and output event streams with distinct advantages, such as high\ntemporal resolution. To exploit event cameras for object/action recognition,\nexisting methods predominantly sample and aggregate events in a second-level\nduration at every fixed temporal interval (or frequency). However, they often\nface difficulties in capturing the spatiotemporal relationships for longer,\ne.g., minute-level, events and generalizing across varying temporal\nfrequencies. To fill the gap, we present a novel framework, dubbed PAST-SSM,\nexhibiting superior capacity in recognizing events with arbitrary duration\n(e.g., 0.1s to 4.5s) and generalizing to varying inference frequencies. Our key\ninsight is to learn the spatiotemporal relationships from the encoded event\nfeatures via the state space model (SSM) -- whose linear complexity makes it\nideal for modeling high temporal resolution events with longer sequences. To\nachieve this goal, we first propose a Path-Adaptive Event Aggregation and Scan\n(PEAS) module to encode events of varying duration into features with fixed\ndimensions by adaptively scanning and selecting aggregated event frames. On top\nof PEAS, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to\nminimize the randomness and redundancy of the encoded features. This subtly\nenhances the model generalization across different inference frequencies.\nLastly, the SSM is employed to better learn the spatiotemporal properties from\nthe encoded features. Moreover, we build a minute-level event-based recognition\ndataset, named ArDVS100, with arbitrary duration for the benefit of the\ncommunity. Extensive experiments prove that our method outperforms prior arts\nby +3.45%, +0.38% and +8.31% on the DVS Action, SeAct and HARDVS datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event cameras are bio-inspired sensors that capture the intensity changes\nasynchronously and output event streams with distinct advantages, such as high\ntemporal resolution. To exploit event cameras for object/action recognition,\nexisting methods predominantly sample and aggregate events in a second-level\nduration at every fixed temporal interval (or frequency). However, they often\nface difficulties in capturing the spatiotemporal relationships for longer,\ne.g., minute-level, events and generalizing across varying temporal\nfrequencies. To fill the gap, we present a novel framework, dubbed PAST-SSM,\nexhibiting superior capacity in recognizing events with arbitrary duration\n(e.g., 0.1s to 4.5s) and generalizing to varying inference frequencies. Our key\ninsight is to learn the spatiotemporal relationships from the encoded event\nfeatures via the state space model (SSM) -- whose linear complexity makes it\nideal for modeling high temporal resolution events with longer sequences. To\nachieve this goal, we first propose a Path-Adaptive Event Aggregation and Scan\n(PEAS) module to encode events of varying duration into features with fixed\ndimensions by adaptively scanning and selecting aggregated event frames. On top\nof PEAS, we introduce a novel Multi-faceted Selection Guiding (MSG) loss to\nminimize the randomness and redundancy of the encoded features. This subtly\nenhances the model generalization across different inference frequencies.\nLastly, the SSM is employed to better learn the spatiotemporal properties from\nthe encoded features. Moreover, we build a minute-level event-based recognition\ndataset, named ArDVS100, with arbitrary duration for the benefit of the\ncommunity. Extensive experiments prove that our method outperforms prior arts\nby +3.45%, +0.38% and +8.31% on the DVS Action, SeAct and HARDVS datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jiazhou Zhou"
                    },
                    {
                        "name": "Kanghao Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Lin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lin Wang"
                },
                "author": "Lin Wang",
                "arxiv_comment": "First version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18751v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18751v2",
                "updated": "2024-09-25T14:03:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    3,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-26T20:30:56Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    20,
                    30,
                    56,
                    2,
                    178,
                    0
                ],
                "title": "Robust Distributed Learning of Functional Data From Simulators through\n  Data Sketching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Distributed Learning of Functional Data From Simulators through\n  Data Sketching"
                },
                "summary": "In environmental studies, realistic simulations are essential for\nunderstanding complex systems. Statistical emulation with Gaussian processes\n(GPs) in functional data models have become a standard tool for this purpose.\nTraditional centralized processing of such models requires substantial\ncomputational and storage resources, leading to emerging distributed Bayesian\nlearning algorithms that partition data into shards for distributed\ncomputations. However, concerns about the sensitivity of distributed inference\nto shard selection arise. Instead of using data shards, our approach employs\nmultiple random matrices to create random linear projections, or sketches, of\nthe dataset. Posterior inference on functional data models is conducted using\nrandom data sketches on various machines in parallel. These individual\ninferences are combined across machines at a central server. The aggregation of\ninference across random matrices makes our approach resilient to the selection\nof data sketches, resulting in robust distributed Bayesian learning. An\nimportant advantage is its ability to maintain the privacy of sampling units,\nas random sketches prevent the recovery of raw data. We highlight the\nsignificance of our approach through simulation examples and showcase the\nperformance of our approach as an emulator using surrogates of the Sea, Lake,\nand Overland Surges from Hurricanes (SLOSH) simulator - an important simulator\nfor government agencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In environmental studies, realistic simulations are essential for\nunderstanding complex systems. Statistical emulation with Gaussian processes\n(GPs) in functional data models have become a standard tool for this purpose.\nTraditional centralized processing of such models requires substantial\ncomputational and storage resources, leading to emerging distributed Bayesian\nlearning algorithms that partition data into shards for distributed\ncomputations. However, concerns about the sensitivity of distributed inference\nto shard selection arise. Instead of using data shards, our approach employs\nmultiple random matrices to create random linear projections, or sketches, of\nthe dataset. Posterior inference on functional data models is conducted using\nrandom data sketches on various machines in parallel. These individual\ninferences are combined across machines at a central server. The aggregation of\ninference across random matrices makes our approach resilient to the selection\nof data sketches, resulting in robust distributed Bayesian learning. An\nimportant advantage is its ability to maintain the privacy of sampling units,\nas random sketches prevent the recovery of raw data. We highlight the\nsignificance of our approach through simulation examples and showcase the\nperformance of our approach as an emulator using surrogates of the Sea, Lake,\nand Overland Surges from Hurricanes (SLOSH) simulator - an important simulator\nfor government agencies."
                },
                "authors": [
                    {
                        "name": "R. Jacob Andros"
                    },
                    {
                        "name": "Rajarshi Guhaniyogi"
                    },
                    {
                        "name": "Devin Francom"
                    },
                    {
                        "name": "Donatella Pasqualini"
                    }
                ],
                "author_detail": {
                    "name": "Donatella Pasqualini"
                },
                "author": "Donatella Pasqualini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18751v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18751v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16949v1",
                "updated": "2024-09-25T14:02:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    2,
                    43,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:02:43Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    2,
                    43,
                    2,
                    269,
                    0
                ],
                "title": "DALDA: Data Augmentation Leveraging Diffusion Model and LLM with\n  Adaptive Guidance Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DALDA: Data Augmentation Leveraging Diffusion Model and LLM with\n  Adaptive Guidance Scaling"
                },
                "summary": "In this paper, we present an effective data augmentation framework leveraging\nthe Large Language Model (LLM) and Diffusion Model (DM) to tackle the\nchallenges inherent in data-scarce scenarios. Recently, DMs have opened up the\npossibility of generating synthetic images to complement a few training images.\nHowever, increasing the diversity of synthetic images also raises the risk of\ngenerating samples outside the target distribution. Our approach addresses this\nissue by embedding novel semantic information into text prompts via LLM and\nutilizing real images as visual prompts, thus generating semantically rich\nimages. To ensure that the generated images remain within the target\ndistribution, we dynamically adjust the guidance weight based on each image's\nCLIPScore to control the diversity. Experimental results show that our method\nproduces synthetic images with enhanced diversity while maintaining adherence\nto the target distribution. Consequently, our approach proves to be more\nefficient in the few-shot setting on several benchmarks. Our code is available\nat https://github.com/kkyuhun94/dalda .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an effective data augmentation framework leveraging\nthe Large Language Model (LLM) and Diffusion Model (DM) to tackle the\nchallenges inherent in data-scarce scenarios. Recently, DMs have opened up the\npossibility of generating synthetic images to complement a few training images.\nHowever, increasing the diversity of synthetic images also raises the risk of\ngenerating samples outside the target distribution. Our approach addresses this\nissue by embedding novel semantic information into text prompts via LLM and\nutilizing real images as visual prompts, thus generating semantically rich\nimages. To ensure that the generated images remain within the target\ndistribution, we dynamically adjust the guidance weight based on each image's\nCLIPScore to control the diversity. Experimental results show that our method\nproduces synthetic images with enhanced diversity while maintaining adherence\nto the target distribution. Consequently, our approach proves to be more\nefficient in the few-shot setting on several benchmarks. Our code is available\nat https://github.com/kkyuhun94/dalda ."
                },
                "authors": [
                    {
                        "name": "Kyuheon Jung"
                    },
                    {
                        "name": "Yongdeuk Seo"
                    },
                    {
                        "name": "Seongwoo Cho"
                    },
                    {
                        "name": "Jaeyoung Kim"
                    },
                    {
                        "name": "Hyun-seok Min"
                    },
                    {
                        "name": "Sungchul Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungchul Choi"
                },
                "author": "Sungchul Choi",
                "arxiv_comment": "Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02135v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02135v2",
                "updated": "2024-09-25T14:00:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    0,
                    18,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-04T09:24:04Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    9,
                    24,
                    4,
                    1,
                    156,
                    0
                ],
                "title": "Robust Interaction-Based Relevance Modeling for Online e-Commerce Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Interaction-Based Relevance Modeling for Online e-Commerce Search"
                },
                "summary": "Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic relevance calculation is crucial for e-commerce search engines, as\nit ensures that the items selected closely align with customer intent.\nInadequate attention to this aspect can detrimentally affect user experience\nand engagement. Traditional text-matching techniques are prevalent but often\nfail to capture the nuances of search intent accurately, so neural networks now\nhave become a preferred solution to processing such complex text matching.\nExisting methods predominantly employ representation-based architectures, which\nstrike a balance between high traffic capacity and low latency. However, they\nexhibit significant shortcomings in generalization and robustness when compared\nto interaction-based architectures. In this work, we introduce a robust\ninteraction-based modeling paradigm to address these shortcomings. It\nencompasses 1) a dynamic length representation scheme for expedited inference,\n2) a professional terms recognition method to identify subjects and core\nattributes from complex sentence structures, and 3) a contrastive adversarial\ntraining protocol to bolster the model's robustness and matching capabilities.\nExtensive offline evaluations demonstrate the superior robustness and\neffectiveness of our approach, and online A/B testing confirms its ability to\nimprove relevance in the same exposure position, resulting in more clicks and\nconversions. To the best of our knowledge, this method is the first\ninteraction-based approach for large e-commerce search relevance calculation.\nNotably, we have deployed it for the entire search traffic on alibaba.com, the\nlargest B2B e-commerce platform in the world."
                },
                "authors": [
                    {
                        "name": "Ben Chen"
                    },
                    {
                        "name": "Huangyu Dai"
                    },
                    {
                        "name": "Xiang Ma"
                    },
                    {
                        "name": "Wen Jiang"
                    },
                    {
                        "name": "Wei Ning"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ning"
                },
                "author": "Wei Ning",
                "arxiv_comment": "Accepted by ECML-PKDD'24 as Outstanding Paper. 8 pages, 2 figures, 7\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02135v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02135v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16945v1",
                "updated": "2024-09-25T13:57:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    57,
                    16,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    57,
                    16,
                    2,
                    269,
                    0
                ],
                "title": "Face Forgery Detection with Elaborate Backbone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face Forgery Detection with Elaborate Backbone"
                },
                "summary": "Face Forgery Detection (FFD), or Deepfake detection, aims to determine\nwhether a digital face is real or fake. Due to different face synthesis\nalgorithms with diverse forgery patterns, FFD models often overfit specific\npatterns in training datasets, resulting in poor generalization to other unseen\nforgeries. This severe challenge requires FFD models to possess strong\ncapabilities in representing complex facial features and extracting subtle\nforgery cues. Although previous FFD models directly employ existing backbones\nto represent and extract facial forgery cues, the critical role of backbones is\noften overlooked, particularly as their knowledge and capabilities are\ninsufficient to address FFD challenges, inevitably limiting generalization.\nTherefore, it is essential to integrate the backbone pre-training\nconfigurations and seek practical solutions by revisiting the complete FFD\nworkflow, from backbone pre-training and fine-tuning to inference of\ndiscriminant results. Specifically, we analyze the crucial contributions of\nbackbones with different configurations in FFD task and propose leveraging the\nViT network with self-supervised learning on real-face datasets to pre-train a\nbackbone, equipping it with superior facial representation capabilities. We\nthen build a competitive backbone fine-tuning framework that strengthens the\nbackbone's ability to extract diverse forgery cues within a competitive\nlearning mechanism. Moreover, we devise a threshold optimization mechanism that\nutilizes prediction confidence to improve the inference reliability.\nComprehensive experiments demonstrate that our FFD model with the elaborate\nbackbone achieves excellent performance in FFD and extra face-related tasks,\ni.e., presentation attack detection. Code and models are available at\nhttps://github.com/zhenglab/FFDBackbone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face Forgery Detection (FFD), or Deepfake detection, aims to determine\nwhether a digital face is real or fake. Due to different face synthesis\nalgorithms with diverse forgery patterns, FFD models often overfit specific\npatterns in training datasets, resulting in poor generalization to other unseen\nforgeries. This severe challenge requires FFD models to possess strong\ncapabilities in representing complex facial features and extracting subtle\nforgery cues. Although previous FFD models directly employ existing backbones\nto represent and extract facial forgery cues, the critical role of backbones is\noften overlooked, particularly as their knowledge and capabilities are\ninsufficient to address FFD challenges, inevitably limiting generalization.\nTherefore, it is essential to integrate the backbone pre-training\nconfigurations and seek practical solutions by revisiting the complete FFD\nworkflow, from backbone pre-training and fine-tuning to inference of\ndiscriminant results. Specifically, we analyze the crucial contributions of\nbackbones with different configurations in FFD task and propose leveraging the\nViT network with self-supervised learning on real-face datasets to pre-train a\nbackbone, equipping it with superior facial representation capabilities. We\nthen build a competitive backbone fine-tuning framework that strengthens the\nbackbone's ability to extract diverse forgery cues within a competitive\nlearning mechanism. Moreover, we devise a threshold optimization mechanism that\nutilizes prediction confidence to improve the inference reliability.\nComprehensive experiments demonstrate that our FFD model with the elaborate\nbackbone achieves excellent performance in FFD and extra face-related tasks,\ni.e., presentation attack detection. Code and models are available at\nhttps://github.com/zhenglab/FFDBackbone."
                },
                "authors": [
                    {
                        "name": "Zonghui Guo"
                    },
                    {
                        "name": "Yingjie Liu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Haiyong Zheng"
                    },
                    {
                        "name": "Shiguang Shan"
                    }
                ],
                "author_detail": {
                    "name": "Shiguang Shan"
                },
                "author": "Shiguang Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1910.03821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1910.03821v5",
                "updated": "2024-09-25T13:45:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    45,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2019-10-09T07:31:59Z",
                "published_parsed": [
                    2019,
                    10,
                    9,
                    7,
                    31,
                    59,
                    2,
                    282,
                    0
                ],
                "title": "Quasi Maximum Likelihood Estimation and Inference of Large Approximate\n  Dynamic Factor Models via the EM algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasi Maximum Likelihood Estimation and Inference of Large Approximate\n  Dynamic Factor Models via the EM algorithm"
                },
                "summary": "We study estimation of large Dynamic Factor models implemented through the\nExpectation Maximization (EM) algorithm, jointly with the Kalman smoother. We\nprove that as both the cross-sectional dimension, $n$, and the sample size,\n$T$, diverge to infinity: (i) the estimated loadings are $\\sqrt T$-consistent,\nasymptotically normal and equivalent to their Quasi Maximum Likelihood\nestimates; (ii) the estimated factors are $\\sqrt n$-consistent, asymptotically\nnormal and equivalent to their Weighted Least Squares estimates. Moreover, the\nestimated loadings are asymptotically as efficient as those obtained by\nPrincipal Components analysis, while the estimated factors are more efficient\nif the idiosyncratic covariance is sparse enough.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study estimation of large Dynamic Factor models implemented through the\nExpectation Maximization (EM) algorithm, jointly with the Kalman smoother. We\nprove that as both the cross-sectional dimension, $n$, and the sample size,\n$T$, diverge to infinity: (i) the estimated loadings are $\\sqrt T$-consistent,\nasymptotically normal and equivalent to their Quasi Maximum Likelihood\nestimates; (ii) the estimated factors are $\\sqrt n$-consistent, asymptotically\nnormal and equivalent to their Weighted Least Squares estimates. Moreover, the\nestimated loadings are asymptotically as efficient as those obtained by\nPrincipal Components analysis, while the estimated factors are more efficient\nif the idiosyncratic covariance is sparse enough."
                },
                "authors": [
                    {
                        "name": "Matteo Barigozzi"
                    },
                    {
                        "name": "Matteo Luciani"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Luciani"
                },
                "author": "Matteo Luciani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1910.03821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1910.03821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19280v3",
                "updated": "2024-09-25T13:36:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    36,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-27T15:50:41Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    50,
                    41,
                    3,
                    179,
                    0
                ],
                "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale"
                },
                "summary": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs."
                },
                "authors": [
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Chi Gui"
                    },
                    {
                        "name": "Ruyi Ouyang"
                    },
                    {
                        "name": "Anningzhe Gao"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Guiming Hardy Chen"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Ruifei Zhang"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06721v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06721v3",
                "updated": "2024-09-25T13:27:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    27,
                    22,
                    2,
                    269,
                    0
                ],
                "published": "2023-10-10T15:48:55Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    15,
                    48,
                    55,
                    1,
                    283,
                    0
                ],
                "title": "Tweedie Moment Projected Diffusions For Inverse Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tweedie Moment Projected Diffusions For Inverse Problems"
                },
                "summary": "Diffusion generative models unlock new possibilities for inverse problems as\nthey allow for the incorporation of strong empirical priors in scientific\ninference. Recently, diffusion models are repurposed for solving inverse\nproblems using Gaussian approximations to conditional densities of the reverse\nprocess via Tweedie's formula to parameterise the mean, complemented with\nvarious heuristics. To address various challenges arising from these\napproximations, we leverage higher order information using Tweedie's formula\nand obtain a statistically principled approximation. We further provide a\ntheoretical guarantee specifically for posterior sampling which can lead to a\nbetter theoretical understanding of diffusion-based conditional sampling.\nFinally, we illustrate the empirical effectiveness of our approach for general\nlinear inverse problems on toy synthetic examples as well as image restoration.\nWe show that our method (i) removes any time-dependent step-size\nhyperparameters required by earlier methods, (ii) brings stability and better\nsample quality across multiple noise levels, (iii) is the only method that\nworks in a stable way with variance exploding (VE) forward processes as opposed\nto earlier works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion generative models unlock new possibilities for inverse problems as\nthey allow for the incorporation of strong empirical priors in scientific\ninference. Recently, diffusion models are repurposed for solving inverse\nproblems using Gaussian approximations to conditional densities of the reverse\nprocess via Tweedie's formula to parameterise the mean, complemented with\nvarious heuristics. To address various challenges arising from these\napproximations, we leverage higher order information using Tweedie's formula\nand obtain a statistically principled approximation. We further provide a\ntheoretical guarantee specifically for posterior sampling which can lead to a\nbetter theoretical understanding of diffusion-based conditional sampling.\nFinally, we illustrate the empirical effectiveness of our approach for general\nlinear inverse problems on toy synthetic examples as well as image restoration.\nWe show that our method (i) removes any time-dependent step-size\nhyperparameters required by earlier methods, (ii) brings stability and better\nsample quality across multiple noise levels, (iii) is the only method that\nworks in a stable way with variance exploding (VE) forward processes as opposed\nto earlier works."
                },
                "authors": [
                    {
                        "name": "Benjamin Boys"
                    },
                    {
                        "name": "Mark Girolami"
                    },
                    {
                        "name": "Jakiw Pidstrigach"
                    },
                    {
                        "name": "Sebastian Reich"
                    },
                    {
                        "name": "Alan Mosca"
                    },
                    {
                        "name": "O. Deniz Akyildiz"
                    }
                ],
                "author_detail": {
                    "name": "O. Deniz Akyildiz"
                },
                "author": "O. Deniz Akyildiz",
                "arxiv_comment": "12 pages, 2 figures, 2 tables when excluding abstract and\n  bibliography; 45 pages, 17 figures, 13 tables when including abstract and\n  bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06721v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06721v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.03663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.03663v2",
                "updated": "2024-09-25T13:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    23,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2023-06-06T13:26:35Z",
                "published_parsed": [
                    2023,
                    6,
                    6,
                    13,
                    26,
                    35,
                    1,
                    157,
                    0
                ],
                "title": "Bayesian inference for group-level cortical surface\n  image-on-scalar-regression with Gaussian process priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for group-level cortical surface\n  image-on-scalar-regression with Gaussian process priors"
                },
                "summary": "In regression-based analyses of group-level neuroimage data researchers\ntypically fit a series of marginal general linear models to image outcomes at\neach spatially-referenced pixel. Spatial regularization of effects of interest\nis usually induced indirectly by applying spatial smoothing to the data during\npreprocessing. While this procedure often works well, resulting inference can\nbe poorly calibrated. Spatial modeling of effects of interest leads to more\npowerful analyses, however the number of locations in a typical neuroimage can\npreclude standard computation with explicitly spatial models. Here we\ncontribute a Bayesian spatial regression model for group-level neuroimaging\nanalyses. We induce regularization of spatially varying regression coefficient\nfunctions through Gaussian process priors. When combined with a simple\nnonstationary model for the error process, our prior hierarchy can lead to more\ndata-adaptive smoothing than standard methods. We achieve computational\ntractability through Vecchia approximation of our prior which, critically, can\nbe constructed for a wide class of spatial correlation functions and results in\nprior models that retain full spatial rank. We outline several ways to work\nwith our model in practice and compare performance against standard vertex-wise\nanalyses. Finally we illustrate our method in an analysis of cortical surface\nfMRI task contrast data from a large cohort of children enrolled in the\nAdolescent Brain Cognitive Development study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In regression-based analyses of group-level neuroimage data researchers\ntypically fit a series of marginal general linear models to image outcomes at\neach spatially-referenced pixel. Spatial regularization of effects of interest\nis usually induced indirectly by applying spatial smoothing to the data during\npreprocessing. While this procedure often works well, resulting inference can\nbe poorly calibrated. Spatial modeling of effects of interest leads to more\npowerful analyses, however the number of locations in a typical neuroimage can\npreclude standard computation with explicitly spatial models. Here we\ncontribute a Bayesian spatial regression model for group-level neuroimaging\nanalyses. We induce regularization of spatially varying regression coefficient\nfunctions through Gaussian process priors. When combined with a simple\nnonstationary model for the error process, our prior hierarchy can lead to more\ndata-adaptive smoothing than standard methods. We achieve computational\ntractability through Vecchia approximation of our prior which, critically, can\nbe constructed for a wide class of spatial correlation functions and results in\nprior models that retain full spatial rank. We outline several ways to work\nwith our model in practice and compare performance against standard vertex-wise\nanalyses. Finally we illustrate our method in an analysis of cortical surface\nfMRI task contrast data from a large cohort of children enrolled in the\nAdolescent Brain Cognitive Development study."
                },
                "authors": [
                    {
                        "name": "Andrew S. Whiteman"
                    },
                    {
                        "name": "Timothy D. Johnson"
                    },
                    {
                        "name": "Jian Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Kang"
                },
                "author": "Jian Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.03663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.03663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16915v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16915v1",
                "updated": "2024-09-25T13:20:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    20,
                    33,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:20:33Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    20,
                    33,
                    2,
                    269,
                    0
                ],
                "title": "Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized\n  Gaussian Splat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized\n  Gaussian Splat"
                },
                "summary": "Neural Radiance Fields and Gaussian Splatting have transformed the field of\ncomputer vision by enabling photo-realistic representation of complex scenes.\nDespite this success, they have seen only limited use in real-world robotics\ntasks such as trajectory optimization. Two key factors have contributed to this\nlimited success. First, it is challenging to reason about collisions in\nradiance models. Second, it is difficult to perform inference of radiance\nmodels fast enough for real-time trajectory synthesis. This paper addresses\nthese challenges by proposing SPLANNING, a risk-aware trajectory optimizer that\noperates in a Gaussian Splatting model. This paper first derives a method for\nrigorously upper-bounding the probability of collision between a robot and a\nradiance field. Second, this paper introduces a normalized reformulation of\nGaussian Splatting that enables the efficient computation of the collision\nbound in a Gaussian Splat. Third, a method is presented to optimize\ntrajectories while avoiding collisions with a scene represented by a Gaussian\nSplat. Experiments demonstrate that SPLANNING outperforms state-of-the-art\nmethods in generating collision-free trajectories in highly cluttered\nenvironments. The proposed system is also tested on a real-world robot\nmanipulator. A project page is available at\nhttps://roahmlab.github.io/splanning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields and Gaussian Splatting have transformed the field of\ncomputer vision by enabling photo-realistic representation of complex scenes.\nDespite this success, they have seen only limited use in real-world robotics\ntasks such as trajectory optimization. Two key factors have contributed to this\nlimited success. First, it is challenging to reason about collisions in\nradiance models. Second, it is difficult to perform inference of radiance\nmodels fast enough for real-time trajectory synthesis. This paper addresses\nthese challenges by proposing SPLANNING, a risk-aware trajectory optimizer that\noperates in a Gaussian Splatting model. This paper first derives a method for\nrigorously upper-bounding the probability of collision between a robot and a\nradiance field. Second, this paper introduces a normalized reformulation of\nGaussian Splatting that enables the efficient computation of the collision\nbound in a Gaussian Splat. Third, a method is presented to optimize\ntrajectories while avoiding collisions with a scene represented by a Gaussian\nSplat. Experiments demonstrate that SPLANNING outperforms state-of-the-art\nmethods in generating collision-free trajectories in highly cluttered\nenvironments. The proposed system is also tested on a real-world robot\nmanipulator. A project page is available at\nhttps://roahmlab.github.io/splanning."
                },
                "authors": [
                    {
                        "name": "Jonathan Michaux"
                    },
                    {
                        "name": "Seth Isaacson"
                    },
                    {
                        "name": "Challen Enninful Adu"
                    },
                    {
                        "name": "Adam Li"
                    },
                    {
                        "name": "Rahul Kashyap Swayampakula"
                    },
                    {
                        "name": "Parker Ewen"
                    },
                    {
                        "name": "Sean Rice"
                    },
                    {
                        "name": "Katherine A. Skinner"
                    },
                    {
                        "name": "Ram Vasudevan"
                    }
                ],
                "author_detail": {
                    "name": "Ram Vasudevan"
                },
                "author": "Ram Vasudevan",
                "arxiv_comment": "First two authors contributed equally. Project Page:\n  https://roahmlab.github.io/splanning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16915v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16915v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16914v1",
                "updated": "2024-09-25T13:18:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    18,
                    57,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:18:57Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    18,
                    57,
                    2,
                    269,
                    0
                ],
                "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness"
                },
                "summary": "The increasing capability and widespread usage of large language models\n(LLMs) highlight the desirability of automatic detection of LLM-generated text.\nZero-shot detectors, due to their training-free nature, have received\nconsiderable attention and notable success. In this paper, we identify a new\nfeature, token cohesiveness, that is useful for zero-shot detection, and we\ndemonstrate that LLM-generated text tends to exhibit higher token cohesiveness\nthan human-written text. Based on this observation, we devise TOCSIN, a generic\ndual-channel detection paradigm that uses token cohesiveness as a plug-and-play\nmodule to improve existing zero-shot detectors. To calculate token\ncohesiveness, TOCSIN only requires a few rounds of random token deletion and\nsemantic difference measurement, making it particularly suitable for a\npractical black-box setting where the source model used for generation is not\naccessible. Extensive experiments with four state-of-the-art base detectors on\nvarious datasets, source models, and evaluation settings demonstrate the\neffectiveness and generality of the proposed approach. Code available at:\n\\url{https://github.com/Shixuan-Ma/TOCSIN}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capability and widespread usage of large language models\n(LLMs) highlight the desirability of automatic detection of LLM-generated text.\nZero-shot detectors, due to their training-free nature, have received\nconsiderable attention and notable success. In this paper, we identify a new\nfeature, token cohesiveness, that is useful for zero-shot detection, and we\ndemonstrate that LLM-generated text tends to exhibit higher token cohesiveness\nthan human-written text. Based on this observation, we devise TOCSIN, a generic\ndual-channel detection paradigm that uses token cohesiveness as a plug-and-play\nmodule to improve existing zero-shot detectors. To calculate token\ncohesiveness, TOCSIN only requires a few rounds of random token deletion and\nsemantic difference measurement, making it particularly suitable for a\npractical black-box setting where the source model used for generation is not\naccessible. Extensive experiments with four state-of-the-art base detectors on\nvarious datasets, source models, and evaluation settings demonstrate the\neffectiveness and generality of the proposed approach. Code available at:\n\\url{https://github.com/Shixuan-Ma/TOCSIN}."
                },
                "authors": [
                    {
                        "name": "Shixuan Ma"
                    },
                    {
                        "name": "Quan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Quan Wang"
                },
                "author": "Quan Wang",
                "arxiv_comment": "To appear at the main conference of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16911v1",
                "updated": "2024-09-25T13:15:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    15,
                    50,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:15:50Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    15,
                    50,
                    2,
                    269,
                    0
                ],
                "title": "Pruning Multilingual Large Language Models for Multilingual Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning Multilingual Large Language Models for Multilingual Inference"
                },
                "summary": "Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (MLLMs), trained on multilingual balanced\ndata, demonstrate better zero-shot learning performance in non-English\nlanguages compared to large language models trained on English-dominant data.\nHowever, the disparity in performance between English and non-English languages\nremains a challenge yet to be fully addressed. A distinctive characteristic of\nMLLMs is their high-quality translation capabilities, indicating an acquired\nproficiency in aligning between languages. This study explores how to enhance\nthe zero-shot performance of MLLMs in non-English languages by leveraging their\nalignment capability between English and non-English languages. To achieve\nthis, we first analyze the behavior of MLLMs when performing translation and\nreveal that there are large magnitude features that play a critical role in the\ntranslation process. Inspired by these findings, we retain the weights\nassociated with operations involving the large magnitude features and prune\nother weights to force MLLMs to rely on these features for tasks beyond\ntranslation. We empirically demonstrate that this pruning strategy can enhance\nthe MLLMs' performance in non-English language."
                },
                "authors": [
                    {
                        "name": "Hwichan Kim"
                    },
                    {
                        "name": "Jun Suzuki"
                    },
                    {
                        "name": "Tosho Hirasawa"
                    },
                    {
                        "name": "Mamoru Komachi"
                    }
                ],
                "author_detail": {
                    "name": "Mamoru Komachi"
                },
                "author": "Mamoru Komachi",
                "arxiv_comment": "Accepted at EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.00126v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.00126v3",
                "updated": "2024-09-25T13:13:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    13,
                    32,
                    2,
                    269,
                    0
                ],
                "published": "2023-04-28T23:43:10Z",
                "published_parsed": [
                    2023,
                    4,
                    28,
                    23,
                    43,
                    10,
                    4,
                    118,
                    0
                ],
                "title": "Event-Free Moving Object Segmentation from Moving Ego Vehicle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Free Moving Object Segmentation from Moving Ego Vehicle"
                },
                "summary": "Moving object segmentation (MOS) in dynamic scenes is an important,\nchallenging, but under-explored research topic for autonomous driving,\nespecially for sequences obtained from moving ego vehicles. Most segmentation\nmethods leverage motion cues obtained from optical flow maps. However, since\nthese methods are often based on optical flows that are pre-computed from\nsuccessive RGB frames, this neglects the temporal consideration of events\noccurring within the inter-frame, consequently constraining its ability to\ndiscern objects exhibiting relative staticity but genuinely in motion. To\naddress these limitations, we propose to exploit event cameras for better video\nunderstanding, which provide rich motion cues without relying on optical flow.\nTo foster research in this area, we first introduce a novel large-scale dataset\ncalled DSEC-MOS for moving object segmentation from moving ego vehicles, which\nis the first of its kind. For benchmarking, we select various mainstream\nmethods and rigorously evaluate them on our dataset. Subsequently, we devise\nEmoFormer, a novel network able to exploit the event data. For this purpose, we\nfuse the event temporal prior with spatial semantic maps to distinguish\ngenuinely moving objects from the static background, adding another level of\ndense supervision around our object of interest. Our proposed network relies\nonly on event data for training but does not require event input during\ninference, making it directly comparable to frame-only methods in terms of\nefficiency and more widely usable in many application cases. The exhaustive\ncomparison highlights a significant performance improvement of our method over\nall other methods. The source code and dataset are publicly available at:\nhttps://github.com/ZZY-Zhou/DSEC-MOS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving object segmentation (MOS) in dynamic scenes is an important,\nchallenging, but under-explored research topic for autonomous driving,\nespecially for sequences obtained from moving ego vehicles. Most segmentation\nmethods leverage motion cues obtained from optical flow maps. However, since\nthese methods are often based on optical flows that are pre-computed from\nsuccessive RGB frames, this neglects the temporal consideration of events\noccurring within the inter-frame, consequently constraining its ability to\ndiscern objects exhibiting relative staticity but genuinely in motion. To\naddress these limitations, we propose to exploit event cameras for better video\nunderstanding, which provide rich motion cues without relying on optical flow.\nTo foster research in this area, we first introduce a novel large-scale dataset\ncalled DSEC-MOS for moving object segmentation from moving ego vehicles, which\nis the first of its kind. For benchmarking, we select various mainstream\nmethods and rigorously evaluate them on our dataset. Subsequently, we devise\nEmoFormer, a novel network able to exploit the event data. For this purpose, we\nfuse the event temporal prior with spatial semantic maps to distinguish\ngenuinely moving objects from the static background, adding another level of\ndense supervision around our object of interest. Our proposed network relies\nonly on event data for training but does not require event input during\ninference, making it directly comparable to frame-only methods in terms of\nefficiency and more widely usable in many application cases. The exhaustive\ncomparison highlights a significant performance improvement of our method over\nall other methods. The source code and dataset are publicly available at:\nhttps://github.com/ZZY-Zhou/DSEC-MOS."
                },
                "authors": [
                    {
                        "name": "Zhuyun Zhou"
                    },
                    {
                        "name": "Zongwei Wu"
                    },
                    {
                        "name": "Danda Pani Paudel"
                    },
                    {
                        "name": "Rémi Boutteau"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Luc Van Gool"
                    },
                    {
                        "name": "Radu Timofte"
                    },
                    {
                        "name": "Dominique Ginhac"
                    }
                ],
                "author_detail": {
                    "name": "Dominique Ginhac"
                },
                "author": "Dominique Ginhac",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.00126v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.00126v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16909v1",
                "updated": "2024-09-25T13:13:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    13,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:13:21Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    13,
                    21,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering"
                },
                "summary": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities.In this paper,\nwe propose a novel framework that enhances temporal awareness and reasoning\nthrough Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities.In this paper,\nwe propose a novel framework that enhances temporal awareness and reasoning\nthrough Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00134v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00134v3",
                "updated": "2024-09-25T13:09:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    9,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-29T12:55:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    12,
                    55,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAPF-GPT: Imitation Learning for Multi-Agent Pathfinding at Scale"
                },
                "summary": "Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent pathfinding (MAPF) is a challenging computational problem that\ntypically requires to find collision-free paths for multiple agents in a shared\nenvironment. Solving MAPF optimally is NP-hard, yet efficient solutions are\ncritical for numerous applications, including automated warehouses and\ntransportation systems. Recently, learning-based approaches to MAPF have gained\nattention, particularly those leveraging deep reinforcement learning. Following\ncurrent trends in machine learning, we have created a foundation model for the\nMAPF problems called MAPF-GPT. Using imitation learning, we have trained a\npolicy on a set of pre-collected sub-optimal expert trajectories that can\ngenerate actions in conditions of partial observability without additional\nheuristics, reward functions, or communication with other agents. The resulting\nMAPF-GPT model demonstrates zero-shot learning abilities when solving the MAPF\nproblem instances that were not present in the training dataset. We show that\nMAPF-GPT notably outperforms the current best-performing learnable-MAPF solvers\non a diverse range of problem instances and is efficient in terms of\ncomputation (in the inference mode)."
                },
                "authors": [
                    {
                        "name": "Anton Andreychuk"
                    },
                    {
                        "name": "Konstantin Yakovlev"
                    },
                    {
                        "name": "Aleksandr Panov"
                    },
                    {
                        "name": "Alexey Skrynnik"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Skrynnik"
                },
                "author": "Alexey Skrynnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00134v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00134v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16900v1",
                "updated": "2024-09-25T13:09:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    9,
                    23,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:09:23Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    9,
                    23,
                    2,
                    269,
                    0
                ],
                "title": "A Roadmap for Embodied and Social Grounding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Roadmap for Embodied and Social Grounding in LLMs"
                },
                "summary": "The fusion of Large Language Models (LLMs) and robotic systems has led to a\ntransformative paradigm in the robotic field, offering unparalleled\ncapabilities not only in the communication domain but also in skills like\nmultimodal input handling, high-level reasoning, and plan generation. The\ngrounding of LLMs knowledge into the empirical world has been considered a\ncrucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,\nconnecting LLMs' representations to the external world with multimodal\napproaches or with robots' bodies is not enough to let them understand the\nmeaning of the language they are manipulating. Taking inspiration from humans,\nthis work draws attention to three necessary elements for an agent to grasp and\nexperience the world. The roadmap for LLMs grounding is envisaged in an active\nbodily system as the reference point for experiencing the environment, a\ntemporally structured experience for a coherent, self-related interaction with\nthe external world, and social skills to acquire a common-grounded shared\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fusion of Large Language Models (LLMs) and robotic systems has led to a\ntransformative paradigm in the robotic field, offering unparalleled\ncapabilities not only in the communication domain but also in skills like\nmultimodal input handling, high-level reasoning, and plan generation. The\ngrounding of LLMs knowledge into the empirical world has been considered a\ncrucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,\nconnecting LLMs' representations to the external world with multimodal\napproaches or with robots' bodies is not enough to let them understand the\nmeaning of the language they are manipulating. Taking inspiration from humans,\nthis work draws attention to three necessary elements for an agent to grasp and\nexperience the world. The roadmap for LLMs grounding is envisaged in an active\nbodily system as the reference point for experiencing the environment, a\ntemporally structured experience for a coherent, self-related interaction with\nthe external world, and social skills to acquire a common-grounded shared\nexperience."
                },
                "authors": [
                    {
                        "name": "Sara Incao"
                    },
                    {
                        "name": "Carlo Mazzola"
                    },
                    {
                        "name": "Giulia Belgiovine"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Sciutti"
                },
                "author": "Alessandra Sciutti",
                "arxiv_comment": "Accepted Version of a conference paper presented at Robophilosophy\n  Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.9; J.4; F.3.2; D.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09285v2",
                "updated": "2024-09-25T13:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    6,
                    53,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-17T20:13:34Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    20,
                    13,
                    34,
                    5,
                    230,
                    0
                ],
                "title": "Evaluating Usability and Engagement of Large Language Models in Virtual\n  Reality for Traditional Scottish Curling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Usability and Engagement of Large Language Models in Virtual\n  Reality for Traditional Scottish Curling"
                },
                "summary": "This paper explores the innovative application of Large Language Models\n(LLMs) in Virtual Reality (VR) environments to promote heritage education,\nfocusing on traditional Scottish curling presented in the game ``Scottish\nBonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\npre-defined scripted chatbots, evaluating key criteria such as usability, user\nengagement, and learning outcomes. The results show that LLM-based chatbots\nsignificantly improve interactivity and engagement, creating a more dynamic and\nimmersive learning environment. This integration helps document and preserve\ncultural heritage and enhances dissemination processes, which are crucial for\nsafeguarding intangible cultural heritage (ICH) amid environmental changes.\nFurthermore, the study highlights the potential of novel technologies in\neducation to provide immersive experiences that foster a deeper appreciation of\ncultural heritage. These findings support the wider application of LLMs and VR\nin cultural education to address global challenges and promote sustainable\npractices to preserve and enhance cultural heritage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the innovative application of Large Language Models\n(LLMs) in Virtual Reality (VR) environments to promote heritage education,\nfocusing on traditional Scottish curling presented in the game ``Scottish\nBonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\npre-defined scripted chatbots, evaluating key criteria such as usability, user\nengagement, and learning outcomes. The results show that LLM-based chatbots\nsignificantly improve interactivity and engagement, creating a more dynamic and\nimmersive learning environment. This integration helps document and preserve\ncultural heritage and enhances dissemination processes, which are crucial for\nsafeguarding intangible cultural heritage (ICH) amid environmental changes.\nFurthermore, the study highlights the potential of novel technologies in\neducation to provide immersive experiences that foster a deeper appreciation of\ncultural heritage. These findings support the wider application of LLMs and VR\nin cultural education to address global challenges and promote sustainable\npractices to preserve and enhance cultural heritage."
                },
                "authors": [
                    {
                        "name": "Ka Hei Carrie Lau"
                    },
                    {
                        "name": "Efe Bozkir"
                    },
                    {
                        "name": "Hong Gao"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2103.14315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2103.14315v2",
                "updated": "2024-09-25T12:53:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    53,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2021-03-26T08:11:05Z",
                "published_parsed": [
                    2021,
                    3,
                    26,
                    8,
                    11,
                    5,
                    4,
                    85,
                    0
                ],
                "title": "Variable Selection Using Nearest Neighbor Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variable Selection Using Nearest Neighbor Gaussian Processes"
                },
                "summary": "We introduce a novel Bayesian approach for variable selection using Gaussian\nprocess regression, which is crucial for enhancing interpretability and model\nregularization. Our method employs nearest neighbor Gaussian processes, serving\nas scalable approximations of classical Gaussian processes. Variable selection\nis achieved by conditioning the process mean and covariance function on a\nrandom set that represents the indices of contributing variables. A priori\nbeliefs regarding this set control the variable selection, while reference\npriors are assigned to the remaining model parameters, ensuring numerical\nrobustness in the process covariance matrix. We propose a\nMetropolis-Within-Gibbs algorithm for model inference. Evaluation using\nsimulated data, a computer experiment approximation, and two real-world data\nsets demonstrate the effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel Bayesian approach for variable selection using Gaussian\nprocess regression, which is crucial for enhancing interpretability and model\nregularization. Our method employs nearest neighbor Gaussian processes, serving\nas scalable approximations of classical Gaussian processes. Variable selection\nis achieved by conditioning the process mean and covariance function on a\nrandom set that represents the indices of contributing variables. A priori\nbeliefs regarding this set control the variable selection, while reference\npriors are assigned to the remaining model parameters, ensuring numerical\nrobustness in the process covariance matrix. We propose a\nMetropolis-Within-Gibbs algorithm for model inference. Evaluation using\nsimulated data, a computer experiment approximation, and two real-world data\nsets demonstrate the effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Konstantin Posch"
                    },
                    {
                        "name": "Maximilian Arbeiter"
                    },
                    {
                        "name": "Christian Truden"
                    },
                    {
                        "name": "Martin Pleschberger"
                    },
                    {
                        "name": "Juergen Pilz"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Pilz"
                },
                "author": "Juergen Pilz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2103.14315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2103.14315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16879v1",
                "updated": "2024-09-25T12:44:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    44,
                    13,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T12:44:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    44,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations"
                },
                "summary": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from Large Language Models (LLMs), and it\nintegrates this knowledge with human explanations through a generative network\narchitecture. The bidirectional structure of GRACE enables robots to refine and\nenhance LLM predictions by utilizing human explanations and makes robots\ncapable of generating such explanations for human-specified actions. Our\nexperimental evaluations show that integrating human explanations boosts\nGRACE's performance, where it outperforms several baselines and provides\nsensible explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from Large Language Models (LLMs), and it\nintegrates this knowledge with human explanations through a generative network\narchitecture. The bidirectional structure of GRACE enables robots to refine and\nenhance LLM predictions by utilizing human explanations and makes robots\ncapable of generating such explanations for human-specified actions. Our\nexperimental evaluations show that integrating human explanations boosts\nGRACE's performance, where it outperforms several baselines and provides\nsensible explanations."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Umut Ozyurt"
                    },
                    {
                        "name": "Gizem Cinar"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "arxiv_comment": "Under review for 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), Supplementary video: https://youtu.be/3gP3euwNBjQ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11022v3",
                "updated": "2024-09-25T12:33:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    33,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-17T09:32:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Tong Shang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16867v1",
                "updated": "2024-09-25T12:32:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    32,
                    41,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T12:32:41Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    32,
                    41,
                    2,
                    269,
                    0
                ],
                "title": "Multi-objective Evolution of Heuristic Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective Evolution of Heuristic Using Large Language Model"
                },
                "summary": "Heuristics are commonly used to tackle diverse search and optimization\nproblems. Design heuristics usually require tedious manual crafting with domain\nknowledge. Recent works have incorporated large language models (LLMs) into\nautomatic heuristic search leveraging their powerful language and coding\ncapacity. However, existing research focuses on the optimal performance on the\ntarget problem as the sole objective, neglecting other criteria such as\nefficiency and scalability, which are vital in practice. To tackle this\nchallenge, we propose to model heuristic search as a multi-objective\noptimization problem and consider introducing other practical criteria beyond\noptimal performance. Due to the complexity of the search space, conventional\nmulti-objective optimization methods struggle to effectively handle\nmulti-objective heuristic search. We propose the first LLM-based\nmulti-objective heuristic search framework, Multi-objective Evolution of\nHeuristic (MEoH), which integrates LLMs in a zero-shot manner to generate a\nnon-dominated set of heuristics to meet multiple design criteria. We design a\nnew dominance-dissimilarity mechanism for effective population management and\nselection, which incorporates both code dissimilarity in the search space and\ndominance in the objective space. MEoH is demonstrated in two well-known\ncombinatorial optimization problems: the online Bin Packing Problem (BPP) and\nthe Traveling Salesman Problem (TSP). Results indicate that a variety of elite\nheuristics are automatically generated in a single run, offering more trade-off\noptions than existing methods. It successfully achieves competitive or superior\nperformance while improving efficiency up to 10 times. Moreover, we also\nobserve that the multi-objective search introduces novel insights into\nheuristic design and leads to the discovery of diverse heuristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristics are commonly used to tackle diverse search and optimization\nproblems. Design heuristics usually require tedious manual crafting with domain\nknowledge. Recent works have incorporated large language models (LLMs) into\nautomatic heuristic search leveraging their powerful language and coding\ncapacity. However, existing research focuses on the optimal performance on the\ntarget problem as the sole objective, neglecting other criteria such as\nefficiency and scalability, which are vital in practice. To tackle this\nchallenge, we propose to model heuristic search as a multi-objective\noptimization problem and consider introducing other practical criteria beyond\noptimal performance. Due to the complexity of the search space, conventional\nmulti-objective optimization methods struggle to effectively handle\nmulti-objective heuristic search. We propose the first LLM-based\nmulti-objective heuristic search framework, Multi-objective Evolution of\nHeuristic (MEoH), which integrates LLMs in a zero-shot manner to generate a\nnon-dominated set of heuristics to meet multiple design criteria. We design a\nnew dominance-dissimilarity mechanism for effective population management and\nselection, which incorporates both code dissimilarity in the search space and\ndominance in the objective space. MEoH is demonstrated in two well-known\ncombinatorial optimization problems: the online Bin Packing Problem (BPP) and\nthe Traveling Salesman Problem (TSP). Results indicate that a variety of elite\nheuristics are automatically generated in a single run, offering more trade-off\noptions than existing methods. It successfully achieves competitive or superior\nperformance while improving efficiency up to 10 times. Moreover, we also\nobserve that the multi-objective search introduces novel insights into\nheuristic design and leads to the discovery of diverse heuristics."
                },
                "authors": [
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02636v2",
                "updated": "2024-09-25T12:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    25,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-04T11:59:53Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "title": "Mamba as a motion encoder for robotic imitation learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba as a motion encoder for robotic imitation learning"
                },
                "summary": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. This paper proposes using Mamba, a state-of-the-art architecture\nwith potential applications in LLMs, for robotic imitation learning,\nhighlighting its ability to function as an encoder that effectively captures\ncontextual information. By reducing the dimensionality of the state space,\nMamba operates similarly to an autoencoder. It effectively compresses the\nsequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. This paper proposes using Mamba, a state-of-the-art architecture\nwith potential applications in LLMs, for robotic imitation learning,\nhighlighting its ability to function as an encoder that effectively captures\ncontextual information. By reducing the dimensionality of the state space,\nMamba operates similarly to an autoencoder. It effectively compresses the\nsequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data."
                },
                "authors": [
                    {
                        "name": "Toshiaki Tsuji"
                    }
                ],
                "author_detail": {
                    "name": "Toshiaki Tsuji"
                },
                "author": "Toshiaki Tsuji",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16860v1",
                "updated": "2024-09-25T12:15:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    15,
                    15,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T12:15:15Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    15,
                    15,
                    2,
                    269,
                    0
                ],
                "title": "The Role of Language Models in Modern Healthcare: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Language Models in Modern Healthcare: A Comprehensive Review"
                },
                "summary": "The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice."
                },
                "authors": [
                    {
                        "name": "Amna Khalid"
                    },
                    {
                        "name": "Ayma Khalid"
                    },
                    {
                        "name": "Umar Khalid"
                    }
                ],
                "author_detail": {
                    "name": "Umar Khalid"
                },
                "author": "Umar Khalid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13781v2",
                "updated": "2024-09-25T12:10:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    10,
                    22,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-25T09:22:07Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    9,
                    22,
                    7,
                    6,
                    238,
                    0
                ],
                "title": "GenOnet: Generative Open xG Network Simulation with Multi-Agent LLM and\n  ns-3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenOnet: Generative Open xG Network Simulation with Multi-Agent LLM and\n  ns-3"
                },
                "summary": "The move toward Sixth-Generation (6G) networks relies on open interfaces and\nprotocols for seamless interoperability across devices, vendors, and\ntechnologies. In this context, open 6G development involves multiple\ndisciplines and requires advanced simulation approaches for testing. In this\ndemo paper, we propose a generative simulation approach based on a multi-agent\nLarge Language Model (LLM) and Network Simulator 3 (ns-3), called Generative\nOpen xG Network Simulation (GenOnet), to effectively generate, debug, execute,\nand interpret simulated Open Fifth-Generation (5G) environments. The first\nversion of GenOnet application represents a specialized adaptation of the\nOpenAI GPT models. It incorporates supplementary tools, agents, 5G standards,\nand seamless integration with ns-3 simulation capabilities, supporting both C++\nvariants and Python implementations. This release complies with the latest Open\nRadio Access Network (O-RAN) and 3GPP standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The move toward Sixth-Generation (6G) networks relies on open interfaces and\nprotocols for seamless interoperability across devices, vendors, and\ntechnologies. In this context, open 6G development involves multiple\ndisciplines and requires advanced simulation approaches for testing. In this\ndemo paper, we propose a generative simulation approach based on a multi-agent\nLarge Language Model (LLM) and Network Simulator 3 (ns-3), called Generative\nOpen xG Network Simulation (GenOnet), to effectively generate, debug, execute,\nand interpret simulated Open Fifth-Generation (5G) environments. The first\nversion of GenOnet application represents a specialized adaptation of the\nOpenAI GPT models. It incorporates supplementary tools, agents, 5G standards,\nand seamless integration with ns-3 simulation capabilities, supporting both C++\nvariants and Python implementations. This release complies with the latest Open\nRadio Access Network (O-RAN) and 3GPP standards."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Amir Ashtari Gargari"
                    },
                    {
                        "name": "Sandra Lagén"
                    },
                    {
                        "name": "Josep Mangues"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "3 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05200v2",
                "updated": "2024-09-25T11:43:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    43,
                    59,
                    2,
                    269,
                    0
                ],
                "published": "2024-02-07T19:10:36Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    19,
                    10,
                    36,
                    2,
                    38,
                    0
                ],
                "title": "Are LLMs Ready for Real-World Materials Discovery?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Ready for Real-World Materials Discovery?"
                },
                "summary": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories."
                },
                "authors": [
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "N M Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N M Anoop Krishnan"
                },
                "author": "N M Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16834v1",
                "updated": "2024-09-25T11:33:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    33,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:33:51Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    33,
                    51,
                    2,
                    269,
                    0
                ],
                "title": "Conditional Generative Denoiser for Nighttime UAV Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Generative Denoiser for Nighttime UAV Tracking"
                },
                "summary": "State-of-the-art (SOTA) visual object tracking methods have significantly\nenhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light\nconditions, the presence of irregular real noise from the environments severely\ndegrades the performance of these SOTA methods. Moreover, existing SOTA\ndenoising techniques often fail to meet the real-time processing requirements\nwhen deployed as plug-and-play denoisers for UAV tracking. To address this\nchallenge, this work proposes a novel conditional generative denoiser\n(CGDenoiser), which breaks free from the limitations of traditional\ndeterministic paradigms and generates the noise conditioning on the input,\nsubsequently removing it. To better align the input dimensions and accelerate\ninference, a novel nested residual Transformer conditionalizer is developed.\nFurthermore, an innovative multi-kernel conditional refiner is designed to\npertinently refine the denoised output. Extensive experiments show that\nCGDenoiser promotes the tracking precision of the SOTA tracker by 18.18\\% on\nDarkTrack2021 whereas working 5.8 times faster than the second well-performed\ndenoiser. Real-world tests with complex challenges also prove the effectiveness\nand practicality of CGDenoiser. Code, video demo and supplementary proof for\nCGDenoier are now available at:\n\\url{https://github.com/vision4robotics/CGDenoiser}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art (SOTA) visual object tracking methods have significantly\nenhanced the autonomy of unmanned aerial vehicles (UAVs). However, in low-light\nconditions, the presence of irregular real noise from the environments severely\ndegrades the performance of these SOTA methods. Moreover, existing SOTA\ndenoising techniques often fail to meet the real-time processing requirements\nwhen deployed as plug-and-play denoisers for UAV tracking. To address this\nchallenge, this work proposes a novel conditional generative denoiser\n(CGDenoiser), which breaks free from the limitations of traditional\ndeterministic paradigms and generates the noise conditioning on the input,\nsubsequently removing it. To better align the input dimensions and accelerate\ninference, a novel nested residual Transformer conditionalizer is developed.\nFurthermore, an innovative multi-kernel conditional refiner is designed to\npertinently refine the denoised output. Extensive experiments show that\nCGDenoiser promotes the tracking precision of the SOTA tracker by 18.18\\% on\nDarkTrack2021 whereas working 5.8 times faster than the second well-performed\ndenoiser. Real-world tests with complex challenges also prove the effectiveness\nand practicality of CGDenoiser. Code, video demo and supplementary proof for\nCGDenoier are now available at:\n\\url{https://github.com/vision4robotics/CGDenoiser}."
                },
                "authors": [
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Changhong Fu"
                    },
                    {
                        "name": "Kunhan Lu"
                    },
                    {
                        "name": "Liangliang Yao"
                    },
                    {
                        "name": "Haobo Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Haobo Zuo"
                },
                "author": "Haobo Zuo",
                "arxiv_journal_ref": "Proceedings of the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16829v1",
                "updated": "2024-09-25T11:30:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    30,
                    14,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:30:14Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    30,
                    14,
                    2,
                    269,
                    0
                ],
                "title": "Conditional Testing based on Localized Conformal p-values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Testing based on Localized Conformal p-values"
                },
                "summary": "In this paper, we address conditional testing problems through the conformal\ninference framework. We define the localized conformal p-values by inverting\nprediction intervals and prove their theoretical properties. These defined\np-values are then applied to several conditional testing problems to illustrate\ntheir practicality. Firstly, we propose a conditional outlier detection\nprocedure to test for outliers in the conditional distribution with\nfinite-sample false discovery rate (FDR) control. We also introduce a novel\nconditional label screening problem with the goal of screening multivariate\nresponse variables and propose a screening procedure to control the family-wise\nerror rate (FWER). Finally, we consider the two-sample conditional distribution\ntest and define a weighted U-statistic through the aggregation of localized\np-values. Numerical simulations and real-data examples validate the superior\nperformance of our proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address conditional testing problems through the conformal\ninference framework. We define the localized conformal p-values by inverting\nprediction intervals and prove their theoretical properties. These defined\np-values are then applied to several conditional testing problems to illustrate\ntheir practicality. Firstly, we propose a conditional outlier detection\nprocedure to test for outliers in the conditional distribution with\nfinite-sample false discovery rate (FDR) control. We also introduce a novel\nconditional label screening problem with the goal of screening multivariate\nresponse variables and propose a screening procedure to control the family-wise\nerror rate (FWER). Finally, we consider the two-sample conditional distribution\ntest and define a weighted U-statistic through the aggregation of localized\np-values. Numerical simulations and real-data examples validate the superior\nperformance of our proposed strategies."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Wu"
                    },
                    {
                        "name": "Lin Lu"
                    },
                    {
                        "name": "Zhaojun Wang"
                    },
                    {
                        "name": "Changliang Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changliang Zou"
                },
                "author": "Changliang Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16824v1",
                "updated": "2024-09-25T11:22:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    22,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:22:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    22,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "Uncertainty Representations in State-Space Layers for Deep Reinforcement\n  Learning under Partial Observability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Representations in State-Space Layers for Deep Reinforcement\n  Learning under Partial Observability"
                },
                "summary": "Optimal decision-making under partial observability requires reasoning about\nthe uncertainty of the environment's hidden state. However, most reinforcement\nlearning architectures handle partial observability with sequence models that\nhave no internal mechanism to incorporate uncertainty in their hidden state\nrepresentation, such as recurrent neural networks, deterministic state-space\nmodels and transformers. Inspired by advances in probabilistic world models for\nreinforcement learning, we propose a standalone Kalman filter layer that\nperforms closed-form Gaussian inference in linear state-space models and train\nit end-to-end within a model-free architecture to maximize returns. Similar to\nefficient linear recurrent layers, the Kalman filter layer processes sequential\ndata using a parallel scan, which scales logarithmically with the sequence\nlength. By design, Kalman filter layers are a drop-in replacement for other\nrecurrent layers in standard model-free architectures, but importantly they\ninclude an explicit mechanism for probabilistic filtering of the latent state\nrepresentation. Experiments in a wide variety of tasks with partial\nobservability show that Kalman filter layers excel in problems where\nuncertainty reasoning is key for decision-making, outperforming other stateful\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal decision-making under partial observability requires reasoning about\nthe uncertainty of the environment's hidden state. However, most reinforcement\nlearning architectures handle partial observability with sequence models that\nhave no internal mechanism to incorporate uncertainty in their hidden state\nrepresentation, such as recurrent neural networks, deterministic state-space\nmodels and transformers. Inspired by advances in probabilistic world models for\nreinforcement learning, we propose a standalone Kalman filter layer that\nperforms closed-form Gaussian inference in linear state-space models and train\nit end-to-end within a model-free architecture to maximize returns. Similar to\nefficient linear recurrent layers, the Kalman filter layer processes sequential\ndata using a parallel scan, which scales logarithmically with the sequence\nlength. By design, Kalman filter layers are a drop-in replacement for other\nrecurrent layers in standard model-free architectures, but importantly they\ninclude an explicit mechanism for probabilistic filtering of the latent state\nrepresentation. Experiments in a wide variety of tasks with partial\nobservability show that Kalman filter layers excel in problems where\nuncertainty reasoning is key for decision-making, outperforming other stateful\nmodels."
                },
                "authors": [
                    {
                        "name": "Carlos E. Luis"
                    },
                    {
                        "name": "Alessandro G. Bottero"
                    },
                    {
                        "name": "Julia Vinogradska"
                    },
                    {
                        "name": "Felix Berkenkamp"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16815v1",
                "updated": "2024-09-25T11:10:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    10,
                    33,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:10:33Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    10,
                    33,
                    2,
                    269,
                    0
                ],
                "title": "Accelerating TinyML Inference on Microcontrollers through Approximate\n  Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating TinyML Inference on Microcontrollers through Approximate\n  Kernels"
                },
                "summary": "The rapid growth of microcontroller-based IoT devices has opened up numerous\napplications, from smart manufacturing to personalized healthcare. Despite the\nwidespread adoption of energy-efficient microcontroller units (MCUs) in the\nTiny Machine Learning (TinyML) domain, they still face significant limitations\nin terms of performance and memory (RAM, Flash). In this work, we combine\napproximate computing and software kernel design to accelerate the inference of\napproximate CNN models on MCUs. Our kernel-based approximation framework\nfirstly unpacks the operands of each convolution layer and then conducts an\noffline calculation to determine the significance of each operand.\nSubsequently, through a design space exploration, it employs a computation\nskipping approximation strategy based on the calculated significance. Our\nevaluation on an STM32-Nucleo board and 2 popular CNNs trained on the CIFAR-10\ndataset shows that, compared to state-of-the-art exact inference, our Pareto\noptimal solutions can feature on average 21% latency reduction with no\ndegradation in Top-1 classification accuracy, while for lower accuracy\nrequirements, the corresponding reduction becomes even more pronounced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of microcontroller-based IoT devices has opened up numerous\napplications, from smart manufacturing to personalized healthcare. Despite the\nwidespread adoption of energy-efficient microcontroller units (MCUs) in the\nTiny Machine Learning (TinyML) domain, they still face significant limitations\nin terms of performance and memory (RAM, Flash). In this work, we combine\napproximate computing and software kernel design to accelerate the inference of\napproximate CNN models on MCUs. Our kernel-based approximation framework\nfirstly unpacks the operands of each convolution layer and then conducts an\noffline calculation to determine the significance of each operand.\nSubsequently, through a design space exploration, it employs a computation\nskipping approximation strategy based on the calculated significance. Our\nevaluation on an STM32-Nucleo board and 2 popular CNNs trained on the CIFAR-10\ndataset shows that, compared to state-of-the-art exact inference, our Pareto\noptimal solutions can feature on average 21% latency reduction with no\ndegradation in Top-1 classification accuracy, while for lower accuracy\nrequirements, the corresponding reduction becomes even more pronounced."
                },
                "authors": [
                    {
                        "name": "Giorgos Armeniakos"
                    },
                    {
                        "name": "Georgios Mentzos"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16813v1",
                "updated": "2024-09-25T11:09:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    9,
                    39,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:09:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    9,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "PeerArg: Argumentative Peer Review with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerArg: Argumentative Peer Review with LLMs"
                },
                "summary": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM."
                },
                "authors": [
                    {
                        "name": "Purin Sukpanichnant"
                    },
                    {
                        "name": "Anna Rapberger"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16808v1",
                "updated": "2024-09-25T10:56:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    56,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T10:56:49Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    56,
                    49,
                    2,
                    269,
                    0
                ],
                "title": "Benchmarking Deep Learning Models for Object Detection on Edge Computing\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Deep Learning Models for Object Detection on Edge Computing\n  Devices"
                },
                "summary": "Modern applications, such as autonomous vehicles, require deploying deep\nlearning algorithms on resource-constrained edge devices for real-time image\nand video processing. However, there is limited understanding of the efficiency\nand performance of various object detection models on these devices. In this\npaper, we evaluate state-of-the-art object detection models, including YOLOv8\n(Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD\nMobileNet V1, SSDLite MobileDet). We deployed these models on popular edge\ndevices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and\nJetson Orin Nano, collecting key performance metrics such as energy\nconsumption, inference time, and Mean Average Precision (mAP). Our findings\nhighlight that lower mAP models such as SSD MobileNet V1 are more\nenergy-efficient and faster in inference, whereas higher mAP models like YOLOv8\nMedium generally consume more energy and have slower inference, though with\nexceptions when accelerators like TPUs are used. Among the edge devices, Jetson\nOrin Nano stands out as the fastest and most energy-efficient option for\nrequest handling, despite having the highest idle energy consumption. These\nresults emphasize the need to balance accuracy, speed, and energy efficiency\nwhen deploying deep learning models on edge devices, offering valuable guidance\nfor practitioners and researchers selecting models and devices for their\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications, such as autonomous vehicles, require deploying deep\nlearning algorithms on resource-constrained edge devices for real-time image\nand video processing. However, there is limited understanding of the efficiency\nand performance of various object detection models on these devices. In this\npaper, we evaluate state-of-the-art object detection models, including YOLOv8\n(Nano, Small, Medium), EfficientDet Lite (Lite0, Lite1, Lite2), and SSD (SSD\nMobileNet V1, SSDLite MobileDet). We deployed these models on popular edge\ndevices like the Raspberry Pi 3, 4, and 5 with/without TPU accelerators, and\nJetson Orin Nano, collecting key performance metrics such as energy\nconsumption, inference time, and Mean Average Precision (mAP). Our findings\nhighlight that lower mAP models such as SSD MobileNet V1 are more\nenergy-efficient and faster in inference, whereas higher mAP models like YOLOv8\nMedium generally consume more energy and have slower inference, though with\nexceptions when accelerators like TPUs are used. Among the edge devices, Jetson\nOrin Nano stands out as the fastest and most energy-efficient option for\nrequest handling, despite having the highest idle energy consumption. These\nresults emphasize the need to balance accuracy, speed, and energy efficiency\nwhen deploying deep learning models on edge devices, offering valuable guidance\nfor practitioners and researchers selecting models and devices for their\napplications."
                },
                "authors": [
                    {
                        "name": "Daghash K. Alqahtani"
                    },
                    {
                        "name": "Aamir Cheema"
                    },
                    {
                        "name": "Adel N. Toosi"
                    }
                ],
                "author_detail": {
                    "name": "Adel N. Toosi"
                },
                "author": "Adel N. Toosi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16807v1",
                "updated": "2024-09-25T10:56:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    56,
                    28,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T10:56:28Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    56,
                    28,
                    2,
                    269,
                    0
                ],
                "title": "A Few Hypocrites: Few-Shot Learning and Subtype Definitions for\n  Detecting Hypocrisy Accusations in Online Climate Change Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Few Hypocrites: Few-Shot Learning and Subtype Definitions for\n  Detecting Hypocrisy Accusations in Online Climate Change Debates"
                },
                "summary": "The climate crisis is a salient issue in online discussions, and hypocrisy\naccusations are a central rhetorical element in these debates. However, for\nlarge-scale text analysis, hypocrisy accusation detection is an understudied\ntool, most often defined as a smaller subtask of fallacious argument detection.\nIn this paper, we define hypocrisy accusation detection as an independent task\nin NLP, and identify different relevant subtypes of hypocrisy accusations. Our\nClimate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate\ndebate comments, expert-annotated into two different types of hypocrisy\naccusations: personal versus political hypocrisy. We evaluate few-shot\nin-context learning with 6 shots and 3 instruction-tuned Large Language Models\n(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate\nthat the GPT-4o and Llama-3 models in particular show promise in detecting\nhypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).\nHowever, context matters for a complex semantic concept such as hypocrisy\naccusations, and we find models struggle especially at identifying political\nhypocrisy accusations compared to personal moral hypocrisy. Our study\ncontributes new insights in hypocrisy detection and climate change discourse,\nand is a stepping stone for large-scale analysis of hypocrisy accusation in\nonline climate debates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The climate crisis is a salient issue in online discussions, and hypocrisy\naccusations are a central rhetorical element in these debates. However, for\nlarge-scale text analysis, hypocrisy accusation detection is an understudied\ntool, most often defined as a smaller subtask of fallacious argument detection.\nIn this paper, we define hypocrisy accusation detection as an independent task\nin NLP, and identify different relevant subtypes of hypocrisy accusations. Our\nClimate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate\ndebate comments, expert-annotated into two different types of hypocrisy\naccusations: personal versus political hypocrisy. We evaluate few-shot\nin-context learning with 6 shots and 3 instruction-tuned Large Language Models\n(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate\nthat the GPT-4o and Llama-3 models in particular show promise in detecting\nhypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).\nHowever, context matters for a complex semantic concept such as hypocrisy\naccusations, and we find models struggle especially at identifying political\nhypocrisy accusations compared to personal moral hypocrisy. Our study\ncontributes new insights in hypocrisy detection and climate change discourse,\nand is a stepping stone for large-scale analysis of hypocrisy accusation in\nonline climate debates."
                },
                "authors": [
                    {
                        "name": "Paulina Garcia Corral"
                    },
                    {
                        "name": "Avishai Green"
                    },
                    {
                        "name": "Hendrik Meyer"
                    },
                    {
                        "name": "Anke Stoll"
                    },
                    {
                        "name": "Xiaoyue Yan"
                    },
                    {
                        "name": "Myrthe Reuver"
                    }
                ],
                "author_detail": {
                    "name": "Myrthe Reuver"
                },
                "author": "Myrthe Reuver",
                "arxiv_comment": "cite the public version, published at CPSS 2024 @ KONVENS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15033v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15033v4",
                "updated": "2024-09-25T10:34:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    34,
                    3,
                    2,
                    269,
                    0
                ],
                "published": "2024-03-22T08:32:30Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    8,
                    32,
                    30,
                    4,
                    82,
                    0
                ],
                "title": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning"
                },
                "summary": "Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs."
                },
                "authors": [
                    {
                        "name": "Qiaoqiao Jin"
                    },
                    {
                        "name": "Xuanhong Chen"
                    },
                    {
                        "name": "Meiguang Jin"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Yucheng Zheng"
                    },
                    {
                        "name": "Yupeng Zhu"
                    },
                    {
                        "name": "Bingbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Bingbing Ni"
                },
                "author": "Bingbing Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15033v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15033v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16799v1",
                "updated": "2024-09-25T10:32:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    32,
                    18,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T10:32:18Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    32,
                    18,
                    2,
                    269,
                    0
                ],
                "title": "Large Language Model Predicts Above Normal All India Summer Monsoon\n  Rainfall in 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Predicts Above Normal All India Summer Monsoon\n  Rainfall in 2024"
                },
                "summary": "Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is\npivotal for informed policymaking for the country, impacting the lives of\nbillions of people. However, accurate simulation of AISMR has been a persistent\nchallenge due to the complex interplay of various muti-scale factors and the\ninherent variability of the monsoon system. This research focuses on adapting\nand fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR\nwith a lead time of three months. The fine-tuned PatchTST model, trained with\nhistorical AISMR data, the Ni\\~no3.4 index, and categorical Indian Ocean Dipole\nvalues, outperforms several popular neural network models and statistical\nmodels. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage\nof 0.07% and a Spearman correlation of 0.976. This is particularly impressive,\nsince it is nearly 80% more accurate than the best-performing NN models. The\nmodel predicts an above-normal monsoon for the year 2024, with an accumulated\nrainfall of 921.6 mm in the month of June-September for the entire country.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is\npivotal for informed policymaking for the country, impacting the lives of\nbillions of people. However, accurate simulation of AISMR has been a persistent\nchallenge due to the complex interplay of various muti-scale factors and the\ninherent variability of the monsoon system. This research focuses on adapting\nand fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR\nwith a lead time of three months. The fine-tuned PatchTST model, trained with\nhistorical AISMR data, the Ni\\~no3.4 index, and categorical Indian Ocean Dipole\nvalues, outperforms several popular neural network models and statistical\nmodels. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage\nof 0.07% and a Spearman correlation of 0.976. This is particularly impressive,\nsince it is nearly 80% more accurate than the best-performing NN models. The\nmodel predicts an above-normal monsoon for the year 2024, with an accumulated\nrainfall of 921.6 mm in the month of June-September for the entire country."
                },
                "authors": [
                    {
                        "name": "Ujjawal Sharma"
                    },
                    {
                        "name": "Madhav Biyani"
                    },
                    {
                        "name": "Akhil Dev Suresh"
                    },
                    {
                        "name": "Debi Prasad Bhuyan"
                    },
                    {
                        "name": "Saroj Kanta Mishra"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04872v2",
                "updated": "2024-09-25T10:00:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    0,
                    3,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-09T05:25:17Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    25,
                    17,
                    4,
                    222,
                    0
                ],
                "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation"
                },
                "summary": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT. Our code is available at https://github.com/JamyDon/SCOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT. Our code is available at https://github.com/JamyDon/SCOI."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "EMNLP 2024 main conference long paper. 16 pages, 2 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16788v1",
                "updated": "2024-09-25T09:52:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    52,
                    44,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:52:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    52,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "Mitigating the Bias of Large Language Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Bias of Large Language Model Evaluation"
                },
                "summary": "Recently, there has been a trend of evaluating the Large Language Model (LLM)\nquality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to\nevaluate the current output quality. However, existing judges are proven to be\nbiased, namely they would favor answers which present better superficial\nquality (such as verbosity, fluency) while ignoring the instruction following\nability. In this work, we propose systematic research about the bias of\nLLM-as-a-Judge. Specifically, for closed-source judge models, we apply\ncalibration to mitigate the significance of superficial quality, both on\nprobability level and prompt level. For open-source judge models, we propose to\nmitigate the bias by contrastive training, with curated negative samples that\ndeviate from instruction but present better superficial quality. We apply our\nmethods on the bias evaluation benchmark, and experiment results show our\nmethods mitigate the bias by a large margin while maintaining a satisfactory\nevaluation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a trend of evaluating the Large Language Model (LLM)\nquality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to\nevaluate the current output quality. However, existing judges are proven to be\nbiased, namely they would favor answers which present better superficial\nquality (such as verbosity, fluency) while ignoring the instruction following\nability. In this work, we propose systematic research about the bias of\nLLM-as-a-Judge. Specifically, for closed-source judge models, we apply\ncalibration to mitigate the significance of superficial quality, both on\nprobability level and prompt level. For open-source judge models, we propose to\nmitigate the bias by contrastive training, with curated negative samples that\ndeviate from instruction but present better superficial quality. We apply our\nmethods on the bias evaluation benchmark, and experiment results show our\nmethods mitigate the bias by a large margin while maintaining a satisfactory\nevaluation accuracy."
                },
                "authors": [
                    {
                        "name": "Hongli Zhou"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Conghui Zhu"
                    },
                    {
                        "name": "Hailong Cao"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Zhao"
                },
                "author": "Tiejun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16783v1",
                "updated": "2024-09-25T09:44:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    44,
                    48,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:44:48Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    44,
                    48,
                    2,
                    269,
                    0
                ],
                "title": "Holistic Automated Red Teaming for Large Language Models through\n  Top-Down Test Case Generation and Multi-turn Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Automated Red Teaming for Large Language Models through\n  Top-Down Test Case Generation and Multi-turn Interaction"
                },
                "summary": "Automated red teaming is an effective method for identifying misaligned\nbehaviors in large language models (LLMs). Existing approaches, however, often\nfocus primarily on improving attack success rates while overlooking the need\nfor comprehensive test case coverage. Additionally, most of these methods are\nlimited to single-turn red teaming, failing to capture the multi-turn dynamics\nof real-world human-machine interactions. To overcome these limitations, we\npropose HARM (Holistic Automated Red teaMing), which scales up the diversity of\ntest cases using a top-down approach based on an extensible, fine-grained risk\ntaxonomy. Our method also leverages a novel fine-tuning strategy and\nreinforcement learning techniques to facilitate multi-turn adversarial probing\nin a human-like manner. Experimental results demonstrate that our framework\nenables a more systematic understanding of model vulnerabilities and offers\nmore targeted guidance for the alignment process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated red teaming is an effective method for identifying misaligned\nbehaviors in large language models (LLMs). Existing approaches, however, often\nfocus primarily on improving attack success rates while overlooking the need\nfor comprehensive test case coverage. Additionally, most of these methods are\nlimited to single-turn red teaming, failing to capture the multi-turn dynamics\nof real-world human-machine interactions. To overcome these limitations, we\npropose HARM (Holistic Automated Red teaMing), which scales up the diversity of\ntest cases using a top-down approach based on an extensible, fine-grained risk\ntaxonomy. Our method also leverages a novel fine-tuning strategy and\nreinforcement learning techniques to facilitate multi-turn adversarial probing\nin a human-like manner. Experimental results demonstrate that our framework\nenables a more systematic understanding of model vulnerabilities and offers\nmore targeted guidance for the alignment process."
                },
                "authors": [
                    {
                        "name": "Jinchuan Zhang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Yaxin Liu"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "EMNLP 2024 camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16779v1",
                "updated": "2024-09-25T09:41:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    41,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:41:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    41,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"
                },
                "summary": "Large Language Models (LLMs) often struggle with tasks requiring mathematical\nreasoning, particularly multiple-choice questions (MCQs). To address this\nissue, we developed LLaMa-SciQ, an educational chatbot designed to assist\ncollege students in solving and understanding MCQs in STEM fields. We begin by\nfine-tuning and aligning the models to human preferences. After comparing the\nperformance of Mistral-7B and LLaMa-8B, we selected the latter as the base\nmodel due to its higher evaluation accuracy. To further enhance accuracy, we\nimplement Retrieval-Augmented Generation (RAG) and apply quantization to\ncompress the model, reducing inference time and increasing accessibility for\nstudents. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the\nGSM8k dataset and 30% on the MATH dataset. However, RAG does not improve\nperformance and even reduces it, likely due to retriever issues or the model's\nunfamiliarity with context. Despite this, the quantized model shows only a 5%\nloss in performance, demonstrating significant efficiency improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with tasks requiring mathematical\nreasoning, particularly multiple-choice questions (MCQs). To address this\nissue, we developed LLaMa-SciQ, an educational chatbot designed to assist\ncollege students in solving and understanding MCQs in STEM fields. We begin by\nfine-tuning and aligning the models to human preferences. After comparing the\nperformance of Mistral-7B and LLaMa-8B, we selected the latter as the base\nmodel due to its higher evaluation accuracy. To further enhance accuracy, we\nimplement Retrieval-Augmented Generation (RAG) and apply quantization to\ncompress the model, reducing inference time and increasing accessibility for\nstudents. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the\nGSM8k dataset and 30% on the MATH dataset. However, RAG does not improve\nperformance and even reduces it, likely due to retriever issues or the model's\nunfamiliarity with context. Despite this, the quantized model shows only a 5%\nloss in performance, demonstrating significant efficiency improvements."
                },
                "authors": [
                    {
                        "name": "Marc-Antoine Allard"
                    },
                    {
                        "name": "Matin Ansaripour"
                    },
                    {
                        "name": "Maria Yuffa"
                    },
                    {
                        "name": "Paul Teiletche"
                    }
                ],
                "author_detail": {
                    "name": "Paul Teiletche"
                },
                "author": "Paul Teiletche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16332v2",
                "updated": "2024-09-25T09:36:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    36,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-24T06:10:13Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    6,
                    10,
                    13,
                    0,
                    176,
                    0
                ],
                "title": "DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task"
                },
                "summary": "Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability."
                },
                "authors": [
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16774v1",
                "updated": "2024-09-25T09:34:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    34,
                    44,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:34:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    34,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "MixPolyp: Integrating Mask, Box and Scribble Supervision for Enhanced\n  Polyp Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixPolyp: Integrating Mask, Box and Scribble Supervision for Enhanced\n  Polyp Segmentation"
                },
                "summary": "Limited by the expensive labeling, polyp segmentation models are plagued by\ndata shortages. To tackle this, we propose the mixed supervised polyp\nsegmentation paradigm (MixPolyp). Unlike traditional models relying on a single\ntype of annotation, MixPolyp combines diverse annotation types (mask, box, and\nscribble) within a single model, thereby expanding the range of available data\nand reducing labeling costs. To achieve this, MixPolyp introduces three novel\nsupervision losses to handle various annotations: Subspace Projection loss\n(L_SP), Binary Minimum Entropy loss (L_BME), and Linear Regularization loss\n(L_LR). For box annotations, L_SP eliminates shape inconsistencies between the\nprediction and the supervision. For scribble annotations, L_BME provides\nsupervision for unlabeled pixels through minimum entropy constraint, thereby\nalleviating supervision sparsity. Furthermore, L_LR provides dense supervision\nby enforcing consistency among the predictions, thus reducing the\nnon-uniqueness. These losses are independent of the model structure, making\nthem generally applicable. They are used only during training, adding no\ncomputational cost during inference. Extensive experiments on five datasets\ndemonstrate MixPolyp's effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited by the expensive labeling, polyp segmentation models are plagued by\ndata shortages. To tackle this, we propose the mixed supervised polyp\nsegmentation paradigm (MixPolyp). Unlike traditional models relying on a single\ntype of annotation, MixPolyp combines diverse annotation types (mask, box, and\nscribble) within a single model, thereby expanding the range of available data\nand reducing labeling costs. To achieve this, MixPolyp introduces three novel\nsupervision losses to handle various annotations: Subspace Projection loss\n(L_SP), Binary Minimum Entropy loss (L_BME), and Linear Regularization loss\n(L_LR). For box annotations, L_SP eliminates shape inconsistencies between the\nprediction and the supervision. For scribble annotations, L_BME provides\nsupervision for unlabeled pixels through minimum entropy constraint, thereby\nalleviating supervision sparsity. Furthermore, L_LR provides dense supervision\nby enforcing consistency among the predictions, thus reducing the\nnon-uniqueness. These losses are independent of the model structure, making\nthem generally applicable. They are used only during training, adding no\ncomputational cost during inference. Extensive experiments on five datasets\ndemonstrate MixPolyp's effectiveness."
                },
                "authors": [
                    {
                        "name": "Yiwen Hu"
                    },
                    {
                        "name": "Jun Wei"
                    },
                    {
                        "name": "Yuncheng Jiang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Song Wu"
                    }
                ],
                "author_detail": {
                    "name": "Song Wu"
                },
                "author": "Song Wu",
                "arxiv_comment": "Accepted in IEEE BIBM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06915v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06915v3",
                "updated": "2024-09-25T09:29:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    29,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-02-10T09:28:03Z",
                "published_parsed": [
                    2024,
                    2,
                    10,
                    9,
                    28,
                    3,
                    5,
                    41,
                    0
                ],
                "title": "Detection and inference of changes in high-dimensional linear regression\n  with non-sparse structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection and inference of changes in high-dimensional linear regression\n  with non-sparse structures"
                },
                "summary": "For data segmentation in high-dimensional linear regression settings, the\nregression parameters are often assumed to be sparse segment-wise, which\nenables many existing methods to estimate the parameters locally via\n$\\ell_1$-regularised maximum likelihood-type estimation and then contrast them\nfor change point detection. Contrary to this common practice, we show that the\nsparsity of neither regression parameters nor their differences, a.k.a.\ndifferential parameters, is necessary for consistency in multiple change point\ndetection. In fact, both statistically and computationally, better efficiency\nis attained by a simple strategy that scans for large discrepancies in local\ncovariance between the regressors and the response. We go a step further and\npropose a suite of tools for directly inferring about the differential\nparameters post-segmentation, which are applicable even when the regression\nparameters themselves are non-sparse. Theoretical investigations are conducted\nunder general conditions permitting non-Gaussianity, temporal dependence and\nultra-high dimensionality. Numerical results from simulated and macroeconomic\ndatasets demonstrate the competitiveness and efficacy of the proposed methods.\nImplementation of all methods is provided in the R package inferchange on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For data segmentation in high-dimensional linear regression settings, the\nregression parameters are often assumed to be sparse segment-wise, which\nenables many existing methods to estimate the parameters locally via\n$\\ell_1$-regularised maximum likelihood-type estimation and then contrast them\nfor change point detection. Contrary to this common practice, we show that the\nsparsity of neither regression parameters nor their differences, a.k.a.\ndifferential parameters, is necessary for consistency in multiple change point\ndetection. In fact, both statistically and computationally, better efficiency\nis attained by a simple strategy that scans for large discrepancies in local\ncovariance between the regressors and the response. We go a step further and\npropose a suite of tools for directly inferring about the differential\nparameters post-segmentation, which are applicable even when the regression\nparameters themselves are non-sparse. Theoretical investigations are conducted\nunder general conditions permitting non-Gaussianity, temporal dependence and\nultra-high dimensionality. Numerical results from simulated and macroeconomic\ndatasets demonstrate the competitiveness and efficacy of the proposed methods.\nImplementation of all methods is provided in the R package inferchange on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Haeran Cho"
                    },
                    {
                        "name": "Tobias Kley"
                    },
                    {
                        "name": "Housen Li"
                    }
                ],
                "author_detail": {
                    "name": "Housen Li"
                },
                "author": "Housen Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06915v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06915v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07054v2",
                "updated": "2024-09-25T09:19:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    19,
                    0,
                    2,
                    269,
                    0
                ],
                "published": "2023-11-13T03:42:17Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    3,
                    42,
                    17,
                    0,
                    317,
                    0
                ],
                "title": "A Study of Implicit Ranking Unfairness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of Implicit Ranking Unfairness in Large Language Models"
                },
                "summary": "Recently, Large Language Models (LLMs) have demonstrated a superior ability\nto serve as ranking models. However, concerns have arisen as LLMs will exhibit\ndiscriminatory ranking behaviors based on users' sensitive attributes (\\eg\ngender). Worse still, in this paper, we identify a subtler form of\ndiscrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs\nexhibit discriminatory ranking patterns based solely on non-sensitive user\nprofiles, such as user names. Such implicit unfairness is more widespread but\nless noticeable, threatening the ethical foundation. To comprehensively explore\nsuch unfairness, our analysis will focus on three research aspects: (1) We\npropose an evaluation method to investigate the severity of implicit ranking\nunfairness. (2) We uncover the reasons for causing such unfairness. (3) To\nmitigate such unfairness effectively, we utilize a pair-wise regression method\nto conduct fair-aware data augmentation for LLM fine-tuning. The experiment\ndemonstrates that our method outperforms existing approaches in ranking\nfairness, achieving this with only a small reduction in accuracy. Lastly, we\nemphasize the need for the community to identify and mitigate the implicit\nunfairness, aiming to avert the potential deterioration in the reinforced\nhuman-LLMs ecosystem deterioration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have demonstrated a superior ability\nto serve as ranking models. However, concerns have arisen as LLMs will exhibit\ndiscriminatory ranking behaviors based on users' sensitive attributes (\\eg\ngender). Worse still, in this paper, we identify a subtler form of\ndiscrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs\nexhibit discriminatory ranking patterns based solely on non-sensitive user\nprofiles, such as user names. Such implicit unfairness is more widespread but\nless noticeable, threatening the ethical foundation. To comprehensively explore\nsuch unfairness, our analysis will focus on three research aspects: (1) We\npropose an evaluation method to investigate the severity of implicit ranking\nunfairness. (2) We uncover the reasons for causing such unfairness. (3) To\nmitigate such unfairness effectively, we utilize a pair-wise regression method\nto conduct fair-aware data augmentation for LLM fine-tuning. The experiment\ndemonstrates that our method outperforms existing approaches in ranking\nfairness, achieving this with only a small reduction in accuracy. Lastly, we\nemphasize the need for the community to identify and mitigate the implicit\nunfairness, aiming to avert the potential deterioration in the reinforced\nhuman-LLMs ecosystem deterioration."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted in EMNLP 2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16760v1",
                "updated": "2024-09-25T09:16:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    16,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:16:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    16,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing Automatic Keyphrase Labelling with Text-to-Text Transfer\n  Transformer (T5) Architecture: A Framework for Keyphrase Generation and\n  Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Automatic Keyphrase Labelling with Text-to-Text Transfer\n  Transformer (T5) Architecture: A Framework for Keyphrase Generation and\n  Filtering"
                },
                "summary": "Automatic keyphrase labelling stands for the ability of models to retrieve\nwords or short phrases that adequately describe documents' content. Previous\nwork has put much effort into exploring extractive techniques to address this\ntask; however, these methods cannot produce keyphrases not found in the text.\nGiven this limitation, keyphrase generation approaches have arisen lately. This\npaper presents a keyphrase generation model based on the Text-to-Text Transfer\nTransformer (T5) architecture. Having a document's title and abstract as input,\nwe learn a T5 model to generate keyphrases which adequately define its content.\nWe name this model docT5keywords. We not only perform the classic inference\napproach, where the output sequence is directly selected as the predicted\nvalues, but we also report results from a majority voting approach. In this\napproach, multiple sequences are generated, and the keyphrases are ranked based\non their frequency of occurrence across these sequences. Along with this model,\nwe present a novel keyphrase filtering technique based on the T5 architecture.\nWe train a T5 model to learn whether a given keyphrase is relevant to a\ndocument. We devise two evaluation methodologies to prove our model's\ncapability to filter inadequate keyphrases. First, we perform a binary\nevaluation where our model has to predict if a keyphrase is relevant for a\ngiven document. Second, we filter the predicted keyphrases by several AKG\nmodels and check if the evaluation scores are improved. Experimental results\ndemonstrate that our keyphrase generation model significantly outperforms all\nthe baselines, with gains exceeding 100\\% in some cases. The proposed filtering\ntechnique also achieves near-perfect accuracy in eliminating false positives\nacross all datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic keyphrase labelling stands for the ability of models to retrieve\nwords or short phrases that adequately describe documents' content. Previous\nwork has put much effort into exploring extractive techniques to address this\ntask; however, these methods cannot produce keyphrases not found in the text.\nGiven this limitation, keyphrase generation approaches have arisen lately. This\npaper presents a keyphrase generation model based on the Text-to-Text Transfer\nTransformer (T5) architecture. Having a document's title and abstract as input,\nwe learn a T5 model to generate keyphrases which adequately define its content.\nWe name this model docT5keywords. We not only perform the classic inference\napproach, where the output sequence is directly selected as the predicted\nvalues, but we also report results from a majority voting approach. In this\napproach, multiple sequences are generated, and the keyphrases are ranked based\non their frequency of occurrence across these sequences. Along with this model,\nwe present a novel keyphrase filtering technique based on the T5 architecture.\nWe train a T5 model to learn whether a given keyphrase is relevant to a\ndocument. We devise two evaluation methodologies to prove our model's\ncapability to filter inadequate keyphrases. First, we perform a binary\nevaluation where our model has to predict if a keyphrase is relevant for a\ngiven document. Second, we filter the predicted keyphrases by several AKG\nmodels and check if the evaluation scores are improved. Experimental results\ndemonstrate that our keyphrase generation model significantly outperforms all\nthe baselines, with gains exceeding 100\\% in some cases. The proposed filtering\ntechnique also achieves near-perfect accuracy in eliminating false positives\nacross all datasets."
                },
                "authors": [
                    {
                        "name": "Jorge Gabín"
                    },
                    {
                        "name": "M. Eduardo Ares"
                    },
                    {
                        "name": "Javier Parapar"
                    }
                ],
                "author_detail": {
                    "name": "Javier Parapar"
                },
                "author": "Javier Parapar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16751v1",
                "updated": "2024-09-25T09:02:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    2,
                    48,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:02:48Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    2,
                    48,
                    2,
                    269,
                    0
                ],
                "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL"
                },
                "summary": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question, bridging the gap between the query and the database\nstructure. The pipeline leverages candidate predicate augmentation to mitigate\nerroneous or incomplete predicates in generated SQLs. We further investigate\nthe impact of schema filtering, a technique widely explored in previous work,\nand demonstrate its diminishing returns when applied alongside advanced large\nlanguage models. Comprehensive evaluations on the BIRD benchmark illustrate\nthat E-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. All code required to\nreproduce the reported results is publicly available on our GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question, bridging the gap between the query and the database\nstructure. The pipeline leverages candidate predicate augmentation to mitigate\nerroneous or incomplete predicates in generated SQLs. We further investigate\nthe impact of schema filtering, a technique widely explored in previous work,\nand demonstrate its diminishing returns when applied alongside advanced large\nlanguage models. Comprehensive evaluations on the BIRD benchmark illustrate\nthat E-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. All code required to\nreproduce the reported results is publicly available on our GitHub repository."
                },
                "authors": [
                    {
                        "name": "Hasan Alp Caferoğlu"
                    },
                    {
                        "name": "Özgür Ulusoy"
                    }
                ],
                "author_detail": {
                    "name": "Özgür Ulusoy"
                },
                "author": "Özgür Ulusoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16154v2",
                "updated": "2024-09-25T09:00:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    0,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T14:58:27Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    58,
                    27,
                    1,
                    268,
                    0
                ],
                "title": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed"
                },
                "summary": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources."
                },
                "authors": [
                    {
                        "name": "Alexander Prutsch"
                    },
                    {
                        "name": "Horst Bischof"
                    },
                    {
                        "name": "Horst Possegger"
                    }
                ],
                "author_detail": {
                    "name": "Horst Possegger"
                },
                "author": "Horst Possegger",
                "arxiv_comment": "Accepted to IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16075v2",
                "updated": "2024-09-25T08:59:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    59,
                    26,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T13:21:21Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    13,
                    21,
                    21,
                    1,
                    268,
                    0
                ],
                "title": "Ultra-low latency quantum-inspired machine learning predictors\n  implemented on FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-low latency quantum-inspired machine learning predictors\n  implemented on FPGA"
                },
                "summary": "Tensor Networks (TNs) are a computational paradigm used for representing\nquantum many-body systems. Recent works have shown how TNs can also be applied\nto perform Machine Learning (ML) tasks, yielding comparable results to standard\nsupervised learning techniques. In this work, we study the use of Tree Tensor\nNetworks (TTNs) in high-frequency real-time applications by exploiting the\nlow-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We\npresent different implementations of TTN classifiers, capable of performing\ninference on classical ML datasets as well as on complex physics data. A\npreparatory analysis of bond dimensions and weight quantization is realized in\nthe training phase, together with entanglement entropy and correlation\nmeasurements, that help setting the choice of the TTN architecture. The\ngenerated TTNs are then deployed on a hardware accelerator; using an FPGA\nintegrated into a server, the inference of the TTN is completely offloaded.\nEventually, a classifier for High Energy Physics (HEP) applications is\nimplemented and executed fully pipelined with sub-microsecond latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Networks (TNs) are a computational paradigm used for representing\nquantum many-body systems. Recent works have shown how TNs can also be applied\nto perform Machine Learning (ML) tasks, yielding comparable results to standard\nsupervised learning techniques. In this work, we study the use of Tree Tensor\nNetworks (TTNs) in high-frequency real-time applications by exploiting the\nlow-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We\npresent different implementations of TTN classifiers, capable of performing\ninference on classical ML datasets as well as on complex physics data. A\npreparatory analysis of bond dimensions and weight quantization is realized in\nthe training phase, together with entanglement entropy and correlation\nmeasurements, that help setting the choice of the TTN architecture. The\ngenerated TTNs are then deployed on a hardware accelerator; using an FPGA\nintegrated into a server, the inference of the TTN is completely offloaded.\nEventually, a classifier for High Energy Physics (HEP) applications is\nimplemented and executed fully pipelined with sub-microsecond latency."
                },
                "authors": [
                    {
                        "name": "Lorenzo Borella"
                    },
                    {
                        "name": "Alberto Coppi"
                    },
                    {
                        "name": "Jacopo Pazzini"
                    },
                    {
                        "name": "Andrea Stanco"
                    },
                    {
                        "name": "Marco Trenti"
                    },
                    {
                        "name": "Andrea Triossi"
                    },
                    {
                        "name": "Marco Zanetti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Zanetti"
                },
                "author": "Marco Zanetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14913v2",
                "updated": "2024-09-25T08:52:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-23T11:08:04Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    8,
                    4,
                    0,
                    267,
                    0
                ],
                "title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents"
                },
                "summary": "We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web."
                },
                "authors": [
                    {
                        "name": "Peter Mühlbacher"
                    },
                    {
                        "name": "Nikos I. Bosse"
                    },
                    {
                        "name": "Lawrence Phillips"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence Phillips"
                },
                "author": "Lawrence Phillips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16742v1",
                "updated": "2024-09-25T08:49:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    49,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:49:27Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    49,
                    27,
                    2,
                    269,
                    0
                ],
                "title": "A numerical-relativity surrogate model for hyperbolic encounters of\n  black holes: challenges in parameter estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A numerical-relativity surrogate model for hyperbolic encounters of\n  black holes: challenges in parameter estimation"
                },
                "summary": "We present a surrogate numerical-relativity model for close hyperbolic\nblack-hole encounters with equal masses and spins aligned with the orbital\nmomentum. Our model, generated in terms of the Newman-Penrose scalar $\\psi_4$,\nspans impact parameters $b/M\\in [11, 15]$ and spin components $\\chi_{i} \\in\n[-0.5,0.5]$, modeling the $(\\ell,m)=(2,0)$, $(2, \\pm 2)$, $(3,\\pm 2)$ and\n$(4,\\pm 4)$ emission multipoles. The model is faithful to numerical relativity\nsimulations, yielding mismatches lower than $10^{-3}$. We test the ability of\nour model to recover the parameters of numerically simulated signals. We find\nthat, despite the high accuracy of the model, parameter inference struggles to\ncorrectly capture the parameters of the source even for SNRs as large as 50 due\nto the strong degeneracies present in the parameter space. This indicates that\ncorrectly identifying these systems will require of extremely large signal\nloudness, only typical of third generation detectors. Nevertheless, we also\nfind that, if one attempts to infer certain combinations of such degenerated\nparameters, there might be a chance to prove the existence of this type of\nevents, even with the current ground-based detectors, as long as these\ncombinations make sense astrophysically and cosmologically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a surrogate numerical-relativity model for close hyperbolic\nblack-hole encounters with equal masses and spins aligned with the orbital\nmomentum. Our model, generated in terms of the Newman-Penrose scalar $\\psi_4$,\nspans impact parameters $b/M\\in [11, 15]$ and spin components $\\chi_{i} \\in\n[-0.5,0.5]$, modeling the $(\\ell,m)=(2,0)$, $(2, \\pm 2)$, $(3,\\pm 2)$ and\n$(4,\\pm 4)$ emission multipoles. The model is faithful to numerical relativity\nsimulations, yielding mismatches lower than $10^{-3}$. We test the ability of\nour model to recover the parameters of numerically simulated signals. We find\nthat, despite the high accuracy of the model, parameter inference struggles to\ncorrectly capture the parameters of the source even for SNRs as large as 50 due\nto the strong degeneracies present in the parameter space. This indicates that\ncorrectly identifying these systems will require of extremely large signal\nloudness, only typical of third generation detectors. Nevertheless, we also\nfind that, if one attempts to infer certain combinations of such degenerated\nparameters, there might be a chance to prove the existence of this type of\nevents, even with the current ground-based detectors, as long as these\ncombinations make sense astrophysically and cosmologically."
                },
                "authors": [
                    {
                        "name": "Joan Fontbuté"
                    },
                    {
                        "name": "Tomas Andrade"
                    },
                    {
                        "name": "Raimon Luna"
                    },
                    {
                        "name": "Juan Calderón Bustillo"
                    },
                    {
                        "name": "Gonzalo Morrás"
                    },
                    {
                        "name": "Santiago Jaraba"
                    },
                    {
                        "name": "Juan García-Bellido"
                    },
                    {
                        "name": "Germán López Izquierdo"
                    }
                ],
                "author_detail": {
                    "name": "Germán López Izquierdo"
                },
                "author": "Germán López Izquierdo",
                "arxiv_comment": "18 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16739v1",
                "updated": "2024-09-25T08:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    42,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    42,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "Context-Enhanced LLM-Based Framework for Automatic Test Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Enhanced LLM-Based Framework for Automatic Test Refactoring"
                },
                "summary": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16732v1",
                "updated": "2024-09-25T08:31:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    31,
                    11,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:31:11Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    31,
                    11,
                    2,
                    269,
                    0
                ],
                "title": "\"It Explains What I am Currently Going Through Perfectly to a Tee\":\n  Understanding User Perceptions on LLM-Enhanced Narrative Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It Explains What I am Currently Going Through Perfectly to a Tee\":\n  Understanding User Perceptions on LLM-Enhanced Narrative Interventions"
                },
                "summary": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that LLM-enhanced stories\nwere perceived to be better than human-written ones in conveying key takeaways,\npromoting reflection, and reducing belief in negative thoughts. These stories\nwere not only seen as more relatable but also similarly authentic to\nhuman-written ones, highlighting the potential of LLMs in helping young adults\nmanage their struggles. The findings of this work provide crucial design\nconsiderations for future narrative-based digital mental health interventions,\nsuch as the need to maintain relatability without veering into implausibility\nand refining the wording and tone of AI-enhanced content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that LLM-enhanced stories\nwere perceived to be better than human-written ones in conveying key takeaways,\npromoting reflection, and reducing belief in negative thoughts. These stories\nwere not only seen as more relatable but also similarly authentic to\nhuman-written ones, highlighting the potential of LLMs in helping young adults\nmanage their struggles. The findings of this work provide crucial design\nconsiderations for future narrative-based digital mental health interventions,\nsuch as the need to maintain relatability without veering into implausibility\nand refining the wording and tone of AI-enhanced content."
                },
                "authors": [
                    {
                        "name": "Ananya Bhattacharjee"
                    },
                    {
                        "name": "Sarah Yi Xu"
                    },
                    {
                        "name": "Pranav Rao"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Jonah Meyerhoff"
                    },
                    {
                        "name": "Syed Ishtiaque Ahmed"
                    },
                    {
                        "name": "David C Mohr"
                    },
                    {
                        "name": "Michael Liut"
                    },
                    {
                        "name": "Alex Mariakakis"
                    },
                    {
                        "name": "Rachel Kornfield"
                    },
                    {
                        "name": "Joseph Jay Williams"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Jay Williams"
                },
                "author": "Joseph Jay Williams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16727v1",
                "updated": "2024-09-25T08:23:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:23:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing\n  Systems"
                },
                "summary": "Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence."
                },
                "authors": [
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Dongming Zhao"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Jijun Zhang"
                    },
                    {
                        "name": "Ruifang He"
                    },
                    {
                        "name": "Yuexian Hou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Hou"
                },
                "author": "Yuexian Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16722v1",
                "updated": "2024-09-25T08:20:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    20,
                    24,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:20:24Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    20,
                    24,
                    2,
                    269,
                    0
                ],
                "title": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning"
                },
                "summary": "Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon."
                },
                "authors": [
                    {
                        "name": "Qibin Wang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02982v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02982v2",
                "updated": "2024-09-25T08:17:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    17,
                    34,
                    2,
                    269,
                    0
                ],
                "published": "2024-04-03T18:07:02Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    18,
                    7,
                    2,
                    2,
                    94,
                    0
                ],
                "title": "Spatio-temporal count autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal count autoregression"
                },
                "summary": "We study the problem of modeling and inference for spatio-temporal count\nprocesses. Our approach uses parsimonious parameterisations of multivariate\nautoregressive count time series models, including possible regression on\ncovariates. We control the number of parameters by specifying spatial\nneighbourhood structures for possibly huge matrices that take into account\nspatio-temporal dependencies. This work is motivated by real data applications\nwhich call for suitable models. Extensive simulation studies show that our\napproach yields reliable estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of modeling and inference for spatio-temporal count\nprocesses. Our approach uses parsimonious parameterisations of multivariate\nautoregressive count time series models, including possible regression on\ncovariates. We control the number of parameters by specifying spatial\nneighbourhood structures for possibly huge matrices that take into account\nspatio-temporal dependencies. This work is motivated by real data applications\nwhich call for suitable models. Extensive simulation studies show that our\napproach yields reliable estimators."
                },
                "authors": [
                    {
                        "name": "Steffen Maletz"
                    },
                    {
                        "name": "Konstantinos Fokianos"
                    },
                    {
                        "name": "Roland Fried"
                    }
                ],
                "author_detail": {
                    "name": "Roland Fried"
                },
                "author": "Roland Fried",
                "arxiv_comment": "24 pages, 16 figures and 22 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02982v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02982v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12246v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12246v3",
                "updated": "2024-09-25T08:17:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    17,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-18T03:42:00Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    3,
                    42,
                    0,
                    1,
                    170,
                    0
                ],
                "title": "TroL: Traversal of Layers for Large Language and Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TroL: Traversal of Layers for Large Language and Vision Models"
                },
                "summary": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes."
                },
                "authors": [
                    {
                        "name": "Byung-Kwan Lee"
                    },
                    {
                        "name": "Sangyun Chung"
                    },
                    {
                        "name": "Chae Won Kim"
                    },
                    {
                        "name": "Beomchan Park"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12246v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12246v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16719v1",
                "updated": "2024-09-25T08:09:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    9,
                    43,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:09:43Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    9,
                    43,
                    2,
                    269,
                    0
                ],
                "title": "Multi-functional reservoir computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-functional reservoir computing"
                },
                "summary": "Whereas the power of reservoir computing (RC) in inferring chaotic systems\nhas been well established in the literature, the studies are mostly restricted\nto mono-functional machines where the training and testing data are acquired\nfrom the same attractor. Here, using the strategies of attractor labeling and\ntrajectory separation, we propose a new scheme of RC capable of learning\nmultiple attractors generated by entirely different dynamics, namely\nmulti-functional RC. Specifically, we demonstrate that by incorporating a label\nchannel into the standard RC, a single machine is able to learn from data the\ndynamics of multiple chaotic attractors, while each attractor can be accurately\nretrieved by inputting just a scalar in the prediction phase. The dependence of\nthe machine performance on the labeling and separation parameters is\ninvestigated, and it is found that the machine performance is optimized when\nthe parameters take intermediate values. The working mechanism of\nmulti-functional RC is analyzed by the method of functional networks in\nneuroscience, and it is revealed that each attractor is represented by a\nstable, unique functional network in the reservoir, and the optimal performance\narises as a balance between the stability, complexity, and distinguishability\nof the functional networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whereas the power of reservoir computing (RC) in inferring chaotic systems\nhas been well established in the literature, the studies are mostly restricted\nto mono-functional machines where the training and testing data are acquired\nfrom the same attractor. Here, using the strategies of attractor labeling and\ntrajectory separation, we propose a new scheme of RC capable of learning\nmultiple attractors generated by entirely different dynamics, namely\nmulti-functional RC. Specifically, we demonstrate that by incorporating a label\nchannel into the standard RC, a single machine is able to learn from data the\ndynamics of multiple chaotic attractors, while each attractor can be accurately\nretrieved by inputting just a scalar in the prediction phase. The dependence of\nthe machine performance on the labeling and separation parameters is\ninvestigated, and it is found that the machine performance is optimized when\nthe parameters take intermediate values. The working mechanism of\nmulti-functional RC is analyzed by the method of functional networks in\nneuroscience, and it is revealed that each attractor is represented by a\nstable, unique functional network in the reservoir, and the optimal performance\narises as a balance between the stability, complexity, and distinguishability\nof the functional networks."
                },
                "authors": [
                    {
                        "name": "Yao Du"
                    },
                    {
                        "name": "Haibo Luo"
                    },
                    {
                        "name": "Jianmin Guo"
                    },
                    {
                        "name": "Jinghua Xiao"
                    },
                    {
                        "name": "Yizhen Yu"
                    },
                    {
                        "name": "Xingang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Wang"
                },
                "author": "Xingang Wang",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nlin.CD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01204v2",
                "updated": "2024-09-25T07:59:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    59,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-04-01T16:00:01Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    16,
                    0,
                    1,
                    0,
                    92,
                    0
                ],
                "title": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis"
                },
                "summary": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zhaoliang Chen"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Shawn Yue"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16710v1",
                "updated": "2024-09-25T07:55:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    55,
                    36,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:55:36Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    55,
                    36,
                    2,
                    269,
                    0
                ],
                "title": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?"
                },
                "summary": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research."
                },
                "authors": [
                    {
                        "name": "Takehiro Takayanagi"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16701v1",
                "updated": "2024-09-25T07:47:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:47:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "Unit Test Generation for Vulnerability Exploitation in Java Third-Party\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit Test Generation for Vulnerability Exploitation in Java Third-Party\n  Libraries"
                },
                "summary": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00646v2",
                "updated": "2024-09-25T07:40:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    40,
                    20,
                    2,
                    269,
                    0
                ],
                "published": "2023-10-01T12:02:57Z",
                "published_parsed": [
                    2023,
                    10,
                    1,
                    12,
                    2,
                    57,
                    6,
                    274,
                    0
                ],
                "title": "Source Attribution for Large Language Model-Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source Attribution for Large Language Model-Generated Data"
                },
                "summary": "The impressive performances of Large Language Models (LLMs) and their immense\npotential for commercialization have given rise to serious concerns over the\nIntellectual Property (IP) of their training data. In particular, the synthetic\ntexts generated by LLMs may infringe the IP of the data being used to train the\nLLMs. To this end, it is imperative to be able to perform source attribution by\nidentifying the data provider who contributed to the generation of a synthetic\ntext by an LLM. In this paper, we show that this problem can be tackled by\nwatermarking, i.e., by enabling an LLM to generate synthetic texts with\nembedded watermarks that contain information about their source(s). We identify\nthe key properties of such watermarking frameworks (e.g., source attribution\naccuracy, robustness against adversaries), and propose a source attribution\nframework that satisfies these key properties due to our algorithmic designs.\nOur framework enables an LLM to learn an accurate mapping from the generated\ntexts to data providers, which sets the foundation for effective source\nattribution. Extensive empirical evaluations show that our framework achieves\neffective source attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive performances of Large Language Models (LLMs) and their immense\npotential for commercialization have given rise to serious concerns over the\nIntellectual Property (IP) of their training data. In particular, the synthetic\ntexts generated by LLMs may infringe the IP of the data being used to train the\nLLMs. To this end, it is imperative to be able to perform source attribution by\nidentifying the data provider who contributed to the generation of a synthetic\ntext by an LLM. In this paper, we show that this problem can be tackled by\nwatermarking, i.e., by enabling an LLM to generate synthetic texts with\nembedded watermarks that contain information about their source(s). We identify\nthe key properties of such watermarking frameworks (e.g., source attribution\naccuracy, robustness against adversaries), and propose a source attribution\nframework that satisfies these key properties due to our algorithmic designs.\nOur framework enables an LLM to learn an accurate mapping from the generated\ntexts to data providers, which sets the foundation for effective source\nattribution. Extensive empirical evaluations show that our framework achieves\neffective source attribution."
                },
                "authors": [
                    {
                        "name": "Jingtan Wang"
                    },
                    {
                        "name": "Xinyang Lu"
                    },
                    {
                        "name": "Zitong Zhao"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16695v1",
                "updated": "2024-09-25T07:38:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    24,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:38:24Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    24,
                    2,
                    269,
                    0
                ],
                "title": "In which fields can ChatGPT detect journal article quality? An\n  evaluation of REF2021 results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In which fields can ChatGPT detect journal article quality? An\n  evaluation of REF2021 results"
                },
                "summary": "Time spent by academics on research quality assessment might be reduced if\nautomated approaches can help. Whilst citation-based indicators have been\nextensively developed and evaluated for this, they have substantial limitations\nand Large Language Models (LLMs) like ChatGPT provide an alternative approach.\nThis article assesses whether ChatGPT 4o-mini can be used to estimate the\nquality of journal articles across academia. It samples up to 200 articles from\nall 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework\n(REF) 2021, comparing ChatGPT scores with departmental average scores. There\nwas an almost universally positive Spearman correlation between ChatGPT scores\nand departmental averages, varying between 0.08 (Philosophy) and 0.78\n(Psychology, Psychiatry and Neuroscience), except for Clinical Medicine\n(rho=-0.12). Although other explanations are possible, especially because REF\nscore profiles are public, the results suggest that LLMs can provide reasonable\nresearch quality estimates in most areas of science, and particularly the\nphysical and health sciences and engineering, even before citation data is\navailable. Nevertheless, ChatGPT assessments seem to be more positive for most\nhealth and physical sciences than for other fields, a concern for\nmultidisciplinary assessments, and the ChatGPT scores are only based on titles\nand abstracts, so cannot be research evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time spent by academics on research quality assessment might be reduced if\nautomated approaches can help. Whilst citation-based indicators have been\nextensively developed and evaluated for this, they have substantial limitations\nand Large Language Models (LLMs) like ChatGPT provide an alternative approach.\nThis article assesses whether ChatGPT 4o-mini can be used to estimate the\nquality of journal articles across academia. It samples up to 200 articles from\nall 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework\n(REF) 2021, comparing ChatGPT scores with departmental average scores. There\nwas an almost universally positive Spearman correlation between ChatGPT scores\nand departmental averages, varying between 0.08 (Philosophy) and 0.78\n(Psychology, Psychiatry and Neuroscience), except for Clinical Medicine\n(rho=-0.12). Although other explanations are possible, especially because REF\nscore profiles are public, the results suggest that LLMs can provide reasonable\nresearch quality estimates in most areas of science, and particularly the\nphysical and health sciences and engineering, even before citation data is\navailable. Nevertheless, ChatGPT assessments seem to be more positive for most\nhealth and physical sciences than for other fields, a concern for\nmultidisciplinary assessments, and the ChatGPT scores are only based on titles\nand abstracts, so cannot be research evaluations."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Abdallah Yaghi"
                    }
                ],
                "author_detail": {
                    "name": "Abdallah Yaghi"
                },
                "author": "Abdallah Yaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16694v1",
                "updated": "2024-09-25T07:38:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:38:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms"
                },
                "summary": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization."
                },
                "authors": [
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Chengtao Lv"
                    },
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Jinyang Du"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_comment": "Ruihao Gong leads the overall organization of the survey, with Yifu\n  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is\n  responsible for authoring Section 4, while Chengtao Lv and Zining Wang\n  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and\n  Xianglong Liu provide guidance during the whole process and assist in\n  refining the final manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16686v1",
                "updated": "2024-09-25T07:21:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    21,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:21:51Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    21,
                    51,
                    2,
                    269,
                    0
                ],
                "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for\n  Superior Planning and Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for\n  Superior Planning and Decision-Making"
                },
                "summary": "Long-term memory is significant for agents, in which insights play a crucial\nrole. However, the emergence of irrelevant insight and the lack of general\ninsight can greatly undermine the effectiveness of insight. To solve this\nproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an\nembodied agent designed to improve LLMs' planning and decision-making ability\nby summarizing and utilizing insight effectively across different scales. MSI\nachieves this through the experience selector, insight generator, and insight\nselector. Leveraging a three-part pipeline, MSI can generate task-specific and\nhigh-level insight, store it in a database, and then use relevant insight from\nit to aid in decision-making. Our experiments show that MSI outperforms another\ninsight strategy when planning by GPT3.5. Moreover, We delve into the\nstrategies for selecting seed experience and insight, aiming to provide LLM\nwith more useful and relevant insight for better decision-making. Our\nobservations also indicate that MSI exhibits better robustness when facing\ndomain-shifting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term memory is significant for agents, in which insights play a crucial\nrole. However, the emergence of irrelevant insight and the lack of general\ninsight can greatly undermine the effectiveness of insight. To solve this\nproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an\nembodied agent designed to improve LLMs' planning and decision-making ability\nby summarizing and utilizing insight effectively across different scales. MSI\nachieves this through the experience selector, insight generator, and insight\nselector. Leveraging a three-part pipeline, MSI can generate task-specific and\nhigh-level insight, store it in a database, and then use relevant insight from\nit to aid in decision-making. Our experiments show that MSI outperforms another\ninsight strategy when planning by GPT3.5. Moreover, We delve into the\nstrategies for selecting seed experience and insight, aiming to provide LLM\nwith more useful and relevant insight for better decision-making. Our\nobservations also indicate that MSI exhibits better robustness when facing\ndomain-shifting scenarios."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Yihuai Gao"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_journal_ref": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16683v1",
                "updated": "2024-09-25T07:20:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    20,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:20:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    20,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Robust Max Statistics for High-Dimensional Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Max Statistics for High-Dimensional Inference"
                },
                "summary": "Although much progress has been made in the theory and application of\nbootstrap approximations for max statistics in high dimensions, the literature\nhas largely been restricted to cases involving light-tailed data. To address\nthis issue, we propose an approach to inference based on robust max statistics,\nand we show that their distributions can be accurately approximated via\nbootstrapping when the data are both high-dimensional and heavy-tailed. In\nparticular, the data are assumed to satisfy an extended version of the\nwell-established $L^{4}$-$L^2$ moment equivalence condition, as well as a weak\nvariance decay condition. In this setting, we show that near-parametric rates\nof bootstrap approximation can be achieved in the Kolmogorov metric,\nindependently of the data dimension. Moreover, this theoretical result is\ncomplemented by favorable empirical results involving both synthetic data and\nan application to financial data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although much progress has been made in the theory and application of\nbootstrap approximations for max statistics in high dimensions, the literature\nhas largely been restricted to cases involving light-tailed data. To address\nthis issue, we propose an approach to inference based on robust max statistics,\nand we show that their distributions can be accurately approximated via\nbootstrapping when the data are both high-dimensional and heavy-tailed. In\nparticular, the data are assumed to satisfy an extended version of the\nwell-established $L^{4}$-$L^2$ moment equivalence condition, as well as a weak\nvariance decay condition. In this setting, we show that near-parametric rates\nof bootstrap approximation can be achieved in the Kolmogorov metric,\nindependently of the data dimension. Moreover, this theoretical result is\ncomplemented by favorable empirical results involving both synthetic data and\nan application to financial data."
                },
                "authors": [
                    {
                        "name": "Mingshuo Liu"
                    },
                    {
                        "name": "Miles E. Lopes"
                    }
                ],
                "author_detail": {
                    "name": "Miles E. Lopes"
                },
                "author": "Miles E. Lopes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16682v1",
                "updated": "2024-09-25T07:18:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    18,
                    45,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:18:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    18,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "SynTQA: Synergistic Table-based Question Answering via Mixture of\n  Text-to-SQL and E2E TQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynTQA: Synergistic Table-based Question Answering via Mixture of\n  Text-to-SQL and E2E TQA"
                },
                "summary": "Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main\napproaches for Table-based Question Answering task. Despite success on multiple\nbenchmarks, they have yet to be compared and their synergy remains unexplored.\nIn this paper, we identify different strengths and weaknesses through\nevaluating state-of-the-art models on benchmark datasets: Text-to-SQL\ndemonstrates superiority in handling questions involving arithmetic operations\nand long tables; E2E TQA excels in addressing ambiguous questions, non-standard\ntable schema, and complex table contents. To combine both strengths, we propose\na Synergistic Table-based Question Answering approach that integrate different\nmodels via answer selection, which is agnostic to any model types. Further\nexperiments validate that ensembling models by either feature-based or\nLLM-based answer selector significantly improves the performance over\nindividual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main\napproaches for Table-based Question Answering task. Despite success on multiple\nbenchmarks, they have yet to be compared and their synergy remains unexplored.\nIn this paper, we identify different strengths and weaknesses through\nevaluating state-of-the-art models on benchmark datasets: Text-to-SQL\ndemonstrates superiority in handling questions involving arithmetic operations\nand long tables; E2E TQA excels in addressing ambiguous questions, non-standard\ntable schema, and complex table contents. To combine both strengths, we propose\na Synergistic Table-based Question Answering approach that integrate different\nmodels via answer selection, which is agnostic to any model types. Further\nexperiments validate that ensembling models by either feature-based or\nLLM-based answer selector significantly improves the performance over\nindividual models."
                },
                "authors": [
                    {
                        "name": "Siyue Zhang"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Chen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhao"
                },
                "author": "Chen Zhao",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16674v1",
                "updated": "2024-09-25T07:06:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    6,
                    14,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:06:14Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    6,
                    14,
                    2,
                    269,
                    0
                ],
                "title": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models"
                },
                "summary": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Toyotaro Suzumura"
                    }
                ],
                "author_detail": {
                    "name": "Toyotaro Suzumura"
                },
                "author": "Toyotaro Suzumura",
                "arxiv_comment": "Risks: The 1st International Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommendation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03536v2",
                "updated": "2024-09-25T07:05:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    5,
                    16,
                    2,
                    269,
                    0
                ],
                "published": "2024-07-03T22:45:36Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    22,
                    45,
                    36,
                    2,
                    185,
                    0
                ],
                "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla (2) a curated dataset for bias measurement\nbenchmarking (3) testing two different probing techniques for bias detection in\nthe context of Bangla. This is the first work of such kind involving bias\nassessment of LLMs for Bangla to the best of our knowledge. All our code and\nresources are publicly available for the progress of bias related research in\nBangla NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla (2) a curated dataset for bias measurement\nbenchmarking (3) testing two different probing techniques for bias detection in\nthe context of Bangla. This is the first work of such kind involving bias\nassessment of LLMs for Bangla to the best of our knowledge. All our code and\nresources are publicly available for the progress of bias related research in\nBangla NLP."
                },
                "authors": [
                    {
                        "name": "Jayanta Sadhu"
                    },
                    {
                        "name": "Maneesha Rani Saha"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15868v2",
                "updated": "2024-09-25T07:03:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T08:41:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    41,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Privacy Evaluation Benchmarks for NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Evaluation Benchmarks for NLP Models"
                },
                "summary": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Needs further optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2409.17143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17143v1",
                "updated": "2024-09-25T17:59:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    13,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:59:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    59,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "Attention Prompting on Image for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Prompting on Image for Large Vision-Language Models"
                },
                "summary": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Website, see https://yu-rp.github.io/api-prompting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17141v1",
                "updated": "2024-09-25T17:58:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:58:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineZip : Pushing the Limits of Large Language Models for Practical\n  Lossless Text Compression"
                },
                "summary": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the language modeling objective has been shown to be deeply connected\nwith compression, it is surprising that modern LLMs are not employed in\npractical text compression systems. In this paper, we provide an in-depth\nanalysis of neural network and transformer-based compression techniques to\nanswer this question. We compare traditional text compression systems with\nneural network and LLM-based text compression methods. Although LLM-based\nsystems significantly outperform conventional compression methods, they are\nhighly impractical. Specifically, LLMZip, a recent text compression system\nusing Llama3-8B requires 9.5 days to compress just 10 MB of text, although with\nhuge improvements in compression ratios. To overcome this, we present FineZip -\na novel LLM-based text compression system that combines ideas of online\nmemorization and dynamic context to reduce the compression time immensely.\nFineZip can compress the above corpus in approximately 4 hours compared to 9.5\ndays, a 54 times improvement over LLMZip and comparable performance. FineZip\noutperforms traditional algorithmic compression methods with a large margin,\nimproving compression ratios by approximately 50\\%. With this work, we take the\nfirst step towards making lossless text compression with LLMs a reality. While\nFineZip presents a significant step in that direction, LLMs are still not a\nviable solution for large-scale text compression. We hope our work paves the\nway for future research and innovation to solve this problem."
                },
                "authors": [
                    {
                        "name": "Fazal Mittu"
                    },
                    {
                        "name": "Yihuan Bu"
                    },
                    {
                        "name": "Akshat Gupta"
                    },
                    {
                        "name": "Ashok Devireddy"
                    },
                    {
                        "name": "Alp Eren Ozdarendeli"
                    },
                    {
                        "name": "Anant Singh"
                    },
                    {
                        "name": "Gopala Anumanchipalli"
                    }
                ],
                "author_detail": {
                    "name": "Gopala Anumanchipalli"
                },
                "author": "Gopala Anumanchipalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17140v1",
                "updated": "2024-09-25T17:58:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    8,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:58:08Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    58,
                    8,
                    2,
                    269,
                    0
                ],
                "title": "Turn Every Application into an Agent: Towards Efficient\n  Human-Agent-Computer Interaction with API-First LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turn Every Application into an Agent: Towards Efficient\n  Human-Agent-Computer Interaction with API-First LLM-Based Agents"
                },
                "summary": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Office Word\ndemonstrate that AXIS reduces task completion time by 65%-70% and cognitive\nworkload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.\nOur work contributes to a new human-agent-computer interaction (HACI) framework\nand a fresh UI design principle for application providers in the era of LLMs.\nIt also explores the possibility of turning every applications into agents,\npaving the way towards an agent-centric operating system (Agent OS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enabled LLM-based agents to\ndirectly interact with application user interfaces (UIs), enhancing agents'\nperformance in complex tasks. However, these agents often suffer from high\nlatency and low reliability due to the extensive sequential UI interactions. To\naddress this issue, we propose AXIS, a novel LLM-based agents framework\nprioritize actions through application programming interfaces (APIs) over UI\nactions. This framework also facilitates the creation and expansion of APIs\nthrough automated exploration of applications. Our experiments on Office Word\ndemonstrate that AXIS reduces task completion time by 65%-70% and cognitive\nworkload by 38%-53%, while maintaining accuracy of 97%-98% compare to humans.\nOur work contributes to a new human-agent-computer interaction (HACI) framework\nand a fresh UI design principle for application providers in the era of LLMs.\nIt also explores the possibility of turning every applications into agents,\npaving the way towards an agent-centric operating system (Agent OS)."
                },
                "authors": [
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Zhiyang Zhang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17132v1",
                "updated": "2024-09-25T17:49:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    49,
                    34,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:49:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    49,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Complex-Phase, Data-Driven Identification of Grid-Forming Inverter\n  Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex-Phase, Data-Driven Identification of Grid-Forming Inverter\n  Dynamics"
                },
                "summary": "The increasing integration of renewable energy sources (RESs) into power\nsystems requires the deployment of grid-forming inverters to ensure a stable\noperation. Accurate modeling of these devices is necessary. In this paper, a\nsystem identification approach to obtain low-dimensional models of grid-forming\ninverters is presented. The proposed approach is based on a Hammerstein-Wiener\nparametrization of the normal-form model. The normal-form is a gray-box model\nthat utilizes complex frequency and phase to capture non-linear inverter\ndynamics. The model is validated on two well-known control strategies:\ndroop-control and dispatchable virtual oscillators. Simulations and\nhardware-in-the-loop experiments demonstrate that the normal-form accurately\nmodels inverter dynamics across various operating conditions. The approach\nshows great potential for enhancing the modeling of RES-dominated power\nsystems, especially when component models are unavailable or computationally\nexpensive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing integration of renewable energy sources (RESs) into power\nsystems requires the deployment of grid-forming inverters to ensure a stable\noperation. Accurate modeling of these devices is necessary. In this paper, a\nsystem identification approach to obtain low-dimensional models of grid-forming\ninverters is presented. The proposed approach is based on a Hammerstein-Wiener\nparametrization of the normal-form model. The normal-form is a gray-box model\nthat utilizes complex frequency and phase to capture non-linear inverter\ndynamics. The model is validated on two well-known control strategies:\ndroop-control and dispatchable virtual oscillators. Simulations and\nhardware-in-the-loop experiments demonstrate that the normal-form accurately\nmodels inverter dynamics across various operating conditions. The approach\nshows great potential for enhancing the modeling of RES-dominated power\nsystems, especially when component models are unavailable or computationally\nexpensive."
                },
                "authors": [
                    {
                        "name": "Anna Büttner"
                    },
                    {
                        "name": "Hans Würfel"
                    },
                    {
                        "name": "Sebastian Liemann"
                    },
                    {
                        "name": "Johannes Schiffer"
                    },
                    {
                        "name": "Frank Hellmann"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hellmann"
                },
                "author": "Frank Hellmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17131v1",
                "updated": "2024-09-25T17:49:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    49,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:49:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    49,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing robot reliability for health-care facilities by means of\n  Human-Aware Navigation Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing robot reliability for health-care facilities by means of\n  Human-Aware Navigation Planning"
                },
                "summary": "With the aim of enabling robots to cooperate with humans, carry out\nhuman-like tasks, or navigate among humans, we need to ensure that they are\nequipped with the ability to comprehend human behaviors and use the extracted\nknowledge for intelligent decision-making. This ability is particularly\nimportant in the safety-critical and human-centred environment of health-care\ninstitutions. In the field of robotic navigation, the most cutting-edge\napproaches to enhancing robot reliability in the application domain of\nhealthcare facilities and in general pertain to augmenting navigation systems\nwith human-aware properties. To implement this in our work, the Co-operative\nHuman-Aware Navigation planner has been integrated into the ROS-based\ndifferential-drive robot MARRtina and exhaustively challenged within various\nsimulated contexts and scenarios (mainly modelling the situations relevant in\nthe medical domain) to draw attention to the integrated system's benefits and\nidentify its drawbacks or instances of poor performance while exploring the\nscope of system capabilities and creating a full characterization of its\napplicability. The simulation results are then presented to medical experts,\nand the enhanced robot acceptability within the domain is validated with them\nas the robot is further planned for deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the aim of enabling robots to cooperate with humans, carry out\nhuman-like tasks, or navigate among humans, we need to ensure that they are\nequipped with the ability to comprehend human behaviors and use the extracted\nknowledge for intelligent decision-making. This ability is particularly\nimportant in the safety-critical and human-centred environment of health-care\ninstitutions. In the field of robotic navigation, the most cutting-edge\napproaches to enhancing robot reliability in the application domain of\nhealthcare facilities and in general pertain to augmenting navigation systems\nwith human-aware properties. To implement this in our work, the Co-operative\nHuman-Aware Navigation planner has been integrated into the ROS-based\ndifferential-drive robot MARRtina and exhaustively challenged within various\nsimulated contexts and scenarios (mainly modelling the situations relevant in\nthe medical domain) to draw attention to the integrated system's benefits and\nidentify its drawbacks or instances of poor performance while exploring the\nscope of system capabilities and creating a full characterization of its\napplicability. The simulation results are then presented to medical experts,\nand the enhanced robot acceptability within the domain is validated with them\nas the robot is further planned for deployment."
                },
                "authors": [
                    {
                        "name": "Olga E. Sorokoletova"
                    },
                    {
                        "name": "Lucca Iocchi"
                    }
                ],
                "author_detail": {
                    "name": "Lucca Iocchi"
                },
                "author": "Lucca Iocchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17115v1",
                "updated": "2024-09-25T17:28:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    28,
                    13,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    28,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Every Example: Lifting Pre-training Data Quality like\n  Experts at Scale"
                },
                "summary": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb.\nFurthermore, ProX exhibits significant potential in domain-specific continual\npre-training: without domain specific design, models trained on OpenWebMath\nrefined by ProX outperform human-crafted rule-based methods, improving average\naccuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for\nCodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B\ntrained on 200B tokens. Further analysis highlights that ProX significantly\nsaves training FLOPs, offering a promising path for efficient LLM\npre-training.We are open-sourcing ProX with >100B corpus, models, and sharing\nall training and implementation details for reproducible research and future\ninnovation. Code: https://github.com/GAIR-NLP/ProX"
                },
                "authors": [
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "45 pages, 13 figures, 34 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17113v1",
                "updated": "2024-09-25T17:27:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:27:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    27,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Characterizing stable regions in the residual stream of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing stable regions in the residual stream of LLMs"
                },
                "summary": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We identify \"stable regions\" in the residual stream of Transformers, where\nthe model's output remains insensitive to small activation changes, but\nexhibits high sensitivity at region boundaries. These regions emerge during\ntraining and become more defined as training progresses or model size\nincreases. The regions appear to be much larger than previously studied\npolytopes. Our analysis suggests that these stable regions align with semantic\ndistinctions, where similar prompts cluster within regions, and activations\nfrom the same region lead to similar next token predictions."
                },
                "authors": [
                    {
                        "name": "Jett Janiak"
                    },
                    {
                        "name": "Jacek Karwowski"
                    },
                    {
                        "name": "Chatrik Singh Mangat"
                    },
                    {
                        "name": "Giorgi Giglemiani"
                    },
                    {
                        "name": "Nora Petrova"
                    },
                    {
                        "name": "Stefan Heimersheim"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Heimersheim"
                },
                "author": "Stefan Heimersheim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17100v1",
                "updated": "2024-09-25T17:12:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    12,
                    34,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:12:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    12,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Generic Diagonalizability, Structural Functional Observability and\n  Output Controllability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generic Diagonalizability, Structural Functional Observability and\n  Output Controllability"
                },
                "summary": "This paper investigates the structural functional observability (SFO) and\nstructural output controllability (SOC) of a class of systems with generically\ndiagonalizable state matrices and explores the associated minimal sensor and\nactuator placement problems. The verification of SOC and the corresponding\nsensor and actuator placement problems, i.e., the problems of determining the\nminimum number of outputs and inputs required to achieve SFO and SOC,\nrespectively, are yet open for general systems, which motivates our focus on a\nclass of systems enabling polynomial-time solutions. In this line, we first\ndefine and characterize generically diagonalizable systems, referring to\nstructured systems for which almost all realizations of the state matrices are\ndiagonalizable. We then develop computationally efficient criteria for SFO and\nSOC within the context of generically diagonalizable systems. Our work expands\nthe class of systems amenable to polynomial-time SOC verification. Thanks to\nthe simplicity of the obtained criteria, we derive closed-form solutions for\ndetermining the minimal sensor placement to achieve SFO and the minimal\nactuator deployment to achieve SOC in such systems, along with efficient\nweighted maximum matching based and weighted maximum flow based algorithms. For\nmore general systems to achieve SFO, an upper bound is given by identifying a\nnon-decreasing property of SFO with respect to a specific class of edge\nadditions, which is shown to be optimal under certain circumstances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the structural functional observability (SFO) and\nstructural output controllability (SOC) of a class of systems with generically\ndiagonalizable state matrices and explores the associated minimal sensor and\nactuator placement problems. The verification of SOC and the corresponding\nsensor and actuator placement problems, i.e., the problems of determining the\nminimum number of outputs and inputs required to achieve SFO and SOC,\nrespectively, are yet open for general systems, which motivates our focus on a\nclass of systems enabling polynomial-time solutions. In this line, we first\ndefine and characterize generically diagonalizable systems, referring to\nstructured systems for which almost all realizations of the state matrices are\ndiagonalizable. We then develop computationally efficient criteria for SFO and\nSOC within the context of generically diagonalizable systems. Our work expands\nthe class of systems amenable to polynomial-time SOC verification. Thanks to\nthe simplicity of the obtained criteria, we derive closed-form solutions for\ndetermining the minimal sensor placement to achieve SFO and the minimal\nactuator deployment to achieve SOC in such systems, along with efficient\nweighted maximum matching based and weighted maximum flow based algorithms. For\nmore general systems to achieve SFO, an upper bound is given by identifying a\nnon-decreasing property of SFO with respect to a specific class of edge\nadditions, which is shown to be optimal under certain circumstances."
                },
                "authors": [
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Tyrone Fernando"
                    },
                    {
                        "name": "Mohamed Darouach"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Darouach"
                },
                "author": "Mohamed Darouach",
                "arxiv_comment": "Under review in a Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17092v1",
                "updated": "2024-09-25T16:58:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:58:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    58,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "Accumulator-Aware Post-Training Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accumulator-Aware Post-Training Quantization"
                },
                "summary": "Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent studies have investigated low-precision accumulation,\nreporting improvements in throughput, power, and area across various platforms.\nHowever, the accompanying proposals have only considered the quantization-aware\ntraining (QAT) paradigm, in which models are fine-tuned or trained from scratch\nwith quantization in the loop. As models continue to grow in size, QAT\ntechniques become increasingly more expensive, which has motivated the recent\nsurge in post-training quantization (PTQ) research. To the best of our\nknowledge, ours marks the first formal study of accumulator-aware quantization\nin the PTQ setting. To bridge this gap, we introduce AXE, a practical framework\nof accumulator-aware extensions designed to endow overflow avoidance guarantees\nto existing layer-wise PTQ algorithms. We theoretically motivate AXE and\ndemonstrate its flexibility by implementing it on top of two state-of-the-art\nPTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage\naccumulation for the first time, opening the door for full datapath\noptimization and scaling to large language models (LLMs). We evaluate AXE\nacross image classification and language generation models, and observe\nsignificant improvements in the trade-off between accumulator bit width and\nmodel accuracy over baseline methods."
                },
                "authors": [
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Fabian Grob"
                    },
                    {
                        "name": "Giuseppe Franco"
                    },
                    {
                        "name": "Jinjie Zhang"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17012v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17012v3",
                "updated": "2024-09-25T16:57:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    57,
                    20,
                    2,
                    269,
                    0
                ],
                "published": "2023-09-29T06:53:10Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    6,
                    53,
                    10,
                    4,
                    272,
                    0
                ],
                "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
                },
                "summary": "Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are cognitively biased judges. Large Language Models\n(LLMs) have recently been shown to be effective as automatic evaluators with\nsimple prompting and in-context learning. In this work, we assemble 15 LLMs of\nfour different size ranges and evaluate their output responses by preference\nranking from the other LLMs as evaluators, such as System Star is better than\nSystem Square. We then evaluate the quality of ranking outputs introducing the\nCognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to\nmeasure six different cognitive biases in LLM evaluation outputs, such as the\nEgocentric bias where a model prefers to rank its own outputs highly in\nevaluation. We find that LLMs are biased text quality evaluators, exhibiting\nstrong indications on our bias benchmark (average of 40% of comparisons across\nall models) within each of their evaluations that question their robustness as\nevaluators. Furthermore, we examine the correlation between human and machine\npreferences and calculate the average Rank-Biased Overlap (RBO) score to be\n49.6%, indicating that machine preferences are misaligned with humans.\nAccording to our findings, LLMs may still be unable to be utilized for\nautomatic annotation aligned with human preferences. Our project page is at:\nhttps://minnesotanlp.github.io/cobbler."
                },
                "authors": [
                    {
                        "name": "Ryan Koo"
                    },
                    {
                        "name": "Minhwa Lee"
                    },
                    {
                        "name": "Vipul Raheja"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "Publishsed at ACL 2024. 29 pages, 9 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17012v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17012v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17073v1",
                "updated": "2024-09-25T16:32:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    32,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:32:35Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    32,
                    35,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Post-Hoc Attributions in Long Document Comprehension via\n  Coarse Grained Answer Decomposition"
                },
                "summary": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nprecisely should be attributed, with an emphasis on identifying the information\nunits within an answer that necessitate grounding? In this paper, we propose\nand investigate a novel approach to the factual decomposition of generated\nanswers for attribution, employing template-based in-context learning. To\naccomplish this, we utilize the question and integrate negative sampling during\nfew-shot in-context learning for decomposition. This approach enhances the\nsemantic understanding of both abstractive and extractive answers. We examine\nthe impact of answer decomposition by providing a thorough examination of\nvarious attribution approaches, ranging from retrieval-based techniques to\nLLM-based attributors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately attributing answer text to its source document is crucial for\ndeveloping a reliable question-answering system. However, attribution for long\ndocuments remains largely unexplored. Post-hoc attribution systems are designed\nto map answer text back to the source document, yet the granularity of this\nmapping has not been addressed. Furthermore, a critical question arises: What\nprecisely should be attributed, with an emphasis on identifying the information\nunits within an answer that necessitate grounding? In this paper, we propose\nand investigate a novel approach to the factual decomposition of generated\nanswers for attribution, employing template-based in-context learning. To\naccomplish this, we utilize the question and integrate negative sampling during\nfew-shot in-context learning for decomposition. This approach enhances the\nsemantic understanding of both abstractive and extractive answers. We examine\nthe impact of answer decomposition by providing a thorough examination of\nvarious attribution approaches, ranging from retrieval-based techniques to\nLLM-based attributors."
                },
                "authors": [
                    {
                        "name": "Pritika Ramu"
                    },
                    {
                        "name": "Koustava Goswami"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Balaji Vasan Srinivavsan"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Vasan Srinivavsan"
                },
                "author": "Balaji Vasan Srinivavsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17066v1",
                "updated": "2024-09-25T16:25:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    25,
                    45,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:25:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    25,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large\n  Language Models"
                },
                "summary": "Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling model size significantly challenges the deployment and inference of\nLarge Language Models (LLMs). Due to the redundancy in LLM weights, recent\nresearch has focused on pushing weight-only quantization to extremely low-bit\n(even down to 2 bits). It reduces memory requirements, optimizes storage costs,\nand decreases memory bandwidth needs during inference. However, due to\nnumerical representation limitations, traditional scalar-based weight\nquantization struggles to achieve such extreme low-bit. Recent research on\nVector Quantization (VQ) for LLMs has demonstrated the potential for extremely\nlow-bit model quantization by compressing vectors into indices using lookup\ntables.\n  In this paper, we introduce Vector Post-Training Quantization (VPTQ) for\nextremely low-bit quantization of LLMs. We use Second-Order Optimization to\nformulate the LLM VQ problem and guide our quantization algorithm design by\nsolving the optimization. We further refine the weights using\nChannel-Independent Second-Order Optimization for a granular VQ. In addition,\nby decomposing the optimization problem, we propose a brief and effective\ncodebook initialization algorithm. We also extend VPTQ to support residual and\noutlier quantization, which enhances model accuracy and further compresses the\nmodel. Our experimental results show that VPTQ reduces model quantization\nperplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B,\n$4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy\nimprovement of $0.79$-$1.5\\%$ on LLaMA-2, $1\\%$ on Mistral-7B, $11$-$22\\%$ on\nLLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\\%$ of the\nquantization algorithm execution time, resulting in a $1.6$-$1.8\\times$\nincrease in inference throughput compared to SOTA."
                },
                "authors": [
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Jicheng Wen"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Shengyu Ye"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17054v1",
                "updated": "2024-09-25T16:13:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    13,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T16:13:42Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    16,
                    13,
                    42,
                    2,
                    269,
                    0
                ],
                "title": "Using LLM for Real-Time Transcription and Summarization of\n  Doctor-Patient Interactions into ePuskesmas in Indonesia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLM for Real-Time Transcription and Summarization of\n  Doctor-Patient Interactions into ePuskesmas in Indonesia"
                },
                "summary": "One of the key issues contributing to inefficiency in Puskesmas is the\ntime-consuming nature of doctor-patient interactions. Doctors need to conduct\nthorough consultations, which include diagnosing the patient's condition,\nproviding treatment advice, and transcribing detailed notes into medical\nrecords. In regions with diverse linguistic backgrounds, doctors often have to\nask clarifying questions, further prolonging the process. While diagnosing is\nessential, transcription and summarization can often be automated using AI to\nimprove time efficiency and help doctors enhance care quality and enable early\ndiagnosis and intervention. This paper proposes a solution using a localized\nlarge language model (LLM) to transcribe, translate, and summarize\ndoctor-patient conversations. We utilize the Whisper model for transcription\nand GPT-3 to summarize them into the ePuskemas medical records format. This\nsystem is implemented as an add-on to an existing web browser extension,\nallowing doctors to fill out patient forms while talking. By leveraging this\nsolution for real-time transcription, translation, and summarization, doctors\ncan improve the turnaround time for patient care while enhancing the quality of\nrecords, which become more detailed and insightful for future visits. This\ninnovation addresses challenges like overcrowded facilities and the\nadministrative burden on healthcare providers in Indonesia. We believe this\nsolution will help doctors save time, provide better care, and produce more\naccurate medical records, representing a significant step toward modernizing\nhealthcare and ensuring patients receive timely, high-quality care, even in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key issues contributing to inefficiency in Puskesmas is the\ntime-consuming nature of doctor-patient interactions. Doctors need to conduct\nthorough consultations, which include diagnosing the patient's condition,\nproviding treatment advice, and transcribing detailed notes into medical\nrecords. In regions with diverse linguistic backgrounds, doctors often have to\nask clarifying questions, further prolonging the process. While diagnosing is\nessential, transcription and summarization can often be automated using AI to\nimprove time efficiency and help doctors enhance care quality and enable early\ndiagnosis and intervention. This paper proposes a solution using a localized\nlarge language model (LLM) to transcribe, translate, and summarize\ndoctor-patient conversations. We utilize the Whisper model for transcription\nand GPT-3 to summarize them into the ePuskemas medical records format. This\nsystem is implemented as an add-on to an existing web browser extension,\nallowing doctors to fill out patient forms while talking. By leveraging this\nsolution for real-time transcription, translation, and summarization, doctors\ncan improve the turnaround time for patient care while enhancing the quality of\nrecords, which become more detailed and insightful for future visits. This\ninnovation addresses challenges like overcrowded facilities and the\nadministrative burden on healthcare providers in Indonesia. We believe this\nsolution will help doctors save time, provide better care, and produce more\naccurate medical records, representing a significant step toward modernizing\nhealthcare and ensuring patients receive timely, high-quality care, even in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Azmul Asmar Irfan"
                    },
                    {
                        "name": "Nur Ahmad Khatim"
                    },
                    {
                        "name": "Mansur M. Arief"
                    }
                ],
                "author_detail": {
                    "name": "Mansur M. Arief"
                },
                "author": "Mansur M. Arief",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17044v1",
                "updated": "2024-09-25T15:54:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not"
                },
                "summary": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM."
                },
                "authors": [
                    {
                        "name": "Francesco Verdini"
                    },
                    {
                        "name": "Pierfrancesco Melucci"
                    },
                    {
                        "name": "Stefano Perna"
                    },
                    {
                        "name": "Francesco Cariaggi"
                    },
                    {
                        "name": "Marco Gaido"
                    },
                    {
                        "name": "Sara Papi"
                    },
                    {
                        "name": "Szymon Mazurek"
                    },
                    {
                        "name": "Marek Kasztelnik"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    },
                    {
                        "name": "Sébastien Bratières"
                    },
                    {
                        "name": "Paolo Merialdo"
                    },
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14507v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14507v3",
                "updated": "2024-09-25T15:50:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    50,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-22T16:11:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    11,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse\n  Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose\nthe activations of Large Language Models (LLMs) into human-interpretable\nlatents. In this paper, we pose two questions. First, to what extent do SAEs\nextract monosemantic and interpretable latents? Second, to what extent does\nvarying the sparsity or the size of the SAE affect monosemanticity /\ninterpretability? By investigating these questions in the context of a simple\nfirst-letter identification task where we have complete access to ground truth\nlabels for all tokens in the vocabulary, we are able to provide more detail\nthan prior investigations. Critically, we identify a problematic form of\nfeature-splitting we call feature absorption where seemingly monosemantic\nlatents fail to fire in cases where they clearly should. Our investigation\nsuggests that varying SAE size or sparsity is insufficient to solve this issue,\nand that there are deeper conceptual issues in need of resolution."
                },
                "authors": [
                    {
                        "name": "David Chanin"
                    },
                    {
                        "name": "James Wilken-Smith"
                    },
                    {
                        "name": "Tomáš Dulka"
                    },
                    {
                        "name": "Hardik Bhatnagar"
                    },
                    {
                        "name": "Joseph Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Bloom"
                },
                "author": "Joseph Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14507v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07368v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07368v3",
                "updated": "2024-09-25T15:17:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    17,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-11T15:56:15Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    56,
                    15,
                    2,
                    255,
                    0
                ],
                "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation\n  of Code"
                },
                "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: https://sgcode.codes/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces SGCode, a flexible prompt-optimizing system to generate\nsecure code with large language models (LLMs). SGCode integrates recent\nprompt-optimization approaches with LLMs in a unified system accessible through\nfront-end and back-end APIs, enabling users to 1) generate secure code, which\nis free of vulnerabilities, 2) review and share security analysis, and 3)\neasily switch from one prompt optimization approach to another, while providing\ninsights on model and system performance. We populated SGCode on an AWS server\nwith PromSec, an approach that optimizes prompts by combining an LLM and\nsecurity tools with a lightweight generative adversarial graph neural network\nto detect and fix security vulnerabilities in the generated code. Extensive\nexperiments show that SGCode is practical as a public tool to gain insights\ninto the trade-offs between model utility, secure code generation, and system\ncost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is\navailable at: https://sgcode.codes/."
                },
                "authors": [
                    {
                        "name": "Khiem Ton"
                    },
                    {
                        "name": "Nhi Nguyen"
                    },
                    {
                        "name": "Mahmoud Nazzal"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "Cristian Borcea"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Issa Khalil"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_doi": "10.1145/3658644.3691367",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3691367",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.07368v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07368v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17011v1",
                "updated": "2024-09-25T15:15:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    15,
                    57,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T15:15:57Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    15,
                    57,
                    2,
                    269,
                    0
                ],
                "title": "LLM-CARD: Towards a Description and Landscape of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-CARD: Towards a Description and Landscape of Large Language Models"
                },
                "summary": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As an increasing number of papers are presented, researchers and\ndevelopers face the challenge of information overload. Thus, it is particularly\nimportant to develop a system that can automatically extract and organise key\ninformation about LLMs from academic papers (\\textbf{LLM model card}). This\nwork is to develop such a pioneer system by using Named Entity Recognition\n(\\textbf{NER}) and Relation Extraction (\\textbf{RE}) methods that automatically\nextract key information about large language models from the papers, helping\nresearchers to efficiently access information about LLMs. These features\ninclude model \\textit{licence}, model \\textit{name}, and model\n\\textit{application}. With these features, we can form a model card for each\npaper. \\textbf{Data-contribution} wise, 106 academic papers were processed by\ndefining three dictionaries - LLMs name, licence, and application. 11,051\nsentences were extracted through dictionary lookup, and the dataset was\nconstructed through manual review of the final selection of 129 sentences that\nhave a link between the name and the licence, and 106 sentences that have a\nlink between the model name and the application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of the Natural Language Processing (NLP) field, a vast\nvariety of Large Language Models (LLMs) continue to emerge for diverse NLP\ntasks. As an increasing number of papers are presented, researchers and\ndevelopers face the challenge of information overload. Thus, it is particularly\nimportant to develop a system that can automatically extract and organise key\ninformation about LLMs from academic papers (\\textbf{LLM model card}). This\nwork is to develop such a pioneer system by using Named Entity Recognition\n(\\textbf{NER}) and Relation Extraction (\\textbf{RE}) methods that automatically\nextract key information about large language models from the papers, helping\nresearchers to efficiently access information about LLMs. These features\ninclude model \\textit{licence}, model \\textit{name}, and model\n\\textit{application}. With these features, we can form a model card for each\npaper. \\textbf{Data-contribution} wise, 106 academic papers were processed by\ndefining three dictionaries - LLMs name, licence, and application. 11,051\nsentences were extracted through dictionary lookup, and the dataset was\nconstructed through manual review of the final selection of 129 sentences that\nhave a link between the name and the licence, and 106 sentences that have a\nlink between the model name and the application."
                },
                "authors": [
                    {
                        "name": "Shengwei Tian"
                    },
                    {
                        "name": "Lifeng Han"
                    },
                    {
                        "name": "Erick Mendez Guzman"
                    },
                    {
                        "name": "Goran Nenadic"
                    }
                ],
                "author_detail": {
                    "name": "Goran Nenadic"
                },
                "author": "Goran Nenadic",
                "arxiv_comment": "ongoing work, 16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16997v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16997v2",
                "updated": "2024-09-26T06:13:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    6,
                    13,
                    4,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-25T15:02:25Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    15,
                    2,
                    25,
                    2,
                    269,
                    0
                ],
                "title": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INT-FlashAttention: Enabling Flash Attention for INT8 Quantization"
                },
                "summary": "As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the foundation of large language models (LLMs), self-attention module\nfaces the challenge of quadratic time and memory complexity with respect to\nsequence length. FlashAttention accelerates attention computation and reduces\nits memory usage by leveraging the GPU memory hierarchy. A promising research\ndirection is to integrate FlashAttention with quantization methods. This paper\nintroduces INT-FlashAttention, the first INT8 quantization architecture\ncompatible with the forward workflow of FlashAttention, which significantly\nimproves the inference speed of FlashAttention on Ampere GPUs. We implement our\nINT-FlashAttention prototype with fully INT8 activations and general\nmatrix-multiplication (GEMM) kernels, making it the first attention operator\nwith fully INT8 input. As a general token-level post-training quantization\nframework, INT-FlashAttention is also compatible with other data formats like\nINT4, etc. Experimental results show INT-FlashAttention achieves 72% faster\ninference speed and 82% smaller quantization error compared to standard\nFlashAttention with FP16 and FP8 data format."
                },
                "authors": [
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Zhiying Wu"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Peizhuang Cong"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Yuhan Wu"
                    },
                    {
                        "name": "Lei Su"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16997v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16997v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03589v3",
                "updated": "2024-09-25T14:59:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    59,
                    24,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-05T19:14:21Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    19,
                    14,
                    21,
                    2,
                    157,
                    0
                ],
                "title": "Ranking Manipulation for Conversational Search Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ranking Manipulation for Conversational Search Engines"
                },
                "summary": "Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity$.$ai. Given the strong\nfinancial incentive for website owners to boost their search ranking, we argue\nthat our problem formulation is of critical importance for future robustness\nwork.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Major search engine providers are rapidly incorporating Large Language Model\n(LLM)-generated content in response to user queries. These conversational\nsearch engines operate by loading retrieved website text into the LLM context\nfor summarization and interpretation. Recent research demonstrates that LLMs\nare highly vulnerable to jailbreaking and prompt injection attacks, which\ndisrupt the safety and quality goals of LLMs using adversarial strings. This\nwork investigates the impact of prompt injections on the ranking order of\nsources referenced by conversational search engines. To this end, we introduce\na focused dataset of real-world consumer product websites and formalize\nconversational search ranking as an adversarial problem. Experimentally, we\nanalyze conversational search rankings in the absence of adversarial injections\nand show that different LLMs vary significantly in prioritizing product name,\ndocument content, and context position. We then present a tree-of-attacks-based\njailbreaking technique which reliably promotes low-ranked products.\nImportantly, these attacks transfer effectively to state-of-the-art\nconversational search engines such as perplexity$.$ai. Given the strong\nfinancial incentive for website owners to boost their search ranking, we argue\nthat our problem formulation is of critical importance for future robustness\nwork."
                },
                "authors": [
                    {
                        "name": "Samuel Pfrommer"
                    },
                    {
                        "name": "Yatong Bai"
                    },
                    {
                        "name": "Tanmay Gautam"
                    },
                    {
                        "name": "Somayeh Sojoudi"
                    }
                ],
                "author_detail": {
                    "name": "Somayeh Sojoudi"
                },
                "author": "Somayeh Sojoudi",
                "arxiv_comment": "2024 Conference on Empirical Methods in Natural Language Processing\n  (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16984v1",
                "updated": "2024-09-25T14:45:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    45,
                    52,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:45:52Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    45,
                    52,
                    2,
                    269,
                    0
                ],
                "title": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXCEL: Automated eXplainable Consistency Evaluation using LLMs"
                },
                "summary": "Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in both industry and academia\nfor various tasks, yet evaluating the consistency of generated text responses\ncontinues to be a challenge. Traditional metrics like ROUGE and BLEU show a\nweak correlation with human judgment. More sophisticated metrics using Natural\nLanguage Inference (NLI) have shown improved correlations but are complex to\nimplement, require domain-specific training due to poor cross-domain\ngeneralization, and lack explainability. More recently, prompt-based metrics\nusing LLMs as evaluators have emerged; while they are easier to implement, they\nstill lack explainability and depend on task-specific prompts, which limits\ntheir generalizability. This work introduces Automated eXplainable Consistency\nEvaluation using LLMs (AXCEL), a prompt-based consistency metric which offers\nexplanations for the consistency scores by providing detailed reasoning and\npinpointing inconsistent text spans. AXCEL is also a generalizable metric which\ncan be adopted to multiple tasks without changing the prompt. AXCEL outperforms\nboth non-prompt and prompt-based state-of-the-art (SOTA) metrics in detecting\ninconsistencies across summarization by 8.7%, free text generation by 6.2%, and\ndata-to-text conversion tasks by 29.4%. We also evaluate the influence of\nunderlying LLMs on prompt based metric performance and recalibrate the SOTA\nprompt-based metrics with the latest LLMs for fair comparison. Further, we show\nthat AXCEL demonstrates strong performance using open source LLMs."
                },
                "authors": [
                    {
                        "name": "P Aditya Sreekar"
                    },
                    {
                        "name": "Sahil Verma"
                    },
                    {
                        "name": "Suransh Chopra"
                    },
                    {
                        "name": "Sarik Ghazarian"
                    },
                    {
                        "name": "Abhishek Persad"
                    },
                    {
                        "name": "Narayanan Sadagopan"
                    }
                ],
                "author_detail": {
                    "name": "Narayanan Sadagopan"
                },
                "author": "Narayanan Sadagopan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11728v2",
                "updated": "2024-09-25T14:41:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    41,
                    47,
                    2,
                    269,
                    0
                ],
                "published": "2024-01-22T07:10:06Z",
                "published_parsed": [
                    2024,
                    1,
                    22,
                    7,
                    10,
                    6,
                    0,
                    22,
                    0
                ],
                "title": "The Design and Construction of the Chips Water Cherenkov Neutrino\n  Detector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Design and Construction of the Chips Water Cherenkov Neutrino\n  Detector"
                },
                "summary": "CHIPS (CHerenkov detectors In mine PitS) was a prototype large-scale water\nCherenkov detector located in northern Minnesota. The main aim of the R&D\nproject was to demonstrate that construction costs of neutrino oscillation\ndetectors could be reduced by at least an order of magnitude compared to other\nequivalent experiments. This article presents design features of the CHIPS\ndetector along with details of the implementation and deployment of the\nprototype. While issues during and after the deployment of the detector\nprevented data taking, a number of key concepts and designs were successfully\ndemonstrated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIPS (CHerenkov detectors In mine PitS) was a prototype large-scale water\nCherenkov detector located in northern Minnesota. The main aim of the R&D\nproject was to demonstrate that construction costs of neutrino oscillation\ndetectors could be reduced by at least an order of magnitude compared to other\nequivalent experiments. This article presents design features of the CHIPS\ndetector along with details of the implementation and deployment of the\nprototype. While issues during and after the deployment of the detector\nprevented data taking, a number of key concepts and designs were successfully\ndemonstrated."
                },
                "authors": [
                    {
                        "name": "B. Alonso Rancurel"
                    },
                    {
                        "name": "N. Angelides"
                    },
                    {
                        "name": "G. Augustoni"
                    },
                    {
                        "name": "S. Bash"
                    },
                    {
                        "name": "B. Bergmann"
                    },
                    {
                        "name": "N. Bertschinger"
                    },
                    {
                        "name": "P. Bizouard"
                    },
                    {
                        "name": "M. Campbell"
                    },
                    {
                        "name": "S. Cao"
                    },
                    {
                        "name": "T. J. Carroll"
                    },
                    {
                        "name": "R. Castellan"
                    },
                    {
                        "name": "E. Catano-Mur"
                    },
                    {
                        "name": "J. P. Cesar"
                    },
                    {
                        "name": "J. A. B. Coelho"
                    },
                    {
                        "name": "P. Dills"
                    },
                    {
                        "name": "T. Dodwell"
                    },
                    {
                        "name": "J. Edmondson"
                    },
                    {
                        "name": "D. van Eijk"
                    },
                    {
                        "name": "Q. Fetterly"
                    },
                    {
                        "name": "Z. Garbal"
                    },
                    {
                        "name": "S. Germani"
                    },
                    {
                        "name": "T. Gilpin"
                    },
                    {
                        "name": "A. Giraudo"
                    },
                    {
                        "name": "A. Habig"
                    },
                    {
                        "name": "D. Hanuska"
                    },
                    {
                        "name": "H. Hausner"
                    },
                    {
                        "name": "W. Y. Hernandez"
                    },
                    {
                        "name": "A. Holin"
                    },
                    {
                        "name": "J. Huang"
                    },
                    {
                        "name": "S. B. Jones"
                    },
                    {
                        "name": "A. Karle"
                    },
                    {
                        "name": "G. Kileff"
                    },
                    {
                        "name": "K. R. Jenkins"
                    },
                    {
                        "name": "P. Kooijman"
                    },
                    {
                        "name": "A. Kreymer"
                    },
                    {
                        "name": "D. A. Loving"
                    },
                    {
                        "name": "G. M. LaFond"
                    },
                    {
                        "name": "K. Lang"
                    },
                    {
                        "name": "J. P. Lazar"
                    },
                    {
                        "name": "R. Li"
                    },
                    {
                        "name": "K. Liu"
                    },
                    {
                        "name": "P. Mánek"
                    },
                    {
                        "name": "M. L. Marshak"
                    },
                    {
                        "name": "J. R. Meier"
                    },
                    {
                        "name": "W. Miller"
                    },
                    {
                        "name": "J. K. Nelson"
                    },
                    {
                        "name": "C. Ng"
                    },
                    {
                        "name": "R. J. Nichol"
                    },
                    {
                        "name": "V. Paolone"
                    },
                    {
                        "name": "A. Perch"
                    },
                    {
                        "name": "M. M. Pfützner"
                    },
                    {
                        "name": "A. Radovic"
                    },
                    {
                        "name": "K. Rawlins"
                    },
                    {
                        "name": "P. Roedl"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "I. Safa"
                    },
                    {
                        "name": "A. Sousa"
                    },
                    {
                        "name": "J. Tingey"
                    },
                    {
                        "name": "J. Thomas"
                    },
                    {
                        "name": "J. Trokan-Tenorio"
                    },
                    {
                        "name": "P. Vahle"
                    },
                    {
                        "name": "R. Wade"
                    },
                    {
                        "name": "C. Wendt"
                    },
                    {
                        "name": "D. Wendt"
                    },
                    {
                        "name": "L. H. Whitehead"
                    },
                    {
                        "name": "S. Wolcott"
                    },
                    {
                        "name": "T. Yuan"
                    }
                ],
                "author_detail": {
                    "name": "T. Yuan"
                },
                "author": "T. Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16974v1",
                "updated": "2024-09-25T14:36:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    36,
                    30,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:36:30Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    36,
                    30,
                    2,
                    269,
                    0
                ],
                "title": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Large-Language Models: A Systematic Overview of Socio-Technical\n  Impacts, Constraints, and Emerging Questions"
                },
                "summary": "There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been rapid advancements in the capabilities of large language\nmodels (LLMs) in recent years, greatly revolutionizing the field of natural\nlanguage processing (NLP) and artificial intelligence (AI) to understand and\ninteract with human language. Therefore, in this work, we conduct a systematic\ninvestigation of the literature to identify the prominent themes and directions\nof LLM developments, impacts, and limitations. Our findings illustrate the\naims, methodologies, limitations, and future directions of LLM research. It\nincludes responsible development considerations, algorithmic improvements,\nethical challenges, and societal implications of LLM development. Overall, this\npaper provides a rigorous and comprehensive overview of current research in LLM\nand identifies potential directions for future development. The article\nhighlights the application areas that could have a positive impact on society\nalong with the ethical considerations."
                },
                "authors": [
                    {
                        "name": "Zeyneb N. Kaya"
                    },
                    {
                        "name": "Souvick Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Souvick Ghosh"
                },
                "author": "Souvick Ghosh",
                "arxiv_comment": "28 pages, 5 figures, preprint submitted to journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16973v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16973v1",
                "updated": "2024-09-25T14:35:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    35,
                    6,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:35:06Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    35,
                    6,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM\n  Personalization"
                },
                "summary": "Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized how we interact with\ntechnology, but their personalization to individual user preferences remains a\nsignificant challenge, particularly in on-device applications. Traditional\nmethods often depend heavily on labeled datasets and can be resource-intensive.\nTo address these issues, we present Adaptive Self-Supervised Learning\nStrategies (ASLS), which utilizes self-supervised learning techniques to\npersonalize LLMs dynamically. The framework comprises a user profiling layer\nfor collecting interaction data and a neural adaptation layer for real-time\nmodel fine-tuning. This innovative approach enables continuous learning from\nuser feedback, allowing the model to generate responses that align closely with\nuser-specific contexts. The adaptive mechanisms of ASLS minimize computational\ndemands and enhance personalization efficiency. Experimental results across\nvarious user scenarios illustrate the superior performance of ASLS in boosting\nuser engagement and satisfaction, highlighting its potential to redefine LLMs\nas highly responsive and context-aware systems on-device."
                },
                "authors": [
                    {
                        "name": "Rafael Mendoza"
                    },
                    {
                        "name": "Isabella Cruz"
                    },
                    {
                        "name": "Richard Liu"
                    },
                    {
                        "name": "Aarav Deshmukh"
                    },
                    {
                        "name": "David Williams"
                    },
                    {
                        "name": "Jesscia Peng"
                    },
                    {
                        "name": "Rohan Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Rohan Iyer"
                },
                "author": "Rohan Iyer",
                "arxiv_comment": "First ASLS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16973v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16973v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16972v1",
                "updated": "2024-09-25T14:32:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    32,
                    59,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:32:59Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    32,
                    59,
                    2,
                    269,
                    0
                ],
                "title": "Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial\n  SLAM Configurable for LiDARs or Depth Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial\n  SLAM Configurable for LiDARs or Depth Cameras"
                },
                "summary": "Autonomous exploration of unknown space is an essential component for the\ndeployment of mobile robots in the real world. Safe navigation is crucial for\nall robotics applications and requires accurate and consistent maps of the\nrobot's surroundings. To achieve full autonomy and allow deployment in a wide\nvariety of environments, the robot must rely on on-board state estimation which\nis prone to drift over time. We propose a Micro Aerial Vehicle (MAV)\nexploration framework based on local submaps to allow retaining global\nconsistency by applying loop-closure corrections to the relative submap poses.\nTo enable large-scale exploration we efficiently compute global,\nenvironment-wide frontiers from the local submap frontiers and use a\nsampling-based next-best-view exploration planner. Our method seamlessly\nsupports using either a LiDAR sensor or a depth camera, making it suitable for\ndifferent kinds of MAV platforms. We perform comparative evaluations in\nsimulation against a state-of-the-art submap-based exploration framework to\nshowcase the efficiency and reconstruction quality of our approach. Finally, we\ndemonstrate the applicability of our method to real-world MAVs, one equipped\nwith a LiDAR and the other with a depth camera. Video available at\nhttps://youtu.be/Uf5fwmYcuq4 .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous exploration of unknown space is an essential component for the\ndeployment of mobile robots in the real world. Safe navigation is crucial for\nall robotics applications and requires accurate and consistent maps of the\nrobot's surroundings. To achieve full autonomy and allow deployment in a wide\nvariety of environments, the robot must rely on on-board state estimation which\nis prone to drift over time. We propose a Micro Aerial Vehicle (MAV)\nexploration framework based on local submaps to allow retaining global\nconsistency by applying loop-closure corrections to the relative submap poses.\nTo enable large-scale exploration we efficiently compute global,\nenvironment-wide frontiers from the local submap frontiers and use a\nsampling-based next-best-view exploration planner. Our method seamlessly\nsupports using either a LiDAR sensor or a depth camera, making it suitable for\ndifferent kinds of MAV platforms. We perform comparative evaluations in\nsimulation against a state-of-the-art submap-based exploration framework to\nshowcase the efficiency and reconstruction quality of our approach. Finally, we\ndemonstrate the applicability of our method to real-world MAVs, one equipped\nwith a LiDAR and the other with a depth camera. Video available at\nhttps://youtu.be/Uf5fwmYcuq4 ."
                },
                "authors": [
                    {
                        "name": "Sotiris Papatheodorou"
                    },
                    {
                        "name": "Simon Boche"
                    },
                    {
                        "name": "Sebastián Barbas Laina"
                    },
                    {
                        "name": "Stefan Leutenegger"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Leutenegger"
                },
                "author": "Stefan Leutenegger",
                "arxiv_comment": "7 pages, 8 figures, for the accompanying video see\n  https://youtu.be/Uf5fwmYcuq4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16949v1",
                "updated": "2024-09-25T14:02:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    2,
                    43,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T14:02:43Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    14,
                    2,
                    43,
                    2,
                    269,
                    0
                ],
                "title": "DALDA: Data Augmentation Leveraging Diffusion Model and LLM with\n  Adaptive Guidance Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DALDA: Data Augmentation Leveraging Diffusion Model and LLM with\n  Adaptive Guidance Scaling"
                },
                "summary": "In this paper, we present an effective data augmentation framework leveraging\nthe Large Language Model (LLM) and Diffusion Model (DM) to tackle the\nchallenges inherent in data-scarce scenarios. Recently, DMs have opened up the\npossibility of generating synthetic images to complement a few training images.\nHowever, increasing the diversity of synthetic images also raises the risk of\ngenerating samples outside the target distribution. Our approach addresses this\nissue by embedding novel semantic information into text prompts via LLM and\nutilizing real images as visual prompts, thus generating semantically rich\nimages. To ensure that the generated images remain within the target\ndistribution, we dynamically adjust the guidance weight based on each image's\nCLIPScore to control the diversity. Experimental results show that our method\nproduces synthetic images with enhanced diversity while maintaining adherence\nto the target distribution. Consequently, our approach proves to be more\nefficient in the few-shot setting on several benchmarks. Our code is available\nat https://github.com/kkyuhun94/dalda .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present an effective data augmentation framework leveraging\nthe Large Language Model (LLM) and Diffusion Model (DM) to tackle the\nchallenges inherent in data-scarce scenarios. Recently, DMs have opened up the\npossibility of generating synthetic images to complement a few training images.\nHowever, increasing the diversity of synthetic images also raises the risk of\ngenerating samples outside the target distribution. Our approach addresses this\nissue by embedding novel semantic information into text prompts via LLM and\nutilizing real images as visual prompts, thus generating semantically rich\nimages. To ensure that the generated images remain within the target\ndistribution, we dynamically adjust the guidance weight based on each image's\nCLIPScore to control the diversity. Experimental results show that our method\nproduces synthetic images with enhanced diversity while maintaining adherence\nto the target distribution. Consequently, our approach proves to be more\nefficient in the few-shot setting on several benchmarks. Our code is available\nat https://github.com/kkyuhun94/dalda ."
                },
                "authors": [
                    {
                        "name": "Kyuheon Jung"
                    },
                    {
                        "name": "Yongdeuk Seo"
                    },
                    {
                        "name": "Seongwoo Cho"
                    },
                    {
                        "name": "Jaeyoung Kim"
                    },
                    {
                        "name": "Hyun-seok Min"
                    },
                    {
                        "name": "Sungchul Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungchul Choi"
                },
                "author": "Sungchul Choi",
                "arxiv_comment": "Accepted to ECCV Synthetic Data for Computer Vision Workshop (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19280v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19280v3",
                "updated": "2024-09-25T13:36:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    36,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-27T15:50:41Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    50,
                    41,
                    3,
                    179,
                    0
                ],
                "title": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into\n  Multimodal LLMs at Scale"
                },
                "summary": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs."
                },
                "authors": [
                    {
                        "name": "Junying Chen"
                    },
                    {
                        "name": "Chi Gui"
                    },
                    {
                        "name": "Ruyi Ouyang"
                    },
                    {
                        "name": "Anningzhe Gao"
                    },
                    {
                        "name": "Shunian Chen"
                    },
                    {
                        "name": "Guiming Hardy Chen"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Ruifei Zhang"
                    },
                    {
                        "name": "Zhenyang Cai"
                    },
                    {
                        "name": "Ke Ji"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Benyou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Benyou Wang"
                },
                "author": "Benyou Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19280v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19280v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16914v1",
                "updated": "2024-09-25T13:18:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    18,
                    57,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:18:57Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    18,
                    57,
                    2,
                    269,
                    0
                ],
                "title": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness"
                },
                "summary": "The increasing capability and widespread usage of large language models\n(LLMs) highlight the desirability of automatic detection of LLM-generated text.\nZero-shot detectors, due to their training-free nature, have received\nconsiderable attention and notable success. In this paper, we identify a new\nfeature, token cohesiveness, that is useful for zero-shot detection, and we\ndemonstrate that LLM-generated text tends to exhibit higher token cohesiveness\nthan human-written text. Based on this observation, we devise TOCSIN, a generic\ndual-channel detection paradigm that uses token cohesiveness as a plug-and-play\nmodule to improve existing zero-shot detectors. To calculate token\ncohesiveness, TOCSIN only requires a few rounds of random token deletion and\nsemantic difference measurement, making it particularly suitable for a\npractical black-box setting where the source model used for generation is not\naccessible. Extensive experiments with four state-of-the-art base detectors on\nvarious datasets, source models, and evaluation settings demonstrate the\neffectiveness and generality of the proposed approach. Code available at:\n\\url{https://github.com/Shixuan-Ma/TOCSIN}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capability and widespread usage of large language models\n(LLMs) highlight the desirability of automatic detection of LLM-generated text.\nZero-shot detectors, due to their training-free nature, have received\nconsiderable attention and notable success. In this paper, we identify a new\nfeature, token cohesiveness, that is useful for zero-shot detection, and we\ndemonstrate that LLM-generated text tends to exhibit higher token cohesiveness\nthan human-written text. Based on this observation, we devise TOCSIN, a generic\ndual-channel detection paradigm that uses token cohesiveness as a plug-and-play\nmodule to improve existing zero-shot detectors. To calculate token\ncohesiveness, TOCSIN only requires a few rounds of random token deletion and\nsemantic difference measurement, making it particularly suitable for a\npractical black-box setting where the source model used for generation is not\naccessible. Extensive experiments with four state-of-the-art base detectors on\nvarious datasets, source models, and evaluation settings demonstrate the\neffectiveness and generality of the proposed approach. Code available at:\n\\url{https://github.com/Shixuan-Ma/TOCSIN}."
                },
                "authors": [
                    {
                        "name": "Shixuan Ma"
                    },
                    {
                        "name": "Quan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Quan Wang"
                },
                "author": "Quan Wang",
                "arxiv_comment": "To appear at the main conference of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16909v1",
                "updated": "2024-09-25T13:13:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    13,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:13:21Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    13,
                    21,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Sensitivity and Reasoning for Time-Sensitive Question\n  Answering"
                },
                "summary": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities.In this paper,\nwe propose a novel framework that enhances temporal awareness and reasoning\nthrough Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Sensitive Question Answering (TSQA) demands the effective utilization of\nspecific temporal contexts, encompassing multiple time-evolving facts, to\naddress time-sensitive questions. This necessitates not only the parsing of\ntemporal information within questions but also the identification and\nunderstanding of time-evolving facts to generate accurate answers. However,\ncurrent large language models still have limited sensitivity to temporal\ninformation and their inadequate temporal reasoning capabilities.In this paper,\nwe propose a novel framework that enhances temporal awareness and reasoning\nthrough Temporal Information-Aware Embedding and Granular Contrastive\nReinforcement Learning. Experimental results on four TSQA datasets demonstrate\nthat our framework significantly outperforms existing LLMs in TSQA tasks,\nmarking a step forward in bridging the performance gap between machine and\nhuman temporal understanding and reasoning."
                },
                "authors": [
                    {
                        "name": "Wanqi Yang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_comment": "Accepted by EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16900v1",
                "updated": "2024-09-25T13:09:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    9,
                    23,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T13:09:23Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    9,
                    23,
                    2,
                    269,
                    0
                ],
                "title": "A Roadmap for Embodied and Social Grounding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Roadmap for Embodied and Social Grounding in LLMs"
                },
                "summary": "The fusion of Large Language Models (LLMs) and robotic systems has led to a\ntransformative paradigm in the robotic field, offering unparalleled\ncapabilities not only in the communication domain but also in skills like\nmultimodal input handling, high-level reasoning, and plan generation. The\ngrounding of LLMs knowledge into the empirical world has been considered a\ncrucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,\nconnecting LLMs' representations to the external world with multimodal\napproaches or with robots' bodies is not enough to let them understand the\nmeaning of the language they are manipulating. Taking inspiration from humans,\nthis work draws attention to three necessary elements for an agent to grasp and\nexperience the world. The roadmap for LLMs grounding is envisaged in an active\nbodily system as the reference point for experiencing the environment, a\ntemporally structured experience for a coherent, self-related interaction with\nthe external world, and social skills to acquire a common-grounded shared\nexperience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fusion of Large Language Models (LLMs) and robotic systems has led to a\ntransformative paradigm in the robotic field, offering unparalleled\ncapabilities not only in the communication domain but also in skills like\nmultimodal input handling, high-level reasoning, and plan generation. The\ngrounding of LLMs knowledge into the empirical world has been considered a\ncrucial pathway to exploit the efficiency of LLMs in robotics. Nevertheless,\nconnecting LLMs' representations to the external world with multimodal\napproaches or with robots' bodies is not enough to let them understand the\nmeaning of the language they are manipulating. Taking inspiration from humans,\nthis work draws attention to three necessary elements for an agent to grasp and\nexperience the world. The roadmap for LLMs grounding is envisaged in an active\nbodily system as the reference point for experiencing the environment, a\ntemporally structured experience for a coherent, self-related interaction with\nthe external world, and social skills to acquire a common-grounded shared\nexperience."
                },
                "authors": [
                    {
                        "name": "Sara Incao"
                    },
                    {
                        "name": "Carlo Mazzola"
                    },
                    {
                        "name": "Giulia Belgiovine"
                    },
                    {
                        "name": "Alessandra Sciutti"
                    }
                ],
                "author_detail": {
                    "name": "Alessandra Sciutti"
                },
                "author": "Alessandra Sciutti",
                "arxiv_comment": "Accepted Version of a conference paper presented at Robophilosophy\n  Conference 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.9; J.4; F.3.2; D.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09285v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09285v2",
                "updated": "2024-09-25T13:06:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    13,
                    6,
                    53,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-17T20:13:34Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    20,
                    13,
                    34,
                    5,
                    230,
                    0
                ],
                "title": "Evaluating Usability and Engagement of Large Language Models in Virtual\n  Reality for Traditional Scottish Curling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Usability and Engagement of Large Language Models in Virtual\n  Reality for Traditional Scottish Curling"
                },
                "summary": "This paper explores the innovative application of Large Language Models\n(LLMs) in Virtual Reality (VR) environments to promote heritage education,\nfocusing on traditional Scottish curling presented in the game ``Scottish\nBonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\npre-defined scripted chatbots, evaluating key criteria such as usability, user\nengagement, and learning outcomes. The results show that LLM-based chatbots\nsignificantly improve interactivity and engagement, creating a more dynamic and\nimmersive learning environment. This integration helps document and preserve\ncultural heritage and enhances dissemination processes, which are crucial for\nsafeguarding intangible cultural heritage (ICH) amid environmental changes.\nFurthermore, the study highlights the potential of novel technologies in\neducation to provide immersive experiences that foster a deeper appreciation of\ncultural heritage. These findings support the wider application of LLMs and VR\nin cultural education to address global challenges and promote sustainable\npractices to preserve and enhance cultural heritage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the innovative application of Large Language Models\n(LLMs) in Virtual Reality (VR) environments to promote heritage education,\nfocusing on traditional Scottish curling presented in the game ``Scottish\nBonspiel VR''. Our study compares the effectiveness of LLM-based chatbots with\npre-defined scripted chatbots, evaluating key criteria such as usability, user\nengagement, and learning outcomes. The results show that LLM-based chatbots\nsignificantly improve interactivity and engagement, creating a more dynamic and\nimmersive learning environment. This integration helps document and preserve\ncultural heritage and enhances dissemination processes, which are crucial for\nsafeguarding intangible cultural heritage (ICH) amid environmental changes.\nFurthermore, the study highlights the potential of novel technologies in\neducation to provide immersive experiences that foster a deeper appreciation of\ncultural heritage. These findings support the wider application of LLMs and VR\nin cultural education to address global challenges and promote sustainable\npractices to preserve and enhance cultural heritage."
                },
                "authors": [
                    {
                        "name": "Ka Hei Carrie Lau"
                    },
                    {
                        "name": "Efe Bozkir"
                    },
                    {
                        "name": "Hong Gao"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09285v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09285v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16879v1",
                "updated": "2024-09-25T12:44:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    44,
                    13,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T12:44:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    44,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRACE: Generating Socially Appropriate Robot Actions Leveraging LLMs and\n  Human Explanations"
                },
                "summary": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from Large Language Models (LLMs), and it\nintegrates this knowledge with human explanations through a generative network\narchitecture. The bidirectional structure of GRACE enables robots to refine and\nenhance LLM predictions by utilizing human explanations and makes robots\ncapable of generating such explanations for human-specified actions. Our\nexperimental evaluations show that integrating human explanations boosts\nGRACE's performance, where it outperforms several baselines and provides\nsensible explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When operating in human environments, robots need to handle complex tasks\nwhile both adhering to social norms and accommodating individual preferences.\nFor instance, based on common sense knowledge, a household robot can predict\nthat it should avoid vacuuming during a social gathering, but it may still be\nuncertain whether it should vacuum before or after having guests. In such\ncases, integrating common-sense knowledge with human preferences, often\nconveyed through human explanations, is fundamental yet a challenge for\nexisting systems. In this paper, we introduce GRACE, a novel approach\naddressing this while generating socially appropriate robot actions. GRACE\nleverages common sense knowledge from Large Language Models (LLMs), and it\nintegrates this knowledge with human explanations through a generative network\narchitecture. The bidirectional structure of GRACE enables robots to refine and\nenhance LLM predictions by utilizing human explanations and makes robots\ncapable of generating such explanations for human-specified actions. Our\nexperimental evaluations show that integrating human explanations boosts\nGRACE's performance, where it outperforms several baselines and provides\nsensible explanations."
                },
                "authors": [
                    {
                        "name": "Fethiye Irmak Dogan"
                    },
                    {
                        "name": "Umut Ozyurt"
                    },
                    {
                        "name": "Gizem Cinar"
                    },
                    {
                        "name": "Hatice Gunes"
                    }
                ],
                "author_detail": {
                    "name": "Hatice Gunes"
                },
                "author": "Hatice Gunes",
                "arxiv_comment": "Under review for 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), Supplementary video: https://youtu.be/3gP3euwNBjQ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11022v3",
                "updated": "2024-09-25T12:33:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    33,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-17T09:32:12Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    9,
                    32,
                    12,
                    1,
                    261,
                    0
                ],
                "title": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEIC: Universal and Multilingual Named Entity Recognition with Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have supplanted traditional methods in numerous\nnatural language processing tasks. Nonetheless, in Named Entity Recognition\n(NER), existing LLM-based methods underperform compared to baselines and\nrequire significantly more computational resources, limiting their application.\nIn this paper, we introduce the task of generation-based extraction and\nin-context classification (GEIC), designed to leverage LLMs' prior knowledge\nand self-attention mechanisms for NER tasks. We then propose CascadeNER, a\nuniversal and multilingual GEIC framework for few-shot and zero-shot NER.\nCascadeNER employs model cascading to utilize two small-parameter LLMs to\nextract and classify independently, reducing resource consumption while\nenhancing accuracy. We also introduce AnythingNER, the first NER dataset\nspecifically designed for LLMs, including 8 languages, 155 entity types and a\nnovel dynamic categorization system. Experiments show that CascadeNER achieves\nstate-of-the-art performance on low-resource and fine-grained scenarios,\nincluding CrossNER and FewNERD. Our work is openly accessible."
                },
                "authors": [
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xuecheng Liu"
                    },
                    {
                        "name": "Tong Shang"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16867v1",
                "updated": "2024-09-25T12:32:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    32,
                    41,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T12:32:41Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    32,
                    41,
                    2,
                    269,
                    0
                ],
                "title": "Multi-objective Evolution of Heuristic Using Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-objective Evolution of Heuristic Using Large Language Model"
                },
                "summary": "Heuristics are commonly used to tackle diverse search and optimization\nproblems. Design heuristics usually require tedious manual crafting with domain\nknowledge. Recent works have incorporated large language models (LLMs) into\nautomatic heuristic search leveraging their powerful language and coding\ncapacity. However, existing research focuses on the optimal performance on the\ntarget problem as the sole objective, neglecting other criteria such as\nefficiency and scalability, which are vital in practice. To tackle this\nchallenge, we propose to model heuristic search as a multi-objective\noptimization problem and consider introducing other practical criteria beyond\noptimal performance. Due to the complexity of the search space, conventional\nmulti-objective optimization methods struggle to effectively handle\nmulti-objective heuristic search. We propose the first LLM-based\nmulti-objective heuristic search framework, Multi-objective Evolution of\nHeuristic (MEoH), which integrates LLMs in a zero-shot manner to generate a\nnon-dominated set of heuristics to meet multiple design criteria. We design a\nnew dominance-dissimilarity mechanism for effective population management and\nselection, which incorporates both code dissimilarity in the search space and\ndominance in the objective space. MEoH is demonstrated in two well-known\ncombinatorial optimization problems: the online Bin Packing Problem (BPP) and\nthe Traveling Salesman Problem (TSP). Results indicate that a variety of elite\nheuristics are automatically generated in a single run, offering more trade-off\noptions than existing methods. It successfully achieves competitive or superior\nperformance while improving efficiency up to 10 times. Moreover, we also\nobserve that the multi-objective search introduces novel insights into\nheuristic design and leads to the discovery of diverse heuristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heuristics are commonly used to tackle diverse search and optimization\nproblems. Design heuristics usually require tedious manual crafting with domain\nknowledge. Recent works have incorporated large language models (LLMs) into\nautomatic heuristic search leveraging their powerful language and coding\ncapacity. However, existing research focuses on the optimal performance on the\ntarget problem as the sole objective, neglecting other criteria such as\nefficiency and scalability, which are vital in practice. To tackle this\nchallenge, we propose to model heuristic search as a multi-objective\noptimization problem and consider introducing other practical criteria beyond\noptimal performance. Due to the complexity of the search space, conventional\nmulti-objective optimization methods struggle to effectively handle\nmulti-objective heuristic search. We propose the first LLM-based\nmulti-objective heuristic search framework, Multi-objective Evolution of\nHeuristic (MEoH), which integrates LLMs in a zero-shot manner to generate a\nnon-dominated set of heuristics to meet multiple design criteria. We design a\nnew dominance-dissimilarity mechanism for effective population management and\nselection, which incorporates both code dissimilarity in the search space and\ndominance in the objective space. MEoH is demonstrated in two well-known\ncombinatorial optimization problems: the online Bin Packing Problem (BPP) and\nthe Traveling Salesman Problem (TSP). Results indicate that a variety of elite\nheuristics are automatically generated in a single run, offering more trade-off\noptions than existing methods. It successfully achieves competitive or superior\nperformance while improving efficiency up to 10 times. Moreover, we also\nobserve that the multi-objective search introduces novel insights into\nheuristic design and leads to the discovery of diverse heuristics."
                },
                "authors": [
                    {
                        "name": "Shunyu Yao"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Zhenkun Wang"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02636v2",
                "updated": "2024-09-25T12:25:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    25,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-04T11:59:53Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    11,
                    59,
                    53,
                    2,
                    248,
                    0
                ],
                "title": "Mamba as a motion encoder for robotic imitation learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba as a motion encoder for robotic imitation learning"
                },
                "summary": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. This paper proposes using Mamba, a state-of-the-art architecture\nwith potential applications in LLMs, for robotic imitation learning,\nhighlighting its ability to function as an encoder that effectively captures\ncontextual information. By reducing the dimensionality of the state space,\nMamba operates similarly to an autoencoder. It effectively compresses the\nsequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in imitation learning, particularly with the integration\nof LLM techniques, are set to significantly improve robots' dexterity and\nadaptability. This paper proposes using Mamba, a state-of-the-art architecture\nwith potential applications in LLMs, for robotic imitation learning,\nhighlighting its ability to function as an encoder that effectively captures\ncontextual information. By reducing the dimensionality of the state space,\nMamba operates similarly to an autoencoder. It effectively compresses the\nsequential information into state variables while preserving the essential\ntemporal dynamics necessary for accurate motion prediction. Experimental\nresults in tasks such as cup placing and case loading demonstrate that despite\nexhibiting higher estimation errors, Mamba achieves superior success rates\ncompared to Transformers in practical task execution. This performance is\nattributed to Mamba's structure, which encompasses the state space model.\nAdditionally, the study investigates Mamba's capacity to serve as a real-time\nmotion generator with a limited amount of training data."
                },
                "authors": [
                    {
                        "name": "Toshiaki Tsuji"
                    }
                ],
                "author_detail": {
                    "name": "Toshiaki Tsuji"
                },
                "author": "Toshiaki Tsuji",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16860v1",
                "updated": "2024-09-25T12:15:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    15,
                    15,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T12:15:15Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    15,
                    15,
                    2,
                    269,
                    0
                ],
                "title": "The Role of Language Models in Modern Healthcare: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Language Models in Modern Healthcare: A Comprehensive Review"
                },
                "summary": "The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of large language models (LLMs) in healthcare has gained\nsignificant attention due to their ability to process complex medical data and\nprovide insights for clinical decision-making. These models have demonstrated\nsubstantial capabilities in understanding and generating natural language,\nwhich is crucial for medical documentation, diagnostics, and patient\ninteraction. This review examines the trajectory of language models from their\nearly stages to the current state-of-the-art LLMs, highlighting their strengths\nin healthcare applications and discussing challenges such as data privacy,\nbias, and ethical considerations. The potential of LLMs to enhance healthcare\ndelivery is explored, alongside the necessary steps to ensure their ethical and\neffective integration into medical practice."
                },
                "authors": [
                    {
                        "name": "Amna Khalid"
                    },
                    {
                        "name": "Ayma Khalid"
                    },
                    {
                        "name": "Umar Khalid"
                    }
                ],
                "author_detail": {
                    "name": "Umar Khalid"
                },
                "author": "Umar Khalid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13781v2",
                "updated": "2024-09-25T12:10:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    12,
                    10,
                    22,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-25T09:22:07Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    9,
                    22,
                    7,
                    6,
                    238,
                    0
                ],
                "title": "GenOnet: Generative Open xG Network Simulation with Multi-Agent LLM and\n  ns-3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenOnet: Generative Open xG Network Simulation with Multi-Agent LLM and\n  ns-3"
                },
                "summary": "The move toward Sixth-Generation (6G) networks relies on open interfaces and\nprotocols for seamless interoperability across devices, vendors, and\ntechnologies. In this context, open 6G development involves multiple\ndisciplines and requires advanced simulation approaches for testing. In this\ndemo paper, we propose a generative simulation approach based on a multi-agent\nLarge Language Model (LLM) and Network Simulator 3 (ns-3), called Generative\nOpen xG Network Simulation (GenOnet), to effectively generate, debug, execute,\nand interpret simulated Open Fifth-Generation (5G) environments. The first\nversion of GenOnet application represents a specialized adaptation of the\nOpenAI GPT models. It incorporates supplementary tools, agents, 5G standards,\nand seamless integration with ns-3 simulation capabilities, supporting both C++\nvariants and Python implementations. This release complies with the latest Open\nRadio Access Network (O-RAN) and 3GPP standards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The move toward Sixth-Generation (6G) networks relies on open interfaces and\nprotocols for seamless interoperability across devices, vendors, and\ntechnologies. In this context, open 6G development involves multiple\ndisciplines and requires advanced simulation approaches for testing. In this\ndemo paper, we propose a generative simulation approach based on a multi-agent\nLarge Language Model (LLM) and Network Simulator 3 (ns-3), called Generative\nOpen xG Network Simulation (GenOnet), to effectively generate, debug, execute,\nand interpret simulated Open Fifth-Generation (5G) environments. The first\nversion of GenOnet application represents a specialized adaptation of the\nOpenAI GPT models. It incorporates supplementary tools, agents, 5G standards,\nand seamless integration with ns-3 simulation capabilities, supporting both C++\nvariants and Python implementations. This release complies with the latest Open\nRadio Access Network (O-RAN) and 3GPP standards."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Amir Ashtari Gargari"
                    },
                    {
                        "name": "Sandra Lagén"
                    },
                    {
                        "name": "Josep Mangues"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "3 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16851v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16851v1",
                "updated": "2024-09-25T11:57:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    57,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:57:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    57,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "Communication Backbone Reconfiguration with Connectivity Maintenance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Backbone Reconfiguration with Connectivity Maintenance"
                },
                "summary": "The exchange of information is key in applications that involve multiple\nagents, such as search and rescue, military operations, and disaster response.\nIn this work, we propose a simple and effective trajectory planning framework\nthat tackles the design, deployment, and reconfiguration of a communication\nbackbone by reframing the problem of networked multi-agent motion planning as a\nmanipulator motion planning problem. Our approach works for backbones of\nvariable configurations both in terms of the number of robots utilized and the\ndistance limit between each robot. While research has been conducted on\nconnection-restricted navigation for multi-robot systems in the last years, the\nfield of manipulators is arguably more developed both in theory and practice.\nHence, our methodology facilitates practical applications built on top of\nwidely available motion planning algorithms and frameworks for manipulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exchange of information is key in applications that involve multiple\nagents, such as search and rescue, military operations, and disaster response.\nIn this work, we propose a simple and effective trajectory planning framework\nthat tackles the design, deployment, and reconfiguration of a communication\nbackbone by reframing the problem of networked multi-agent motion planning as a\nmanipulator motion planning problem. Our approach works for backbones of\nvariable configurations both in terms of the number of robots utilized and the\ndistance limit between each robot. While research has been conducted on\nconnection-restricted navigation for multi-robot systems in the last years, the\nfield of manipulators is arguably more developed both in theory and practice.\nHence, our methodology facilitates practical applications built on top of\nwidely available motion planning algorithms and frameworks for manipulators."
                },
                "authors": [
                    {
                        "name": "Leonardo Santos"
                    },
                    {
                        "name": "Caio C. G. Ribeiro"
                    },
                    {
                        "name": "Douglas G. Macharet"
                    }
                ],
                "author_detail": {
                    "name": "Douglas G. Macharet"
                },
                "author": "Douglas G. Macharet",
                "arxiv_comment": "Submitted to IEEE Latin America Transactions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16851v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16851v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05200v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05200v2",
                "updated": "2024-09-25T11:43:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    43,
                    59,
                    2,
                    269,
                    0
                ],
                "published": "2024-02-07T19:10:36Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    19,
                    10,
                    36,
                    2,
                    38,
                    0
                ],
                "title": "Are LLMs Ready for Real-World Materials Discovery?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Ready for Real-World Materials Discovery?"
                },
                "summary": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) create exciting possibilities for powerful\nlanguage processing tools to accelerate research in materials science. While\nLLMs have great potential to accelerate materials understanding and discovery,\nthey currently fall short in being practical materials science tools. In this\nposition paper, we show relevant failure cases of LLMs in materials science\nthat reveal current limitations of LLMs related to comprehending and reasoning\nover complex, interconnected materials science knowledge. Given those\nshortcomings, we outline a framework for developing Materials Science LLMs\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\ngeneration followed by hypothesis testing. The path to attaining performant\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\nsourced from scientific literature where various information extraction\nchallenges persist. As such, we describe key materials science information\nextraction challenges which need to be overcome in order to build large-scale,\nmulti-modal datasets that capture valuable materials science knowledge.\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\nLaboratories."
                },
                "authors": [
                    {
                        "name": "Santiago Miret"
                    },
                    {
                        "name": "N M Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N M Anoop Krishnan"
                },
                "author": "N M Anoop Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05200v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05200v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16813v1",
                "updated": "2024-09-25T11:09:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    9,
                    39,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T11:09:39Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    11,
                    9,
                    39,
                    2,
                    269,
                    0
                ],
                "title": "PeerArg: Argumentative Peer Review with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerArg: Argumentative Peer Review with LLMs"
                },
                "summary": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is an essential process to determine the quality of papers\nsubmitted to scientific conferences or journals. However, it is subjective and\nprone to biases. Several studies have been conducted to apply techniques from\nNLP to support peer review, but they are based on black-box techniques and\ntheir outputs are difficult to interpret and trust. In this paper, we propose a\nnovel pipeline to support and understand the reviewing and decision-making\nprocesses of peer review: the PeerArg system combining LLMs with methods from\nknowledge representation. PeerArg takes in input a set of reviews for a paper\nand outputs the paper acceptance prediction. We evaluate the performance of the\nPeerArg pipeline on three different datasets, in comparison with a novel\nend-2-end LLM that uses few-shot learning to predict paper acceptance given\nreviews. The results indicate that the end-2-end LLM is capable of predicting\npaper acceptance from reviews, but a variant of the PeerArg pipeline\noutperforms this LLM."
                },
                "authors": [
                    {
                        "name": "Purin Sukpanichnant"
                    },
                    {
                        "name": "Anna Rapberger"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16807v1",
                "updated": "2024-09-25T10:56:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    56,
                    28,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T10:56:28Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    56,
                    28,
                    2,
                    269,
                    0
                ],
                "title": "A Few Hypocrites: Few-Shot Learning and Subtype Definitions for\n  Detecting Hypocrisy Accusations in Online Climate Change Debates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Few Hypocrites: Few-Shot Learning and Subtype Definitions for\n  Detecting Hypocrisy Accusations in Online Climate Change Debates"
                },
                "summary": "The climate crisis is a salient issue in online discussions, and hypocrisy\naccusations are a central rhetorical element in these debates. However, for\nlarge-scale text analysis, hypocrisy accusation detection is an understudied\ntool, most often defined as a smaller subtask of fallacious argument detection.\nIn this paper, we define hypocrisy accusation detection as an independent task\nin NLP, and identify different relevant subtypes of hypocrisy accusations. Our\nClimate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate\ndebate comments, expert-annotated into two different types of hypocrisy\naccusations: personal versus political hypocrisy. We evaluate few-shot\nin-context learning with 6 shots and 3 instruction-tuned Large Language Models\n(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate\nthat the GPT-4o and Llama-3 models in particular show promise in detecting\nhypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).\nHowever, context matters for a complex semantic concept such as hypocrisy\naccusations, and we find models struggle especially at identifying political\nhypocrisy accusations compared to personal moral hypocrisy. Our study\ncontributes new insights in hypocrisy detection and climate change discourse,\nand is a stepping stone for large-scale analysis of hypocrisy accusation in\nonline climate debates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The climate crisis is a salient issue in online discussions, and hypocrisy\naccusations are a central rhetorical element in these debates. However, for\nlarge-scale text analysis, hypocrisy accusation detection is an understudied\ntool, most often defined as a smaller subtask of fallacious argument detection.\nIn this paper, we define hypocrisy accusation detection as an independent task\nin NLP, and identify different relevant subtypes of hypocrisy accusations. Our\nClimate Hypocrisy Accusation Corpus (CHAC) consists of 420 Reddit climate\ndebate comments, expert-annotated into two different types of hypocrisy\naccusations: personal versus political hypocrisy. We evaluate few-shot\nin-context learning with 6 shots and 3 instruction-tuned Large Language Models\n(LLMs) for detecting hypocrisy accusations in this dataset. Results indicate\nthat the GPT-4o and Llama-3 models in particular show promise in detecting\nhypocrisy accusations (F1 reaching 0.68, while previous work shows F1 of 0.44).\nHowever, context matters for a complex semantic concept such as hypocrisy\naccusations, and we find models struggle especially at identifying political\nhypocrisy accusations compared to personal moral hypocrisy. Our study\ncontributes new insights in hypocrisy detection and climate change discourse,\nand is a stepping stone for large-scale analysis of hypocrisy accusation in\nonline climate debates."
                },
                "authors": [
                    {
                        "name": "Paulina Garcia Corral"
                    },
                    {
                        "name": "Avishai Green"
                    },
                    {
                        "name": "Hendrik Meyer"
                    },
                    {
                        "name": "Anke Stoll"
                    },
                    {
                        "name": "Xiaoyue Yan"
                    },
                    {
                        "name": "Myrthe Reuver"
                    }
                ],
                "author_detail": {
                    "name": "Myrthe Reuver"
                },
                "author": "Myrthe Reuver",
                "arxiv_comment": "cite the public version, published at CPSS 2024 @ KONVENS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15033v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15033v4",
                "updated": "2024-09-25T10:34:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    34,
                    3,
                    2,
                    269,
                    0
                ],
                "published": "2024-03-22T08:32:30Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    8,
                    32,
                    30,
                    4,
                    82,
                    0
                ],
                "title": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Tiny and High-quality Facial Makeup with Data Amplify Learning"
                },
                "summary": "Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary makeup approaches primarily hinge on unpaired learning\nparadigms, yet they grapple with the challenges of inaccurate supervision\n(e.g., face misalignment) and sophisticated facial prompts (including face\nparsing, and landmark detection). These challenges prohibit low-cost deployment\nof facial makeup models, especially on mobile devices. To solve above problems,\nwe propose a brand-new learning paradigm, termed \"Data Amplify Learning (DAL),\"\nalongside a compact makeup model named \"TinyBeauty.\" The core idea of DAL lies\nin employing a Diffusion-based Data Amplifier (DDA) to \"amplify\" limited images\nfor the model training, thereby enabling accurate pixel-to-pixel supervision\nwith merely a handful of annotations. Two pivotal innovations in DDA facilitate\nthe above training approach: (1) A Residual Diffusion Model (RDM) is designed\nto generate high-fidelity detail and circumvent the detail vanishing problem in\nthe vanilla diffusion models; (2) A Fine-Grained Makeup Module (FGMM) is\nproposed to achieve precise makeup control and combination while retaining face\nidentity. Coupled with DAL, TinyBeauty necessitates merely 80K parameters to\nachieve a state-of-the-art performance without intricate face prompts.\nMeanwhile, TinyBeauty achieves a remarkable inference speed of up to 460 fps on\nthe iPhone 13. Extensive experiments show that DAL can produce highly\ncompetitive makeup models using only 5 image pairs."
                },
                "authors": [
                    {
                        "name": "Qiaoqiao Jin"
                    },
                    {
                        "name": "Xuanhong Chen"
                    },
                    {
                        "name": "Meiguang Jin"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Rui Shi"
                    },
                    {
                        "name": "Yucheng Zheng"
                    },
                    {
                        "name": "Yupeng Zhu"
                    },
                    {
                        "name": "Bingbing Ni"
                    }
                ],
                "author_detail": {
                    "name": "Bingbing Ni"
                },
                "author": "Bingbing Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15033v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15033v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16799v1",
                "updated": "2024-09-25T10:32:18Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    32,
                    18,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T10:32:18Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    32,
                    18,
                    2,
                    269,
                    0
                ],
                "title": "Large Language Model Predicts Above Normal All India Summer Monsoon\n  Rainfall in 2024",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Predicts Above Normal All India Summer Monsoon\n  Rainfall in 2024"
                },
                "summary": "Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is\npivotal for informed policymaking for the country, impacting the lives of\nbillions of people. However, accurate simulation of AISMR has been a persistent\nchallenge due to the complex interplay of various muti-scale factors and the\ninherent variability of the monsoon system. This research focuses on adapting\nand fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR\nwith a lead time of three months. The fine-tuned PatchTST model, trained with\nhistorical AISMR data, the Ni\\~no3.4 index, and categorical Indian Ocean Dipole\nvalues, outperforms several popular neural network models and statistical\nmodels. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage\nof 0.07% and a Spearman correlation of 0.976. This is particularly impressive,\nsince it is nearly 80% more accurate than the best-performing NN models. The\nmodel predicts an above-normal monsoon for the year 2024, with an accumulated\nrainfall of 921.6 mm in the month of June-September for the entire country.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is\npivotal for informed policymaking for the country, impacting the lives of\nbillions of people. However, accurate simulation of AISMR has been a persistent\nchallenge due to the complex interplay of various muti-scale factors and the\ninherent variability of the monsoon system. This research focuses on adapting\nand fine-tuning the latest LLM model, PatchTST, to accurately predict AISMR\nwith a lead time of three months. The fine-tuned PatchTST model, trained with\nhistorical AISMR data, the Ni\\~no3.4 index, and categorical Indian Ocean Dipole\nvalues, outperforms several popular neural network models and statistical\nmodels. This fine-tuned LLM model exhibits an exceptionally low RMSE percentage\nof 0.07% and a Spearman correlation of 0.976. This is particularly impressive,\nsince it is nearly 80% more accurate than the best-performing NN models. The\nmodel predicts an above-normal monsoon for the year 2024, with an accumulated\nrainfall of 921.6 mm in the month of June-September for the entire country."
                },
                "authors": [
                    {
                        "name": "Ujjawal Sharma"
                    },
                    {
                        "name": "Madhav Biyani"
                    },
                    {
                        "name": "Akhil Dev Suresh"
                    },
                    {
                        "name": "Debi Prasad Bhuyan"
                    },
                    {
                        "name": "Saroj Kanta Mishra"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04872v2",
                "updated": "2024-09-25T10:00:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    10,
                    0,
                    3,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-09T05:25:17Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    25,
                    17,
                    4,
                    222,
                    0
                ],
                "title": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOI: Syntax-augmented Coverage-based In-context Example Selection for\n  Machine Translation"
                },
                "summary": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT. Our code is available at https://github.com/JamyDon/SCOI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) greatly improves the performance of large language\nmodels (LLMs) on various down-stream tasks, where the improvement highly\ndepends on the quality of demonstrations. In this work, we introduce syntactic\nknowledge to select better in-context examples for machine translation (MT). We\npropose a new strategy, namely Syntax-augmented COverage-based In-context\nexample selection (SCOI), leveraging the deep syntactic structure beyond\nconventional word matching. Specifically, we measure the set-level syntactic\ncoverage by computing the coverage of polynomial terms with the help of a\nsimplified tree-to-polynomial algorithm, and lexical coverage using word\noverlap. Furthermore, we devise an alternate selection approach to combine both\ncoverage measures, taking advantage of syntactic and lexical information. We\nconduct experiments with two multi-lingual LLMs on six translation directions.\nEmpirical results show that our proposed SCOI obtains the highest average COMET\nscore among all learning-free methods, indicating that combining syntactic and\nlexical coverage successfully helps to select better in-context examples for\nMT. Our code is available at https://github.com/JamyDon/SCOI."
                },
                "authors": [
                    {
                        "name": "Chenming Tang"
                    },
                    {
                        "name": "Zhixiang Wang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "arxiv_comment": "EMNLP 2024 main conference long paper. 16 pages, 2 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16788v1",
                "updated": "2024-09-25T09:52:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    52,
                    44,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:52:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    52,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "Mitigating the Bias of Large Language Model Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating the Bias of Large Language Model Evaluation"
                },
                "summary": "Recently, there has been a trend of evaluating the Large Language Model (LLM)\nquality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to\nevaluate the current output quality. However, existing judges are proven to be\nbiased, namely they would favor answers which present better superficial\nquality (such as verbosity, fluency) while ignoring the instruction following\nability. In this work, we propose systematic research about the bias of\nLLM-as-a-Judge. Specifically, for closed-source judge models, we apply\ncalibration to mitigate the significance of superficial quality, both on\nprobability level and prompt level. For open-source judge models, we propose to\nmitigate the bias by contrastive training, with curated negative samples that\ndeviate from instruction but present better superficial quality. We apply our\nmethods on the bias evaluation benchmark, and experiment results show our\nmethods mitigate the bias by a large margin while maintaining a satisfactory\nevaluation accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been a trend of evaluating the Large Language Model (LLM)\nquality in the flavor of LLM-as-a-Judge, namely leveraging another LLM to\nevaluate the current output quality. However, existing judges are proven to be\nbiased, namely they would favor answers which present better superficial\nquality (such as verbosity, fluency) while ignoring the instruction following\nability. In this work, we propose systematic research about the bias of\nLLM-as-a-Judge. Specifically, for closed-source judge models, we apply\ncalibration to mitigate the significance of superficial quality, both on\nprobability level and prompt level. For open-source judge models, we propose to\nmitigate the bias by contrastive training, with curated negative samples that\ndeviate from instruction but present better superficial quality. We apply our\nmethods on the bias evaluation benchmark, and experiment results show our\nmethods mitigate the bias by a large margin while maintaining a satisfactory\nevaluation accuracy."
                },
                "authors": [
                    {
                        "name": "Hongli Zhou"
                    },
                    {
                        "name": "Hui Huang"
                    },
                    {
                        "name": "Yunfei Long"
                    },
                    {
                        "name": "Bing Xu"
                    },
                    {
                        "name": "Conghui Zhu"
                    },
                    {
                        "name": "Hailong Cao"
                    },
                    {
                        "name": "Muyun Yang"
                    },
                    {
                        "name": "Tiejun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Zhao"
                },
                "author": "Tiejun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16783v1",
                "updated": "2024-09-25T09:44:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    44,
                    48,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:44:48Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    44,
                    48,
                    2,
                    269,
                    0
                ],
                "title": "Holistic Automated Red Teaming for Large Language Models through\n  Top-Down Test Case Generation and Multi-turn Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Automated Red Teaming for Large Language Models through\n  Top-Down Test Case Generation and Multi-turn Interaction"
                },
                "summary": "Automated red teaming is an effective method for identifying misaligned\nbehaviors in large language models (LLMs). Existing approaches, however, often\nfocus primarily on improving attack success rates while overlooking the need\nfor comprehensive test case coverage. Additionally, most of these methods are\nlimited to single-turn red teaming, failing to capture the multi-turn dynamics\nof real-world human-machine interactions. To overcome these limitations, we\npropose HARM (Holistic Automated Red teaMing), which scales up the diversity of\ntest cases using a top-down approach based on an extensible, fine-grained risk\ntaxonomy. Our method also leverages a novel fine-tuning strategy and\nreinforcement learning techniques to facilitate multi-turn adversarial probing\nin a human-like manner. Experimental results demonstrate that our framework\nenables a more systematic understanding of model vulnerabilities and offers\nmore targeted guidance for the alignment process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated red teaming is an effective method for identifying misaligned\nbehaviors in large language models (LLMs). Existing approaches, however, often\nfocus primarily on improving attack success rates while overlooking the need\nfor comprehensive test case coverage. Additionally, most of these methods are\nlimited to single-turn red teaming, failing to capture the multi-turn dynamics\nof real-world human-machine interactions. To overcome these limitations, we\npropose HARM (Holistic Automated Red teaMing), which scales up the diversity of\ntest cases using a top-down approach based on an extensible, fine-grained risk\ntaxonomy. Our method also leverages a novel fine-tuning strategy and\nreinforcement learning techniques to facilitate multi-turn adversarial probing\nin a human-like manner. Experimental results demonstrate that our framework\nenables a more systematic understanding of model vulnerabilities and offers\nmore targeted guidance for the alignment process."
                },
                "authors": [
                    {
                        "name": "Jinchuan Zhang"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Yaxin Liu"
                    },
                    {
                        "name": "Ziming Li"
                    },
                    {
                        "name": "Songlin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Songlin Hu"
                },
                "author": "Songlin Hu",
                "arxiv_comment": "EMNLP 2024 camera ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16779v1",
                "updated": "2024-09-25T09:41:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    41,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:41:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    41,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ"
                },
                "summary": "Large Language Models (LLMs) often struggle with tasks requiring mathematical\nreasoning, particularly multiple-choice questions (MCQs). To address this\nissue, we developed LLaMa-SciQ, an educational chatbot designed to assist\ncollege students in solving and understanding MCQs in STEM fields. We begin by\nfine-tuning and aligning the models to human preferences. After comparing the\nperformance of Mistral-7B and LLaMa-8B, we selected the latter as the base\nmodel due to its higher evaluation accuracy. To further enhance accuracy, we\nimplement Retrieval-Augmented Generation (RAG) and apply quantization to\ncompress the model, reducing inference time and increasing accessibility for\nstudents. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the\nGSM8k dataset and 30% on the MATH dataset. However, RAG does not improve\nperformance and even reduces it, likely due to retriever issues or the model's\nunfamiliarity with context. Despite this, the quantized model shows only a 5%\nloss in performance, demonstrating significant efficiency improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often struggle with tasks requiring mathematical\nreasoning, particularly multiple-choice questions (MCQs). To address this\nissue, we developed LLaMa-SciQ, an educational chatbot designed to assist\ncollege students in solving and understanding MCQs in STEM fields. We begin by\nfine-tuning and aligning the models to human preferences. After comparing the\nperformance of Mistral-7B and LLaMa-8B, we selected the latter as the base\nmodel due to its higher evaluation accuracy. To further enhance accuracy, we\nimplement Retrieval-Augmented Generation (RAG) and apply quantization to\ncompress the model, reducing inference time and increasing accessibility for\nstudents. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the\nGSM8k dataset and 30% on the MATH dataset. However, RAG does not improve\nperformance and even reduces it, likely due to retriever issues or the model's\nunfamiliarity with context. Despite this, the quantized model shows only a 5%\nloss in performance, demonstrating significant efficiency improvements."
                },
                "authors": [
                    {
                        "name": "Marc-Antoine Allard"
                    },
                    {
                        "name": "Matin Ansaripour"
                    },
                    {
                        "name": "Maria Yuffa"
                    },
                    {
                        "name": "Paul Teiletche"
                    }
                ],
                "author_detail": {
                    "name": "Paul Teiletche"
                },
                "author": "Paul Teiletche",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16332v2",
                "updated": "2024-09-25T09:36:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    36,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-24T06:10:13Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    6,
                    10,
                    13,
                    0,
                    176,
                    0
                ],
                "title": "DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DemoRank: Selecting Effective Demonstrations for Large Language Models\n  in Ranking Task"
                },
                "summary": "Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, there has been increasing interest in applying large language\nmodels (LLMs) as zero-shot passage rankers. However, few studies have explored\nhow to select appropriate in-context demonstrations for the passage ranking\ntask, which is the focus of this paper. Previous studies mainly use LLM's\nfeedback to train a retriever for demonstration selection. These studies apply\nthe LLM to score each demonstration independently, which ignores the\ndependencies between demonstrations (especially important in ranking task),\nleading to inferior performance of top-$k$ retrieved demonstrations. To\nmitigate this issue, we introduce a demonstration reranker to rerank the\nretrieved demonstrations so that top-$k$ ranked ones are more suitable for ICL.\nHowever, generating training data for such reranker is quite challenging. On\nthe one hand, different from demonstration retriever, the training samples of\nreranker need to incorporate demonstration dependencies. On the other hand,\nobtaining the gold ranking from the retrieved demonstrations is an NP-hard\nproblem, which is hard to implement. To overcome these challenges, we propose a\nmethod to approximate the optimal demonstration list iteratively and utilize\nLLM to score demonstration lists of varying lengths. By doing so, the search\nspace is greatly reduced and demonstration dependencies are considered. Based\non these scored demonstration lists, we further design a list-pairwise training\napproach which compares a pair of lists that only differ in the last\ndemonstration, to teach the reranker how to select the next demonstration given\na previous sequence. In this paper, we propose a demonstration selection\nframework DemoRank for ranking task and conduct extensive experiments to prove\nits strong ability."
                },
                "authors": [
                    {
                        "name": "Wenhan Liu"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07054v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07054v2",
                "updated": "2024-09-25T09:19:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    19,
                    0,
                    2,
                    269,
                    0
                ],
                "published": "2023-11-13T03:42:17Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    3,
                    42,
                    17,
                    0,
                    317,
                    0
                ],
                "title": "A Study of Implicit Ranking Unfairness in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of Implicit Ranking Unfairness in Large Language Models"
                },
                "summary": "Recently, Large Language Models (LLMs) have demonstrated a superior ability\nto serve as ranking models. However, concerns have arisen as LLMs will exhibit\ndiscriminatory ranking behaviors based on users' sensitive attributes (\\eg\ngender). Worse still, in this paper, we identify a subtler form of\ndiscrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs\nexhibit discriminatory ranking patterns based solely on non-sensitive user\nprofiles, such as user names. Such implicit unfairness is more widespread but\nless noticeable, threatening the ethical foundation. To comprehensively explore\nsuch unfairness, our analysis will focus on three research aspects: (1) We\npropose an evaluation method to investigate the severity of implicit ranking\nunfairness. (2) We uncover the reasons for causing such unfairness. (3) To\nmitigate such unfairness effectively, we utilize a pair-wise regression method\nto conduct fair-aware data augmentation for LLM fine-tuning. The experiment\ndemonstrates that our method outperforms existing approaches in ranking\nfairness, achieving this with only a small reduction in accuracy. Lastly, we\nemphasize the need for the community to identify and mitigate the implicit\nunfairness, aiming to avert the potential deterioration in the reinforced\nhuman-LLMs ecosystem deterioration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have demonstrated a superior ability\nto serve as ranking models. However, concerns have arisen as LLMs will exhibit\ndiscriminatory ranking behaviors based on users' sensitive attributes (\\eg\ngender). Worse still, in this paper, we identify a subtler form of\ndiscrimination in LLMs, termed \\textit{implicit ranking unfairness}, where LLMs\nexhibit discriminatory ranking patterns based solely on non-sensitive user\nprofiles, such as user names. Such implicit unfairness is more widespread but\nless noticeable, threatening the ethical foundation. To comprehensively explore\nsuch unfairness, our analysis will focus on three research aspects: (1) We\npropose an evaluation method to investigate the severity of implicit ranking\nunfairness. (2) We uncover the reasons for causing such unfairness. (3) To\nmitigate such unfairness effectively, we utilize a pair-wise regression method\nto conduct fair-aware data augmentation for LLM fine-tuning. The experiment\ndemonstrates that our method outperforms existing approaches in ranking\nfairness, achieving this with only a small reduction in accuracy. Lastly, we\nemphasize the need for the community to identify and mitigate the implicit\nunfairness, aiming to avert the potential deterioration in the reinforced\nhuman-LLMs ecosystem deterioration."
                },
                "authors": [
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Wenjie Wang"
                    },
                    {
                        "name": "Yuxin Li"
                    },
                    {
                        "name": "Liang Pang"
                    },
                    {
                        "name": "Jun Xu"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "Accepted in EMNLP 2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07054v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07054v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16751v1",
                "updated": "2024-09-25T09:02:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    2,
                    48,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T09:02:48Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    2,
                    48,
                    2,
                    269,
                    0
                ],
                "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL"
                },
                "summary": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question, bridging the gap between the query and the database\nstructure. The pipeline leverages candidate predicate augmentation to mitigate\nerroneous or incomplete predicates in generated SQLs. We further investigate\nthe impact of schema filtering, a technique widely explored in previous work,\nand demonstrate its diminishing returns when applied alongside advanced large\nlanguage models. Comprehensive evaluations on the BIRD benchmark illustrate\nthat E-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. All code required to\nreproduce the reported results is publicly available on our GitHub repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question, bridging the gap between the query and the database\nstructure. The pipeline leverages candidate predicate augmentation to mitigate\nerroneous or incomplete predicates in generated SQLs. We further investigate\nthe impact of schema filtering, a technique widely explored in previous work,\nand demonstrate its diminishing returns when applied alongside advanced large\nlanguage models. Comprehensive evaluations on the BIRD benchmark illustrate\nthat E-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. All code required to\nreproduce the reported results is publicly available on our GitHub repository."
                },
                "authors": [
                    {
                        "name": "Hasan Alp Caferoğlu"
                    },
                    {
                        "name": "Özgür Ulusoy"
                    }
                ],
                "author_detail": {
                    "name": "Özgür Ulusoy"
                },
                "author": "Özgür Ulusoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16154v2",
                "updated": "2024-09-25T09:00:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    9,
                    0,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T14:58:27Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    58,
                    27,
                    1,
                    268,
                    0
                ],
                "title": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Motion Prediction: A Lightweight & Accurate Trajectory\n  Prediction Model With Fast Training and Inference Speed"
                },
                "summary": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For efficient and safe autonomous driving, it is essential that autonomous\nvehicles can predict the motion of other traffic agents. While highly accurate,\ncurrent motion prediction models often impose significant challenges in terms\nof training resource requirements and deployment on embedded hardware. We\npropose a new efficient motion prediction model, which achieves highly\ncompetitive benchmark results while training only a few hours on a single GPU.\nDue to our lightweight architectural choices and the focus on reducing the\nrequired training resources, our model can easily be applied to custom\ndatasets. Furthermore, its low inference latency makes it particularly suitable\nfor deployment in autonomous applications with limited computing resources."
                },
                "authors": [
                    {
                        "name": "Alexander Prutsch"
                    },
                    {
                        "name": "Horst Bischof"
                    },
                    {
                        "name": "Horst Possegger"
                    }
                ],
                "author_detail": {
                    "name": "Horst Possegger"
                },
                "author": "Horst Possegger",
                "arxiv_comment": "Accepted to IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14913v2",
                "updated": "2024-09-25T08:52:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-23T11:08:04Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    8,
                    4,
                    0,
                    267,
                    0
                ],
                "title": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Realistic Long-Term Benchmark for Open-Web Research Agents"
                },
                "summary": "We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present initial results of a forthcoming benchmark for evaluating LLM\nagents on white-collar tasks of economic value. We evaluate agents on\nreal-world \"messy\" open-web research tasks of the type that are routine in\nfinance and consulting. In doing so, we lay the groundwork for an LLM agent\nevaluation suite where good performance directly corresponds to a large\neconomic and societal impact. We built and tested several agent architectures\nwith o1-preview, GPT-4o, Claude-3.5 Sonnet, Llama 3.1 (405b), and GPT-4o-mini.\nOn average, LLM agents powered by Claude-3.5 Sonnet and o1-preview\nsubstantially outperformed agents using GPT-4o, with agents based on Llama 3.1\n(405b) and GPT-4o-mini lagging noticeably behind. Across LLMs, a ReAct\narchitecture with the ability to delegate subtasks to subagents performed best.\nIn addition to quantitative evaluations, we qualitatively assessed the\nperformance of the LLM agents by inspecting their traces and reflecting on\ntheir observations. Our evaluation represents the first in-depth assessment of\nagents' abilities to conduct challenging, economically valuable analyst-style\nresearch on the real open web."
                },
                "authors": [
                    {
                        "name": "Peter Mühlbacher"
                    },
                    {
                        "name": "Nikos I. Bosse"
                    },
                    {
                        "name": "Lawrence Phillips"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence Phillips"
                },
                "author": "Lawrence Phillips",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16739v1",
                "updated": "2024-09-25T08:42:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    42,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:42:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    42,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "Context-Enhanced LLM-Based Framework for Automatic Test Refactoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Enhanced LLM-Based Framework for Automatic Test Refactoring"
                },
                "summary": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test smells arise from poor design practices and insufficient domain\nknowledge, which can lower the quality of test code and make it harder to\nmaintain and update. Manually refactoring test smells is time-consuming and\nerror-prone, highlighting the necessity for automated approaches. Current\nrule-based refactoring methods often struggle in scenarios not covered by\npredefined rules and lack the flexibility needed to handle diverse cases\neffectively. In this paper, we propose a novel approach called UTRefactor, a\ncontext-enhanced, LLM-based framework for automatic test refactoring in Java\nprojects. UTRefactor extracts relevant context from test code and leverages an\nexternal knowledge base that includes test smell definitions, descriptions, and\nDSL-based refactoring rules. By simulating the manual refactoring process\nthrough a chain-of-thought approach, UTRefactor guides the LLM to eliminate\ntest smells in a step-by-step process, ensuring both accuracy and consistency\nthroughout the refactoring. Additionally, we implement a checkpoint mechanism\nto facilitate comprehensive refactoring, particularly when multiple smells are\npresent. We evaluate UTRefactor on 879 tests from six open-source Java\nprojects, reducing the number of test smells from 2,375 to 265, achieving an\n89% reduction. UTRefactor outperforms direct LLM-based refactoring methods by\n61.82% in smell elimination and significantly surpasses the performance of a\nrule-based test smell refactoring tool. Our results demonstrate the\neffectiveness of UTRefactor in enhancing test code quality while minimizing\nmanual involvement."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16732v1",
                "updated": "2024-09-25T08:31:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    31,
                    11,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:31:11Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    31,
                    11,
                    2,
                    269,
                    0
                ],
                "title": "\"It Explains What I am Currently Going Through Perfectly to a Tee\":\n  Understanding User Perceptions on LLM-Enhanced Narrative Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It Explains What I am Currently Going Through Perfectly to a Tee\":\n  Understanding User Perceptions on LLM-Enhanced Narrative Interventions"
                },
                "summary": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that LLM-enhanced stories\nwere perceived to be better than human-written ones in conveying key takeaways,\npromoting reflection, and reducing belief in negative thoughts. These stories\nwere not only seen as more relatable but also similarly authentic to\nhuman-written ones, highlighting the potential of LLMs in helping young adults\nmanage their struggles. The findings of this work provide crucial design\nconsiderations for future narrative-based digital mental health interventions,\nsuch as the need to maintain relatability without veering into implausibility\nand refining the wording and tone of AI-enhanced content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stories about overcoming personal struggles can effectively illustrate the\napplication of psychological theories in real life, yet they may fail to\nresonate with individuals' experiences. In this work, we employ large language\nmodels (LLMs) to create tailored narratives that acknowledge and address unique\nchallenging thoughts and situations faced by individuals. Our study, involving\n346 young adults across two settings, demonstrates that LLM-enhanced stories\nwere perceived to be better than human-written ones in conveying key takeaways,\npromoting reflection, and reducing belief in negative thoughts. These stories\nwere not only seen as more relatable but also similarly authentic to\nhuman-written ones, highlighting the potential of LLMs in helping young adults\nmanage their struggles. The findings of this work provide crucial design\nconsiderations for future narrative-based digital mental health interventions,\nsuch as the need to maintain relatability without veering into implausibility\nand refining the wording and tone of AI-enhanced content."
                },
                "authors": [
                    {
                        "name": "Ananya Bhattacharjee"
                    },
                    {
                        "name": "Sarah Yi Xu"
                    },
                    {
                        "name": "Pranav Rao"
                    },
                    {
                        "name": "Yuchen Zeng"
                    },
                    {
                        "name": "Jonah Meyerhoff"
                    },
                    {
                        "name": "Syed Ishtiaque Ahmed"
                    },
                    {
                        "name": "David C Mohr"
                    },
                    {
                        "name": "Michael Liut"
                    },
                    {
                        "name": "Alex Mariakakis"
                    },
                    {
                        "name": "Rachel Kornfield"
                    },
                    {
                        "name": "Joseph Jay Williams"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Jay Williams"
                },
                "author": "Joseph Jay Williams",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16727v1",
                "updated": "2024-09-25T08:23:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:23:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    23,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoleBreak: Character Hallucination as a Jailbreak Attack in Role-Playing\n  Systems"
                },
                "summary": "Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing systems powered by large language models (LLMs) have become\nincreasingly influential in emotional communication applications. However,\nthese systems are susceptible to character hallucinations, where the model\ndeviates from predefined character roles and generates responses that are\ninconsistent with the intended persona. This paper presents the first\nsystematic analysis of character hallucination from an attack perspective,\nintroducing the RoleBreak framework. Our framework identifies two core\nmechanisms-query sparsity and role-query conflict-as key factors driving\ncharacter hallucination. Leveraging these insights, we construct a novel\ndataset, RoleBreakEval, to evaluate existing hallucination mitigation\ntechniques. Our experiments reveal that even enhanced models trained to\nminimize hallucination remain vulnerable to attacks. To address these\nvulnerabilities, we propose a novel defence strategy, the Narrator Mode, which\ngenerates supplemental context through narration to mitigate role-query\nconflicts and improve query generalization. Experimental results demonstrate\nthat Narrator Mode significantly outperforms traditional refusal-based\nstrategies by reducing hallucinations, enhancing fidelity to character roles\nand queries, and improving overall narrative coherence."
                },
                "authors": [
                    {
                        "name": "Yihong Tang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Dongming Zhao"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Jijun Zhang"
                    },
                    {
                        "name": "Ruifang He"
                    },
                    {
                        "name": "Yuexian Hou"
                    }
                ],
                "author_detail": {
                    "name": "Yuexian Hou"
                },
                "author": "Yuexian Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16722v1",
                "updated": "2024-09-25T08:20:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    20,
                    24,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:20:24Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    20,
                    24,
                    2,
                    269,
                    0
                ],
                "title": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMSS: Pretrained Matrices Skeleton Selection for LLM Fine-tuning"
                },
                "summary": "Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adaptation (LoRA) and its variants have recently gained much\ninterest due to their ability to avoid excessive inference costs. However, LoRA\nstill encounters the following challenges: (1) Limitation of low-rank\nassumption; and (2) Its initialization method may be suboptimal. To this end,\nwe propose PMSS(Pre-trained Matrices Skeleton Selection), which enables\nhigh-rank updates with low costs while leveraging semantic and linguistic\ninformation inherent in pre-trained weight. It achieves this by selecting\nskeletons from the pre-trained weight matrix and only learning a small matrix\ninstead. Experiments demonstrate that PMSS outperforms LoRA and other\nfine-tuning methods across tasks with much less trainable parameters. We\ndemonstrate its effectiveness, especially in handling complex tasks such as\nDROP benchmark(+3.4%/+5.9% on LLaMA2-7B/13B) and math\nreasoning(+12.89%/+5.61%/+3.11% on LLaMA2-7B, Mistral-7B and Gemma-7B of\nGSM8K). The code and model will be released soon."
                },
                "authors": [
                    {
                        "name": "Qibin Wang"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12246v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12246v3",
                "updated": "2024-09-25T08:17:21Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    17,
                    21,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-18T03:42:00Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    3,
                    42,
                    0,
                    1,
                    170,
                    0
                ],
                "title": "TroL: Traversal of Layers for Large Language and Vision Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TroL: Traversal of Layers for Large Language and Vision Models"
                },
                "summary": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes."
                },
                "authors": [
                    {
                        "name": "Byung-Kwan Lee"
                    },
                    {
                        "name": "Sangyun Chung"
                    },
                    {
                        "name": "Chae Won Kim"
                    },
                    {
                        "name": "Beomchan Park"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "EMNLP 2024. Code is available in https://github.com/ByungKwanLee/TroL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12246v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12246v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01204v2",
                "updated": "2024-09-25T07:59:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    59,
                    49,
                    2,
                    269,
                    0
                ],
                "published": "2024-04-01T16:00:01Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    16,
                    0,
                    1,
                    0,
                    92,
                    0
                ],
                "title": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Fine Line: Navigating Large Language Model Pretraining with\n  Down-streaming Capability Analysis"
                },
                "summary": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering early-stage metrics that reflect final model performance is one\ncore principle for large-scale pretraining. The existing scaling law\ndemonstrates the power-law correlation between pretraining loss and training\nflops, which serves as an important indicator of the current training state for\nlarge language models. However, this principle only focuses on the model's\ncompression properties on the training data, resulting in an inconsistency with\nthe ability improvements on the downstream tasks. Some follow-up works\nattempted to extend the scaling-law to more complex metrics (such as\nhyperparameters), but still lacked a comprehensive analysis of the dynamic\ndifferences among various capabilities during pretraining. To address the\naforementioned limitations, this paper undertakes a comprehensive comparison of\nmodel capabilities at various pretraining intermediate checkpoints. Through\nthis analysis, we confirm that specific downstream metrics exhibit similar\ntraining dynamics across models of different sizes, up to 67 billion\nparameters. In addition to our core findings, we've reproduced Amber and\nOpenLLaMA, releasing their intermediate checkpoints. This initiative offers\nvaluable resources to the research community and facilitates the verification\nand exploration of LLM pretraining by open-source researchers. Besides, we\nprovide empirical summaries, including performance comparisons of different\nmodels and capabilities, and tuition of key metrics for different training\nphases. Based on these findings, we provide a more user-friendly strategy for\nevaluating the optimization state, offering guidance for establishing a stable\npretraining process."
                },
                "authors": [
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Junzhuo Li"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Xinrun Du"
                    },
                    {
                        "name": "Songyang Gao"
                    },
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zhaoliang Chen"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Stephen W. Huang"
                    },
                    {
                        "name": "Shawn Yue"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Ge Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ge Zhang"
                },
                "author": "Ge Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16710v1",
                "updated": "2024-09-25T07:55:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    55,
                    36,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:55:36Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    55,
                    36,
                    2,
                    269,
                    0
                ],
                "title": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?"
                },
                "summary": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-Turing era, evaluating large language models (LLMs) involves\nassessing generated text based on readers' reactions rather than merely its\nindistinguishability from human-produced content. This paper explores how\nLLM-generated text impacts readers' decisions, focusing on both amateur and\nexpert audiences. Our findings indicate that GPT-4 can generate persuasive\nanalyses affecting the decisions of both amateurs and professionals.\nFurthermore, we evaluate the generated text from the aspects of grammar,\nconvincingness, logical coherence, and usefulness. The results highlight a high\ncorrelation between real-world evaluation through audience reactions and the\ncurrent multi-dimensional evaluators commonly used for generative models.\nOverall, this paper shows the potential and risk of using generated text to\nsway human decisions and also points out a new direction for evaluating\ngenerated text, i.e., leveraging the reactions and decisions of readers. We\nrelease our dataset to assist future research."
                },
                "authors": [
                    {
                        "name": "Takehiro Takayanagi"
                    },
                    {
                        "name": "Hiroya Takamura"
                    },
                    {
                        "name": "Kiyoshi Izumi"
                    },
                    {
                        "name": "Chung-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chung-Chi Chen"
                },
                "author": "Chung-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16701v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16701v1",
                "updated": "2024-09-25T07:47:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:47:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    47,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "Unit Test Generation for Vulnerability Exploitation in Java Third-Party\n  Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit Test Generation for Vulnerability Exploitation in Java Third-Party\n  Libraries"
                },
                "summary": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source third-party libraries are widely used in software development.\nThese libraries offer substantial advantages in terms of time and resource\nsavings. However, a significant concern arises due to the publicly disclosed\nvulnerabilities within these libraries. Existing automated vulnerability\ndetection tools often suffer from false positives and fail to accurately assess\nthe propagation of inputs capable of triggering vulnerabilities from client\nprojects to vulnerable code in libraries. In this paper, we propose a novel\napproach called VULEUT (Vulnerability Exploit Unit Test Generation), which\ncombines vulnerability exploitation reachability analysis and LLM-based unit\ntest generation. VULEUT is designed to automatically verify the exploitability\nof vulnerabilities in third-party libraries commonly used in client software\nprojects. VULEUT first analyzes the client projects to determine the\nreachability of vulnerability conditions. And then, it leverages the Large\nLanguage Model (LLM) to generate unit tests for vulnerability confirmation. To\nevaluate the effectiveness of VULEUT, we collect 32 vulnerabilities from\nvarious third-party libraries and conduct experiments on 70 real client\nprojects. Besides, we also compare our approach with two representative tools,\ni.e., TRANSFER and VESTA. Our results demonstrate the effectiveness of VULEUT,\nwith 229 out of 292 generated unit tests successfully confirming vulnerability\nexploitation across 70 client projects, which outperforms baselines by 24%."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Xiaohu Yang"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16701v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16701v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00646v2",
                "updated": "2024-09-25T07:40:20Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    40,
                    20,
                    2,
                    269,
                    0
                ],
                "published": "2023-10-01T12:02:57Z",
                "published_parsed": [
                    2023,
                    10,
                    1,
                    12,
                    2,
                    57,
                    6,
                    274,
                    0
                ],
                "title": "Source Attribution for Large Language Model-Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Source Attribution for Large Language Model-Generated Data"
                },
                "summary": "The impressive performances of Large Language Models (LLMs) and their immense\npotential for commercialization have given rise to serious concerns over the\nIntellectual Property (IP) of their training data. In particular, the synthetic\ntexts generated by LLMs may infringe the IP of the data being used to train the\nLLMs. To this end, it is imperative to be able to perform source attribution by\nidentifying the data provider who contributed to the generation of a synthetic\ntext by an LLM. In this paper, we show that this problem can be tackled by\nwatermarking, i.e., by enabling an LLM to generate synthetic texts with\nembedded watermarks that contain information about their source(s). We identify\nthe key properties of such watermarking frameworks (e.g., source attribution\naccuracy, robustness against adversaries), and propose a source attribution\nframework that satisfies these key properties due to our algorithmic designs.\nOur framework enables an LLM to learn an accurate mapping from the generated\ntexts to data providers, which sets the foundation for effective source\nattribution. Extensive empirical evaluations show that our framework achieves\neffective source attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The impressive performances of Large Language Models (LLMs) and their immense\npotential for commercialization have given rise to serious concerns over the\nIntellectual Property (IP) of their training data. In particular, the synthetic\ntexts generated by LLMs may infringe the IP of the data being used to train the\nLLMs. To this end, it is imperative to be able to perform source attribution by\nidentifying the data provider who contributed to the generation of a synthetic\ntext by an LLM. In this paper, we show that this problem can be tackled by\nwatermarking, i.e., by enabling an LLM to generate synthetic texts with\nembedded watermarks that contain information about their source(s). We identify\nthe key properties of such watermarking frameworks (e.g., source attribution\naccuracy, robustness against adversaries), and propose a source attribution\nframework that satisfies these key properties due to our algorithmic designs.\nOur framework enables an LLM to learn an accurate mapping from the generated\ntexts to data providers, which sets the foundation for effective source\nattribution. Extensive empirical evaluations show that our framework achieves\neffective source attribution."
                },
                "authors": [
                    {
                        "name": "Jingtan Wang"
                    },
                    {
                        "name": "Xinyang Lu"
                    },
                    {
                        "name": "Zitong Zhao"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16695v1",
                "updated": "2024-09-25T07:38:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    24,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:38:24Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    24,
                    2,
                    269,
                    0
                ],
                "title": "In which fields can ChatGPT detect journal article quality? An\n  evaluation of REF2021 results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In which fields can ChatGPT detect journal article quality? An\n  evaluation of REF2021 results"
                },
                "summary": "Time spent by academics on research quality assessment might be reduced if\nautomated approaches can help. Whilst citation-based indicators have been\nextensively developed and evaluated for this, they have substantial limitations\nand Large Language Models (LLMs) like ChatGPT provide an alternative approach.\nThis article assesses whether ChatGPT 4o-mini can be used to estimate the\nquality of journal articles across academia. It samples up to 200 articles from\nall 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework\n(REF) 2021, comparing ChatGPT scores with departmental average scores. There\nwas an almost universally positive Spearman correlation between ChatGPT scores\nand departmental averages, varying between 0.08 (Philosophy) and 0.78\n(Psychology, Psychiatry and Neuroscience), except for Clinical Medicine\n(rho=-0.12). Although other explanations are possible, especially because REF\nscore profiles are public, the results suggest that LLMs can provide reasonable\nresearch quality estimates in most areas of science, and particularly the\nphysical and health sciences and engineering, even before citation data is\navailable. Nevertheless, ChatGPT assessments seem to be more positive for most\nhealth and physical sciences than for other fields, a concern for\nmultidisciplinary assessments, and the ChatGPT scores are only based on titles\nand abstracts, so cannot be research evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time spent by academics on research quality assessment might be reduced if\nautomated approaches can help. Whilst citation-based indicators have been\nextensively developed and evaluated for this, they have substantial limitations\nand Large Language Models (LLMs) like ChatGPT provide an alternative approach.\nThis article assesses whether ChatGPT 4o-mini can be used to estimate the\nquality of journal articles across academia. It samples up to 200 articles from\nall 34 Units of Assessment (UoAs) in the UK's Research Excellence Framework\n(REF) 2021, comparing ChatGPT scores with departmental average scores. There\nwas an almost universally positive Spearman correlation between ChatGPT scores\nand departmental averages, varying between 0.08 (Philosophy) and 0.78\n(Psychology, Psychiatry and Neuroscience), except for Clinical Medicine\n(rho=-0.12). Although other explanations are possible, especially because REF\nscore profiles are public, the results suggest that LLMs can provide reasonable\nresearch quality estimates in most areas of science, and particularly the\nphysical and health sciences and engineering, even before citation data is\navailable. Nevertheless, ChatGPT assessments seem to be more positive for most\nhealth and physical sciences than for other fields, a concern for\nmultidisciplinary assessments, and the ChatGPT scores are only based on titles\nand abstracts, so cannot be research evaluations."
                },
                "authors": [
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Abdallah Yaghi"
                    }
                ],
                "author_detail": {
                    "name": "Abdallah Yaghi"
                },
                "author": "Abdallah Yaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16694v1",
                "updated": "2024-09-25T07:38:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:38:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    38,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Low-bit Large Language Models: Basics, Systems, and\n  Algorithms"
                },
                "summary": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable advancements in natural\nlanguage processing, showcasing exceptional performance across various tasks.\nHowever, the expensive memory and computational requirements present\nsignificant challenges for their practical deployment. Low-bit quantization has\nemerged as a critical approach to mitigate these challenges by reducing the\nbit-width of model parameters, activations, and gradients, thus decreasing\nmemory usage and computational demands. This paper presents a comprehensive\nsurvey of low-bit quantization methods tailored for LLMs, covering the\nfundamental principles, system implementations, and algorithmic strategies. An\noverview of basic concepts and new data formats specific to low-bit LLMs is\nfirst introduced, followed by a review of frameworks and systems that\nfacilitate low-bit LLMs across various hardware platforms. Then, we categorize\nand analyze techniques and toolkits for efficient low-bit training and\ninference of LLMs. Finally, we conclude with a discussion of future trends and\npotential advancements of low-bit LLMs. Our systematic overview from basic,\nsystem, and algorithm perspectives can offer valuable insights and guidelines\nfor future works to enhance the efficiency and applicability of LLMs through\nlow-bit quantization."
                },
                "authors": [
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Chengtao Lv"
                    },
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Jinyang Du"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Xianglong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xianglong Liu"
                },
                "author": "Xianglong Liu",
                "arxiv_comment": "Ruihao Gong leads the overall organization of the survey, with Yifu\n  Ding and Jinyang Du contributing to Sections 2 and 3. Xingyu Zheng is\n  responsible for authoring Section 4, while Chengtao Lv and Zining Wang\n  collaborate on Section 5. Haotong Qin, Jinyang Guo, Michele Magno, and\n  Xianglong Liu provide guidance during the whole process and assist in\n  refining the final manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16686v1",
                "updated": "2024-09-25T07:21:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    21,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:21:51Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    21,
                    51,
                    2,
                    269,
                    0
                ],
                "title": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for\n  Superior Planning and Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSI-Agent: Incorporating Multi-Scale Insight into Embodied Agents for\n  Superior Planning and Decision-Making"
                },
                "summary": "Long-term memory is significant for agents, in which insights play a crucial\nrole. However, the emergence of irrelevant insight and the lack of general\ninsight can greatly undermine the effectiveness of insight. To solve this\nproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an\nembodied agent designed to improve LLMs' planning and decision-making ability\nby summarizing and utilizing insight effectively across different scales. MSI\nachieves this through the experience selector, insight generator, and insight\nselector. Leveraging a three-part pipeline, MSI can generate task-specific and\nhigh-level insight, store it in a database, and then use relevant insight from\nit to aid in decision-making. Our experiments show that MSI outperforms another\ninsight strategy when planning by GPT3.5. Moreover, We delve into the\nstrategies for selecting seed experience and insight, aiming to provide LLM\nwith more useful and relevant insight for better decision-making. Our\nobservations also indicate that MSI exhibits better robustness when facing\ndomain-shifting scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term memory is significant for agents, in which insights play a crucial\nrole. However, the emergence of irrelevant insight and the lack of general\ninsight can greatly undermine the effectiveness of insight. To solve this\nproblem, in this paper, we introduce Multi-Scale Insight Agent (MSI-Agent), an\nembodied agent designed to improve LLMs' planning and decision-making ability\nby summarizing and utilizing insight effectively across different scales. MSI\nachieves this through the experience selector, insight generator, and insight\nselector. Leveraging a three-part pipeline, MSI can generate task-specific and\nhigh-level insight, store it in a database, and then use relevant insight from\nit to aid in decision-making. Our experiments show that MSI outperforms another\ninsight strategy when planning by GPT3.5. Moreover, We delve into the\nstrategies for selecting seed experience and insight, aiming to provide LLM\nwith more useful and relevant insight for better decision-making. Our\nobservations also indicate that MSI exhibits better robustness when facing\ndomain-shifting scenarios."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Yihuai Gao"
                    },
                    {
                        "name": "Che Jiang"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_journal_ref": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16682v1",
                "updated": "2024-09-25T07:18:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    18,
                    45,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:18:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    18,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "SynTQA: Synergistic Table-based Question Answering via Mixture of\n  Text-to-SQL and E2E TQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynTQA: Synergistic Table-based Question Answering via Mixture of\n  Text-to-SQL and E2E TQA"
                },
                "summary": "Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main\napproaches for Table-based Question Answering task. Despite success on multiple\nbenchmarks, they have yet to be compared and their synergy remains unexplored.\nIn this paper, we identify different strengths and weaknesses through\nevaluating state-of-the-art models on benchmark datasets: Text-to-SQL\ndemonstrates superiority in handling questions involving arithmetic operations\nand long tables; E2E TQA excels in addressing ambiguous questions, non-standard\ntable schema, and complex table contents. To combine both strengths, we propose\na Synergistic Table-based Question Answering approach that integrate different\nmodels via answer selection, which is agnostic to any model types. Further\nexperiments validate that ensembling models by either feature-based or\nLLM-based answer selector significantly improves the performance over\nindividual models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL parsing and end-to-end question answering (E2E TQA) are two main\napproaches for Table-based Question Answering task. Despite success on multiple\nbenchmarks, they have yet to be compared and their synergy remains unexplored.\nIn this paper, we identify different strengths and weaknesses through\nevaluating state-of-the-art models on benchmark datasets: Text-to-SQL\ndemonstrates superiority in handling questions involving arithmetic operations\nand long tables; E2E TQA excels in addressing ambiguous questions, non-standard\ntable schema, and complex table contents. To combine both strengths, we propose\na Synergistic Table-based Question Answering approach that integrate different\nmodels via answer selection, which is agnostic to any model types. Further\nexperiments validate that ensembling models by either feature-based or\nLLM-based answer selector significantly improves the performance over\nindividual models."
                },
                "authors": [
                    {
                        "name": "Siyue Zhang"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    },
                    {
                        "name": "Chen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhao"
                },
                "author": "Chen Zhao",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16674v1",
                "updated": "2024-09-25T07:06:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    6,
                    14,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T07:06:14Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    6,
                    14,
                    2,
                    269,
                    0
                ],
                "title": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Prompting-Based Representation Learning Method for Recommendation with\n  Large Language Models"
                },
                "summary": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Recommender Systems (RS) have witnessed a transformative\nshift with the advent of Large Language Models (LLMs) in the field of Natural\nLanguage Processing (NLP). Models such as GPT-3.5/4, Llama, have demonstrated\nunprecedented capabilities in understanding and generating human-like text. The\nextensive information pre-trained by these LLMs allows for the potential to\ncapture a more profound semantic representation from different contextual\ninformation of users and items.\n  While the great potential lies behind the thriving of LLMs, the challenge of\nleveraging user-item preferences from contextual information and its alignment\nwith the improvement of Recommender Systems needs to be addressed. Believing\nthat a better understanding of the user or item itself can be the key factor in\nimproving recommendation performance, we conduct research on generating\ninformative profiles using state-of-the-art LLMs.\n  To boost the linguistic abilities of LLMs in Recommender Systems, we\nintroduce the Prompting-Based Representation Learning Method for Recommendation\n(P4R). In our P4R framework, we utilize the LLM prompting strategy to create\npersonalized item profiles. These profiles are then transformed into semantic\nrepresentation spaces using a pre-trained BERT model for text embedding.\nFurthermore, we incorporate a Graph Convolution Network (GCN) for collaborative\nfiltering representation. The P4R framework aligns these two embedding spaces\nin order to address the general recommendation tasks. In our evaluation, we\ncompare P4R with state-of-the-art Recommender models and assess the quality of\nprompt-based profile generation."
                },
                "authors": [
                    {
                        "name": "Junyi Chen"
                    },
                    {
                        "name": "Toyotaro Suzumura"
                    }
                ],
                "author_detail": {
                    "name": "Toyotaro Suzumura"
                },
                "author": "Toyotaro Suzumura",
                "arxiv_comment": "Risks: The 1st International Workshop on Risks, Opportunities, and\n  Evaluation of Generative Models in Recommendation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03536v2",
                "updated": "2024-09-25T07:05:16Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    5,
                    16,
                    2,
                    269,
                    0
                ],
                "published": "2024-07-03T22:45:36Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    22,
                    45,
                    36,
                    2,
                    185,
                    0
                ],
                "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla (2) a curated dataset for bias measurement\nbenchmarking (3) testing two different probing techniques for bias detection in\nthe context of Bangla. This is the first work of such kind involving bias\nassessment of LLMs for Bangla to the best of our knowledge. All our code and\nresources are publicly available for the progress of bias related research in\nBangla NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla (2) a curated dataset for bias measurement\nbenchmarking (3) testing two different probing techniques for bias detection in\nthe context of Bangla. This is the first work of such kind involving bias\nassessment of LLMs for Bangla to the best of our knowledge. All our code and\nresources are publicly available for the progress of bias related research in\nBangla NLP."
                },
                "authors": [
                    {
                        "name": "Jayanta Sadhu"
                    },
                    {
                        "name": "Maneesha Rani Saha"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15868v2",
                "updated": "2024-09-25T07:03:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    7,
                    3,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T08:41:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    8,
                    41,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Privacy Evaluation Benchmarks for NLP Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy Evaluation Benchmarks for NLP Models"
                },
                "summary": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By inducing privacy attacks on NLP models, attackers can obtain sensitive\ninformation such as training data and model parameters, etc. Although\nresearchers have studied, in-depth, several kinds of attacks in NLP models,\nthey are non-systematic analyses. It lacks a comprehensive understanding of the\nimpact caused by the attacks. For example, we must consider which scenarios can\napply to which attacks, what the common factors are that affect the performance\nof different attacks, the nature of the relationships between different\nattacks, and the influence of various datasets and models on the effectiveness\nof the attacks, etc. Therefore, we need a benchmark to holistically assess the\nprivacy risks faced by NLP models. In this paper, we present a privacy attack\nand defense evaluation benchmark in the field of NLP, which includes the\nconventional/small models and large language models (LLMs). This benchmark\nsupports a variety of models, datasets, and protocols, along with standardized\nmodules for comprehensive evaluation of attacks and defense strategies. Based\non the above framework, we present a study on the association between auxiliary\ndata from different domains and the strength of privacy attacks. And we provide\nan improved attack method in this scenario with the help of Knowledge\nDistillation (KD). Furthermore, we propose a chained framework for privacy\nattacks. Allowing a practitioner to chain multiple attacks to achieve a\nhigher-level attack objective. Based on this, we provide some defense and\nenhanced attack strategies. The code for reproducing the results can be found\nat https://github.com/user2311717757/nlp_doctor."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "arxiv_comment": "Needs further optimization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16667v1",
                "updated": "2024-09-25T06:54:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T06:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "A Character-Centric Creative Story Generation via Imagination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Character-Centric Creative Story Generation via Imagination"
                },
                "summary": "Creative story generation with diverse and detailed story elements is a\nlong-standing goal for large language models. While existing methodologies\ngenerate long and coherent stories, they fall significantly short of human\ncapabilities in terms of diversity and character detail. To address this, we\nintroduce a novel story generation framework called CCI (Character-centric\nCreative story generation via Imagination). CCI features two innovative modules\nfor creative story generation: IG (Image-Guided Imagination) and MW\n(Multi-Writer model). In the IG module, we utilize DALL-E 3 to create visual\nrepresentations of key story elements. The IG generates more novel and concrete\ncharacters, backgrounds, and main plots than text-only methods. The MW module\nuses these story elements created by IG to generate multiple description\ncandidates for the protagonist and select the best one. This method\nincorporates vivid and rich character descriptions into the story. We compared\nthe stories generated by CCI and baseline models through human evaluation and\nstatistical analysis. The results showed significant improvements in the\ncreativity. Furthermore, by enabling interactive multi-modal story generation\nwith users, we have opened up possibilities for human-LLM integration in\ncultural development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative story generation with diverse and detailed story elements is a\nlong-standing goal for large language models. While existing methodologies\ngenerate long and coherent stories, they fall significantly short of human\ncapabilities in terms of diversity and character detail. To address this, we\nintroduce a novel story generation framework called CCI (Character-centric\nCreative story generation via Imagination). CCI features two innovative modules\nfor creative story generation: IG (Image-Guided Imagination) and MW\n(Multi-Writer model). In the IG module, we utilize DALL-E 3 to create visual\nrepresentations of key story elements. The IG generates more novel and concrete\ncharacters, backgrounds, and main plots than text-only methods. The MW module\nuses these story elements created by IG to generate multiple description\ncandidates for the protagonist and select the best one. This method\nincorporates vivid and rich character descriptions into the story. We compared\nthe stories generated by CCI and baseline models through human evaluation and\nstatistical analysis. The results showed significant improvements in the\ncreativity. Furthermore, by enabling interactive multi-modal story generation\nwith users, we have opened up possibilities for human-LLM integration in\ncultural development."
                },
                "authors": [
                    {
                        "name": "Kyeongman Park"
                    },
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v3",
                "updated": "2024-09-25T06:50:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    50,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cyber Incident Timeline Analysis Through Rule Based AI and\n  Large Language Models"
                },
                "summary": "Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeline Analysis (TA) plays a crucial role in Timeline Forensics (TF) within\nthe field of Digital Forensics (DF). It focuses on examining and analyzing\ntime-based digital artefacts, such as timestamps derived from event logs, file\nmetadata, and other relevant data, to correlate events linked to cyber\nincidents and reconstruct their chronological sequence. Traditional tools often\nstruggle to efficiently handle the large volume and variety of data generated\nduring DF investigations and Incident Response (IR) processes. This paper\nintroduces a novel framework, GenDFIR, which combines Rule-Based Artificial\nIntelligence (R-BAI) algorithms with Large Language Models (LLMs) to enhance\nand automate the TA process. The proposed approach consists of two key stages:\n(1) R-BAI is used to identify and select anomalous digital artefacts based on\npredefined rules. (2) The selected artefacts are then transformed into\nembeddings for processing by an LLM with the assistance of a\nRetrieval-Augmented Generation (RAG) agent. The LLM uses its capabilities to\nperform automated TA on the artefacts and predict potential incident outcomes.\nTo validate the framework, we evaluated its performance, efficiency, and\nreliability. Several metrics were applied to simulated cyber incident\nscenarios, which were presented as forensic case documents. Our findings\ndemonstrate the significant potential of integrating R-BAI and LLMs for TA.\nThis innovative approach underscores the power of Generative AI (GenAI),\nparticularly LLMs, and opens up new possibilities for advanced threat detection\nand incident reconstruction, marking a significant advancement in the field."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Chahine Ghanem"
                },
                "author": "Mohamed Chahine Ghanem",
                "arxiv_comment": "22 pages V3.1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18181v2",
                "updated": "2024-09-25T06:47:10Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    47,
                    10,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-26T08:57:03Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    57,
                    3,
                    2,
                    178,
                    0
                ],
                "title": "On the Evaluation of Large Language Models in Unit Test Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Evaluation of Large Language Models in Unit Test Generation"
                },
                "summary": "Unit testing is an essential activity in software development for verifying\nthe correctness of software components. However, manually writing unit tests is\nchallenging and time-consuming. The emergence of Large Language Models (LLMs)\noffers a new direction for automating unit test generation. Existing research\nprimarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed\nprompting strategies, leaving the capabilities of advanced open-source LLMs\nwith various prompting settings unexplored. Particularly, open-source LLMs\noffer advantages in data privacy protection and have demonstrated superior\nperformance in some tasks. Moreover, effective prompting is crucial for\nmaximizing LLMs' capabilities. In this paper, we conduct the first empirical\nstudy to fill this gap, based on 17 Java projects, five widely-used open-source\nLLMs with different structures and parameter sizes, and comprehensive\nevaluation metrics. Our findings highlight the significant influence of various\nprompt factors, show the performance of open-source LLMs compared to the\ncommercial GPT-4 and the traditional Evosuite, and identify limitations in\nLLM-based unit test generation. We then derive a series of implications from\nour study to guide future research and practical use of LLM-based unit test\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit testing is an essential activity in software development for verifying\nthe correctness of software components. However, manually writing unit tests is\nchallenging and time-consuming. The emergence of Large Language Models (LLMs)\noffers a new direction for automating unit test generation. Existing research\nprimarily focuses on closed-source LLMs (e.g., ChatGPT and CodeX) with fixed\nprompting strategies, leaving the capabilities of advanced open-source LLMs\nwith various prompting settings unexplored. Particularly, open-source LLMs\noffer advantages in data privacy protection and have demonstrated superior\nperformance in some tasks. Moreover, effective prompting is crucial for\nmaximizing LLMs' capabilities. In this paper, we conduct the first empirical\nstudy to fill this gap, based on 17 Java projects, five widely-used open-source\nLLMs with different structures and parameter sizes, and comprehensive\nevaluation metrics. Our findings highlight the significant influence of various\nprompt factors, show the performance of open-source LLMs compared to the\ncommercial GPT-4 and the traditional Evosuite, and identify limitations in\nLLM-based unit test generation. We then derive a series of implications from\nour study to guide future research and practical use of LLM-based unit test\ngeneration."
                },
                "authors": [
                    {
                        "name": "Lin Yang"
                    },
                    {
                        "name": "Chen Yang"
                    },
                    {
                        "name": "Shutao Gao"
                    },
                    {
                        "name": "Weijing Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Xiao Chu"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Guangtai Liang"
                    },
                    {
                        "name": "Qianxiang Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Chen"
                },
                "author": "Junjie Chen",
                "arxiv_comment": "Accepted by ASE 2024, Research Paper Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v2",
                "updated": "2024-09-25T06:46:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    46,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Unlike existing works that encodes the whole context, its main idea\nlies in dividing the retrieved documents into blocks, where each block\ncalculates key-value (KV) states independently except for the final block. In\nRAG scenarios, by defining each passage as a block, Block-Attention enables us\nto pre-compute the KV states for all passages and cache them in memory,\nsignificantly reducing the latency and the computation cost during inference.\nThe implementation involves block segmentation, positional encoding\ncalculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism.\nExperiments on four RAG benchmarks demonstrate that after block fine-tuning,\nthe Block Attention model can achieve performance comparable to (68.4\\% vs\n67.9\\% on Llama3) or even better (62.8\\% vs 59.6\\% on Mistral) than\nself-attention models. Notably, Block-Attention reduces the TTFT (the time to\nfirst token) and FLOPs (floating point operations) to a very low level. It only\ntakes 45 ms to output the first token for an input sequence with a total length\nof 32K. Compared with the self-attention model, the time consumption and\ncorresponding FLOPs are reduced by 98.7\\% and 99.8\\%, respectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.00781v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.00781v3",
                "updated": "2024-09-25T06:31:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    31,
                    9,
                    2,
                    269,
                    0
                ],
                "published": "2024-02-18T06:07:17Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    6,
                    7,
                    17,
                    6,
                    49,
                    0
                ],
                "title": "ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender\n  Chatbots through an LLM-Augmented Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender\n  Chatbots through an LLM-Augmented Framework"
                },
                "summary": "The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The profound impact of food on health necessitates advanced\nnutrition-oriented food recommendation services. Conventional methods often\nlack the crucial elements of personalization, explainability, and\ninteractivity. While Large Language Models (LLMs) bring interpretability and\nexplainability, their standalone use falls short of achieving true\npersonalization. In this paper, we introduce ChatDiet, a novel LLM-powered\nframework designed specifically for personalized nutrition-oriented food\nrecommendation chatbots. ChatDiet integrates personal and population models,\ncomplemented by an orchestrator, to seamlessly retrieve and process pertinent\ninformation. The personal model leverages causal discovery and inference\ntechniques to assess personalized nutritional effects for a specific user,\nwhereas the population model provides generalized information on food\nnutritional content. The orchestrator retrieves, synergizes and delivers the\noutput of both models to the LLM, providing tailored food recommendations\ndesigned to support targeted health outcomes. The result is a dynamic delivery\nof personalized and explainable food recommendations, tailored to individual\nuser preferences. Our evaluation of ChatDiet includes a compelling case study,\nwhere we establish a causal personal model to estimate individual nutrition\neffects. Our assessments, including a food recommendation test showcasing a\n92\\% effectiveness rate, coupled with illustrative dialogue examples,\nunderscore ChatDiet's strengths in explainability, personalization, and\ninteractivity."
                },
                "authors": [
                    {
                        "name": "Zhongqi Yang"
                    },
                    {
                        "name": "Elahe Khatibi"
                    },
                    {
                        "name": "Nitish Nagesh"
                    },
                    {
                        "name": "Mahyar Abbasian"
                    },
                    {
                        "name": "Iman Azimi"
                    },
                    {
                        "name": "Ramesh Jain"
                    },
                    {
                        "name": "Amir M. Rahmani"
                    }
                ],
                "author_detail": {
                    "name": "Amir M. Rahmani"
                },
                "author": "Amir M. Rahmani",
                "arxiv_doi": "10.1016/j.smhl.2024.100465",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.smhl.2024.100465",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.00781v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.00781v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published on Smart Health",
                "arxiv_journal_ref": "Smart Health 32 (2024): 100465",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11871v2",
                "updated": "2024-09-25T06:21:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    21,
                    26,
                    2,
                    269,
                    0
                ],
                "published": "2024-08-19T13:27:07Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    13,
                    27,
                    7,
                    0,
                    232,
                    0
                ],
                "title": "MegaFake: A Theory-Driven Dataset of Fake News Generated by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFake: A Theory-Driven Dataset of Fake News Generated by Large\n  Language Models"
                },
                "summary": "The advent of large language models (LLMs) has revolutionized online content\ncreation, making it much easier to generate high-quality fake news. This misuse\nthreatens the integrity of our digital environment and ethical standards.\nTherefore, understanding the motivations and mechanisms behind LLM-generated\nfake news is crucial. In this study, we analyze the creation of fake news from\na social psychology perspective and develop a comprehensive LLM-based\ntheoretical framework, LLM-Fake Theory. We introduce a novel pipeline that\nautomates the generation of fake news using LLMs, thereby eliminating the need\nfor manual annotation. Utilizing this pipeline, we create a theoretically\ninformed Machine-generated Fake news dataset, MegaFake, derived from the\nGossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake\ndataset. We believe that our dataset and insights will provide valuable\ncontributions to future research focused on the detection and governance of\nfake news in the era of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs) has revolutionized online content\ncreation, making it much easier to generate high-quality fake news. This misuse\nthreatens the integrity of our digital environment and ethical standards.\nTherefore, understanding the motivations and mechanisms behind LLM-generated\nfake news is crucial. In this study, we analyze the creation of fake news from\na social psychology perspective and develop a comprehensive LLM-based\ntheoretical framework, LLM-Fake Theory. We introduce a novel pipeline that\nautomates the generation of fake news using LLMs, thereby eliminating the need\nfor manual annotation. Utilizing this pipeline, we create a theoretically\ninformed Machine-generated Fake news dataset, MegaFake, derived from the\nGossipCop dataset. We conduct comprehensive analyses to evaluate our MegaFake\ndataset. We believe that our dataset and insights will provide valuable\ncontributions to future research focused on the detection and governance of\nfake news in the era of LLMs."
                },
                "authors": [
                    {
                        "name": "Lionel Z. Wang"
                    },
                    {
                        "name": "Yiming Ma"
                    },
                    {
                        "name": "Renfei Gao"
                    },
                    {
                        "name": "Beichen Guo"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Zexin Lu"
                    },
                    {
                        "name": "Ka Chung Ng"
                    }
                ],
                "author_detail": {
                    "name": "Ka Chung Ng"
                },
                "author": "Ka Chung Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16656v1",
                "updated": "2024-09-25T06:19:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    19,
                    54,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T06:19:54Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    19,
                    54,
                    2,
                    269,
                    0
                ],
                "title": "A Rule-Based Approach for UI Migration from Android to iOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Rule-Based Approach for UI Migration from Android to iOS"
                },
                "summary": "In the mobile development process, creating the user interface (UI) is highly\nresource intensive. Consequently, numerous studies have focused on automating\nUI development, such as generating UI from screenshots or design\nspecifications. However, they heavily rely on computer vision techniques for\nimage recognition. Any recognition errors can cause invalid UI element\ngeneration, compromising the effectiveness of these automated approaches.\nMoreover, developing an app UI from scratch remains a time consuming and labor\nintensive task.\n  To address this challenge, we propose a novel approach called GUIMIGRATOR,\nwhich enables the cross platform migration of existing Android app UIs to iOS,\nthereby automatically generating UI to facilitate the reuse of existing UI.\nThis approach not only avoids errors from screenshot recognition but also\nreduces the cost of developing UIs from scratch. GUIMIGRATOR extracts and\nparses Android UI layouts, views, and resources to construct a UI skeleton\ntree. GUIMIGRATOR generates the final UI code files utilizing target code\ntemplates, which are then compiled and validated in the iOS development\nplatform, i.e., Xcode. We evaluate the effectiveness of GUIMIGRATOR on 31\nAndroid open source applications across ten domains. The results show that\nGUIMIGRATOR achieves a UI similarity score of 78 between migration screenshots,\noutperforming two popular existing LLMs substantially. Additionally,\nGUIMIGRATOR demonstrates high efficiency, taking only 7.6 seconds to migrate\nthe datasets. These findings indicate that GUIMIGRATOR effectively facilitates\nthe reuse of Android UI code on iOS, leveraging the strengths of both platforms\nUI frameworks and making new contributions to cross platform development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the mobile development process, creating the user interface (UI) is highly\nresource intensive. Consequently, numerous studies have focused on automating\nUI development, such as generating UI from screenshots or design\nspecifications. However, they heavily rely on computer vision techniques for\nimage recognition. Any recognition errors can cause invalid UI element\ngeneration, compromising the effectiveness of these automated approaches.\nMoreover, developing an app UI from scratch remains a time consuming and labor\nintensive task.\n  To address this challenge, we propose a novel approach called GUIMIGRATOR,\nwhich enables the cross platform migration of existing Android app UIs to iOS,\nthereby automatically generating UI to facilitate the reuse of existing UI.\nThis approach not only avoids errors from screenshot recognition but also\nreduces the cost of developing UIs from scratch. GUIMIGRATOR extracts and\nparses Android UI layouts, views, and resources to construct a UI skeleton\ntree. GUIMIGRATOR generates the final UI code files utilizing target code\ntemplates, which are then compiled and validated in the iOS development\nplatform, i.e., Xcode. We evaluate the effectiveness of GUIMIGRATOR on 31\nAndroid open source applications across ten domains. The results show that\nGUIMIGRATOR achieves a UI similarity score of 78 between migration screenshots,\noutperforming two popular existing LLMs substantially. Additionally,\nGUIMIGRATOR demonstrates high efficiency, taking only 7.6 seconds to migrate\nthe datasets. These findings indicate that GUIMIGRATOR effectively facilitates\nthe reuse of Android UI code on iOS, leveraging the strengths of both platforms\nUI frameworks and making new contributions to cross platform development."
                },
                "authors": [
                    {
                        "name": "Yi Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16654v1",
                "updated": "2024-09-25T06:17:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    17,
                    23,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T06:17:23Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    17,
                    23,
                    2,
                    269,
                    0
                ],
                "title": "Speech Recognition Rescoring with Large Speech-Text Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Recognition Rescoring with Large Speech-Text Foundation Models"
                },
                "summary": "Large language models (LLM) have demonstrated the ability to understand human\nlanguage by leveraging large amount of text data. Automatic speech recognition\n(ASR) systems are often limited by available transcribed speech data and\nbenefit from a second pass rescoring using LLM. Recently multi-modal large\nlanguage models, particularly speech and text foundational models have\ndemonstrated strong spoken language understanding. Speech-Text foundational\nmodels leverage large amounts of unlabelled and labelled data both in speech\nand text modalities to model human language. In this work, we propose novel\ntechniques to use multi-modal LLM for ASR rescoring. We also explore\ndiscriminative training to further improve the foundational model rescoring\nperformance. We demonstrate cross-modal knowledge transfer in speech-text LLM\ncan benefit rescoring. Our experiments demonstrate up-to 20% relative\nimprovements over Whisper large ASR and up-to 15% relative improvements over\ntext-only LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) have demonstrated the ability to understand human\nlanguage by leveraging large amount of text data. Automatic speech recognition\n(ASR) systems are often limited by available transcribed speech data and\nbenefit from a second pass rescoring using LLM. Recently multi-modal large\nlanguage models, particularly speech and text foundational models have\ndemonstrated strong spoken language understanding. Speech-Text foundational\nmodels leverage large amounts of unlabelled and labelled data both in speech\nand text modalities to model human language. In this work, we propose novel\ntechniques to use multi-modal LLM for ASR rescoring. We also explore\ndiscriminative training to further improve the foundational model rescoring\nperformance. We demonstrate cross-modal knowledge transfer in speech-text LLM\ncan benefit rescoring. Our experiments demonstrate up-to 20% relative\nimprovements over Whisper large ASR and up-to 15% relative improvements over\ntext-only LLM."
                },
                "authors": [
                    {
                        "name": "Prashanth Gurunath Shivakumar"
                    },
                    {
                        "name": "Jari Kolehmainen"
                    },
                    {
                        "name": "Aditya Gourav"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Ankur Gandhe"
                    },
                    {
                        "name": "Ariya Rastrow"
                    },
                    {
                        "name": "Ivan Bulyko"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Bulyko"
                },
                "author": "Ivan Bulyko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15146v2",
                "updated": "2024-09-25T05:59:08Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    59,
                    8,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-23T15:53:41Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    53,
                    41,
                    0,
                    267,
                    0
                ],
                "title": "COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large\n  Language Models"
                },
                "summary": "Leveraging the powerful reasoning capabilities of large language models\n(LLMs), recent LLM-based robot task planning methods yield promising results.\nHowever, they mainly focus on single or multiple homogeneous robots on simple\ntasks. Practically, complex long-horizon tasks always require collaborations\namong multiple heterogeneous robots especially with more complex action spaces,\nwhich makes these tasks more challenging. To this end, we propose COHERENT, a\nnovel LLM-based task planning framework for collaboration of heterogeneous\nmulti-robot systems including quadrotors, robotic dogs, and robotic arms.\nSpecifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is\ndesigned to decompose and assign actions for individual robots, where a\ncentralized task assigner makes a task planning proposal to decompose the\ncomplex task into subtasks, and then assigns subtasks to robot executors. Each\nrobot executor selects a feasible action to implement the assigned subtask and\nreports self-reflection feedback to the task assigner for plan adjustment. The\nPEFA loops until the task is completed. Moreover, we create a challenging\nheterogeneous multi-robot task planning benchmark encompassing 100 complex\nlong-horizon tasks. The experimental results show that our work surpasses the\nprevious methods by a large margin in terms of success rate and execution\nefficiency. The experimental videos, code, and benchmark are released at\nhttps://github.com/MrKeee/COHERENT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the powerful reasoning capabilities of large language models\n(LLMs), recent LLM-based robot task planning methods yield promising results.\nHowever, they mainly focus on single or multiple homogeneous robots on simple\ntasks. Practically, complex long-horizon tasks always require collaborations\namong multiple heterogeneous robots especially with more complex action spaces,\nwhich makes these tasks more challenging. To this end, we propose COHERENT, a\nnovel LLM-based task planning framework for collaboration of heterogeneous\nmulti-robot systems including quadrotors, robotic dogs, and robotic arms.\nSpecifically, a Proposal-Execution-Feedback-Adjustment (PEFA) mechanism is\ndesigned to decompose and assign actions for individual robots, where a\ncentralized task assigner makes a task planning proposal to decompose the\ncomplex task into subtasks, and then assigns subtasks to robot executors. Each\nrobot executor selects a feasible action to implement the assigned subtask and\nreports self-reflection feedback to the task assigner for plan adjustment. The\nPEFA loops until the task is completed. Moreover, we create a challenging\nheterogeneous multi-robot task planning benchmark encompassing 100 complex\nlong-horizon tasks. The experimental results show that our work surpasses the\nprevious methods by a large margin in terms of success rate and execution\nefficiency. The experimental videos, code, and benchmark are released at\nhttps://github.com/MrKeee/COHERENT."
                },
                "authors": [
                    {
                        "name": "Kehui Liu"
                    },
                    {
                        "name": "Zixin Tang"
                    },
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Bin Zhao"
                    },
                    {
                        "name": "Xuelong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuelong Li"
                },
                "author": "Xuelong Li",
                "arxiv_comment": "7 pages, 5 figures. Submitted to IEEE International Conference on\n  Robotics and Automation (ICRA), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.05920v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.05920v3",
                "updated": "2024-09-25T05:57:51Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    57,
                    51,
                    2,
                    269,
                    0
                ],
                "published": "2023-05-10T06:17:50Z",
                "published_parsed": [
                    2023,
                    5,
                    10,
                    6,
                    17,
                    50,
                    2,
                    130,
                    0
                ],
                "title": "Fast Distributed Inference Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Distributed Inference Serving for Large Language Models"
                },
                "summary": "Large language models (LLMs) power a new generation of interactive AI\napplications exemplified by ChatGPT. The interactive nature of these\napplications demands low latency for LLM inference. Existing LLM serving\nsystems use run-to-completion processing for inference jobs, which suffers from\nhead-of-line blocking and long latency.\n  We present FastServe, a distributed inference serving system for LLMs.\nFastServe exploits the autoregressive pattern of LLM inference to enable\npreemption at the granularity of each output token. FastServe uses preemptive\nscheduling to minimize latency with a novel skip-join Multi-Level Feedback\nQueue scheduler. Based on the new semi-information-agnostic setting of LLM\ninference, the scheduler leverages the input length information to assign an\nappropriate initial queue for each arrival job to join. The higher priority\nqueues than the joined queue are skipped to reduce demotions. We design an\nefficient GPU memory management mechanism that proactively offloads and uploads\nintermediate state between GPU memory and host memory for LLM inference. We\nbuild a system prototype of FastServe and experimental results show that\ncompared to the state-of-the-art solution vLLM, FastServe improves the\nthroughput by up to 31.4x and 17.9x under the same average and tail latency\nrequirements, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) power a new generation of interactive AI\napplications exemplified by ChatGPT. The interactive nature of these\napplications demands low latency for LLM inference. Existing LLM serving\nsystems use run-to-completion processing for inference jobs, which suffers from\nhead-of-line blocking and long latency.\n  We present FastServe, a distributed inference serving system for LLMs.\nFastServe exploits the autoregressive pattern of LLM inference to enable\npreemption at the granularity of each output token. FastServe uses preemptive\nscheduling to minimize latency with a novel skip-join Multi-Level Feedback\nQueue scheduler. Based on the new semi-information-agnostic setting of LLM\ninference, the scheduler leverages the input length information to assign an\nappropriate initial queue for each arrival job to join. The higher priority\nqueues than the joined queue are skipped to reduce demotions. We design an\nefficient GPU memory management mechanism that proactively offloads and uploads\nintermediate state between GPU memory and host memory for LLM inference. We\nbuild a system prototype of FastServe and experimental results show that\ncompared to the state-of-the-art solution vLLM, FastServe improves the\nthroughput by up to 31.4x and 17.9x under the same average and tail latency\nrequirements, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Fangyue Liu"
                    },
                    {
                        "name": "Yuanhang Sun"
                    },
                    {
                        "name": "Gang Huang"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.05920v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.05920v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16644v1",
                "updated": "2024-09-25T05:44:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    44,
                    44,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T05:44:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    44,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation"
                },
                "summary": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Siyin Wang"
                    },
                    {
                        "name": "Wenyi Yu"
                    },
                    {
                        "name": "Yudong Yang"
                    },
                    {
                        "name": "Changli Tang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Jimin Zhuang"
                    },
                    {
                        "name": "Xianzhao Chen"
                    },
                    {
                        "name": "Xiaohai Tian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Guangzhi Sun"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16635v1",
                "updated": "2024-09-25T05:28:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    28,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T05:28:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    28,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Judgment of Thoughts: Courtroom of the Binary Logical Reasoning in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgment of Thoughts: Courtroom of the Binary Logical Reasoning in Large\n  Language Models"
                },
                "summary": "This paper proposes a novel prompt engineering technique called Judgment of\nThought (JoT) that is specifically tailored for binary logical reasoning tasks.\nJoT employs three roles$\\unicode{x2014}$lawyer, prosecutor, and\njudge$\\unicode{x2014}$to facilitate more reliable and accurate reasoning by the\nmodel. In this framework, the judge utilizes a high$\\unicode{x2010}$level\nmodel, while the lawyer and prosecutor utilize low$\\unicode{x2010}$level\nmodels. This structure helps the judge better understand the responses from\nboth the lawyer and prosecutor, enabling a more accurate judgment. Experimental\nresults on large language model (LLM) benchmark datasets, such as BigBenchHard\nand Winogrande, demonstrate that JoT outperforms existing methods, including\nChain of Thought (CoT) and Self$\\unicode{x2010}$Consistency (SC), in binary\nlogical reasoning tasks. Additionally, in real$\\unicode{x2010}$world tasks,\nsuch as Fake News Detection and SMS Spam Detection, JoT shows comparable or\nimproved performance compared to existing techniques. JoT significantly\nenhances the accuracy and reliability of models in binary reasoning tasks and\nshow potential for practical applicability across various domains. Future\nresearch should aim to further broaden the applicability of JoT and optimize\nits implementation for real$\\unicode{x2010}$world\nproblem$\\unicode{x2010}$solving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel prompt engineering technique called Judgment of\nThought (JoT) that is specifically tailored for binary logical reasoning tasks.\nJoT employs three roles$\\unicode{x2014}$lawyer, prosecutor, and\njudge$\\unicode{x2014}$to facilitate more reliable and accurate reasoning by the\nmodel. In this framework, the judge utilizes a high$\\unicode{x2010}$level\nmodel, while the lawyer and prosecutor utilize low$\\unicode{x2010}$level\nmodels. This structure helps the judge better understand the responses from\nboth the lawyer and prosecutor, enabling a more accurate judgment. Experimental\nresults on large language model (LLM) benchmark datasets, such as BigBenchHard\nand Winogrande, demonstrate that JoT outperforms existing methods, including\nChain of Thought (CoT) and Self$\\unicode{x2010}$Consistency (SC), in binary\nlogical reasoning tasks. Additionally, in real$\\unicode{x2010}$world tasks,\nsuch as Fake News Detection and SMS Spam Detection, JoT shows comparable or\nimproved performance compared to existing techniques. JoT significantly\nenhances the accuracy and reliability of models in binary reasoning tasks and\nshow potential for practical applicability across various domains. Future\nresearch should aim to further broaden the applicability of JoT and optimize\nits implementation for real$\\unicode{x2010}$world\nproblem$\\unicode{x2010}$solving."
                },
                "authors": [
                    {
                        "name": "Sungjune Park"
                    },
                    {
                        "name": "Daeseon Choi"
                    }
                ],
                "author_detail": {
                    "name": "Daeseon Choi"
                },
                "author": "Daeseon Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16626v1",
                "updated": "2024-09-25T05:11:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    11,
                    58,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T05:11:58Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    11,
                    58,
                    2,
                    269,
                    0
                ],
                "title": "Ascend HiFloat8 Format for Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ascend HiFloat8 Format for Deep Learning"
                },
                "summary": "This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponents with 3-bit\nmantissa, 8 exponents with 2-bit mantissa, and 16 exponents with 1-bit\nmantissa. For denormal or subnormal value encoding, it extends the dynamic\nrange by 7 extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40\nbinades). Meanwhile, HiF8 encodes all the special values except that positive\nzero and negative zero are represented by only one bit-pattern. Thanks to the\nbetter balance between precision and dynamic range, HiF8 can be simultaneously\nused in both forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8 format,\nmassive simulation results on various neural networks, including traditional\nneural networks and large language models (LLMs), will also be presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This preliminary white paper proposes a novel 8-bit floating-point data\nformat HiFloat8 (abbreviated as HiF8) for deep learning. HiF8 features tapered\nprecision. For normal value encoding, it provides 7 exponents with 3-bit\nmantissa, 8 exponents with 2-bit mantissa, and 16 exponents with 1-bit\nmantissa. For denormal or subnormal value encoding, it extends the dynamic\nrange by 7 extra powers of 2, from 31 to 38 binades (notice that FP16 covers 40\nbinades). Meanwhile, HiF8 encodes all the special values except that positive\nzero and negative zero are represented by only one bit-pattern. Thanks to the\nbetter balance between precision and dynamic range, HiF8 can be simultaneously\nused in both forward and backward passes of AI training. In this paper, we will\ndescribe the definition and rounding methods of HiF8, as well as the tentative\ntraining and inference solutions. To demonstrate the efficacy of HiF8 format,\nmassive simulation results on various neural networks, including traditional\nneural networks and large language models (LLMs), will also be presented."
                },
                "authors": [
                    {
                        "name": "Yuanyong Luo"
                    },
                    {
                        "name": "Zhongxing Zhang"
                    },
                    {
                        "name": "Richard Wu"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Ying Jin"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Minmin Wang"
                    },
                    {
                        "name": "Zhanying He"
                    },
                    {
                        "name": "Guipeng Hu"
                    },
                    {
                        "name": "Luyao Chen"
                    },
                    {
                        "name": "Tianchi Hu"
                    },
                    {
                        "name": "Junsong Wang"
                    },
                    {
                        "name": "Minqi Chen"
                    },
                    {
                        "name": "Mikhaylov Dmitry"
                    },
                    {
                        "name": "Korviakov Vladimir"
                    },
                    {
                        "name": "Bobrin Maxim"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Guanfu Chen"
                    },
                    {
                        "name": "Zeyi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zeyi Huang"
                },
                "author": "Zeyi Huang",
                "arxiv_comment": "13 Pages, 4 Figures, 9 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16621v1",
                "updated": "2024-09-25T05:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    7,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T05:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    5,
                    7,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Entailment-Driven Privacy Policy Classification with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entailment-Driven Privacy Policy Classification with LLMs"
                },
                "summary": "While many online services provide privacy policies for end users to read and\nunderstand what personal data are being collected, these documents are often\nlengthy and complicated. As a result, the vast majority of users do not read\nthem at all, leading to data collection under uninformed consent. Several\nattempts have been made to make privacy policies more user friendly by\nsummarising them, providing automatic annotations or labels for key sections,\nor by offering chat interfaces to ask specific questions. With recent advances\nin Large Language Models (LLMs), there is an opportunity to develop more\neffective tools to parse privacy policies and help users make informed\ndecisions. In this paper, we propose an entailment-driven LLM based framework\nto classify paragraphs of privacy policies into meaningful labels that are\neasily understood by users. The results demonstrate that our framework\noutperforms traditional LLM methods, improving the F1 score in average by\n11.2%. Additionally, our framework provides inherently explainable and\nmeaningful predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While many online services provide privacy policies for end users to read and\nunderstand what personal data are being collected, these documents are often\nlengthy and complicated. As a result, the vast majority of users do not read\nthem at all, leading to data collection under uninformed consent. Several\nattempts have been made to make privacy policies more user friendly by\nsummarising them, providing automatic annotations or labels for key sections,\nor by offering chat interfaces to ask specific questions. With recent advances\nin Large Language Models (LLMs), there is an opportunity to develop more\neffective tools to parse privacy policies and help users make informed\ndecisions. In this paper, we propose an entailment-driven LLM based framework\nto classify paragraphs of privacy policies into meaningful labels that are\neasily understood by users. The results demonstrate that our framework\noutperforms traditional LLM methods, improving the F1 score in average by\n11.2%. Additionally, our framework provides inherently explainable and\nmeaningful predictions."
                },
                "authors": [
                    {
                        "name": "Bhanuka Silva"
                    },
                    {
                        "name": "Dishanika Denipitiyage"
                    },
                    {
                        "name": "Suranga Seneviratne"
                    },
                    {
                        "name": "Anirban Mahanti"
                    },
                    {
                        "name": "Aruna Seneviratne"
                    }
                ],
                "author_detail": {
                    "name": "Aruna Seneviratne"
                },
                "author": "Aruna Seneviratne",
                "arxiv_comment": "8 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14509v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14509v3",
                "updated": "2024-09-26T03:15:53Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    3,
                    15,
                    53,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-22T16:13:00Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    16,
                    13,
                    0,
                    6,
                    266,
                    0
                ],
                "title": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving\n  Human-AI Alignment in the Writing Process through Edits"
                },
                "summary": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human-written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM-generated text, formalizing it into a seven-category\ntaxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nwe explored automatic editing methods to improve LLM-generated text. A\nlarge-scale preference annotation confirms that although experts largely prefer\ntext edited by other experts, automatic editing methods show promise in\nimproving alignment between LLM-generated and human-written text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based applications are helping people write, and LLM-generated text is\nmaking its way into social media, journalism, and our classrooms. However, the\ndifferences between LLM-generated and human-written text remain unclear. To\nexplore this, we hired professional writers to edit paragraphs in several\ncreative domains. We first found these writers agree on undesirable\nidiosyncrasies in LLM-generated text, formalizing it into a seven-category\ntaxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP\ncorpus: 1,057 LLM-generated paragraphs edited by professional writers according\nto our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our\nstudy (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms\nof writing quality, revealing common limitations across model families. Third,\nwe explored automatic editing methods to improve LLM-generated text. A\nlarge-scale preference annotation confirms that although experts largely prefer\ntext edited by other experts, automatic editing methods show promise in\nimproving alignment between LLM-generated and human-written text."
                },
                "authors": [
                    {
                        "name": "Tuhin Chakrabarty"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Chien-Sheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Sheng Wu"
                },
                "author": "Chien-Sheng Wu",
                "arxiv_comment": "NLP+HCI, Behavioral Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14509v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14509v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02374v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02374v5",
                "updated": "2024-09-25T04:50:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    4,
                    50,
                    38,
                    2,
                    269,
                    0
                ],
                "published": "2023-10-03T18:54:10Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    18,
                    54,
                    10,
                    1,
                    276,
                    0
                ],
                "title": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Health Agents: A Personalized LLM-Powered Agent Framework"
                },
                "summary": "Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance and diagnosis. Current CHAs, especially\nthose utilizing Large Language Models (LLMs), primarily focus on conversation\naspects. However, they offer limited agent capabilities, specifically lacking\nmulti-step problem-solving, personalized conversations, and multimodal data\nanalysis. Our aim is to overcome these limitations. We propose openCHA, an\nopen-source LLM-powered framework, to empower conversational agents to generate\na personalized response for users' healthcare queries. This framework enables\ndevelopers to integrate external sources including data sources, knowledge\nbases, and analysis models, into their LLM-based solutions. openCHA includes an\norchestrator to plan and execute actions for gathering information from\nexternal sources, essential for formulating responses to user inquiries. It\nfacilitates knowledge acquisition, problem-solving capabilities, multilingual\nand multimodal conversations, and fosters interaction with various AI\nplatforms. We illustrate the framework's proficiency in handling complex\nhealthcare tasks via two demonstrations and four use cases. Moreover, we\nrelease openCHA as open source available to the community via GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Health Agents (CHAs) are interactive systems that provide\nhealthcare services, such as assistance and diagnosis. Current CHAs, especially\nthose utilizing Large Language Models (LLMs), primarily focus on conversation\naspects. However, they offer limited agent capabilities, specifically lacking\nmulti-step problem-solving, personalized conversations, and multimodal data\nanalysis. Our aim is to overcome these limitations. We propose openCHA, an\nopen-source LLM-powered framework, to empower conversational agents to generate\na personalized response for users' healthcare queries. This framework enables\ndevelopers to integrate external sources including data sources, knowledge\nbases, and analysis models, into their LLM-based solutions. openCHA includes an\norchestrator to plan and execute actions for gathering information from\nexternal sources, essential for formulating responses to user inquiries. It\nfacilitates knowledge acquisition, problem-solving capabilities, multilingual\nand multimodal conversations, and fosters interaction with various AI\nplatforms. We illustrate the framework's proficiency in handling complex\nhealthcare tasks via two demonstrations and four use cases. Moreover, we\nrelease openCHA as open source available to the community via GitHub."
                },
                "authors": [
                    {
                        "name": "Mahyar Abbasian"
                    },
                    {
                        "name": "Iman Azimi"
                    },
                    {
                        "name": "Amir M. Rahmani"
                    },
                    {
                        "name": "Ramesh Jain"
                    }
                ],
                "author_detail": {
                    "name": "Ramesh Jain"
                },
                "author": "Ramesh Jain",
                "arxiv_comment": "23 pages, 6 figures, 2 tables, 4 appendices, journal paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02374v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02374v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04511v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04511v2",
                "updated": "2024-09-25T04:36:14Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    4,
                    36,
                    14,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-06T21:08:07Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    21,
                    8,
                    7,
                    3,
                    158,
                    0
                ],
                "title": "Classification of Non-native Handwritten Characters Using Convolutional\n  Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of Non-native Handwritten Characters Using Convolutional\n  Neural Network"
                },
                "summary": "The use of convolutional neural networks (CNNs) has accelerated the progress\nof handwritten character classification/recognition. Handwritten character\nrecognition (HCR) has found applications in various domains, such as traffic\nsignal detection, language translation, and document information extraction.\nHowever, the widespread use of existing HCR technology is yet to be seen as it\ndoes not provide reliable character recognition with outstanding accuracy. One\nof the reasons for unreliable HCR is that existing HCR methods do not take the\nhandwriting styles of non-native writers into account. Hence, further\nimprovement is needed to ensure the reliability and extensive deployment of\ncharacter recognition technologies for critical tasks. In this work, the\nclassification of English characters written by non-native users is performed\nby proposing a custom-tailored CNN model. We train this CNN with a new dataset\ncalled the handwritten isolated English character (HIEC) dataset. This dataset\nconsists of 16,496 images collected from 260 persons. This paper also includes\nan ablation study of our CNN by adjusting hyperparameters to identify the best\nmodel for the HIEC dataset. The proposed model with five convolutional layers\nand one hidden layer outperforms state-of-the-art models in terms of character\nrecognition accuracy and achieves an accuracy of $\\mathbf{97.04}$%. Compared\nwith the second-best model, the relative improvement of our model in terms of\nclassification accuracy is $\\mathbf{4.38}$%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of convolutional neural networks (CNNs) has accelerated the progress\nof handwritten character classification/recognition. Handwritten character\nrecognition (HCR) has found applications in various domains, such as traffic\nsignal detection, language translation, and document information extraction.\nHowever, the widespread use of existing HCR technology is yet to be seen as it\ndoes not provide reliable character recognition with outstanding accuracy. One\nof the reasons for unreliable HCR is that existing HCR methods do not take the\nhandwriting styles of non-native writers into account. Hence, further\nimprovement is needed to ensure the reliability and extensive deployment of\ncharacter recognition technologies for critical tasks. In this work, the\nclassification of English characters written by non-native users is performed\nby proposing a custom-tailored CNN model. We train this CNN with a new dataset\ncalled the handwritten isolated English character (HIEC) dataset. This dataset\nconsists of 16,496 images collected from 260 persons. This paper also includes\nan ablation study of our CNN by adjusting hyperparameters to identify the best\nmodel for the HIEC dataset. The proposed model with five convolutional layers\nand one hidden layer outperforms state-of-the-art models in terms of character\nrecognition accuracy and achieves an accuracy of $\\mathbf{97.04}$%. Compared\nwith the second-best model, the relative improvement of our model in terms of\nclassification accuracy is $\\mathbf{4.38}$%."
                },
                "authors": [
                    {
                        "name": "F. A. Mamun"
                    },
                    {
                        "name": "S. A. H. Chowdhury"
                    },
                    {
                        "name": "J. E. Giti"
                    },
                    {
                        "name": "H. Sarker"
                    }
                ],
                "author_detail": {
                    "name": "H. Sarker"
                },
                "author": "H. Sarker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04511v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04511v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06964v2",
                "updated": "2024-09-25T04:21:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    4,
                    21,
                    6,
                    2,
                    269,
                    0
                ],
                "published": "2024-05-11T09:18:37Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    9,
                    18,
                    37,
                    5,
                    132,
                    0
                ],
                "title": "ManiFoundation Model for General-Purpose Robotic Manipulation of Contact\n  Synthesis with Arbitrary Objects and Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ManiFoundation Model for General-Purpose Robotic Manipulation of Contact\n  Synthesis with Arbitrary Objects and Robots"
                },
                "summary": "To substantially enhance robot intelligence, there is a pressing need to\ndevelop a large model that enables general-purpose robots to proficiently\nundertake a broad spectrum of manipulation tasks, akin to the versatile\ntask-planning ability exhibited by LLMs. The vast diversity in objects, robots,\nand manipulation tasks presents huge challenges. Our work introduces a\ncomprehensive framework to develop a foundation model for general robotic\nmanipulation that formalizes a manipulation task as contact synthesis.\nSpecifically, our model takes as input object and robot manipulator point\nclouds, object physical attributes, target motions, and manipulation region\nmasks. It outputs contact points on the object and associated contact forces or\npost-contact motions for robots to achieve the desired manipulation task. We\nperform extensive experiments both in the simulation and real-world settings,\nmanipulating articulated rigid objects, rigid objects, and deformable objects\nthat vary in dimensionality, ranging from one-dimensional objects like ropes to\ntwo-dimensional objects like cloth and extending to three-dimensional objects\nsuch as plasticine. Our model achieves average success rates of around 90\\%.\nSupplementary materials and videos are available on our project website at\nhttps://manifoundationmodel.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To substantially enhance robot intelligence, there is a pressing need to\ndevelop a large model that enables general-purpose robots to proficiently\nundertake a broad spectrum of manipulation tasks, akin to the versatile\ntask-planning ability exhibited by LLMs. The vast diversity in objects, robots,\nand manipulation tasks presents huge challenges. Our work introduces a\ncomprehensive framework to develop a foundation model for general robotic\nmanipulation that formalizes a manipulation task as contact synthesis.\nSpecifically, our model takes as input object and robot manipulator point\nclouds, object physical attributes, target motions, and manipulation region\nmasks. It outputs contact points on the object and associated contact forces or\npost-contact motions for robots to achieve the desired manipulation task. We\nperform extensive experiments both in the simulation and real-world settings,\nmanipulating articulated rigid objects, rigid objects, and deformable objects\nthat vary in dimensionality, ranging from one-dimensional objects like ropes to\ntwo-dimensional objects like cloth and extending to three-dimensional objects\nsuch as plasticine. Our model achieves average success rates of around 90\\%.\nSupplementary materials and videos are available on our project website at\nhttps://manifoundationmodel.github.io/."
                },
                "authors": [
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Zixuan Liu"
                    },
                    {
                        "name": "Gang Yang"
                    },
                    {
                        "name": "Chenrui Tie"
                    },
                    {
                        "name": "Haozhuo Zheng"
                    },
                    {
                        "name": "Haoyu Zhou"
                    },
                    {
                        "name": "Weikun Peng"
                    },
                    {
                        "name": "Debang Wang"
                    },
                    {
                        "name": "Tianrun Hu"
                    },
                    {
                        "name": "Tianyi Chen"
                    },
                    {
                        "name": "Zhouliang Yu"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16605v1",
                "updated": "2024-09-25T04:12:38Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    4,
                    12,
                    38,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T04:12:38Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    4,
                    12,
                    38,
                    2,
                    269,
                    0
                ],
                "title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in\n  Scholarly Publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Enhancing Large Language Models for Novelty Assessment in\n  Scholarly Publications"
                },
                "summary": "Recent studies have evaluated the creativity/novelty of large language models\n(LLMs) primarily from a semantic perspective, using benchmarks from cognitive\nscience. However, accessing the novelty in scholarly publications is a largely\nunexplored area in evaluating LLMs. In this paper, we introduce a scholarly\nnovelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in\nscholarly papers. SchNovel consists of 15000 pairs of papers across six fields\nsampled from the arXiv dataset with publication dates spanning 2 to 10 years\napart. In each pair, the more recently published paper is assumed to be more\nnovel. Additionally, we propose RAG-Novelty, which simulates the review process\ntaken by human reviewers by leveraging the retrieval of similar papers to\nassess novelty. Extensive experiments provide insights into the capabilities of\ndifferent LLMs to assess novelty and demonstrate that RAG-Novelty outperforms\nrecent baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have evaluated the creativity/novelty of large language models\n(LLMs) primarily from a semantic perspective, using benchmarks from cognitive\nscience. However, accessing the novelty in scholarly publications is a largely\nunexplored area in evaluating LLMs. In this paper, we introduce a scholarly\nnovelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in\nscholarly papers. SchNovel consists of 15000 pairs of papers across six fields\nsampled from the arXiv dataset with publication dates spanning 2 to 10 years\napart. In each pair, the more recently published paper is assumed to be more\nnovel. Additionally, we propose RAG-Novelty, which simulates the review process\ntaken by human reviewers by leveraging the retrieval of similar papers to\nassess novelty. Extensive experiments provide insights into the capabilities of\ndifferent LLMs to assess novelty and demonstrate that RAG-Novelty outperforms\nrecent baseline models."
                },
                "authors": [
                    {
                        "name": "Ethan Lin"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07267v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07267v3",
                "updated": "2024-09-25T03:53:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    53,
                    39,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-11T13:43:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters."
                },
                "authors": [
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Qinghai Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qinghai Miao"
                },
                "author": "Qinghai Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07267v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07267v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16597v1",
                "updated": "2024-09-25T03:49:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    49,
                    46,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T03:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    49,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we first propose EventHallusion, a\nnovel benchmark that focuses on assessing the VideoLMMs' hallucination\nphenomenon on video event comprehension. Based on the observation that existing\nVideoLLMs are entangled with the priors stemming from their foundation models,\nour EventHallusion is curated by meticulously collecting videos and annotating\nquestions to intentionally mislead the VideoLLMs into interpreting events based\non these priors rather than accurately understanding the video content. On the\nother hand, we also propose a simple yet effective method, called Temporal\nContrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs.\nThe proposed TCD suppresses the model's preference toward their priors by\ncomparing the original video with a constructed counterpart, whose temporal\ncues are disrupted, during the autoregressive decoding stage. Through\ncomprehensive evaluation of eight open-source and two closed-source VideoLLMs\non the proposed EventHallusion benchmark, we find that the open-source models\nsuffer significantly from hallucination problems, whereas the closed-source\nmodels perform markedly better. By further equipping open-sourced VideoLLMs\nwith the proposed TCD approach, evident performance improvements are achieved\nacross most metrics in the EventHallusion benchmark. Our codes and benchmark\ndata are available at https://github.com/Stevetich/EventHallusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we first propose EventHallusion, a\nnovel benchmark that focuses on assessing the VideoLMMs' hallucination\nphenomenon on video event comprehension. Based on the observation that existing\nVideoLLMs are entangled with the priors stemming from their foundation models,\nour EventHallusion is curated by meticulously collecting videos and annotating\nquestions to intentionally mislead the VideoLLMs into interpreting events based\non these priors rather than accurately understanding the video content. On the\nother hand, we also propose a simple yet effective method, called Temporal\nContrastive Decoding (TCD), to tackle the hallucination problems of VideoLLMs.\nThe proposed TCD suppresses the model's preference toward their priors by\ncomparing the original video with a constructed counterpart, whose temporal\ncues are disrupted, during the autoregressive decoding stage. Through\ncomprehensive evaluation of eight open-source and two closed-source VideoLLMs\non the proposed EventHallusion benchmark, we find that the open-source models\nsuffer significantly from hallucination problems, whereas the closed-source\nmodels perform markedly better. By further equipping open-sourced VideoLLMs\nwith the proposed TCD approach, evident performance improvements are achieved\nacross most metrics in the EventHallusion benchmark. Our codes and benchmark\ndata are available at https://github.com/Stevetich/EventHallusion."
                },
                "authors": [
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Shaoxiang Chen"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16202v2",
                "updated": "2024-09-25T03:35:35Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    35,
                    35,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-24T16:00:28Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    16,
                    0,
                    28,
                    1,
                    268,
                    0
                ],
                "title": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese\n  Junior High School Exam Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CJEval: A Benchmark for Assessing Large Language Models Using Chinese\n  Junior High School Exam Data"
                },
                "summary": "Online education platforms have significantly transformed the dissemination\nof educational resources by providing a dynamic and digital infrastructure.\nWith the further enhancement of this transformation, the advent of Large\nLanguage Models (LLMs) has elevated the intelligence levels of these platforms.\nHowever, current academic benchmarks provide limited guidance for real-world\nindustry scenarios. This limitation arises because educational applications\nrequire more than mere test question responses. To bridge this gap, we\nintroduce CJEval, a benchmark based on Chinese Junior High School Exam\nEvaluations. CJEval consists of 26,136 samples across four application-level\neducational tasks covering ten subjects. These samples include not only\nquestions and answers but also detailed annotations such as question types,\ndifficulty levels, knowledge concepts, and answer explanations. By utilizing\nthis benchmark, we assessed LLMs' potential applications and conducted a\ncomprehensive analysis of their performance by fine-tuning on various\neducational tasks. Extensive experiments and discussions have highlighted the\nopportunities and challenges of applying LLMs in the field of education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online education platforms have significantly transformed the dissemination\nof educational resources by providing a dynamic and digital infrastructure.\nWith the further enhancement of this transformation, the advent of Large\nLanguage Models (LLMs) has elevated the intelligence levels of these platforms.\nHowever, current academic benchmarks provide limited guidance for real-world\nindustry scenarios. This limitation arises because educational applications\nrequire more than mere test question responses. To bridge this gap, we\nintroduce CJEval, a benchmark based on Chinese Junior High School Exam\nEvaluations. CJEval consists of 26,136 samples across four application-level\neducational tasks covering ten subjects. These samples include not only\nquestions and answers but also detailed annotations such as question types,\ndifficulty levels, knowledge concepts, and answer explanations. By utilizing\nthis benchmark, we assessed LLMs' potential applications and conducted a\ncomprehensive analysis of their performance by fine-tuning on various\neducational tasks. Extensive experiments and discussions have highlighted the\nopportunities and challenges of applying LLMs in the field of education."
                },
                "authors": [
                    {
                        "name": "Qian-Wen Zhang"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Fang Li"
                    },
                    {
                        "name": "Siyu An"
                    },
                    {
                        "name": "Lingfeng Qiao"
                    },
                    {
                        "name": "Liangcai Gao"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xing Sun"
                },
                "author": "Xing Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16578v1",
                "updated": "2024-09-25T03:15:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    15,
                    17,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T03:15:17Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    15,
                    17,
                    2,
                    269,
                    0
                ],
                "title": "FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale\n  Reinforcement Learning Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale\n  Reinforcement Learning Fine-Tuning"
                },
                "summary": "In recent years, the Robotics field has initiated several efforts toward\nbuilding generalist robot policies through large-scale multi-task Behavior\nCloning. However, direct deployments of these policies have led to\nunsatisfactory performance, where the policy struggles with unseen states and\ntasks. How can we break through the performance plateau of these models and\nelevate their capabilities to new heights? In this paper, we propose FLaRe, a\nlarge-scale Reinforcement Learning fine-tuning framework that integrates robust\npre-trained representations, large-scale training, and gradient stabilization\ntechniques. Our method aligns pre-trained policies towards task completion,\nachieving state-of-the-art (SoTA) performance both on previously demonstrated\nand on entirely novel tasks and embodiments. Specifically, on a set of\nlong-horizon mobile manipulation tasks, FLaRe achieves an average success rate\nof 79.5% in unseen environments, with absolute improvements of +23.6% in\nsimulation and +30.7% on real robots over prior SoTA methods. By utilizing only\nsparse rewards, our approach can enable generalizing to new capabilities beyond\nthe pretraining data with minimal human effort. Moreover, we demonstrate rapid\nadaptation to new embodiments and behaviors with less than a day of\nfine-tuning. Videos can be found on the project website at\nhttps://robot-flare.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Robotics field has initiated several efforts toward\nbuilding generalist robot policies through large-scale multi-task Behavior\nCloning. However, direct deployments of these policies have led to\nunsatisfactory performance, where the policy struggles with unseen states and\ntasks. How can we break through the performance plateau of these models and\nelevate their capabilities to new heights? In this paper, we propose FLaRe, a\nlarge-scale Reinforcement Learning fine-tuning framework that integrates robust\npre-trained representations, large-scale training, and gradient stabilization\ntechniques. Our method aligns pre-trained policies towards task completion,\nachieving state-of-the-art (SoTA) performance both on previously demonstrated\nand on entirely novel tasks and embodiments. Specifically, on a set of\nlong-horizon mobile manipulation tasks, FLaRe achieves an average success rate\nof 79.5% in unseen environments, with absolute improvements of +23.6% in\nsimulation and +30.7% on real robots over prior SoTA methods. By utilizing only\nsparse rewards, our approach can enable generalizing to new capabilities beyond\nthe pretraining data with minimal human effort. Moreover, we demonstrate rapid\nadaptation to new embodiments and behaviors with less than a day of\nfine-tuning. Videos can be found on the project website at\nhttps://robot-flare.github.io/"
                },
                "authors": [
                    {
                        "name": "Jiaheng Hu"
                    },
                    {
                        "name": "Rose Hendrix"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aniruddha Kembhavi"
                    },
                    {
                        "name": "Roberto Martin-Martin"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Kuo-Hao Zeng"
                    },
                    {
                        "name": "Kiana Ehsan"
                    }
                ],
                "author_detail": {
                    "name": "Kiana Ehsan"
                },
                "author": "Kiana Ehsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16573v1",
                "updated": "2024-09-25T03:03:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    3,
                    34,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T03:03:34Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    3,
                    34,
                    2,
                    269,
                    0
                ],
                "title": "Task-driven SLAM Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-driven SLAM Benchmarking"
                },
                "summary": "For assistive robots, one critical use case of SLAM is to support\nlocalization as they navigate through an environment completing tasks. Current\nSLAM benchmarks do not consider task-based deployments where repeatability\n(precision) is more critical than accuracy. To address this gap, we propose a\ntask-driven benchmarking framework for evaluating SLAM methods. The framework\naccounts for SLAM's mapping capabilities, employs precision as a key metric,\nand has low resource requirements to implement. Testing of state-of-the-art\nSLAM methods in both simulated and real-world scenarios provides insights into\nthe performance properties of modern SLAM solutions. In particular, it shows\nthat passive stereo SLAM operates at a level of precision comparable to\nLiDAR-based SLAM in typical indoor environments. The benchmarking approach\noffers a more relevant and accurate assessment of SLAM performance in\ntask-driven applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For assistive robots, one critical use case of SLAM is to support\nlocalization as they navigate through an environment completing tasks. Current\nSLAM benchmarks do not consider task-based deployments where repeatability\n(precision) is more critical than accuracy. To address this gap, we propose a\ntask-driven benchmarking framework for evaluating SLAM methods. The framework\naccounts for SLAM's mapping capabilities, employs precision as a key metric,\nand has low resource requirements to implement. Testing of state-of-the-art\nSLAM methods in both simulated and real-world scenarios provides insights into\nthe performance properties of modern SLAM solutions. In particular, it shows\nthat passive stereo SLAM operates at a level of precision comparable to\nLiDAR-based SLAM in typical indoor environments. The benchmarking approach\noffers a more relevant and accurate assessment of SLAM performance in\ntask-driven applications."
                },
                "authors": [
                    {
                        "name": "Yanwei Du"
                    },
                    {
                        "name": "Shiyu Feng"
                    },
                    {
                        "name": "Carlton G. Cort"
                    },
                    {
                        "name": "Patricio A. Vela"
                    }
                ],
                "author_detail": {
                    "name": "Patricio A. Vela"
                },
                "author": "Patricio A. Vela",
                "arxiv_comment": "7 pages, 7 figures, 1 table. Submitted to ICRA2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16570v1",
                "updated": "2024-09-25T02:53:27Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    53,
                    27,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T02:53:27Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    53,
                    27,
                    2,
                    269,
                    0
                ],
                "title": "Disentangling Questions from Query Generation for Task-Adaptive\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Questions from Query Generation for Task-Adaptive\n  Retrieval"
                },
                "summary": "This paper studies the problem of information retrieval, to adapt to unseen\ntasks. Existing work generates synthetic queries from domain-specific documents\nto jointly train the retriever. However, the conventional query generator\nassumes the query as a question, thus failing to accommodate general search\nintents. A more lenient approach incorporates task-adaptive elements, such as\nfew-shot learning with an 137B LLM. In this paper, we challenge a trend\nequating query and question, and instead conceptualize query generation task as\na \"compilation\" of high-level intent into task-adaptive query. Specifically, we\npropose EGG, a query generator that better adapts to wide search intents\nexpressed in the BeIR benchmark. Our method outperforms baselines and existing\nmodels on four tasks with underexplored intents, while utilizing a query\ngenerator 47 times smaller than the previous state-of-the-art. Our findings\nreveal that instructing the LM with explicit search intent is a key aspect of\nmodeling an effective query generator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the problem of information retrieval, to adapt to unseen\ntasks. Existing work generates synthetic queries from domain-specific documents\nto jointly train the retriever. However, the conventional query generator\nassumes the query as a question, thus failing to accommodate general search\nintents. A more lenient approach incorporates task-adaptive elements, such as\nfew-shot learning with an 137B LLM. In this paper, we challenge a trend\nequating query and question, and instead conceptualize query generation task as\na \"compilation\" of high-level intent into task-adaptive query. Specifically, we\npropose EGG, a query generator that better adapts to wide search intents\nexpressed in the BeIR benchmark. Our method outperforms baselines and existing\nmodels on four tasks with underexplored intents, while utilizing a query\ngenerator 47 times smaller than the previous state-of-the-art. Our findings\nreveal that instructing the LM with explicit search intent is a key aspect of\nmodeling an effective query generator."
                },
                "authors": [
                    {
                        "name": "Yoonsang Lee"
                    },
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Seung-won Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Seung-won Hwang"
                },
                "author": "Seung-won Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16563v1",
                "updated": "2024-09-25T02:29:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    29,
                    44,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T02:29:44Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    29,
                    44,
                    2,
                    269,
                    0
                ],
                "title": "Enhancing disease detection in radiology reports through fine-tuning\n  lightweight LLM on weak labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing disease detection in radiology reports through fine-tuning\n  lightweight LLM on weak labels"
                },
                "summary": "Despite significant progress in applying large language models (LLMs) to the\nmedical domain, several limitations still prevent them from practical\napplications. Among these are the constraints on model size and the lack of\ncohort-specific labeled datasets. In this work, we investigated the potential\nof improving a lightweight LLM, such as Llama 3.1-8B, through fine-tuning with\ndatasets using synthetic labels. Two tasks are jointly trained by combining\ntheir respective instruction datasets. When the quality of the task-specific\nsynthetic labels is relatively high (e.g., generated by GPT4- o), Llama 3.1-8B\nachieves satisfactory performance on the open-ended disease detection task,\nwith a micro F1 score of 0.91. Conversely, when the quality of the\ntask-relevant synthetic labels is relatively low (e.g., from the MIMIC-CXR\ndataset), fine-tuned Llama 3.1-8B is able to surpass its noisy teacher labels\n(micro F1 score of 0.67 v.s. 0.63) when calibrated against curated labels,\nindicating the strong inherent underlying capability of the model. These\nfindings demonstrate the potential of fine-tuning LLMs with synthetic labels,\noffering a promising direction for future research on LLM specialization in the\nmedical domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress in applying large language models (LLMs) to the\nmedical domain, several limitations still prevent them from practical\napplications. Among these are the constraints on model size and the lack of\ncohort-specific labeled datasets. In this work, we investigated the potential\nof improving a lightweight LLM, such as Llama 3.1-8B, through fine-tuning with\ndatasets using synthetic labels. Two tasks are jointly trained by combining\ntheir respective instruction datasets. When the quality of the task-specific\nsynthetic labels is relatively high (e.g., generated by GPT4- o), Llama 3.1-8B\nachieves satisfactory performance on the open-ended disease detection task,\nwith a micro F1 score of 0.91. Conversely, when the quality of the\ntask-relevant synthetic labels is relatively low (e.g., from the MIMIC-CXR\ndataset), fine-tuned Llama 3.1-8B is able to surpass its noisy teacher labels\n(micro F1 score of 0.67 v.s. 0.63) when calibrated against curated labels,\nindicating the strong inherent underlying capability of the model. These\nfindings demonstrate the potential of fine-tuning LLMs with synthetic labels,\noffering a promising direction for future research on LLM specialization in the\nmedical domain."
                },
                "authors": [
                    {
                        "name": "Yishu Wei"
                    },
                    {
                        "name": "Xindi Wang"
                    },
                    {
                        "name": "Hanley Ong"
                    },
                    {
                        "name": "Yiliang Zhou"
                    },
                    {
                        "name": "Adam Flanders"
                    },
                    {
                        "name": "George Shih"
                    },
                    {
                        "name": "Yifan Peng"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Peng"
                },
                "author": "Yifan Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16560v1",
                "updated": "2024-09-25T02:20:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    20,
                    42,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T02:20:42Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    20,
                    42,
                    2,
                    269,
                    0
                ],
                "title": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-Width Speculative Beam Decoding for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) have shown outstanding performance across\nnumerous real-world tasks. However, the autoregressive nature of these models\nmakes the inference process slow and costly. Speculative decoding has emerged\nas a promising solution, leveraging a smaller auxiliary model to draft future\ntokens, which are then validated simultaneously by the larger model, achieving\na speed-up of 1-2x. Although speculative decoding matches the same distribution\nas multinomial sampling, multinomial sampling itself is prone to suboptimal\noutputs, whereas beam sampling is widely recognized for producing\nhigher-quality results by maintaining multiple candidate sequences at each\nstep. This paper explores the novel integration of speculative decoding with\nbeam sampling. However, there are four key challenges: (1) how to generate\nmultiple sequences from the larger model's distribution given drafts sequences\nfrom the small model; (2) how to dynamically optimize the number of beams to\nbalance efficiency and accuracy; (3) how to efficiently verify the multiple\ndrafts in parallel; and (4) how to address the extra memory costs inherent in\nbeam sampling. To address these challenges, we propose dynamic-width\nspeculative beam decoding (DSBD). Specifically, we first introduce a novel\ndraft and verification scheme that generates multiple sequences following the\nlarge model's distribution based on beam sampling trajectories from the small\nmodel. Then, we introduce an adaptive mechanism to dynamically tune the number\nof beams based on the context, optimizing efficiency and effectiveness.\nBesides, we extend tree-based parallel verification to handle multiple trees\nsimultaneously, accelerating the verification process. Finally, we illustrate a\nsimple modification to our algorithm to mitigate the memory overhead of beam\nsampling...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown outstanding performance across\nnumerous real-world tasks. However, the autoregressive nature of these models\nmakes the inference process slow and costly. Speculative decoding has emerged\nas a promising solution, leveraging a smaller auxiliary model to draft future\ntokens, which are then validated simultaneously by the larger model, achieving\na speed-up of 1-2x. Although speculative decoding matches the same distribution\nas multinomial sampling, multinomial sampling itself is prone to suboptimal\noutputs, whereas beam sampling is widely recognized for producing\nhigher-quality results by maintaining multiple candidate sequences at each\nstep. This paper explores the novel integration of speculative decoding with\nbeam sampling. However, there are four key challenges: (1) how to generate\nmultiple sequences from the larger model's distribution given drafts sequences\nfrom the small model; (2) how to dynamically optimize the number of beams to\nbalance efficiency and accuracy; (3) how to efficiently verify the multiple\ndrafts in parallel; and (4) how to address the extra memory costs inherent in\nbeam sampling. To address these challenges, we propose dynamic-width\nspeculative beam decoding (DSBD). Specifically, we first introduce a novel\ndraft and verification scheme that generates multiple sequences following the\nlarge model's distribution based on beam sampling trajectories from the small\nmodel. Then, we introduce an adaptive mechanism to dynamically tune the number\nof beams based on the context, optimizing efficiency and effectiveness.\nBesides, we extend tree-based parallel verification to handle multiple trees\nsimultaneously, accelerating the verification process. Finally, we illustrate a\nsimple modification to our algorithm to mitigate the memory overhead of beam\nsampling..."
                },
                "authors": [
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Neha Prakriya"
                    },
                    {
                        "name": "Jason Cong"
                    },
                    {
                        "name": "Yizhou Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Sun"
                },
                "author": "Yizhou Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16559v1",
                "updated": "2024-09-25T02:16:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    16,
                    45,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T02:16:45Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    2,
                    16,
                    45,
                    2,
                    269,
                    0
                ],
                "title": "Demystifying Issues, Causes and Solutions in LLM Open-Source Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Issues, Causes and Solutions in LLM Open-Source Projects"
                },
                "summary": "With the advancements of Large Language Models (LLMs), an increasing number\nof open-source software projects are using LLMs as their core functional\ncomponent. Although research and practice on LLMs are capturing considerable\ninterest, no dedicated studies explored the challenges faced by practitioners\nof LLM open-source projects, the causes of these challenges, and potential\nsolutions. To fill this research gap, we conducted an empirical study to\nunderstand the issues that practitioners encounter when developing and using\nLLM open-source software, the possible causes of these issues, and potential\nsolutions.We collected all closed issues from 15 LLM open-source projects and\nlabelled issues that met our requirements. We then randomly selected 994 issues\nfrom the labelled issues as the sample for data extraction and analysis to\nunderstand the prevalent issues, their underlying causes, and potential\nsolutions. Our study results show that (1) Model Issue is the most common issue\nfaced by practitioners, (2) Model Problem, Configuration and Connection\nProblem, and Feature and Method Problem are identified as the most frequent\ncauses of the issues, and (3) Optimize Model is the predominant solution to the\nissues. Based on the study results, we provide implications for practitioners\nand researchers of LLM open-source projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancements of Large Language Models (LLMs), an increasing number\nof open-source software projects are using LLMs as their core functional\ncomponent. Although research and practice on LLMs are capturing considerable\ninterest, no dedicated studies explored the challenges faced by practitioners\nof LLM open-source projects, the causes of these challenges, and potential\nsolutions. To fill this research gap, we conducted an empirical study to\nunderstand the issues that practitioners encounter when developing and using\nLLM open-source software, the possible causes of these issues, and potential\nsolutions.We collected all closed issues from 15 LLM open-source projects and\nlabelled issues that met our requirements. We then randomly selected 994 issues\nfrom the labelled issues as the sample for data extraction and analysis to\nunderstand the prevalent issues, their underlying causes, and potential\nsolutions. Our study results show that (1) Model Issue is the most common issue\nfaced by practitioners, (2) Model Problem, Configuration and Connection\nProblem, and Feature and Method Problem are identified as the most frequent\ncauses of the issues, and (3) Optimize Model is the predominant solution to the\nissues. Based on the study results, we provide implications for practitioners\nand researchers of LLM open-source projects."
                },
                "authors": [
                    {
                        "name": "Yangxiao Cai"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Zengyang Li"
                    },
                    {
                        "name": "Mojtaba Shahin"
                    }
                ],
                "author_detail": {
                    "name": "Mojtaba Shahin"
                },
                "author": "Mojtaba Shahin",
                "arxiv_comment": "22 pages, 2 images, 6 tables, Manuscript submitted to a journal\n  (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11283v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11283v3",
                "updated": "2024-09-25T01:55:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    55,
                    29,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-17T15:38:36Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    38,
                    36,
                    1,
                    261,
                    0
                ],
                "title": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-resource Hallucination Detection for Text Generation via\n  Graph-based Contextual Knowledge Triples Modeling"
                },
                "summary": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs obtain remarkable performance but suffer from hallucinations. Most\nresearch on detecting hallucination focuses on the questions with short and\nconcrete correct answers that are easy to check the faithfulness. Hallucination\ndetections for text generation with open-ended answers are more challenging.\nSome researchers use external knowledge to detect hallucinations in generated\ntexts, but external resources for specific scenarios are hard to access. Recent\nstudies on detecting hallucinations in long text without external resources\nconduct consistency comparison among multiple sampled outputs. To handle long\ntexts, researchers split long texts into multiple facts and individually\ncompare the consistency of each pairs of facts. However, these methods (1)\nhardly achieve alignment among multiple facts; (2) overlook dependencies\nbetween multiple contextual facts. In this paper, we propose a graph-based\ncontext-aware (GCA) hallucination detection for text generations, which aligns\nknowledge facts and considers the dependencies between contextual knowledge\ntriples in consistency comparison. Particularly, to align multiple facts, we\nconduct a triple-oriented response segmentation to extract multiple knowledge\ntriples. To model dependencies among contextual knowledge triple (facts), we\nconstruct contextual triple into a graph and enhance triples' interactions via\nmessage passing and aggregating via RGCN. To avoid the omission of knowledge\ntriples in long text, we conduct a LLM-based reverse verification via\nreconstructing the knowledge triples. Experiments show that our model enhances\nhallucination detection and excels all baselines."
                },
                "authors": [
                    {
                        "name": "Xinyue Fang"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhiliang Tian"
                    },
                    {
                        "name": "Minghui Fang"
                    },
                    {
                        "name": "Ziyi Pan"
                    },
                    {
                        "name": "Quntian Fang"
                    },
                    {
                        "name": "Zhihua Wen"
                    },
                    {
                        "name": "Hengyue Pan"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11283v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11283v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13919v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13919v4",
                "updated": "2024-09-25T01:48:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    48,
                    32,
                    2,
                    269,
                    0
                ],
                "published": "2024-06-20T01:18:52Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    1,
                    18,
                    52,
                    3,
                    172,
                    0
                ],
                "title": "SPL: A Socratic Playground for Learning Powered by Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPL: A Socratic Playground for Learning Powered by Large Language Model"
                },
                "summary": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue-based Intelligent Tutoring Systems (ITSs) have significantly\nadvanced adaptive and personalized learning by automating sophisticated human\ntutoring strategies within interactive dialogues. However, replicating the\nnuanced patterns of expert human communication remains a challenge in Natural\nLanguage Processing (NLP). Recent advancements in NLP, particularly Large\nLanguage Models (LLMs) such as OpenAI's GPT-4, offer promising solutions by\nproviding human-like and context-aware responses based on extensive pre-trained\nknowledge. Motivated by the effectiveness of LLMs in various educational tasks\n(e.g., content creation and summarization, problem-solving, and automated\nfeedback provision), our study introduces the Socratic Playground for Learning\n(SPL), a dialogue-based ITS powered by the GPT-4 model, which employs the\nSocratic teaching method to foster critical thinking among learners. Through\nextensive prompt engineering, SPL can generate specific learning scenarios and\nfacilitates efficient multi-turn tutoring dialogues. The SPL system aims to\nenhance personalized and adaptive learning experiences tailored to individual\nneeds, specifically focusing on improving critical thinking skills. Our pilot\nexperimental results from essay writing tasks demonstrate SPL has the potential\nto improve tutoring interactions and further enhance dialogue-based ITS\nfunctionalities. Our study, exemplified by SPL, demonstrates how LLMs enhance\ndialogue-based ITSs and expand the accessibility and efficacy of educational\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Ziyi Kuang"
                    },
                    {
                        "name": "Sheng Xu"
                    },
                    {
                        "name": "Xiangen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangen Hu"
                },
                "author": "Xiangen Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13919v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13919v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16530v1",
                "updated": "2024-09-25T00:41:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    0,
                    41,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T00:41:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    0,
                    41,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "T2Pair++: Secure and Usable IoT Pairing with Zero Information Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T2Pair++: Secure and Usable IoT Pairing with Zero Information Loss"
                },
                "summary": "Secure pairing is crucial for ensuring the trustworthy deployment and\noperation of Internet of Things (IoT) devices. However, traditional pairing\nmethods are often unsuitable for IoT devices due to their lack of conventional\nuser interfaces, such as keyboards. Proximity-based pairing approaches are\nusable but vulnerable to exploitation by co-located malicious devices. While\nmethods based on a user's physical operations (such as shaking) on IoT devices\noffer greater security, they typically rely on inertial sensors to sense the\noperations, which most IoT devices lack. We introduce a novel technique called\nUniversal Operation Sensing, enabling IoT devices to sense the user's physical\noperations without the need for inertial sensors. With this technique, users\ncan complete the pairing process within seconds using simple actions such as\npressing a button or twisting a knob, whether they are holding a smartphone or\nwearing a smartwatch. Moreover, we reveal an inaccuracy issue in the fuzzy\ncommitment protocol, which is frequently used for pairing. To address it, we\npropose an accurate pairing protocol, which does not use fuzzy commitment and\nincurs zero information loss. The comprehensive evaluation shows that it is\nsecure, usable and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure pairing is crucial for ensuring the trustworthy deployment and\noperation of Internet of Things (IoT) devices. However, traditional pairing\nmethods are often unsuitable for IoT devices due to their lack of conventional\nuser interfaces, such as keyboards. Proximity-based pairing approaches are\nusable but vulnerable to exploitation by co-located malicious devices. While\nmethods based on a user's physical operations (such as shaking) on IoT devices\noffer greater security, they typically rely on inertial sensors to sense the\noperations, which most IoT devices lack. We introduce a novel technique called\nUniversal Operation Sensing, enabling IoT devices to sense the user's physical\noperations without the need for inertial sensors. With this technique, users\ncan complete the pairing process within seconds using simple actions such as\npressing a button or twisting a knob, whether they are holding a smartphone or\nwearing a smartwatch. Moreover, we reveal an inaccuracy issue in the fuzzy\ncommitment protocol, which is frequently used for pairing. To address it, we\npropose an accurate pairing protocol, which does not use fuzzy commitment and\nincurs zero information loss. The comprehensive evaluation shows that it is\nsecure, usable and efficient."
                },
                "authors": [
                    {
                        "name": "Chuxiong Wu"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Lannan Luo"
                    },
                    {
                        "name": "Qiang Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Zeng"
                },
                "author": "Qiang Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16526v1",
                "updated": "2024-09-25T00:37:40Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    0,
                    37,
                    40,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T00:37:40Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    0,
                    37,
                    40,
                    2,
                    269,
                    0
                ],
                "title": "APILOT: Navigating Large Language Models to Generate Secure Code by\n  Sidestepping Outdated API Pitfalls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APILOT: Navigating Large Language Models to Generate Secure Code by\n  Sidestepping Outdated API Pitfalls"
                },
                "summary": "With the rapid development of large language models (LLMs), their\napplications have expanded into diverse fields, such as code assistance.\nHowever, the substantial size of LLMs makes their training highly resource- and\ntime-intensive, rendering frequent retraining or updates impractical.\nConsequently, time-sensitive data can become outdated, potentially misleading\nLLMs in time-aware tasks. For example, new vulnerabilities are discovered in\nvarious programs every day. Without updating their knowledge, LLMs may\ninadvertently generate code that includes these newly discovered\nvulnerabilities. Current strategies, such as prompt engineering and\nfine-tuning, do not effectively address this issue.\n  To address this issue, we propose solution, named APILOT, which maintains a\nrealtime, quickly updatable dataset of outdated APIs. Additionally, APILOT\nutilizes an augmented generation method that leverages this dataset to navigate\nLLMs in generating secure, version-aware code. We conducted a comprehensive\nevaluation to measure the effectiveness of APILOT in reducing the incidence of\noutdated API recommendations across seven different state-of-the-art LLMs. The\nevaluation results indicate that APILOT can reduce outdated code\nrecommendations by 89.42% on average with limited performance overhead.\nInterestingly, while enhancing security, APILOT also improves the usability of\nthe code generated by LLMs, showing an average increase of 27.54% in usability.\nThis underscores APILOT's dual capability to enhance both the safety and\npractical utility of code suggestions in contemporary software development\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), their\napplications have expanded into diverse fields, such as code assistance.\nHowever, the substantial size of LLMs makes their training highly resource- and\ntime-intensive, rendering frequent retraining or updates impractical.\nConsequently, time-sensitive data can become outdated, potentially misleading\nLLMs in time-aware tasks. For example, new vulnerabilities are discovered in\nvarious programs every day. Without updating their knowledge, LLMs may\ninadvertently generate code that includes these newly discovered\nvulnerabilities. Current strategies, such as prompt engineering and\nfine-tuning, do not effectively address this issue.\n  To address this issue, we propose solution, named APILOT, which maintains a\nrealtime, quickly updatable dataset of outdated APIs. Additionally, APILOT\nutilizes an augmented generation method that leverages this dataset to navigate\nLLMs in generating secure, version-aware code. We conducted a comprehensive\nevaluation to measure the effectiveness of APILOT in reducing the incidence of\noutdated API recommendations across seven different state-of-the-art LLMs. The\nevaluation results indicate that APILOT can reduce outdated code\nrecommendations by 89.42% on average with limited performance overhead.\nInterestingly, while enhancing security, APILOT also improves the usability of\nthe code generated by LLMs, showing an average increase of 27.54% in usability.\nThis underscores APILOT's dual capability to enhance both the safety and\npractical utility of code suggestions in contemporary software development\nenvironments."
                },
                "authors": [
                    {
                        "name": "Weiheng Bai"
                    },
                    {
                        "name": "Keyang Xuan"
                    },
                    {
                        "name": "Pengxiang Huang"
                    },
                    {
                        "name": "Qiushi Wu"
                    },
                    {
                        "name": "Jianing Wen"
                    },
                    {
                        "name": "Jingjing Wu"
                    },
                    {
                        "name": "Kangjie Lu"
                    }
                ],
                "author_detail": {
                    "name": "Kangjie Lu"
                },
                "author": "Kangjie Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]